using symmetric alpha at 0.2
using symmetric eta at 0.2
using serial LDA version on this node
running online LDA training, 5 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.276*"see" + 0.195*"back" + 0.120*"area" + 0.117*"come" + 0.065*"set" + 0.036*"royal" + 0.036*"school" + 0.036*"role" + 0.026*"court" + 0.020*"build"
topic #1 (0.200): 0.196*"long" + 0.159*"school" + 0.158*"come" + 0.121*"role" + 0.084*"build" + 0.047*"player" + 0.046*"royal" + 0.046*"see" + 0.046*"original" + 0.045*"short"
topic #2 (0.200): 0.256*"school" + 0.140*"international" + 0.101*"back" + 0.082*"come" + 0.082*"role" + 0.081*"royal" + 0.063*"player" + 0.062*"short" + 0.043*"area" + 0.043*"set"
topic #3 (0.200): 0.264*"back" + 0.171*"see" + 0.090*"come" + 0.088*"original" + 0.071*"area" + 0.060*"long" + 0.054*"international" + 0.043*"short" + 0.041*"set" + 0.032*"court"
topic #4 (0.200): 0.269*"film" + 0.128*"area" + 0.113*"court" + 0.086*"build" + 0.080*"player" + 0.065*"school" + 0.044*"set" + 0.033*"royal" + 0.028*"international" + 0.028*"short"
topic diff=1.591860, rho=1.000000
-3.616 per-word bound, 12.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.273*"see" + 0.216*"back" + 0.137*"area" + 0.129*"come" + 0.061*"set" + 0.035*"school" + 0.032*"role" + 0.025*"court" + 0.025*"royal" + 0.014*"short"
topic #1 (0.200): 0.225*"long" + 0.145*"come" + 0.137*"role" + 0.131*"school" + 0.089*"build" + 0.052*"short" + 0.047*"player" + 0.046*"original" + 0.046*"see" + 0.032*"royal"
topic #2 (0.200): 0.261*"school" + 0.158*"international" + 0.123*"royal" + 0.083*"come" + 0.081*"back" + 0.069*"role" + 0.058*"player" + 0.058*"short" + 0.040*"set" + 0.027*"area"
topic #3 (0.200): 0.276*"back" + 0.156*"see" + 0.110*"original" + 0.079*"come" + 0.068*"long" + 0.056*"area" + 0.051*"short" + 0.043*"international" + 0.041*"set" + 0.034*"court"
topic #4 (0.200): 0.279*"film" + 0.129*"area" + 0.116*"court" + 0.090*"build" + 0.082*"player" + 0.064*"school" + 0.045*"set" + 0.028*"international" + 0.027*"short" + 0.025*"original"
topic diff=0.179093, rho=0.377964
-3.574 per-word bound, 11.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.267*"see" + 0.220*"back" + 0.154*"area" + 0.137*"come" + 0.058*"set" + 0.035*"school" + 0.030*"role" + 0.024*"court" + 0.020*"royal" + 0.013*"short"
topic #1 (0.200): 0.239*"long" + 0.143*"role" + 0.122*"come" + 0.113*"school" + 0.106*"build" + 0.059*"original" + 0.057*"short" + 0.046*"player" + 0.045*"see" + 0.023*"royal"
topic #2 (0.200): 0.270*"school" + 0.167*"international" + 0.145*"royal" + 0.084*"come" + 0.063*"back" + 0.062*"role" + 0.056*"short" + 0.056*"player" + 0.039*"set" + 0.021*"build"
topic #3 (0.200): 0.293*"back" + 0.142*"see" + 0.127*"original" + 0.074*"long" + 0.069*"come" + 0.056*"short" + 0.044*"area" + 0.041*"set" + 0.036*"court" + 0.034*"international"
topic #4 (0.200): 0.291*"film" + 0.125*"area" + 0.120*"court" + 0.091*"build" + 0.086*"player" + 0.061*"school" + 0.045*"set" + 0.029*"international" + 0.027*"short" + 0.025*"original"
topic diff=0.151590, rho=0.353553
-3.534 per-word bound, 11.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.263*"see" + 0.221*"back" + 0.169*"area" + 0.141*"come" + 0.057*"set" + 0.035*"school" + 0.029*"role" + 0.024*"court" + 0.014*"royal" + 0.013*"short"
topic #1 (0.200): 0.246*"long" + 0.148*"role" + 0.125*"build" + 0.105*"come" + 0.099*"school" + 0.067*"original" + 0.060*"short" + 0.044*"player" + 0.043*"see" + 0.017*"royal"
topic #2 (0.200): 0.278*"school" + 0.171*"international" + 0.160*"royal" + 0.085*"come" + 0.058*"role" + 0.055*"short" + 0.054*"player" + 0.050*"back" + 0.038*"set" + 0.021*"build"
topic #3 (0.200): 0.309*"back" + 0.138*"original" + 0.131*"see" + 0.079*"long" + 0.061*"come" + 0.061*"short" + 0.041*"set" + 0.037*"court" + 0.034*"area" + 0.033*"royal"
topic #4 (0.200): 0.303*"film" + 0.124*"court" + 0.121*"area" + 0.092*"build" + 0.089*"player" + 0.058*"school" + 0.046*"set" + 0.030*"international" + 0.027*"short" + 0.025*"original"
topic diff=0.135615, rho=0.333333
-3.506 per-word bound, 11.4 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.261*"see" + 0.221*"back" + 0.181*"area" + 0.142*"come" + 0.057*"set" + 0.035*"school" + 0.027*"role" + 0.023*"court" + 0.013*"short" + 0.012*"long"
topic #1 (0.200): 0.247*"long" + 0.156*"role" + 0.139*"build" + 0.093*"come" + 0.089*"school" + 0.073*"original" + 0.063*"short" + 0.042*"player" + 0.042*"see" + 0.014*"royal"
topic #2 (0.200): 0.286*"school" + 0.172*"international" + 0.170*"royal" + 0.086*"come" + 0.055*"short" + 0.054*"role" + 0.052*"player" + 0.041*"back" + 0.038*"set" + 0.021*"build"
topic #3 (0.200): 0.321*"back" + 0.147*"original" + 0.123*"see" + 0.083*"long" + 0.064*"short" + 0.055*"come" + 0.040*"set" + 0.037*"court" + 0.035*"royal" + 0.026*"area"
topic #4 (0.200): 0.315*"film" + 0.128*"court" + 0.117*"area" + 0.092*"player" + 0.092*"build" + 0.054*"school" + 0.047*"set" + 0.030*"international" + 0.026*"short" + 0.025*"original"
topic diff=0.120696, rho=0.316228
-3.486 per-word bound, 11.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.259*"see" + 0.221*"back" + 0.190*"area" + 0.143*"come" + 0.057*"set" + 0.036*"school" + 0.023*"court" + 0.023*"role" + 0.013*"short" + 0.012*"long"
topic #1 (0.200): 0.243*"long" + 0.168*"role" + 0.151*"build" + 0.085*"come" + 0.081*"school" + 0.076*"original" + 0.064*"short" + 0.040*"player" + 0.040*"see" + 0.011*"royal"
topic #2 (0.200): 0.294*"school" + 0.176*"royal" + 0.173*"international" + 0.086*"come" + 0.055*"short" + 0.052*"role" + 0.051*"player" + 0.038*"set" + 0.033*"back" + 0.018*"build"
topic #3 (0.200): 0.334*"back" + 0.152*"original" + 0.116*"see" + 0.085*"long" + 0.065*"short" + 0.050*"come" + 0.040*"set" + 0.038*"royal" + 0.038*"court" + 0.021*"area"
topic #4 (0.200): 0.325*"film" + 0.132*"court" + 0.113*"area" + 0.095*"player" + 0.092*"build" + 0.051*"school" + 0.047*"set" + 0.031*"international" + 0.026*"short" + 0.025*"original"
topic diff=0.115044, rho=0.301511
-3.470 per-word bound, 11.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.258*"see" + 0.220*"back" + 0.199*"area" + 0.144*"come" + 0.057*"set" + 0.036*"school" + 0.023*"court" + 0.019*"role" + 0.013*"long" + 0.013*"short"
topic #1 (0.200): 0.240*"long" + 0.180*"role" + 0.160*"build" + 0.079*"come" + 0.078*"original" + 0.076*"school" + 0.064*"short" + 0.039*"player" + 0.035*"see" + 0.009*"royal"
topic #2 (0.200): 0.301*"school" + 0.181*"royal" + 0.174*"international" + 0.087*"come" + 0.055*"short" + 0.051*"role" + 0.050*"player" + 0.038*"set" + 0.026*"back" + 0.016*"build"
topic #3 (0.200): 0.344*"back" + 0.155*"original" + 0.111*"see" + 0.087*"long" + 0.066*"short" + 0.047*"come" + 0.039*"royal" + 0.039*"set" + 0.038*"court" + 0.017*"area"
topic #4 (0.200): 0.333*"film" + 0.135*"court" + 0.109*"area" + 0.097*"player" + 0.093*"build" + 0.047*"set" + 0.047*"school" + 0.031*"international" + 0.025*"short" + 0.025*"original"
topic diff=0.107080, rho=0.288675
-3.457 per-word bound, 11.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.257*"see" + 0.218*"back" + 0.207*"area" + 0.143*"come" + 0.057*"set" + 0.036*"school" + 0.022*"court" + 0.015*"role" + 0.013*"long" + 0.013*"short"
topic #1 (0.200): 0.236*"long" + 0.193*"role" + 0.166*"build" + 0.080*"original" + 0.075*"come" + 0.071*"school" + 0.065*"short" + 0.037*"player" + 0.030*"see" + 0.008*"royal"
topic #2 (0.200): 0.308*"school" + 0.184*"royal" + 0.175*"international" + 0.087*"come" + 0.055*"short" + 0.050*"player" + 0.048*"role" + 0.038*"set" + 0.021*"back" + 0.014*"build"
topic #3 (0.200): 0.352*"back" + 0.157*"original" + 0.107*"see" + 0.089*"long" + 0.067*"short" + 0.044*"come" + 0.042*"royal" + 0.039*"set" + 0.038*"court" + 0.014*"area"
topic #4 (0.200): 0.341*"film" + 0.138*"court" + 0.104*"area" + 0.099*"player" + 0.093*"build" + 0.047*"set" + 0.044*"school" + 0.031*"international" + 0.025*"short" + 0.024*"original"
topic diff=0.100235, rho=0.277350
-3.447 per-word bound, 10.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=5, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:31:39.417290', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
Coherence Score: 0.5503595963830591
Starting K=6
using symmetric alpha at 0.16666666666666666
using symmetric eta at 0.16666666666666666
using serial LDA version on this node
running online LDA training, 6 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #2 (0.167): 0.346*"school" + 0.196*"back" + 0.132*"royal" + 0.068*"come" + 0.068*"short" + 0.046*"original" + 0.046*"international" + 0.025*"area" + 0.025*"role" + 0.025*"set"
topic #0 (0.167): 0.178*"see" + 0.131*"back" + 0.101*"royal" + 0.077*"area" + 0.068*"set" + 0.066*"come" + 0.062*"international" + 0.062*"build" + 0.062*"player" + 0.052*"court"
topic #3 (0.167): 0.194*"see" + 0.152*"original" + 0.120*"back" + 0.088*"long" + 0.087*"short" + 0.082*"international" + 0.071*"area" + 0.061*"come" + 0.034*"set" + 0.026*"school"
topic #4 (0.167): 0.294*"film" + 0.132*"area" + 0.119*"court" + 0.094*"build" + 0.070*"school" + 0.063*"player" + 0.051*"set" + 0.032*"short" + 0.026*"international" + 0.026*"original"
topic #5 (0.167): 0.172*"back" + 0.167*"see" + 0.153*"come" + 0.106*"area" + 0.062*"international" + 0.062*"player" + 0.062*"role" + 0.052*"film" + 0.035*"set" + 0.033*"school"
topic diff=1.977311, rho=1.000000
-3.869 per-word bound, 14.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #2 (0.167): 0.379*"school" + 0.190*"back" + 0.152*"royal" + 0.068*"come" + 0.063*"short" + 0.031*"original" + 0.031*"international" + 0.025*"role" + 0.020*"area" + 0.018*"set"
topic #4 (0.167): 0.309*"film" + 0.135*"area" + 0.123*"court" + 0.097*"build" + 0.061*"school" + 0.057*"player" + 0.051*"set" + 0.030*"short" + 0.027*"original" + 0.026*"international"
topic #3 (0.167): 0.220*"original" + 0.169*"see" + 0.125*"short" + 0.096*"long" + 0.088*"back" + 0.062*"international" + 0.054*"area" + 0.047*"come" + 0.028*"set" + 0.023*"school"
topic #0 (0.167): 0.147*"see" + 0.121*"back" + 0.117*"royal" + 0.076*"set" + 0.075*"come" + 0.072*"build" + 0.066*"area" + 0.063*"international" + 0.057*"original" + 0.056*"court"
topic #5 (0.167): 0.184*"see" + 0.182*"back" + 0.148*"come" + 0.111*"area" + 0.075*"player" + 0.071*"international" + 0.050*"role" + 0.039*"set" + 0.032*"film" + 0.030*"school"
topic diff=0.224872, rho=0.377964
-3.710 per-word bound, 13.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #4 (0.167): 0.324*"film" + 0.141*"area" + 0.129*"court" + 0.101*"build" + 0.054*"school" + 0.053*"set" + 0.038*"player" + 0.029*"short" + 0.027*"original" + 0.027*"international"
topic #0 (0.167): 0.130*"royal" + 0.125*"see" + 0.115*"back" + 0.083*"set" + 0.082*"come" + 0.077*"build" + 0.069*"original" + 0.065*"international" + 0.056*"court" + 0.054*"area"
topic #3 (0.167): 0.270*"original" + 0.151*"see" + 0.150*"short" + 0.100*"long" + 0.066*"back" + 0.048*"international" + 0.042*"area" + 0.038*"come" + 0.024*"set" + 0.021*"school"
topic #1 (0.167): 0.415*"long" + 0.157*"role" + 0.097*"school" + 0.090*"short" + 0.078*"build" + 0.069*"see" + 0.010*"player" + 0.010*"film" + 0.010*"international" + 0.009*"set"
topic #2 (0.167): 0.402*"school" + 0.185*"back" + 0.163*"royal" + 0.069*"come" + 0.060*"short" + 0.025*"role" + 0.022*"original" + 0.022*"international" + 0.016*"area" + 0.014*"set"
topic diff=0.196099, rho=0.353553
-3.582 per-word bound, 12.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #1 (0.167): 0.415*"long" + 0.175*"role" + 0.096*"school" + 0.090*"short" + 0.074*"build" + 0.063*"see" + 0.009*"player" + 0.009*"film" + 0.009*"international" + 0.009*"set"
topic #3 (0.167): 0.305*"original" + 0.167*"short" + 0.139*"see" + 0.103*"long" + 0.050*"back" + 0.038*"international" + 0.034*"area" + 0.031*"come" + 0.022*"set" + 0.019*"school"
topic #4 (0.167): 0.335*"film" + 0.146*"area" + 0.134*"court" + 0.105*"build" + 0.053*"set" + 0.048*"school" + 0.028*"short" + 0.028*"original" + 0.027*"international" + 0.026*"player"
topic #5 (0.167): 0.192*"see" + 0.186*"back" + 0.140*"come" + 0.117*"player" + 0.111*"area" + 0.077*"international" + 0.040*"set" + 0.035*"role" + 0.027*"school" + 0.019*"court"
topic #0 (0.167): 0.141*"royal" + 0.112*"back" + 0.109*"see" + 0.089*"set" + 0.088*"come" + 0.080*"build" + 0.079*"original" + 0.067*"international" + 0.057*"role" + 0.054*"court"
topic diff=0.167605, rho=0.333333
-3.531 per-word bound, 11.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #5 (0.167): 0.197*"back" + 0.193*"see" + 0.136*"come" + 0.124*"player" + 0.110*"area" + 0.077*"international" + 0.040*"set" + 0.032*"role" + 0.026*"school" + 0.018*"court"
topic #1 (0.167): 0.410*"long" + 0.194*"role" + 0.094*"school" + 0.089*"short" + 0.072*"build" + 0.062*"see" + 0.008*"player" + 0.008*"set" + 0.008*"international" + 0.008*"film"
topic #0 (0.167): 0.153*"royal" + 0.113*"back" + 0.095*"set" + 0.095*"come" + 0.092*"see" + 0.088*"original" + 0.079*"build" + 0.070*"international" + 0.053*"role" + 0.051*"court"
topic #2 (0.167): 0.448*"school" + 0.178*"royal" + 0.149*"back" + 0.073*"come" + 0.058*"short" + 0.025*"role" + 0.013*"international" + 0.013*"original" + 0.011*"area" + 0.009*"set"
topic #3 (0.167): 0.344*"original" + 0.179*"short" + 0.125*"see" + 0.104*"long" + 0.038*"back" + 0.030*"international" + 0.028*"area" + 0.025*"come" + 0.019*"set" + 0.018*"school"
topic diff=0.160165, rho=0.316228
-3.486 per-word bound, 11.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.167): 0.145*"royal" + 0.115*"back" + 0.111*"set" + 0.101*"come" + 0.096*"original" + 0.084*"build" + 0.080*"see" + 0.073*"international" + 0.049*"court" + 0.045*"player"
topic #4 (0.167): 0.351*"film" + 0.154*"area" + 0.141*"court" + 0.110*"build" + 0.053*"set" + 0.036*"school" + 0.028*"international" + 0.025*"short" + 0.024*"original" + 0.024*"see"
topic #2 (0.167): 0.464*"school" + 0.194*"royal" + 0.125*"back" + 0.075*"come" + 0.058*"short" + 0.026*"role" + 0.010*"international" + 0.010*"original" + 0.009*"area" + 0.007*"set"
topic #1 (0.167): 0.397*"long" + 0.213*"role" + 0.090*"school" + 0.089*"short" + 0.073*"build" + 0.063*"see" + 0.008*"set" + 0.008*"player" + 0.008*"international" + 0.008*"film"
topic #5 (0.167): 0.204*"back" + 0.192*"see" + 0.134*"come" + 0.129*"player" + 0.110*"area" + 0.077*"international" + 0.040*"set" + 0.029*"role" + 0.025*"school" + 0.018*"court"
topic diff=0.154115, rho=0.301511
-3.449 per-word bound, 10.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #5 (0.167): 0.210*"back" + 0.192*"see" + 0.133*"come" + 0.132*"player" + 0.109*"area" + 0.077*"international" + 0.040*"set" + 0.026*"role" + 0.025*"school" + 0.017*"court"
topic #2 (0.167): 0.476*"school" + 0.204*"royal" + 0.109*"back" + 0.076*"come" + 0.058*"short" + 0.024*"role" + 0.008*"original" + 0.008*"international" + 0.008*"area" + 0.006*"set"
topic #1 (0.167): 0.382*"long" + 0.238*"role" + 0.087*"short" + 0.086*"school" + 0.072*"build" + 0.063*"see" + 0.007*"set" + 0.007*"international" + 0.007*"player" + 0.007*"film"
topic #4 (0.167): 0.357*"film" + 0.157*"area" + 0.144*"court" + 0.111*"build" + 0.053*"set" + 0.031*"school" + 0.028*"international" + 0.025*"see" + 0.024*"short" + 0.023*"original"
topic #0 (0.167): 0.139*"royal" + 0.123*"set" + 0.117*"back" + 0.106*"come" + 0.103*"original" + 0.088*"build" + 0.076*"international" + 0.071*"see" + 0.048*"court" + 0.045*"player"
topic diff=0.138776, rho=0.288675
-3.423 per-word bound, 10.7 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #4 (0.167): 0.362*"film" + 0.159*"area" + 0.146*"court" + 0.112*"build" + 0.052*"set" + 0.029*"international" + 0.027*"school" + 0.026*"see" + 0.023*"short" + 0.022*"original"
topic #1 (0.167): 0.371*"long" + 0.260*"role" + 0.086*"short" + 0.083*"school" + 0.071*"build" + 0.062*"see" + 0.007*"set" + 0.007*"international" + 0.007*"film" + 0.007*"player"
topic #5 (0.167): 0.214*"back" + 0.192*"see" + 0.134*"player" + 0.132*"come" + 0.109*"area" + 0.078*"international" + 0.040*"set" + 0.025*"school" + 0.022*"role" + 0.017*"court"
topic #2 (0.167): 0.487*"school" + 0.210*"royal" + 0.097*"back" + 0.078*"come" + 0.058*"short" + 0.021*"role" + 0.007*"original" + 0.007*"international" + 0.007*"area" + 0.006*"set"
topic #0 (0.167): 0.135*"royal" + 0.133*"set" + 0.118*"back" + 0.110*"come" + 0.107*"original" + 0.090*"build" + 0.078*"international" + 0.065*"see" + 0.047*"court" + 0.045*"player"
topic diff=0.123994, rho=0.277350
-3.405 per-word bound, 10.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=6, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:31:39.728701', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
Coherence Score: 0.5503595963830592
ALL DONE!
