using symmetric alpha at 0.3333333333333333
using symmetric eta at 0.3333333333333333
using serial LDA version on this node
running online LDA training, 3 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.186*"see" + 0.154*"area" + 0.135*"back" + 0.109*"court" + 0.086*"come" + 0.059*"build" + 0.048*"school" + 0.047*"set" + 0.031*"original" + 0.028*"royal"
topic #1 (0.333): 0.197*"film" + 0.173*"player" + 0.113*"long" + 0.083*"court" + 0.066*"build" + 0.057*"school" + 0.050*"role" + 0.039*"come" + 0.037*"short" + 0.035*"see"
topic #2 (0.333): 0.213*"film" + 0.136*"school" + 0.090*"area" + 0.090*"back" + 0.072*"international" + 0.064*"royal" + 0.059*"come" + 0.051*"set" + 0.044*"build" + 0.042*"short"
topic diff=0.692438, rho=1.000000
-3.248 per-word bound, 9.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.181*"see" + 0.160*"area" + 0.154*"back" + 0.121*"court" + 0.093*"come" + 0.057*"build" + 0.045*"set" + 0.043*"school" + 0.029*"original" + 0.023*"royal"
topic #1 (0.333): 0.208*"player" + 0.180*"film" + 0.130*"long" + 0.067*"build" + 0.060*"court" + 0.050*"role" + 0.045*"school" + 0.044*"short" + 0.039*"original" + 0.035*"come"
topic #2 (0.333): 0.242*"film" + 0.149*"school" + 0.079*"area" + 0.074*"international" + 0.071*"royal" + 0.067*"back" + 0.052*"set" + 0.052*"come" + 0.046*"build" + 0.044*"short"
topic diff=0.165443, rho=0.377964
-3.175 per-word bound, 9.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.179*"see" + 0.167*"back" + 0.165*"area" + 0.127*"court" + 0.097*"come" + 0.055*"build" + 0.043*"set" + 0.039*"school" + 0.025*"original" + 0.020*"role"
topic #1 (0.333): 0.234*"player" + 0.154*"film" + 0.143*"long" + 0.068*"build" + 0.053*"short" + 0.051*"original" + 0.049*"role" + 0.043*"court" + 0.040*"international" + 0.036*"school"
topic #2 (0.333): 0.267*"film" + 0.160*"school" + 0.075*"royal" + 0.075*"international" + 0.070*"area" + 0.054*"set" + 0.049*"back" + 0.047*"build" + 0.046*"come" + 0.044*"short"
topic diff=0.148748, rho=0.353553
-3.123 per-word bound, 8.7 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.177*"see" + 0.175*"back" + 0.169*"area" + 0.131*"court" + 0.099*"come" + 0.054*"build" + 0.042*"set" + 0.037*"school" + 0.023*"original" + 0.020*"role"
topic #1 (0.333): 0.255*"player" + 0.152*"long" + 0.126*"film" + 0.070*"build" + 0.062*"original" + 0.060*"short" + 0.048*"role" + 0.047*"international" + 0.033*"come" + 0.032*"court"
topic #2 (0.333): 0.288*"film" + 0.167*"school" + 0.077*"royal" + 0.074*"international" + 0.062*"area" + 0.055*"set" + 0.048*"build" + 0.044*"short" + 0.042*"come" + 0.042*"role"
topic diff=0.129639, rho=0.333333
-3.080 per-word bound, 8.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.180*"back" + 0.176*"see" + 0.173*"area" + 0.133*"court" + 0.100*"come" + 0.053*"build" + 0.041*"set" + 0.034*"school" + 0.021*"role" + 0.020*"original"
topic #1 (0.333): 0.269*"player" + 0.158*"long" + 0.100*"film" + 0.071*"build" + 0.071*"original" + 0.067*"short" + 0.055*"international" + 0.047*"role" + 0.036*"come" + 0.027*"set"
topic #2 (0.333): 0.306*"film" + 0.174*"school" + 0.080*"royal" + 0.071*"international" + 0.056*"set" + 0.055*"area" + 0.048*"build" + 0.043*"short" + 0.043*"role" + 0.038*"come"
topic diff=0.117556, rho=0.316228
-3.047 per-word bound, 8.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.184*"back" + 0.177*"area" + 0.175*"see" + 0.134*"court" + 0.100*"come" + 0.053*"build" + 0.041*"set" + 0.033*"school" + 0.021*"role" + 0.019*"original"
topic #1 (0.333): 0.274*"player" + 0.159*"long" + 0.077*"original" + 0.077*"film" + 0.072*"international" + 0.071*"build" + 0.071*"short" + 0.046*"role" + 0.043*"come" + 0.029*"set"
topic #2 (0.333): 0.323*"film" + 0.180*"school" + 0.082*"royal" + 0.065*"international" + 0.055*"set" + 0.049*"area" + 0.049*"build" + 0.044*"role" + 0.043*"short" + 0.033*"come"
topic diff=0.116500, rho=0.301511
-3.022 per-word bound, 8.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.188*"back" + 0.181*"area" + 0.174*"see" + 0.135*"court" + 0.100*"come" + 0.053*"build" + 0.040*"set" + 0.032*"school" + 0.021*"role" + 0.017*"original"
topic #1 (0.333): 0.272*"player" + 0.157*"long" + 0.092*"international" + 0.082*"original" + 0.073*"short" + 0.070*"build" + 0.059*"film" + 0.051*"come" + 0.045*"role" + 0.031*"set"
topic #2 (0.333): 0.338*"film" + 0.186*"school" + 0.085*"royal" + 0.056*"international" + 0.055*"set" + 0.049*"build" + 0.044*"role" + 0.044*"area" + 0.042*"short" + 0.030*"come"
topic diff=0.106703, rho=0.288675
-3.002 per-word bound, 8.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.190*"back" + 0.184*"area" + 0.174*"see" + 0.135*"court" + 0.099*"come" + 0.053*"build" + 0.040*"set" + 0.031*"school" + 0.021*"role" + 0.015*"original"
topic #1 (0.333): 0.269*"player" + 0.155*"long" + 0.113*"international" + 0.084*"original" + 0.073*"short" + 0.070*"build" + 0.057*"come" + 0.045*"film" + 0.040*"role" + 0.035*"set"
topic #2 (0.333): 0.351*"film" + 0.192*"school" + 0.087*"royal" + 0.054*"set" + 0.049*"build" + 0.047*"role" + 0.046*"international" + 0.043*"short" + 0.039*"area" + 0.027*"come"
topic diff=0.102158, rho=0.277350
-2.989 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=3, decay=0.5, chunksize=20) in 0.19s', 'datetime': '2022-02-06T15:31:38.812791', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
Coherence Score: 0.5503595963830591
Starting K=4
using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.154*"area" + 0.137*"see" + 0.132*"back" + 0.127*"court" + 0.074*"build" + 0.070*"come" + 0.057*"royal" + 0.054*"set" + 0.050*"school" + 0.041*"role"
topic #1 (0.250): 0.357*"player" + 0.096*"long" + 0.075*"build" + 0.074*"court" + 0.073*"short" + 0.073*"come" + 0.052*"school" + 0.051*"original" + 0.030*"film" + 0.030*"international"
topic #2 (0.250): 0.303*"film" + 0.170*"school" + 0.080*"area" + 0.074*"international" + 0.057*"come" + 0.045*"short" + 0.045*"royal" + 0.045*"set" + 0.040*"build" + 0.038*"role"
topic #3 (0.250): 0.222*"back" + 0.161*"see" + 0.155*"film" + 0.111*"long" + 0.068*"area" + 0.065*"come" + 0.050*"original" + 0.032*"short" + 0.030*"set" + 0.023*"build"
topic diff=1.067418, rho=1.000000
-3.098 per-word bound, 8.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.169*"area" + 0.137*"court" + 0.131*"see" + 0.129*"back" + 0.077*"build" + 0.076*"come" + 0.061*"royal" + 0.050*"set" + 0.047*"school" + 0.040*"role"
topic #1 (0.250): 0.385*"player" + 0.096*"long" + 0.078*"short" + 0.077*"come" + 0.075*"build" + 0.060*"original" + 0.048*"court" + 0.038*"international" + 0.037*"school" + 0.031*"set"
topic #2 (0.250): 0.329*"film" + 0.181*"school" + 0.080*"international" + 0.067*"area" + 0.048*"come" + 0.047*"set" + 0.047*"short" + 0.040*"role" + 0.039*"royal" + 0.036*"build"
topic #3 (0.250): 0.244*"back" + 0.181*"see" + 0.129*"film" + 0.120*"long" + 0.062*"come" + 0.056*"area" + 0.055*"original" + 0.032*"short" + 0.031*"set" + 0.022*"build"
topic diff=0.200071, rho=0.377964
-3.044 per-word bound, 8.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.181*"area" + 0.143*"court" + 0.121*"back" + 0.120*"see" + 0.081*"come" + 0.081*"build" + 0.069*"royal" + 0.048*"set" + 0.048*"school" + 0.038*"role"
topic #1 (0.250): 0.396*"player" + 0.093*"long" + 0.083*"come" + 0.080*"short" + 0.073*"build" + 0.071*"original" + 0.047*"international" + 0.033*"court" + 0.033*"set" + 0.026*"school"
topic #2 (0.250): 0.352*"film" + 0.188*"school" + 0.084*"international" + 0.056*"area" + 0.049*"set" + 0.048*"short" + 0.045*"role" + 0.040*"come" + 0.032*"build" + 0.030*"royal"
topic #3 (0.250): 0.265*"back" + 0.206*"see" + 0.121*"long" + 0.108*"film" + 0.062*"come" + 0.050*"original" + 0.047*"area" + 0.033*"set" + 0.032*"short" + 0.021*"build"
topic diff=0.185151, rho=0.353553
-3.011 per-word bound, 8.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.193*"area" + 0.149*"court" + 0.112*"back" + 0.108*"see" + 0.086*"build" + 0.084*"come" + 0.075*"royal" + 0.049*"school" + 0.046*"set" + 0.037*"role"
topic #1 (0.250): 0.393*"player" + 0.091*"long" + 0.088*"come" + 0.081*"short" + 0.076*"original" + 0.071*"build" + 0.068*"international" + 0.034*"set" + 0.023*"court" + 0.021*"role"
topic #2 (0.250): 0.374*"film" + 0.194*"school" + 0.081*"international" + 0.050*"set" + 0.049*"short" + 0.049*"role" + 0.046*"area" + 0.033*"come" + 0.028*"build" + 0.027*"original"
topic #3 (0.250): 0.283*"back" + 0.234*"see" + 0.115*"long" + 0.083*"film" + 0.066*"come" + 0.044*"original" + 0.043*"area" + 0.035*"set" + 0.031*"short" + 0.020*"build"
topic diff=0.174391, rho=0.333333
-2.991 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.204*"area" + 0.155*"court" + 0.103*"back" + 0.094*"see" + 0.091*"build" + 0.085*"come" + 0.082*"royal" + 0.051*"school" + 0.042*"set" + 0.037*"role"
topic #1 (0.250): 0.381*"player" + 0.092*"come" + 0.092*"international" + 0.087*"long" + 0.082*"short" + 0.078*"original" + 0.068*"build" + 0.037*"set" + 0.022*"role" + 0.017*"court"
topic #2 (0.250): 0.395*"film" + 0.198*"school" + 0.074*"international" + 0.053*"set" + 0.050*"role" + 0.050*"short" + 0.038*"area" + 0.029*"original" + 0.025*"come" + 0.024*"build"
topic #3 (0.250): 0.295*"back" + 0.257*"see" + 0.107*"long" + 0.072*"come" + 0.061*"film" + 0.041*"area" + 0.040*"original" + 0.038*"set" + 0.029*"short" + 0.018*"build"
topic diff=0.163239, rho=0.316228
-2.979 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.214*"area" + 0.161*"court" + 0.097*"build" + 0.095*"back" + 0.087*"royal" + 0.083*"come" + 0.081*"see" + 0.054*"school" + 0.039*"set" + 0.037*"role"
topic #1 (0.250): 0.364*"player" + 0.120*"international" + 0.097*"come" + 0.084*"long" + 0.082*"short" + 0.079*"original" + 0.064*"build" + 0.038*"set" + 0.022*"role" + 0.012*"court"
topic #2 (0.250): 0.415*"film" + 0.203*"school" + 0.063*"international" + 0.055*"set" + 0.052*"role" + 0.051*"short" + 0.032*"area" + 0.031*"original" + 0.021*"build" + 0.020*"come"
topic #3 (0.250): 0.304*"back" + 0.275*"see" + 0.099*"long" + 0.080*"come" + 0.045*"film" + 0.041*"set" + 0.040*"area" + 0.037*"original" + 0.026*"short" + 0.017*"build"
topic diff=0.150003, rho=0.301511
-2.974 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.223*"area" + 0.167*"court" + 0.102*"build" + 0.092*"royal" + 0.087*"back" + 0.080*"come" + 0.071*"see" + 0.056*"school" + 0.038*"role" + 0.036*"set"
topic #1 (0.250): 0.347*"player" + 0.145*"international" + 0.098*"come" + 0.082*"short" + 0.081*"long" + 0.079*"original" + 0.060*"build" + 0.042*"set" + 0.021*"role" + 0.010*"court"
topic #2 (0.250): 0.434*"film" + 0.207*"school" + 0.055*"set" + 0.054*"role" + 0.052*"short" + 0.050*"international" + 0.032*"original" + 0.027*"area" + 0.018*"build" + 0.018*"long"
topic #3 (0.250): 0.310*"back" + 0.287*"see" + 0.091*"long" + 0.087*"come" + 0.044*"set" + 0.041*"area" + 0.034*"original" + 0.033*"film" + 0.023*"short" + 0.016*"build"
topic diff=0.137191, rho=0.288675
-2.972 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.230*"area" + 0.171*"court" + 0.106*"build" + 0.096*"royal" + 0.080*"back" + 0.077*"come" + 0.062*"see" + 0.059*"school" + 0.038*"role" + 0.033*"set"
topic #1 (0.250): 0.334*"player" + 0.161*"international" + 0.098*"come" + 0.081*"short" + 0.081*"long" + 0.079*"original" + 0.060*"build" + 0.045*"set" + 0.021*"role" + 0.008*"court"
topic #2 (0.250): 0.448*"film" + 0.209*"school" + 0.055*"set" + 0.055*"role" + 0.054*"short" + 0.040*"international" + 0.033*"original" + 0.023*"area" + 0.020*"long" + 0.016*"build"
topic #3 (0.250): 0.315*"back" + 0.295*"see" + 0.094*"come" + 0.084*"long" + 0.047*"set" + 0.042*"area" + 0.032*"original" + 0.025*"film" + 0.021*"short" + 0.013*"build"
topic diff=0.122455, rho=0.277350
-2.968 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=4, decay=0.5, chunksize=20) in 0.19s', 'datetime': '2022-02-06T15:31:39.117923', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
Coherence Score: 0.550359596383059
Starting K=5
using symmetric alpha at 0.2
using symmetric eta at 0.2
using serial LDA version on this node
running online LDA training, 5 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.276*"see" + 0.195*"back" + 0.120*"area" + 0.117*"come" + 0.065*"set" + 0.036*"royal" + 0.036*"school" + 0.036*"role" + 0.026*"court" + 0.020*"build"
topic #1 (0.200): 0.196*"long" + 0.159*"school" + 0.158*"come" + 0.121*"role" + 0.084*"build" + 0.047*"player" + 0.046*"royal" + 0.046*"see" + 0.046*"original" + 0.045*"short"
topic #2 (0.200): 0.256*"school" + 0.140*"international" + 0.101*"back" + 0.082*"come" + 0.082*"role" + 0.081*"royal" + 0.063*"player" + 0.062*"short" + 0.043*"area" + 0.043*"set"
topic #3 (0.200): 0.264*"back" + 0.171*"see" + 0.090*"come" + 0.088*"original" + 0.071*"area" + 0.060*"long" + 0.054*"international" + 0.043*"short" + 0.041*"set" + 0.032*"court"
topic #4 (0.200): 0.269*"film" + 0.128*"area" + 0.113*"court" + 0.086*"build" + 0.080*"player" + 0.065*"school" + 0.044*"set" + 0.033*"royal" + 0.028*"international" + 0.028*"short"
topic diff=1.591860, rho=1.000000
-3.616 per-word bound, 12.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.273*"see" + 0.216*"back" + 0.137*"area" + 0.129*"come" + 0.061*"set" + 0.035*"school" + 0.032*"role" + 0.025*"court" + 0.025*"royal" + 0.014*"short"
topic #1 (0.200): 0.225*"long" + 0.145*"come" + 0.137*"role" + 0.131*"school" + 0.089*"build" + 0.052*"short" + 0.047*"player" + 0.046*"original" + 0.046*"see" + 0.032*"royal"
topic #2 (0.200): 0.261*"school" + 0.158*"international" + 0.123*"royal" + 0.083*"come" + 0.081*"back" + 0.069*"role" + 0.058*"player" + 0.058*"short" + 0.040*"set" + 0.027*"area"
topic #3 (0.200): 0.276*"back" + 0.156*"see" + 0.110*"original" + 0.079*"come" + 0.068*"long" + 0.056*"area" + 0.051*"short" + 0.043*"international" + 0.041*"set" + 0.034*"court"
topic #4 (0.200): 0.279*"film" + 0.129*"area" + 0.116*"court" + 0.090*"build" + 0.082*"player" + 0.064*"school" + 0.045*"set" + 0.028*"international" + 0.027*"short" + 0.025*"original"
topic diff=0.179093, rho=0.377964
-3.574 per-word bound, 11.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.267*"see" + 0.220*"back" + 0.154*"area" + 0.137*"come" + 0.058*"set" + 0.035*"school" + 0.030*"role" + 0.024*"court" + 0.020*"royal" + 0.013*"short"
topic #1 (0.200): 0.239*"long" + 0.143*"role" + 0.122*"come" + 0.113*"school" + 0.106*"build" + 0.059*"original" + 0.057*"short" + 0.046*"player" + 0.045*"see" + 0.023*"royal"
topic #2 (0.200): 0.270*"school" + 0.167*"international" + 0.145*"royal" + 0.084*"come" + 0.063*"back" + 0.062*"role" + 0.056*"short" + 0.056*"player" + 0.039*"set" + 0.021*"build"
topic #3 (0.200): 0.293*"back" + 0.142*"see" + 0.127*"original" + 0.074*"long" + 0.069*"come" + 0.056*"short" + 0.044*"area" + 0.041*"set" + 0.036*"court" + 0.034*"international"
topic #4 (0.200): 0.291*"film" + 0.125*"area" + 0.120*"court" + 0.091*"build" + 0.086*"player" + 0.061*"school" + 0.045*"set" + 0.029*"international" + 0.027*"short" + 0.025*"original"
topic diff=0.151590, rho=0.353553
-3.534 per-word bound, 11.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.263*"see" + 0.221*"back" + 0.169*"area" + 0.141*"come" + 0.057*"set" + 0.035*"school" + 0.029*"role" + 0.024*"court" + 0.014*"royal" + 0.013*"short"
topic #1 (0.200): 0.246*"long" + 0.148*"role" + 0.125*"build" + 0.105*"come" + 0.099*"school" + 0.067*"original" + 0.060*"short" + 0.044*"player" + 0.043*"see" + 0.017*"royal"
topic #2 (0.200): 0.278*"school" + 0.171*"international" + 0.160*"royal" + 0.085*"come" + 0.058*"role" + 0.055*"short" + 0.054*"player" + 0.050*"back" + 0.038*"set" + 0.021*"build"
topic #3 (0.200): 0.309*"back" + 0.138*"original" + 0.131*"see" + 0.079*"long" + 0.061*"come" + 0.061*"short" + 0.041*"set" + 0.037*"court" + 0.034*"area" + 0.033*"royal"
topic #4 (0.200): 0.303*"film" + 0.124*"court" + 0.121*"area" + 0.092*"build" + 0.089*"player" + 0.058*"school" + 0.046*"set" + 0.030*"international" + 0.027*"short" + 0.025*"original"
topic diff=0.135615, rho=0.333333
-3.506 per-word bound, 11.4 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.261*"see" + 0.221*"back" + 0.181*"area" + 0.142*"come" + 0.057*"set" + 0.035*"school" + 0.027*"role" + 0.023*"court" + 0.013*"short" + 0.012*"long"
topic #1 (0.200): 0.247*"long" + 0.156*"role" + 0.139*"build" + 0.093*"come" + 0.089*"school" + 0.073*"original" + 0.063*"short" + 0.042*"player" + 0.042*"see" + 0.014*"royal"
topic #2 (0.200): 0.286*"school" + 0.172*"international" + 0.170*"royal" + 0.086*"come" + 0.055*"short" + 0.054*"role" + 0.052*"player" + 0.041*"back" + 0.038*"set" + 0.021*"build"
topic #3 (0.200): 0.321*"back" + 0.147*"original" + 0.123*"see" + 0.083*"long" + 0.064*"short" + 0.055*"come" + 0.040*"set" + 0.037*"court" + 0.035*"royal" + 0.026*"area"
topic #4 (0.200): 0.315*"film" + 0.128*"court" + 0.117*"area" + 0.092*"player" + 0.092*"build" + 0.054*"school" + 0.047*"set" + 0.030*"international" + 0.026*"short" + 0.025*"original"
topic diff=0.120696, rho=0.316228
-3.486 per-word bound, 11.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.259*"see" + 0.221*"back" + 0.190*"area" + 0.143*"come" + 0.057*"set" + 0.036*"school" + 0.023*"court" + 0.023*"role" + 0.013*"short" + 0.012*"long"
topic #1 (0.200): 0.243*"long" + 0.168*"role" + 0.151*"build" + 0.085*"come" + 0.081*"school" + 0.076*"original" + 0.064*"short" + 0.040*"player" + 0.040*"see" + 0.011*"royal"
topic #2 (0.200): 0.294*"school" + 0.176*"royal" + 0.173*"international" + 0.086*"come" + 0.055*"short" + 0.052*"role" + 0.051*"player" + 0.038*"set" + 0.033*"back" + 0.018*"build"
topic #3 (0.200): 0.334*"back" + 0.152*"original" + 0.116*"see" + 0.085*"long" + 0.065*"short" + 0.050*"come" + 0.040*"set" + 0.038*"royal" + 0.038*"court" + 0.021*"area"
topic #4 (0.200): 0.325*"film" + 0.132*"court" + 0.113*"area" + 0.095*"player" + 0.092*"build" + 0.051*"school" + 0.047*"set" + 0.031*"international" + 0.026*"short" + 0.025*"original"
topic diff=0.115044, rho=0.301511
-3.470 per-word bound, 11.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.258*"see" + 0.220*"back" + 0.199*"area" + 0.144*"come" + 0.057*"set" + 0.036*"school" + 0.023*"court" + 0.019*"role" + 0.013*"long" + 0.013*"short"
topic #1 (0.200): 0.240*"long" + 0.180*"role" + 0.160*"build" + 0.079*"come" + 0.078*"original" + 0.076*"school" + 0.064*"short" + 0.039*"player" + 0.035*"see" + 0.009*"royal"
topic #2 (0.200): 0.301*"school" + 0.181*"royal" + 0.174*"international" + 0.087*"come" + 0.055*"short" + 0.051*"role" + 0.050*"player" + 0.038*"set" + 0.026*"back" + 0.016*"build"
topic #3 (0.200): 0.344*"back" + 0.155*"original" + 0.111*"see" + 0.087*"long" + 0.066*"short" + 0.047*"come" + 0.039*"royal" + 0.039*"set" + 0.038*"court" + 0.017*"area"
topic #4 (0.200): 0.333*"film" + 0.135*"court" + 0.109*"area" + 0.097*"player" + 0.093*"build" + 0.047*"set" + 0.047*"school" + 0.031*"international" + 0.025*"short" + 0.025*"original"
topic diff=0.107080, rho=0.288675
-3.457 per-word bound, 11.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.200): 0.257*"see" + 0.218*"back" + 0.207*"area" + 0.143*"come" + 0.057*"set" + 0.036*"school" + 0.022*"court" + 0.015*"role" + 0.013*"long" + 0.013*"short"
topic #1 (0.200): 0.236*"long" + 0.193*"role" + 0.166*"build" + 0.080*"original" + 0.075*"come" + 0.071*"school" + 0.065*"short" + 0.037*"player" + 0.030*"see" + 0.008*"royal"
topic #2 (0.200): 0.308*"school" + 0.184*"royal" + 0.175*"international" + 0.087*"come" + 0.055*"short" + 0.050*"player" + 0.048*"role" + 0.038*"set" + 0.021*"back" + 0.014*"build"
topic #3 (0.200): 0.352*"back" + 0.157*"original" + 0.107*"see" + 0.089*"long" + 0.067*"short" + 0.044*"come" + 0.042*"royal" + 0.039*"set" + 0.038*"court" + 0.014*"area"
topic #4 (0.200): 0.341*"film" + 0.138*"court" + 0.104*"area" + 0.099*"player" + 0.093*"build" + 0.047*"set" + 0.044*"school" + 0.031*"international" + 0.025*"short" + 0.024*"original"
topic diff=0.100235, rho=0.277350
-3.447 per-word bound, 10.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=5, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:31:39.417290', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
Coherence Score: 0.5503595963830591
Starting K=6
using symmetric alpha at 0.16666666666666666
using symmetric eta at 0.16666666666666666
using serial LDA version on this node
running online LDA training, 6 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #2 (0.167): 0.346*"school" + 0.196*"back" + 0.132*"royal" + 0.068*"come" + 0.068*"short" + 0.046*"original" + 0.046*"international" + 0.025*"area" + 0.025*"role" + 0.025*"set"
topic #0 (0.167): 0.178*"see" + 0.131*"back" + 0.101*"royal" + 0.077*"area" + 0.068*"set" + 0.066*"come" + 0.062*"international" + 0.062*"build" + 0.062*"player" + 0.052*"court"
topic #3 (0.167): 0.194*"see" + 0.152*"original" + 0.120*"back" + 0.088*"long" + 0.087*"short" + 0.082*"international" + 0.071*"area" + 0.061*"come" + 0.034*"set" + 0.026*"school"
topic #4 (0.167): 0.294*"film" + 0.132*"area" + 0.119*"court" + 0.094*"build" + 0.070*"school" + 0.063*"player" + 0.051*"set" + 0.032*"short" + 0.026*"international" + 0.026*"original"
topic #5 (0.167): 0.172*"back" + 0.167*"see" + 0.153*"come" + 0.106*"area" + 0.062*"international" + 0.062*"player" + 0.062*"role" + 0.052*"film" + 0.035*"set" + 0.033*"school"
topic diff=1.977311, rho=1.000000
-3.869 per-word bound, 14.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #2 (0.167): 0.379*"school" + 0.190*"back" + 0.152*"royal" + 0.068*"come" + 0.063*"short" + 0.031*"original" + 0.031*"international" + 0.025*"role" + 0.020*"area" + 0.018*"set"
topic #4 (0.167): 0.309*"film" + 0.135*"area" + 0.123*"court" + 0.097*"build" + 0.061*"school" + 0.057*"player" + 0.051*"set" + 0.030*"short" + 0.027*"original" + 0.026*"international"
topic #3 (0.167): 0.220*"original" + 0.169*"see" + 0.125*"short" + 0.096*"long" + 0.088*"back" + 0.062*"international" + 0.054*"area" + 0.047*"come" + 0.028*"set" + 0.023*"school"
topic #0 (0.167): 0.147*"see" + 0.121*"back" + 0.117*"royal" + 0.076*"set" + 0.075*"come" + 0.072*"build" + 0.066*"area" + 0.063*"international" + 0.057*"original" + 0.056*"court"
topic #5 (0.167): 0.184*"see" + 0.182*"back" + 0.148*"come" + 0.111*"area" + 0.075*"player" + 0.071*"international" + 0.050*"role" + 0.039*"set" + 0.032*"film" + 0.030*"school"
topic diff=0.224872, rho=0.377964
-3.710 per-word bound, 13.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #4 (0.167): 0.324*"film" + 0.141*"area" + 0.129*"court" + 0.101*"build" + 0.054*"school" + 0.053*"set" + 0.038*"player" + 0.029*"short" + 0.027*"original" + 0.027*"international"
topic #0 (0.167): 0.130*"royal" + 0.125*"see" + 0.115*"back" + 0.083*"set" + 0.082*"come" + 0.077*"build" + 0.069*"original" + 0.065*"international" + 0.056*"court" + 0.054*"area"
topic #3 (0.167): 0.270*"original" + 0.151*"see" + 0.150*"short" + 0.100*"long" + 0.066*"back" + 0.048*"international" + 0.042*"area" + 0.038*"come" + 0.024*"set" + 0.021*"school"
topic #1 (0.167): 0.415*"long" + 0.157*"role" + 0.097*"school" + 0.090*"short" + 0.078*"build" + 0.069*"see" + 0.010*"player" + 0.010*"film" + 0.010*"international" + 0.009*"set"
topic #2 (0.167): 0.402*"school" + 0.185*"back" + 0.163*"royal" + 0.069*"come" + 0.060*"short" + 0.025*"role" + 0.022*"original" + 0.022*"international" + 0.016*"area" + 0.014*"set"
topic diff=0.196099, rho=0.353553
-3.582 per-word bound, 12.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #1 (0.167): 0.415*"long" + 0.175*"role" + 0.096*"school" + 0.090*"short" + 0.074*"build" + 0.063*"see" + 0.009*"player" + 0.009*"film" + 0.009*"international" + 0.009*"set"
topic #3 (0.167): 0.305*"original" + 0.167*"short" + 0.139*"see" + 0.103*"long" + 0.050*"back" + 0.038*"international" + 0.034*"area" + 0.031*"come" + 0.022*"set" + 0.019*"school"
topic #4 (0.167): 0.335*"film" + 0.146*"area" + 0.134*"court" + 0.105*"build" + 0.053*"set" + 0.048*"school" + 0.028*"short" + 0.028*"original" + 0.027*"international" + 0.026*"player"
topic #5 (0.167): 0.192*"see" + 0.186*"back" + 0.140*"come" + 0.117*"player" + 0.111*"area" + 0.077*"international" + 0.040*"set" + 0.035*"role" + 0.027*"school" + 0.019*"court"
topic #0 (0.167): 0.141*"royal" + 0.112*"back" + 0.109*"see" + 0.089*"set" + 0.088*"come" + 0.080*"build" + 0.079*"original" + 0.067*"international" + 0.057*"role" + 0.054*"court"
topic diff=0.167605, rho=0.333333
-3.531 per-word bound, 11.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #5 (0.167): 0.197*"back" + 0.193*"see" + 0.136*"come" + 0.124*"player" + 0.110*"area" + 0.077*"international" + 0.040*"set" + 0.032*"role" + 0.026*"school" + 0.018*"court"
topic #1 (0.167): 0.410*"long" + 0.194*"role" + 0.094*"school" + 0.089*"short" + 0.072*"build" + 0.062*"see" + 0.008*"player" + 0.008*"set" + 0.008*"international" + 0.008*"film"
topic #0 (0.167): 0.153*"royal" + 0.113*"back" + 0.095*"set" + 0.095*"come" + 0.092*"see" + 0.088*"original" + 0.079*"build" + 0.070*"international" + 0.053*"role" + 0.051*"court"
topic #2 (0.167): 0.448*"school" + 0.178*"royal" + 0.149*"back" + 0.073*"come" + 0.058*"short" + 0.025*"role" + 0.013*"international" + 0.013*"original" + 0.011*"area" + 0.009*"set"
topic #3 (0.167): 0.344*"original" + 0.179*"short" + 0.125*"see" + 0.104*"long" + 0.038*"back" + 0.030*"international" + 0.028*"area" + 0.025*"come" + 0.019*"set" + 0.018*"school"
topic diff=0.160165, rho=0.316228
-3.486 per-word bound, 11.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.167): 0.145*"royal" + 0.115*"back" + 0.111*"set" + 0.101*"come" + 0.096*"original" + 0.084*"build" + 0.080*"see" + 0.073*"international" + 0.049*"court" + 0.045*"player"
topic #4 (0.167): 0.351*"film" + 0.154*"area" + 0.141*"court" + 0.110*"build" + 0.053*"set" + 0.036*"school" + 0.028*"international" + 0.025*"short" + 0.024*"original" + 0.024*"see"
topic #2 (0.167): 0.464*"school" + 0.194*"royal" + 0.125*"back" + 0.075*"come" + 0.058*"short" + 0.026*"role" + 0.010*"international" + 0.010*"original" + 0.009*"area" + 0.007*"set"
topic #1 (0.167): 0.397*"long" + 0.213*"role" + 0.090*"school" + 0.089*"short" + 0.073*"build" + 0.063*"see" + 0.008*"set" + 0.008*"player" + 0.008*"international" + 0.008*"film"
topic #5 (0.167): 0.204*"back" + 0.192*"see" + 0.134*"come" + 0.129*"player" + 0.110*"area" + 0.077*"international" + 0.040*"set" + 0.029*"role" + 0.025*"school" + 0.018*"court"
topic diff=0.154115, rho=0.301511
-3.449 per-word bound, 10.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #5 (0.167): 0.210*"back" + 0.192*"see" + 0.133*"come" + 0.132*"player" + 0.109*"area" + 0.077*"international" + 0.040*"set" + 0.026*"role" + 0.025*"school" + 0.017*"court"
topic #2 (0.167): 0.476*"school" + 0.204*"royal" + 0.109*"back" + 0.076*"come" + 0.058*"short" + 0.024*"role" + 0.008*"original" + 0.008*"international" + 0.008*"area" + 0.006*"set"
topic #1 (0.167): 0.382*"long" + 0.238*"role" + 0.087*"short" + 0.086*"school" + 0.072*"build" + 0.063*"see" + 0.007*"set" + 0.007*"international" + 0.007*"player" + 0.007*"film"
topic #4 (0.167): 0.357*"film" + 0.157*"area" + 0.144*"court" + 0.111*"build" + 0.053*"set" + 0.031*"school" + 0.028*"international" + 0.025*"see" + 0.024*"short" + 0.023*"original"
topic #0 (0.167): 0.139*"royal" + 0.123*"set" + 0.117*"back" + 0.106*"come" + 0.103*"original" + 0.088*"build" + 0.076*"international" + 0.071*"see" + 0.048*"court" + 0.045*"player"
topic diff=0.138776, rho=0.288675
-3.423 per-word bound, 10.7 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #4 (0.167): 0.362*"film" + 0.159*"area" + 0.146*"court" + 0.112*"build" + 0.052*"set" + 0.029*"international" + 0.027*"school" + 0.026*"see" + 0.023*"short" + 0.022*"original"
topic #1 (0.167): 0.371*"long" + 0.260*"role" + 0.086*"short" + 0.083*"school" + 0.071*"build" + 0.062*"see" + 0.007*"set" + 0.007*"international" + 0.007*"film" + 0.007*"player"
topic #5 (0.167): 0.214*"back" + 0.192*"see" + 0.134*"player" + 0.132*"come" + 0.109*"area" + 0.078*"international" + 0.040*"set" + 0.025*"school" + 0.022*"role" + 0.017*"court"
topic #2 (0.167): 0.487*"school" + 0.210*"royal" + 0.097*"back" + 0.078*"come" + 0.058*"short" + 0.021*"role" + 0.007*"original" + 0.007*"international" + 0.007*"area" + 0.006*"set"
topic #0 (0.167): 0.135*"royal" + 0.133*"set" + 0.118*"back" + 0.110*"come" + 0.107*"original" + 0.090*"build" + 0.078*"international" + 0.065*"see" + 0.047*"court" + 0.045*"player"
topic diff=0.123994, rho=0.277350
-3.405 per-word bound, 10.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=6, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:31:39.728701', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
Coherence Score: 0.5503595963830592
ALL DONE!
