using symmetric alpha at 0.05
using symmetric eta at 0.05
using serial LDA version on this node
running online LDA training, 20 topics, 10 passes over the supplied corpus of 996 documents, updating every 700 documents, evaluating every ~996 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #9 (0.050): 0.008*"game" + 0.007*"design" + 0.006*"road" + 0.004*"street" + 0.004*"player" + 0.004*"south" + 0.004*"church" + 0.004*"system" + 0.004*"engine" + 0.004*"town"
topic #6 (0.050): 0.009*"study" + 0.009*"child" + 0.007*"development" + 0.005*"park" + 0.005*"south" + 0.005*"effect" + 0.004*"low" + 0.004*"album" + 0.004*"specie" + 0.004*"community"
topic #19 (0.050): 0.008*"population" + 0.006*"album" + 0.006*"band" + 0.005*"sport" + 0.004*"music" + 0.004*"film" + 0.004*"player" + 0.004*"specie" + 0.004*"art" + 0.004*"song"
topic #12 (0.050): 0.012*"season" + 0.011*"game" + 0.007*"player" + 0.007*"club" + 0.005*"football" + 0.005*"final" + 0.004*"child" + 0.004*"point" + 0.004*"tournament" + 0.004*"career"
topic #3 (0.050): 0.008*"film" + 0.007*"district" + 0.006*"book" + 0.005*"temple" + 0.004*"side" + 0.004*"art" + 0.003*"squadron" + 0.003*"program" + 0.003*"story" + 0.003*"government"
topic diff=1.587651, rho=1.000000
-8.345 per-word bound, 325.1 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #12 (0.050): 0.011*"game" + 0.010*"season" + 0.006*"player" + 0.005*"club" + 0.005*"football" + 0.005*"event" + 0.005*"tournament" + 0.005*"final" + 0.003*"title" + 0.003*"child"
topic #5 (0.050): 0.011*"station" + 0.008*"season" + 0.007*"game" + 0.006*"story" + 0.005*"player" + 0.004*"company" + 0.004*"title" + 0.004*"series" + 0.003*"railway" + 0.003*"network"
topic #14 (0.050): 0.007*"child" + 0.006*"event" + 0.006*"student" + 0.005*"village" + 0.005*"company" + 0.005*"population" + 0.005*"book" + 0.004*"study" + 0.004*"effect" + 0.004*"story"
topic #8 (0.050): 0.010*"star" + 0.008*"film" + 0.008*"art" + 0.005*"museum" + 0.005*"train" + 0.004*"type" + 0.004*"meet" + 0.004*"railway" + 0.004*"design" + 0.004*"british"
topic #19 (0.050): 0.007*"album" + 0.007*"band" + 0.007*"population" + 0.005*"tour" + 0.005*"music" + 0.004*"fire" + 0.004*"game" + 0.004*"film" + 0.004*"sport" + 0.004*"song"
topic diff=0.738616, rho=0.353553
-7.975 per-word bound, 251.5 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #5 (0.050): 0.014*"station" + 0.009*"story" + 0.007*"season" + 0.005*"game" + 0.005*"australian" + 0.005*"german" + 0.005*"company" + 0.004*"player" + 0.004*"university" + 0.004*"ship"
topic #16 (0.050): 0.013*"game" + 0.010*"house" + 0.006*"attack" + 0.006*"design" + 0.005*"company" + 0.005*"building" + 0.005*"bridge" + 0.005*"division" + 0.005*"north" + 0.004*"film"
topic #12 (0.050): 0.013*"game" + 0.013*"season" + 0.010*"player" + 0.007*"football" + 0.007*"club" + 0.006*"tournament" + 0.005*"goal" + 0.005*"sport" + 0.005*"coach" + 0.005*"event"
topic #9 (0.050): 0.008*"design" + 0.008*"game" + 0.006*"road" + 0.005*"route" + 0.005*"specie" + 0.005*"system" + 0.005*"engine" + 0.005*"street" + 0.004*"law" + 0.004*"south"
topic #18 (0.050): 0.012*"album" + 0.011*"song" + 0.007*"music" + 0.006*"hit" + 0.005*"chart" + 0.004*"station" + 0.004*"event" + 0.004*"band" + 0.004*"game" + 0.004*"season"
topic diff=0.392645, rho=0.289157
-8.004 per-word bound, 256.7 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #16 (0.050): 0.015*"house" + 0.012*"game" + 0.007*"bridge" + 0.005*"building" + 0.005*"attack" + 0.005*"design" + 0.005*"theatre" + 0.005*"plan" + 0.004*"north" + 0.004*"battle"
topic #11 (0.050): 0.012*"company" + 0.009*"student" + 0.008*"son" + 0.008*"film" + 0.006*"street" + 0.005*"club" + 0.005*"station" + 0.004*"music" + 0.004*"game" + 0.004*"band"
topic #6 (0.050): 0.014*"railway" + 0.012*"station" + 0.011*"meet" + 0.008*"district" + 0.007*"child" + 0.007*"train" + 0.007*"study" + 0.007*"development" + 0.006*"village" + 0.006*"north"
topic #8 (0.050): 0.021*"star" + 0.013*"art" + 0.011*"film" + 0.009*"type" + 0.006*"museum" + 0.005*"train" + 0.005*"black" + 0.005*"artist" + 0.005*"female" + 0.004*"system"
topic #19 (0.050): 0.013*"band" + 0.009*"album" + 0.009*"fire" + 0.007*"tour" + 0.006*"population" + 0.006*"music" + 0.005*"concert" + 0.004*"exhibition" + 0.004*"event" + 0.004*"sport"
topic diff=0.537638, rho=0.289157
-7.541 per-word bound, 186.3 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 896 documents into a model of 996 documents
topic #17 (0.050): 0.013*"winner" + 0.010*"film" + 0.010*"center" + 0.009*"event" + 0.007*"match" + 0.007*"history" + 0.007*"round" + 0.006*"book" + 0.006*"woman" + 0.005*"star"
topic #1 (0.050): 0.016*"building" + 0.010*"church" + 0.006*"university" + 0.006*"house" + 0.006*"government" + 0.006*"child" + 0.006*"program" + 0.006*"board" + 0.006*"international" + 0.005*"district"
topic #7 (0.050): 0.014*"film" + 0.009*"race" + 0.009*"series" + 0.006*"channel" + 0.006*"event" + 0.006*"championship" + 0.006*"attack" + 0.005*"band" + 0.004*"direct" + 0.004*"fail"
topic #13 (0.050): 0.018*"reach" + 0.016*"lose" + 0.016*"bridge" + 0.014*"final" + 0.008*"round" + 0.006*"player" + 0.006*"event" + 0.006*"title" + 0.005*"episode" + 0.005*"art"
topic #10 (0.050): 0.032*"game" + 0.012*"baseball" + 0.010*"specie" + 0.009*"week" + 0.008*"port" + 0.005*"network" + 0.005*"station" + 0.005*"season" + 0.004*"football" + 0.004*"trade"
topic diff=0.446207, rho=0.277778
-7.456 per-word bound, 175.6 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 100 documents into a model of 996 documents
topic #2 (0.050): 0.026*"album" + 0.020*"song" + 0.018*"eagle" + 0.012*"command" + 0.009*"major" + 0.009*"order" + 0.009*"game" + 0.008*"army" + 0.008*"division" + 0.008*"general"
topic #1 (0.050): 0.017*"church" + 0.016*"building" + 0.009*"house" + 0.007*"district" + 0.006*"river" + 0.006*"north" + 0.006*"plant" + 0.006*"water" + 0.006*"office" + 0.005*"government"
topic #0 (0.050): 0.016*"district" + 0.010*"water" + 0.009*"title" + 0.008*"fight" + 0.008*"transfer" + 0.008*"house" + 0.007*"film" + 0.007*"space" + 0.007*"act" + 0.006*"specie"
topic #6 (0.050): 0.016*"railway" + 0.015*"station" + 0.012*"meet" + 0.010*"district" + 0.008*"village" + 0.008*"train" + 0.007*"study" + 0.007*"child" + 0.006*"north" + 0.006*"development"
topic #16 (0.050): 0.019*"game" + 0.012*"character" + 0.012*"house" + 0.008*"job" + 0.007*"advance" + 0.007*"battle" + 0.007*"mission" + 0.006*"ability" + 0.006*"theatre" + 0.005*"attack"
topic diff=0.672876, rho=0.277778
-7.595 per-word bound, 193.4 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.050): 0.015*"district" + 0.010*"title" + 0.010*"space" + 0.009*"fight" + 0.009*"water" + 0.008*"house" + 0.007*"act" + 0.007*"transfer" + 0.007*"film" + 0.007*"government"
topic #17 (0.050): 0.016*"winner" + 0.015*"film" + 0.010*"event" + 0.008*"division" + 0.007*"history" + 0.007*"book" + 0.007*"match" + 0.007*"center" + 0.006*"round" + 0.006*"contest"
topic #9 (0.050): 0.010*"design" + 0.008*"road" + 0.008*"town" + 0.008*"specie" + 0.007*"route" + 0.007*"street" + 0.007*"engine" + 0.006*"system" + 0.006*"game" + 0.005*"water"
topic #4 (0.050): 0.013*"surname" + 0.013*"room" + 0.010*"air" + 0.010*"episode" + 0.010*"game" + 0.010*"series" + 0.008*"specie" + 0.008*"common" + 0.006*"person" + 0.006*"th"
topic #7 (0.050): 0.016*"film" + 0.016*"attack" + 0.011*"series" + 0.009*"race" + 0.008*"internet" + 0.007*"digital" + 0.006*"technology" + 0.006*"company" + 0.005*"channel" + 0.005*"science"
topic diff=0.555664, rho=0.267644
-7.640 per-word bound, 199.5 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #6 (0.050): 0.025*"station" + 0.024*"railway" + 0.021*"meet" + 0.014*"train" + 0.014*"district" + 0.010*"run" + 0.008*"north" + 0.007*"company" + 0.007*"village" + 0.007*"park"
topic #4 (0.050): 0.013*"surname" + 0.012*"episode" + 0.011*"room" + 0.010*"series" + 0.010*"air" + 0.009*"game" + 0.007*"specie" + 0.007*"common" + 0.006*"race" + 0.006*"person"
topic #19 (0.050): 0.017*"fire" + 0.011*"band" + 0.009*"tour" + 0.009*"album" + 0.008*"water" + 0.008*"concert" + 0.006*"exhibition" + 0.006*"still" + 0.005*"music" + 0.005*"population"
topic #9 (0.050): 0.010*"design" + 0.010*"route" + 0.009*"road" + 0.008*"image" + 0.008*"town" + 0.007*"specie" + 0.007*"law" + 0.007*"physical" + 0.007*"system" + 0.006*"engine"
topic #16 (0.050): 0.018*"game" + 0.013*"house" + 0.011*"character" + 0.009*"attack" + 0.009*"battle" + 0.008*"company" + 0.007*"design" + 0.006*"advance" + 0.006*"building" + 0.006*"division"
topic diff=0.702825, rho=0.267644
-7.265 per-word bound, 153.9 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 796 documents into a model of 996 documents
topic #15 (0.050): 0.009*"stage" + 0.007*"tourist" + 0.007*"lake" + 0.006*"society" + 0.006*"festival" + 0.005*"tree" + 0.005*"system" + 0.005*"park" + 0.005*"condition" + 0.005*"garden"
topic #6 (0.050): 0.025*"station" + 0.024*"railway" + 0.021*"meet" + 0.014*"district" + 0.014*"train" + 0.010*"run" + 0.008*"north" + 0.007*"company" + 0.007*"village" + 0.007*"road"
topic #18 (0.050): 0.031*"album" + 0.028*"band" + 0.027*"song" + 0.019*"music" + 0.010*"chart" + 0.009*"track" + 0.008*"hit" + 0.008*"tour" + 0.007*"label" + 0.006*"perform"
topic #10 (0.050): 0.035*"game" + 0.015*"baseball" + 0.012*"week" + 0.012*"specie" + 0.008*"network" + 0.007*"decision" + 0.005*"get" + 0.005*"port" + 0.005*"trade" + 0.005*"broadcast"
topic #9 (0.050): 0.011*"design" + 0.010*"route" + 0.009*"road" + 0.009*"specie" + 0.007*"image" + 0.007*"system" + 0.007*"town" + 0.007*"engine" + 0.006*"physical" + 0.006*"law"
topic diff=0.602786, rho=0.258544
-7.200 per-word bound, 147.0 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 200 documents into a model of 996 documents
topic #16 (0.050): 0.017*"house" + 0.015*"game" + 0.010*"design" + 0.010*"character" + 0.009*"battle" + 0.009*"attack" + 0.008*"building" + 0.007*"company" + 0.007*"theatre" + 0.007*"island"
topic #7 (0.050): 0.021*"film" + 0.012*"series" + 0.010*"direct" + 0.008*"science" + 0.008*"attack" + 0.008*"race" + 0.007*"book" + 0.007*"channel" + 0.006*"technology" + 0.006*"actor"
topic #4 (0.050): 0.021*"room" + 0.020*"air" + 0.016*"episode" + 0.012*"person" + 0.011*"series" + 0.010*"surname" + 0.008*"model" + 0.007*"category" + 0.007*"specie" + 0.007*"game"
topic #18 (0.050): 0.035*"album" + 0.028*"song" + 0.024*"band" + 0.019*"music" + 0.011*"track" + 0.009*"chart" + 0.008*"hit" + 0.007*"tour" + 0.007*"love" + 0.006*"radio"
topic #13 (0.050): 0.044*"bridge" + 0.019*"reach" + 0.018*"final" + 0.017*"lose" + 0.009*"wall" + 0.009*"episode" + 0.008*"get" + 0.008*"title" + 0.007*"art" + 0.007*"round"
topic diff=0.722106, rho=0.258544
-7.287 per-word bound, 156.1 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #10 (0.050): 0.021*"game" + 0.012*"specie" + 0.009*"baseball" + 0.008*"port" + 0.007*"increase" + 0.007*"week" + 0.006*"low" + 0.006*"accord" + 0.006*"sex" + 0.006*"network"
topic #16 (0.050): 0.016*"house" + 0.015*"game" + 0.010*"character" + 0.010*"battle" + 0.010*"attack" + 0.009*"design" + 0.008*"company" + 0.008*"building" + 0.007*"theatre" + 0.007*"island"
topic #13 (0.050): 0.048*"bridge" + 0.017*"final" + 0.016*"reach" + 0.016*"lose" + 0.009*"episode" + 0.009*"wall" + 0.008*"get" + 0.008*"title" + 0.008*"decision" + 0.007*"art"
topic #6 (0.050): 0.030*"station" + 0.025*"railway" + 0.019*"meet" + 0.014*"train" + 0.014*"district" + 0.009*"run" + 0.009*"park" + 0.008*"north" + 0.008*"road" + 0.007*"company"
topic #1 (0.050): 0.027*"building" + 0.023*"church" + 0.011*"house" + 0.009*"board" + 0.009*"office" + 0.008*"site" + 0.008*"government" + 0.007*"river" + 0.007*"century" + 0.007*"right"
topic diff=0.631027, rho=0.250313
-7.351 per-word bound, 163.3 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #3 (0.050): 0.019*"film" + 0.016*"squadron" + 0.008*"force" + 0.007*"british" + 0.006*"army" + 0.006*"military" + 0.006*"attack" + 0.005*"air" + 0.005*"role" + 0.005*"fly"
topic #2 (0.050): 0.013*"song" + 0.013*"album" + 0.013*"eagle" + 0.012*"australian" + 0.011*"game" + 0.011*"vote" + 0.010*"party" + 0.010*"event" + 0.010*"command" + 0.009*"government"
topic #16 (0.050): 0.020*"game" + 0.015*"house" + 0.012*"character" + 0.010*"design" + 0.009*"attack" + 0.009*"battle" + 0.007*"theatre" + 0.007*"company" + 0.007*"building" + 0.006*"story"
topic #18 (0.050): 0.035*"album" + 0.031*"song" + 0.026*"band" + 0.019*"music" + 0.011*"chart" + 0.010*"track" + 0.010*"hit" + 0.008*"tour" + 0.007*"love" + 0.006*"label"
topic #11 (0.050): 0.020*"student" + 0.019*"company" + 0.013*"son" + 0.010*"management" + 0.009*"film" + 0.009*"restaurant" + 0.009*"campus" + 0.008*"business" + 0.007*"program" + 0.007*"found"
topic diff=0.688619, rho=0.250313
-7.061 per-word bound, 133.5 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 796 documents into a model of 996 documents
topic #4 (0.050): 0.023*"episode" + 0.019*"room" + 0.018*"air" + 0.014*"series" + 0.011*"person" + 0.010*"specie" + 0.009*"surname" + 0.007*"game" + 0.007*"human" + 0.007*"queen"
topic #7 (0.050): 0.023*"film" + 0.014*"series" + 0.010*"direct" + 0.009*"technology" + 0.008*"learn" + 0.008*"science" + 0.008*"channel" + 0.008*"book" + 0.007*"study" + 0.007*"director"
topic #13 (0.050): 0.040*"bridge" + 0.029*"reach" + 0.026*"lose" + 0.025*"final" + 0.011*"title" + 0.010*"round" + 0.009*"get" + 0.008*"event" + 0.008*"art" + 0.008*"episode"
topic #6 (0.050): 0.035*"station" + 0.023*"railway" + 0.015*"district" + 0.014*"meet" + 0.014*"train" + 0.011*"park" + 0.010*"run" + 0.009*"north" + 0.008*"road" + 0.007*"south"
topic #16 (0.050): 0.021*"game" + 0.016*"house" + 0.012*"character" + 0.011*"design" + 0.009*"attack" + 0.008*"battle" + 0.007*"theatre" + 0.007*"building" + 0.007*"story" + 0.006*"company"
topic diff=0.607618, rho=0.242821
-7.022 per-word bound, 130.0 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 200 documents into a model of 996 documents
topic #14 (0.050): 0.014*"student" + 0.013*"event" + 0.012*"child" + 0.012*"research" + 0.011*"population" + 0.011*"study" + 0.009*"theory" + 0.009*"law" + 0.009*"physical" + 0.007*"cause"
topic #18 (0.050): 0.039*"album" + 0.033*"song" + 0.031*"band" + 0.022*"music" + 0.011*"track" + 0.011*"tour" + 0.011*"chart" + 0.009*"hit" + 0.008*"perform" + 0.007*"label"
topic #5 (0.050): 0.023*"ship" + 0.016*"story" + 0.011*"secretary" + 0.011*"australian" + 0.009*"elect" + 0.009*"political" + 0.008*"appoint" + 0.008*"european" + 0.008*"foreign" + 0.007*"minister"
topic #0 (0.050): 0.047*"district" + 0.014*"council" + 0.013*"space" + 0.013*"municipality" + 0.012*"population" + 0.012*"water" + 0.012*"government" + 0.011*"house" + 0.011*"fight" + 0.010*"title"
topic #16 (0.050): 0.021*"game" + 0.014*"character" + 0.014*"attack" + 0.012*"house" + 0.012*"battle" + 0.011*"company" + 0.008*"division" + 0.008*"design" + 0.007*"german" + 0.007*"advance"
topic diff=0.702721, rho=0.242821
-7.126 per-word bound, 139.7 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #8 (0.050): 0.022*"star" + 0.017*"art" + 0.013*"type" + 0.010*"museum" + 0.010*"female" + 0.010*"sequence" + 0.009*"black" + 0.008*"length" + 0.008*"male" + 0.008*"specie"
topic #3 (0.050): 0.019*"squadron" + 0.018*"film" + 0.007*"force" + 0.007*"role" + 0.007*"army" + 0.007*"military" + 0.006*"british" + 0.006*"attack" + 0.006*"war" + 0.006*"air"
topic #10 (0.050): 0.024*"game" + 0.013*"specie" + 0.011*"baseball" + 0.008*"week" + 0.008*"network" + 0.006*"increase" + 0.006*"low" + 0.006*"decision" + 0.006*"port" + 0.005*"risk"
topic #0 (0.050): 0.047*"district" + 0.016*"population" + 0.015*"municipality" + 0.014*"space" + 0.013*"council" + 0.012*"government" + 0.011*"village" + 0.011*"town" + 0.011*"province" + 0.011*"water"
topic #11 (0.050): 0.020*"company" + 0.020*"student" + 0.014*"son" + 0.012*"management" + 0.010*"restaurant" + 0.009*"film" + 0.009*"business" + 0.009*"campus" + 0.008*"brother" + 0.008*"program"
topic diff=0.576165, rho=0.235965
-7.183 per-word bound, 145.3 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #9 (0.050): 0.014*"design" + 0.011*"route" + 0.010*"town" + 0.010*"specie" + 0.010*"road" + 0.010*"system" + 0.009*"engine" + 0.008*"water" + 0.007*"image" + 0.007*"cell"
topic #2 (0.050): 0.018*"eagle" + 0.015*"vote" + 0.015*"command" + 0.014*"party" + 0.013*"government" + 0.011*"australian" + 0.010*"order" + 0.010*"event" + 0.010*"general" + 0.010*"major"
topic #17 (0.050): 0.022*"event" + 0.019*"center" + 0.018*"winner" + 0.011*"film" + 0.011*"woman" + 0.010*"book" + 0.010*"olympic" + 0.009*"round" + 0.008*"match" + 0.007*"race"
topic #1 (0.050): 0.029*"building" + 0.027*"church" + 0.014*"house" + 0.011*"office" + 0.011*"site" + 0.009*"board" + 0.008*"river" + 0.008*"government" + 0.007*"century" + 0.006*"provide"
topic #18 (0.050): 0.040*"album" + 0.034*"song" + 0.033*"band" + 0.022*"music" + 0.012*"chart" + 0.011*"track" + 0.011*"tour" + 0.009*"hit" + 0.008*"perform" + 0.008*"label"
topic diff=0.610866, rho=0.235965
-6.972 per-word bound, 125.5 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #19 (0.050): 0.021*"fire" + 0.012*"water" + 0.010*"exhibition" + 0.010*"collection" + 0.009*"artist" + 0.007*"th_century" + 0.007*"tour" + 0.007*"portrait" + 0.006*"art" + 0.006*"still"
topic #18 (0.050): 0.041*"album" + 0.034*"band" + 0.034*"song" + 0.024*"music" + 0.012*"track" + 0.011*"tour" + 0.011*"chart" + 0.009*"hit" + 0.008*"perform" + 0.007*"label"
topic #3 (0.050): 0.022*"squadron" + 0.019*"film" + 0.008*"force" + 0.007*"role" + 0.007*"british" + 0.007*"army" + 0.006*"fly" + 0.006*"attack" + 0.006*"air" + 0.006*"military"
topic #16 (0.050): 0.022*"game" + 0.015*"character" + 0.012*"attack" + 0.012*"battle" + 0.012*"house" + 0.010*"company" + 0.008*"design" + 0.008*"division" + 0.007*"island" + 0.007*"german"
topic #11 (0.050): 0.021*"student" + 0.021*"company" + 0.014*"son" + 0.012*"management" + 0.010*"brother" + 0.009*"business" + 0.009*"film" + 0.009*"campus" + 0.008*"restaurant" + 0.008*"program"
topic diff=0.518600, rho=0.229658
-7.041 per-word bound, 131.7 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #6 (0.050): 0.039*"station" + 0.027*"railway" + 0.021*"meet" + 0.016*"district" + 0.016*"train" + 0.013*"run" + 0.010*"park" + 0.010*"north" + 0.009*"road" + 0.009*"company"
topic #10 (0.050): 0.038*"game" + 0.016*"baseball" + 0.015*"week" + 0.011*"specie" + 0.011*"network" + 0.006*"get" + 0.006*"port" + 0.006*"trade" + 0.006*"decision" + 0.006*"often"
topic #15 (0.050): 0.009*"tourist" + 0.009*"stage" + 0.007*"tree" + 0.007*"lake" + 0.006*"park" + 0.006*"event" + 0.006*"company" + 0.006*"engineer" + 0.006*"different" + 0.006*"develop"
topic #1 (0.050): 0.031*"building" + 0.027*"church" + 0.016*"house" + 0.011*"site" + 0.011*"office" + 0.010*"board" + 0.009*"government" + 0.008*"river" + 0.007*"century" + 0.007*"tower"
topic #19 (0.050): 0.020*"fire" + 0.012*"water" + 0.012*"exhibition" + 0.009*"collection" + 0.009*"artist" + 0.007*"th_century" + 0.007*"painting" + 0.006*"portrait" + 0.006*"still" + 0.006*"tour"
topic diff=0.525063, rho=0.229658
-6.859 per-word bound, 116.1 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #1 (0.050): 0.031*"building" + 0.028*"church" + 0.015*"house" + 0.011*"board" + 0.011*"office" + 0.011*"site" + 0.009*"government" + 0.008*"river" + 0.007*"right" + 0.007*"century"
topic #14 (0.050): 0.016*"child" + 0.014*"study" + 0.012*"research" + 0.010*"population" + 0.010*"student" + 0.009*"theory" + 0.009*"event" + 0.008*"effect" + 0.007*"cause" + 0.007*"law"
topic #17 (0.050): 0.024*"event" + 0.023*"winner" + 0.020*"center" + 0.012*"woman" + 0.012*"book" + 0.011*"race" + 0.010*"olympic" + 0.009*"film" + 0.009*"round" + 0.008*"history"
topic #0 (0.050): 0.052*"district" + 0.024*"population" + 0.019*"village" + 0.017*"council" + 0.016*"town" + 0.015*"space" + 0.015*"municipality" + 0.013*"government" + 0.013*"fight" + 0.012*"title"
topic #18 (0.050): 0.039*"album" + 0.036*"song" + 0.031*"band" + 0.023*"music" + 0.011*"chart" + 0.011*"tour" + 0.011*"hit" + 0.011*"track" + 0.008*"love" + 0.008*"perform"
topic diff=0.453797, rho=0.223831
-6.919 per-word bound, 121.0 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #1 (0.050): 0.031*"building" + 0.028*"church" + 0.017*"house" + 0.012*"site" + 0.012*"office" + 0.010*"board" + 0.009*"government" + 0.009*"river" + 0.007*"tower" + 0.007*"century"
topic #2 (0.050): 0.017*"eagle" + 0.016*"party" + 0.015*"event" + 0.014*"game" + 0.014*"command" + 0.013*"australian" + 0.013*"vote" + 0.012*"government" + 0.012*"medal" + 0.012*"compete"
topic #19 (0.050): 0.021*"fire" + 0.013*"water" + 0.012*"exhibition" + 0.009*"collection" + 0.009*"artist" + 0.008*"th_century" + 0.007*"painting" + 0.006*"still" + 0.006*"art" + 0.006*"village"
topic #13 (0.050): 0.046*"bridge" + 0.033*"reach" + 0.031*"lose" + 0.029*"final" + 0.013*"round" + 0.012*"title" + 0.011*"event" + 0.011*"art" + 0.009*"book" + 0.007*"defeat"
topic #16 (0.050): 0.025*"game" + 0.016*"character" + 0.014*"house" + 0.012*"battle" + 0.011*"attack" + 0.009*"company" + 0.009*"theatre" + 0.008*"design" + 0.008*"island" + 0.007*"story"
topic diff=0.467848, rho=0.223831
-6.798 per-word bound, 111.3 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=1970, num_topics=20, decay=0.5, chunksize=100) in 4.77s', 'datetime': '2022-02-06T04:13:02.400416', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 0 minutes
Note: Log likelihood is per-word ELBO
Note: Perplexity estimate based on a held-out corpus of 4 documents
