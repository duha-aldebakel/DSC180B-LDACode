using symmetric alpha at 0.05
using symmetric eta at 0.05
using serial LDA version on this node
running online LDA training, 20 topics, 4 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.050): 0.009*"page" + 0.007*"lee" + 0.004*"dickinson" + 0.003*"born" + 0.003*"two" + 0.003*"back" + 0.003*"cumbernauld" + 0.003*"inquisivi" + 0.003*"also" + 0.002*"team"
topic #12 (0.050): 0.006*"also" + 0.005*"england" + 0.005*"match" + 0.005*"first" + 0.004*"willey" + 0.003*"series" + 0.003*"schiffer" + 0.003*"systems" + 0.003*"took" + 0.003*"new"
topic #9 (0.050): 0.005*"district" + 0.004*"played" + 0.004*"rural" + 0.003*"born" + 0.003*"also" + 0.003*"bridge" + 0.003*"first" + 0.003*"norwegian" + 0.003*"cumbernauld" + 0.003*"courts"
topic #13 (0.050): 0.007*"page" + 0.007*"lee" + 0.005*"first" + 0.005*"acqui" + 0.004*"division" + 0.004*"two" + 0.004*"infantry" + 0.003*"also" + 0.003*"one" + 0.002*"may"
topic #18 (0.050): 0.004*"cumbernauld" + 0.004*"page" + 0.003*"film" + 0.003*"also" + 0.003*"first" + 0.003*"one" + 0.003*"women" + 0.003*"lee" + 0.002*"cadiz" + 0.002*"found"
topic diff=9.222829, rho=1.000000
-11.791 per-word bound, 3543.5 perplexity estimate based on a held-out corpus of 20 documents with 3097 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #5 (0.050): 0.011*"page" + 0.006*"balls" + 0.005*"cchu" + 0.005*"first" + 0.005*"rubber" + 0.005*"program" + 0.004*"ball" + 0.004*"works" + 0.004*"de" + 0.003*"hospital"
topic #11 (0.050): 0.006*"madsen" + 0.006*"court" + 0.006*"first" + 0.005*"season" + 0.005*"art" + 0.005*"state" + 0.005*"member" + 0.004*"also" + 0.004*"new" + 0.004*"mr_pammer"
topic #18 (0.050): 0.008*"women" + 0.006*"cambridge" + 0.004*"society" + 0.003*"louise_creighton" + 0.003*"darwin" + 0.003*"one" + 0.003*"first" + 0.003*"cumbernauld" + 0.003*"suffrage" + 0.003*"also"
topic #0 (0.050): 0.007*"page" + 0.006*"lee" + 0.004*"inquisivi" + 0.003*"dickinson" + 0.003*"born" + 0.002*"team" + 0.002*"two" + 0.002*"road" + 0.002*"professional" + 0.002*"november"
topic #17 (0.050): 0.009*"castle" + 0.005*"first" + 0.005*"page" + 0.005*"nuremberg" + 0.004*"however" + 0.004*"hohenstein" + 0.003*"also" + 0.003*"back" + 0.003*"city" + 0.003*"lee"
topic diff=1.419719, rho=0.377964
-10.317 per-word bound, 1275.9 perplexity estimate based on a held-out corpus of 20 documents with 3097 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #16 (0.050): 0.019*"dickinson" + 0.014*"wrestling" + 0.008*"debut" + 0.007*"made" + 0.006*"animator" + 0.006*"pro_wrestling" + 0.005*"evolve" + 0.005*"released" + 0.005*"herbert_west" + 0.005*"jersey"
topic #6 (0.050): 0.010*"film" + 0.005*"club" + 0.004*"air" + 0.004*"street_trash" + 0.003*"also" + 0.003*"one" + 0.003*"cumbernauld" + 0.003*"two" + 0.003*"division" + 0.003*"football"
topic #0 (0.050): 0.006*"page" + 0.005*"inquisivi" + 0.004*"lee" + 0.003*"road" + 0.003*"professional" + 0.003*"november" + 0.003*"born" + 0.003*"officially" + 0.003*"province" + 0.003*"capital"
topic #7 (0.050): 0.006*"infantry" + 0.005*"assault_pioneers" + 0.005*"tower" + 0.005*"pioneer" + 0.004*"one" + 0.004*"bazaine" + 0.004*"army" + 0.003*"assault" + 0.003*"page" + 0.003*"azs"
topic #13 (0.050): 0.019*"page" + 0.015*"lee" + 0.009*"division" + 0.008*"acqui" + 0.007*"infantry" + 0.005*"officers" + 0.004*"back" + 0.004*"two" + 0.004*"th" + 0.003*"first"
topic diff=1.221366, rho=0.353553
-9.620 per-word bound, 787.0 perplexity estimate based on a held-out corpus of 20 documents with 3097 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #17 (0.050): 0.011*"castle" + 0.006*"nuremberg" + 0.005*"hohenstein" + 0.005*"gujarati" + 0.004*"gujarat" + 0.004*"theatre" + 0.004*"first" + 0.004*"however" + 0.004*"graham" + 0.004*"several"
topic #5 (0.050): 0.012*"page" + 0.008*"balls" + 0.007*"rubber" + 0.006*"cchu" + 0.006*"ball" + 0.005*"program" + 0.005*"works" + 0.005*"first" + 0.004*"hospital" + 0.004*"de"
topic #6 (0.050): 0.012*"film" + 0.005*"air" + 0.005*"club" + 0.004*"street_trash" + 0.003*"also" + 0.003*"one" + 0.003*"american" + 0.003*"wing" + 0.003*"airfield" + 0.003*"football"
topic #10 (0.050): 0.004*"track" + 0.004*"russian" + 0.003*"cumbernauld" + 0.003*"page" + 0.002*"lee" + 0.002*"along" + 0.002*"team" + 0.002*"born" + 0.002*"world" + 0.002*"october"
topic #1 (0.050): 0.009*"mosque" + 0.008*"school" + 0.007*"south" + 0.006*"alberto" + 0.006*"mirando_city" + 0.005*"district" + 0.004*"mountain" + 0.004*"silvia" + 0.004*"time" + 0.003*"osman"
topic diff=1.031809, rho=0.333333
-9.192 per-word bound, 584.8 perplexity estimate based on a held-out corpus of 20 documents with 3097 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=6847, num_topics=20, decay=0.5, chunksize=20) in 0.82s', 'datetime': '2022-02-06T15:41:50.767004', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 0 minutes
Note: Perplexity estimate based on a held-out corpus of 4 documents


## With Lemmatization
using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 10 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.154*"area" + 0.137*"see" + 0.132*"back" + 0.127*"court" + 0.074*"build" + 0.070*"come" + 0.057*"royal" + 0.054*"set" + 0.050*"school" + 0.041*"role"
topic #1 (0.250): 0.357*"player" + 0.096*"long" + 0.075*"build" + 0.074*"court" + 0.073*"short" + 0.073*"come" + 0.052*"school" + 0.051*"original" + 0.030*"film" + 0.030*"international"
topic #2 (0.250): 0.303*"film" + 0.170*"school" + 0.080*"area" + 0.074*"international" + 0.057*"come" + 0.045*"short" + 0.045*"royal" + 0.045*"set" + 0.040*"build" + 0.038*"role"
topic #3 (0.250): 0.222*"back" + 0.161*"see" + 0.155*"film" + 0.111*"long" + 0.068*"area" + 0.065*"come" + 0.050*"original" + 0.032*"short" + 0.030*"set" + 0.023*"build"
topic diff=1.067418, rho=1.000000
-3.098 per-word bound, 8.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.169*"area" + 0.137*"court" + 0.131*"see" + 0.129*"back" + 0.077*"build" + 0.076*"come" + 0.061*"royal" + 0.050*"set" + 0.047*"school" + 0.040*"role"
topic #1 (0.250): 0.385*"player" + 0.096*"long" + 0.078*"short" + 0.077*"come" + 0.075*"build" + 0.060*"original" + 0.048*"court" + 0.038*"international" + 0.037*"school" + 0.031*"set"
topic #2 (0.250): 0.329*"film" + 0.181*"school" + 0.080*"international" + 0.067*"area" + 0.048*"come" + 0.047*"set" + 0.047*"short" + 0.040*"role" + 0.039*"royal" + 0.036*"build"
topic #3 (0.250): 0.244*"back" + 0.181*"see" + 0.129*"film" + 0.120*"long" + 0.062*"come" + 0.056*"area" + 0.055*"original" + 0.032*"short" + 0.031*"set" + 0.022*"build"
topic diff=0.200071, rho=0.377964
-3.044 per-word bound, 8.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.181*"area" + 0.143*"court" + 0.121*"back" + 0.120*"see" + 0.081*"come" + 0.081*"build" + 0.069*"royal" + 0.048*"set" + 0.048*"school" + 0.040*"role"
topic #1 (0.250): 0.399*"player" + 0.088*"long" + 0.084*"come" + 0.081*"short" + 0.074*"build" + 0.072*"original" + 0.048*"international" + 0.033*"court" + 0.033*"set" + 0.026*"school"
topic #2 (0.250): 0.352*"film" + 0.189*"school" + 0.084*"international" + 0.056*"area" + 0.049*"set" + 0.048*"short" + 0.043*"role" + 0.040*"come" + 0.032*"build" + 0.030*"royal"
topic #3 (0.250): 0.263*"back" + 0.205*"see" + 0.125*"long" + 0.108*"film" + 0.062*"come" + 0.049*"original" + 0.047*"area" + 0.033*"set" + 0.032*"short" + 0.021*"build"
topic diff=0.186907, rho=0.353553
-3.005 per-word bound, 8.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.192*"area" + 0.149*"court" + 0.112*"back" + 0.108*"see" + 0.085*"build" + 0.083*"come" + 0.075*"royal" + 0.049*"school" + 0.046*"set" + 0.039*"role"
topic #1 (0.250): 0.395*"player" + 0.089*"come" + 0.087*"long" + 0.082*"short" + 0.077*"original" + 0.071*"build" + 0.068*"international" + 0.035*"set" + 0.023*"court" + 0.020*"role"
topic #2 (0.250): 0.374*"film" + 0.194*"school" + 0.081*"international" + 0.050*"set" + 0.049*"short" + 0.047*"role" + 0.046*"area" + 0.033*"come" + 0.028*"build" + 0.027*"original"
topic #3 (0.250): 0.282*"back" + 0.233*"see" + 0.118*"long" + 0.083*"film" + 0.066*"come" + 0.044*"original" + 0.043*"area" + 0.035*"set" + 0.031*"short" + 0.020*"build"
topic diff=0.174436, rho=0.333333
-2.984 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.204*"area" + 0.155*"court" + 0.103*"back" + 0.094*"see" + 0.091*"build" + 0.085*"come" + 0.081*"royal" + 0.051*"school" + 0.042*"set" + 0.038*"role"
topic #1 (0.250): 0.384*"player" + 0.093*"come" + 0.092*"international" + 0.085*"long" + 0.081*"short" + 0.078*"original" + 0.068*"build" + 0.037*"set" + 0.021*"role" + 0.017*"court"
topic #2 (0.250): 0.395*"film" + 0.199*"school" + 0.074*"international" + 0.053*"set" + 0.050*"short" + 0.049*"role" + 0.038*"area" + 0.029*"original" + 0.025*"come" + 0.024*"build"
topic #3 (0.250): 0.294*"back" + 0.256*"see" + 0.109*"long" + 0.072*"come" + 0.061*"film" + 0.041*"original" + 0.040*"area" + 0.038*"set" + 0.030*"short" + 0.018*"build"
topic diff=0.162894, rho=0.316228
-2.975 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.214*"area" + 0.161*"court" + 0.096*"build" + 0.095*"back" + 0.087*"royal" + 0.083*"come" + 0.082*"see" + 0.053*"school" + 0.039*"set" + 0.038*"role"
topic #1 (0.250): 0.366*"player" + 0.121*"international" + 0.097*"come" + 0.082*"long" + 0.082*"short" + 0.079*"original" + 0.064*"build" + 0.038*"set" + 0.021*"role" + 0.013*"court"
topic #2 (0.250): 0.415*"film" + 0.203*"school" + 0.063*"international" + 0.055*"set" + 0.051*"role" + 0.051*"short" + 0.032*"area" + 0.031*"original" + 0.021*"build" + 0.020*"come"
topic #3 (0.250): 0.303*"back" + 0.274*"see" + 0.101*"long" + 0.079*"come" + 0.045*"film" + 0.041*"set" + 0.040*"area" + 0.037*"original" + 0.027*"short" + 0.017*"build"
topic diff=0.150138, rho=0.301511
-2.970 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.222*"area" + 0.166*"court" + 0.102*"build" + 0.092*"royal" + 0.087*"back" + 0.080*"come" + 0.071*"see" + 0.056*"school" + 0.038*"role" + 0.036*"set"
topic #1 (0.250): 0.348*"player" + 0.146*"international" + 0.098*"come" + 0.081*"short" + 0.080*"long" + 0.079*"original" + 0.060*"build" + 0.043*"set" + 0.021*"role" + 0.010*"court"
topic #2 (0.250): 0.434*"film" + 0.207*"school" + 0.055*"set" + 0.053*"role" + 0.052*"short" + 0.050*"international" + 0.032*"original" + 0.027*"area" + 0.018*"build" + 0.018*"long"
topic #3 (0.250): 0.310*"back" + 0.286*"see" + 0.093*"long" + 0.086*"come" + 0.044*"set" + 0.040*"area" + 0.034*"original" + 0.033*"film" + 0.024*"short" + 0.015*"build"
topic diff=0.138635, rho=0.288675
-2.969 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.229*"area" + 0.171*"court" + 0.106*"build" + 0.096*"royal" + 0.080*"back" + 0.077*"come" + 0.062*"see" + 0.059*"school" + 0.039*"role" + 0.033*"set"
topic #1 (0.250): 0.335*"player" + 0.161*"international" + 0.099*"come" + 0.081*"short" + 0.080*"long" + 0.079*"original" + 0.060*"build" + 0.045*"set" + 0.021*"role" + 0.008*"court"
topic #2 (0.250): 0.448*"film" + 0.210*"school" + 0.055*"set" + 0.055*"role" + 0.054*"short" + 0.040*"international" + 0.033*"original" + 0.023*"area" + 0.020*"long" + 0.016*"build"
topic #3 (0.250): 0.315*"back" + 0.295*"see" + 0.093*"come" + 0.085*"long" + 0.046*"set" + 0.042*"area" + 0.032*"original" + 0.025*"film" + 0.022*"short" + 0.013*"school"
topic diff=0.123112, rho=0.277350
-2.967 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.235*"area" + 0.175*"court" + 0.110*"build" + 0.099*"royal" + 0.075*"come" + 0.074*"back" + 0.061*"school" + 0.055*"see" + 0.039*"role" + 0.031*"set"
topic #1 (0.250): 0.326*"player" + 0.171*"international" + 0.098*"come" + 0.081*"long" + 0.081*"short" + 0.079*"original" + 0.061*"build" + 0.047*"set" + 0.020*"role" + 0.007*"court"
topic #2 (0.250): 0.459*"film" + 0.211*"school" + 0.056*"role" + 0.055*"set" + 0.054*"short" + 0.034*"original" + 0.033*"international" + 0.023*"long" + 0.019*"area" + 0.014*"build"
topic #3 (0.250): 0.319*"back" + 0.300*"see" + 0.099*"come" + 0.077*"long" + 0.048*"set" + 0.044*"area" + 0.031*"original" + 0.020*"short" + 0.019*"film" + 0.013*"school"
topic diff=0.111285, rho=0.267261
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.239*"area" + 0.178*"court" + 0.113*"build" + 0.102*"royal" + 0.072*"come" + 0.069*"back" + 0.063*"school" + 0.050*"see" + 0.040*"role" + 0.029*"set"
topic #1 (0.250): 0.320*"player" + 0.178*"international" + 0.094*"come" + 0.084*"long" + 0.081*"short" + 0.079*"original" + 0.062*"build" + 0.048*"set" + 0.020*"role" + 0.006*"court"
topic #2 (0.250): 0.468*"film" + 0.211*"school" + 0.057*"role" + 0.056*"set" + 0.055*"short" + 0.035*"original" + 0.027*"international" + 0.025*"long" + 0.016*"area" + 0.013*"see"
topic #3 (0.250): 0.321*"back" + 0.303*"see" + 0.107*"come" + 0.070*"long" + 0.050*"set" + 0.047*"area" + 0.029*"original" + 0.019*"short" + 0.015*"film" + 0.014*"school"
topic diff=0.101506, rho=0.258199
-2.962 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=4, decay=0.5, chunksize=20) in 0.24s', 'datetime': '2022-02-06T15:41:51.332180', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 0 minutes
Note: Log likelihood is per-word ELBO
Note: Perplexity estimate based on a held-out corpus of 4 documents


## Finding the right value of K
Starting K=1
using symmetric alpha at 1.0
using symmetric eta at 1.0
using serial LDA version on this node
running online LDA training, 1 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"see" + 0.086*"school" + 0.065*"come" + 0.065*"court" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.406030, rho=1.000000
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"see" + 0.086*"school" + 0.065*"come" + 0.065*"court" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.377964
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"see" + 0.086*"school" + 0.065*"come" + 0.065*"court" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.353553
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"school" + 0.086*"see" + 0.065*"court" + 0.065*"come" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.333333
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"see" + 0.086*"school" + 0.065*"come" + 0.065*"court" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.316228
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"see" + 0.086*"school" + 0.065*"come" + 0.065*"court" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.301511
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"school" + 0.086*"see" + 0.065*"come" + 0.065*"court" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.288675
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (1.000): 0.138*"film" + 0.099*"area" + 0.094*"back" + 0.086*"school" + 0.086*"see" + 0.065*"court" + 0.065*"come" + 0.055*"build" + 0.052*"player" + 0.044*"set"
topic diff=0.000000, rho=0.277350
-2.964 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=1, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:41:51.833619', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
K=1, Coherence Score: 0.5503595963830591
Starting K=2
using symmetric alpha at 0.5
using symmetric eta at 0.5
using serial LDA version on this node
running online LDA training, 2 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.141*"back" + 0.141*"area" + 0.127*"see" + 0.082*"come" + 0.068*"court" + 0.060*"build" + 0.059*"school" + 0.052*"set" + 0.052*"international" + 0.044*"royal"
topic #1 (0.500): 0.268*"film" + 0.121*"school" + 0.074*"player" + 0.060*"court" + 0.060*"long" + 0.048*"build" + 0.045*"short" + 0.044*"area" + 0.042*"come" + 0.038*"role"
topic diff=0.435885, rho=1.000000
-3.203 per-word bound, 9.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.146*"area" + 0.145*"back" + 0.129*"see" + 0.089*"come" + 0.077*"court" + 0.061*"build" + 0.054*"international" + 0.053*"school" + 0.052*"set" + 0.048*"royal"
topic #1 (0.500): 0.289*"film" + 0.131*"school" + 0.085*"player" + 0.065*"long" + 0.050*"short" + 0.048*"court" + 0.045*"build" + 0.040*"original" + 0.039*"role" + 0.034*"area"
topic diff=0.125831, rho=0.377964
-3.160 per-word bound, 8.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.149*"area" + 0.147*"back" + 0.130*"see" + 0.093*"come" + 0.085*"court" + 0.063*"build" + 0.055*"international" + 0.051*"royal" + 0.051*"set" + 0.049*"school"
topic #1 (0.500): 0.303*"film" + 0.137*"school" + 0.094*"player" + 0.068*"long" + 0.055*"short" + 0.044*"original" + 0.043*"build" + 0.041*"role" + 0.037*"court" + 0.034*"set"
topic diff=0.112017, rho=0.353553
-3.126 per-word bound, 8.7 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.152*"area" + 0.149*"back" + 0.131*"see" + 0.096*"come" + 0.091*"court" + 0.064*"build" + 0.056*"international" + 0.054*"royal" + 0.051*"set" + 0.047*"school"
topic #1 (0.500): 0.313*"film" + 0.142*"school" + 0.101*"player" + 0.071*"long" + 0.058*"short" + 0.047*"original" + 0.042*"role" + 0.041*"build" + 0.035*"set" + 0.028*"court"
topic diff=0.099104, rho=0.333333
-3.101 per-word bound, 8.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.154*"area" + 0.150*"back" + 0.131*"see" + 0.098*"come" + 0.095*"court" + 0.065*"build" + 0.056*"international" + 0.056*"royal" + 0.050*"set" + 0.045*"school"
topic #1 (0.500): 0.320*"film" + 0.145*"school" + 0.107*"player" + 0.073*"long" + 0.061*"short" + 0.050*"original" + 0.043*"role" + 0.040*"build" + 0.036*"set" + 0.026*"international"
topic diff=0.088262, rho=0.316228
-3.082 per-word bound, 8.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.156*"area" + 0.151*"back" + 0.132*"see" + 0.100*"come" + 0.098*"court" + 0.066*"build" + 0.058*"royal" + 0.057*"international" + 0.050*"set" + 0.044*"school"
topic #1 (0.500): 0.324*"film" + 0.147*"school" + 0.111*"player" + 0.074*"long" + 0.063*"short" + 0.052*"original" + 0.044*"role" + 0.038*"build" + 0.036*"set" + 0.025*"international"
topic diff=0.079325, rho=0.301511
-3.069 per-word bound, 8.4 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.157*"area" + 0.151*"back" + 0.133*"see" + 0.101*"come" + 0.100*"court" + 0.067*"build" + 0.059*"royal" + 0.057*"international" + 0.049*"set" + 0.043*"school"
topic #1 (0.500): 0.328*"film" + 0.149*"school" + 0.114*"player" + 0.075*"long" + 0.064*"short" + 0.054*"original" + 0.044*"role" + 0.037*"build" + 0.037*"set" + 0.025*"international"
topic diff=0.071781, rho=0.288675
-3.060 per-word bound, 8.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.500): 0.158*"area" + 0.152*"back" + 0.133*"see" + 0.102*"come" + 0.101*"court" + 0.067*"build" + 0.060*"royal" + 0.058*"international" + 0.049*"set" + 0.042*"school"
topic #1 (0.500): 0.330*"film" + 0.150*"school" + 0.117*"player" + 0.075*"long" + 0.065*"short" + 0.056*"original" + 0.045*"role" + 0.037*"set" + 0.036*"build" + 0.024*"international"
topic diff=0.065211, rho=0.277350
-3.053 per-word bound, 8.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=2, decay=0.5, chunksize=20) in 0.19s', 'datetime': '2022-02-06T15:41:52.128271', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
K=2, Coherence Score: 0.550359596383059
Starting K=3
using symmetric alpha at 0.3333333333333333
using symmetric eta at 0.3333333333333333
using serial LDA version on this node
running online LDA training, 3 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.186*"see" + 0.154*"area" + 0.135*"back" + 0.109*"court" + 0.086*"come" + 0.059*"build" + 0.048*"school" + 0.047*"set" + 0.031*"original" + 0.028*"royal"
topic #1 (0.333): 0.197*"film" + 0.173*"player" + 0.113*"long" + 0.083*"court" + 0.066*"build" + 0.057*"school" + 0.050*"role" + 0.039*"come" + 0.037*"short" + 0.035*"see"
topic #2 (0.333): 0.213*"film" + 0.136*"school" + 0.090*"area" + 0.090*"back" + 0.072*"international" + 0.064*"royal" + 0.059*"come" + 0.051*"set" + 0.044*"build" + 0.042*"short"
topic diff=0.692438, rho=1.000000
-3.248 per-word bound, 9.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.181*"see" + 0.160*"area" + 0.154*"back" + 0.121*"court" + 0.093*"come" + 0.057*"build" + 0.045*"set" + 0.043*"school" + 0.029*"original" + 0.023*"royal"
topic #1 (0.333): 0.208*"player" + 0.180*"film" + 0.130*"long" + 0.067*"build" + 0.060*"court" + 0.050*"role" + 0.045*"school" + 0.044*"short" + 0.039*"original" + 0.035*"come"
topic #2 (0.333): 0.242*"film" + 0.149*"school" + 0.079*"area" + 0.074*"international" + 0.071*"royal" + 0.067*"back" + 0.052*"set" + 0.052*"come" + 0.046*"build" + 0.044*"short"
topic diff=0.165443, rho=0.377964
-3.175 per-word bound, 9.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.179*"see" + 0.167*"back" + 0.165*"area" + 0.127*"court" + 0.097*"come" + 0.055*"build" + 0.043*"set" + 0.039*"school" + 0.025*"original" + 0.020*"role"
topic #1 (0.333): 0.234*"player" + 0.154*"film" + 0.143*"long" + 0.068*"build" + 0.053*"short" + 0.051*"original" + 0.049*"role" + 0.043*"court" + 0.040*"international" + 0.036*"school"
topic #2 (0.333): 0.267*"film" + 0.160*"school" + 0.075*"royal" + 0.075*"international" + 0.070*"area" + 0.054*"set" + 0.049*"back" + 0.047*"build" + 0.046*"come" + 0.044*"short"
topic diff=0.148743, rho=0.353553
-3.123 per-word bound, 8.7 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.177*"see" + 0.175*"back" + 0.169*"area" + 0.131*"court" + 0.099*"come" + 0.054*"build" + 0.042*"set" + 0.037*"school" + 0.023*"original" + 0.020*"role"
topic #1 (0.333): 0.255*"player" + 0.152*"long" + 0.126*"film" + 0.070*"build" + 0.062*"original" + 0.060*"short" + 0.048*"role" + 0.047*"international" + 0.033*"come" + 0.032*"court"
topic #2 (0.333): 0.288*"film" + 0.167*"school" + 0.077*"royal" + 0.074*"international" + 0.062*"area" + 0.055*"set" + 0.048*"build" + 0.044*"short" + 0.042*"come" + 0.042*"role"
topic diff=0.129639, rho=0.333333
-3.080 per-word bound, 8.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.180*"back" + 0.176*"see" + 0.173*"area" + 0.133*"court" + 0.100*"come" + 0.053*"build" + 0.041*"set" + 0.034*"school" + 0.021*"role" + 0.020*"original"
topic #1 (0.333): 0.269*"player" + 0.158*"long" + 0.100*"film" + 0.071*"build" + 0.071*"original" + 0.067*"short" + 0.055*"international" + 0.047*"role" + 0.036*"come" + 0.027*"set"
topic #2 (0.333): 0.306*"film" + 0.174*"school" + 0.080*"royal" + 0.071*"international" + 0.056*"set" + 0.055*"area" + 0.048*"build" + 0.043*"short" + 0.043*"role" + 0.038*"come"
topic diff=0.117558, rho=0.316228
-3.047 per-word bound, 8.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.184*"back" + 0.177*"area" + 0.175*"see" + 0.134*"court" + 0.100*"come" + 0.053*"build" + 0.041*"set" + 0.033*"school" + 0.021*"role" + 0.019*"original"
topic #1 (0.333): 0.274*"player" + 0.159*"long" + 0.077*"original" + 0.077*"film" + 0.072*"international" + 0.071*"build" + 0.071*"short" + 0.046*"role" + 0.043*"come" + 0.029*"set"
topic #2 (0.333): 0.323*"film" + 0.180*"school" + 0.082*"royal" + 0.065*"international" + 0.055*"set" + 0.049*"area" + 0.049*"build" + 0.044*"role" + 0.043*"short" + 0.033*"come"
topic diff=0.116449, rho=0.301511
-3.022 per-word bound, 8.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.188*"back" + 0.181*"area" + 0.174*"see" + 0.135*"court" + 0.100*"come" + 0.053*"build" + 0.040*"set" + 0.032*"school" + 0.021*"role" + 0.017*"original"
topic #1 (0.333): 0.272*"player" + 0.157*"long" + 0.092*"international" + 0.082*"original" + 0.073*"short" + 0.070*"build" + 0.059*"film" + 0.051*"come" + 0.045*"role" + 0.031*"set"
topic #2 (0.333): 0.338*"film" + 0.186*"school" + 0.085*"royal" + 0.056*"international" + 0.055*"set" + 0.049*"build" + 0.044*"role" + 0.044*"area" + 0.042*"short" + 0.030*"come"
topic diff=0.106722, rho=0.288675
-3.002 per-word bound, 8.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.190*"back" + 0.184*"area" + 0.174*"see" + 0.136*"court" + 0.099*"come" + 0.053*"build" + 0.040*"set" + 0.031*"school" + 0.021*"role" + 0.015*"original"
topic #1 (0.333): 0.269*"player" + 0.155*"long" + 0.113*"international" + 0.084*"original" + 0.073*"short" + 0.070*"build" + 0.057*"come" + 0.046*"film" + 0.040*"role" + 0.035*"set"
topic #2 (0.333): 0.351*"film" + 0.192*"school" + 0.087*"royal" + 0.054*"set" + 0.049*"build" + 0.047*"role" + 0.046*"international" + 0.043*"short" + 0.039*"area" + 0.027*"come"
topic diff=0.102160, rho=0.277350
-2.989 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=3, decay=0.5, chunksize=20) in 0.18s', 'datetime': '2022-02-06T15:41:52.414732', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
K=3, Coherence Score: 0.5503595963830591
Starting K=4
using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.154*"area" + 0.137*"see" + 0.132*"back" + 0.127*"court" + 0.074*"build" + 0.070*"come" + 0.057*"royal" + 0.054*"set" + 0.050*"school" + 0.041*"role"
topic #1 (0.250): 0.357*"player" + 0.096*"long" + 0.075*"build" + 0.074*"court" + 0.073*"short" + 0.073*"come" + 0.052*"school" + 0.051*"original" + 0.030*"film" + 0.030*"international"
topic #2 (0.250): 0.303*"film" + 0.170*"school" + 0.080*"area" + 0.074*"international" + 0.057*"come" + 0.045*"short" + 0.045*"royal" + 0.045*"set" + 0.040*"build" + 0.038*"role"
topic #3 (0.250): 0.222*"back" + 0.161*"see" + 0.155*"film" + 0.111*"long" + 0.068*"area" + 0.065*"come" + 0.050*"original" + 0.032*"short" + 0.030*"set" + 0.023*"build"
topic diff=1.067418, rho=1.000000
-3.098 per-word bound, 8.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.169*"area" + 0.137*"court" + 0.131*"see" + 0.129*"back" + 0.077*"build" + 0.076*"come" + 0.061*"royal" + 0.050*"set" + 0.047*"school" + 0.040*"role"
topic #1 (0.250): 0.385*"player" + 0.096*"long" + 0.078*"short" + 0.077*"come" + 0.075*"build" + 0.060*"original" + 0.048*"court" + 0.038*"international" + 0.037*"school" + 0.031*"set"
topic #2 (0.250): 0.329*"film" + 0.181*"school" + 0.080*"international" + 0.067*"area" + 0.048*"come" + 0.047*"set" + 0.047*"short" + 0.040*"role" + 0.039*"royal" + 0.036*"build"
topic #3 (0.250): 0.244*"back" + 0.181*"see" + 0.129*"film" + 0.120*"long" + 0.062*"come" + 0.056*"area" + 0.055*"original" + 0.032*"short" + 0.031*"set" + 0.022*"build"
topic diff=0.200071, rho=0.377964
-3.044 per-word bound, 8.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.181*"area" + 0.143*"court" + 0.121*"back" + 0.120*"see" + 0.081*"come" + 0.081*"build" + 0.069*"royal" + 0.048*"set" + 0.048*"school" + 0.038*"role"
topic #1 (0.250): 0.399*"player" + 0.088*"long" + 0.084*"come" + 0.081*"short" + 0.074*"build" + 0.072*"original" + 0.048*"international" + 0.033*"court" + 0.033*"set" + 0.026*"school"
topic #2 (0.250): 0.352*"film" + 0.188*"school" + 0.083*"international" + 0.056*"area" + 0.049*"set" + 0.048*"short" + 0.046*"role" + 0.040*"come" + 0.032*"build" + 0.030*"royal"
topic #3 (0.250): 0.263*"back" + 0.205*"see" + 0.125*"long" + 0.108*"film" + 0.062*"come" + 0.049*"original" + 0.047*"area" + 0.033*"set" + 0.032*"short" + 0.021*"build"
topic diff=0.188703, rho=0.353553
-3.005 per-word bound, 8.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.193*"area" + 0.149*"court" + 0.112*"back" + 0.108*"see" + 0.086*"build" + 0.084*"come" + 0.075*"royal" + 0.049*"school" + 0.046*"set" + 0.037*"role"
topic #1 (0.250): 0.401*"player" + 0.090*"come" + 0.083*"short" + 0.081*"long" + 0.078*"original" + 0.072*"build" + 0.062*"international" + 0.035*"set" + 0.024*"court" + 0.020*"role"
topic #2 (0.250): 0.372*"film" + 0.193*"school" + 0.083*"international" + 0.050*"set" + 0.049*"role" + 0.049*"short" + 0.046*"area" + 0.032*"come" + 0.028*"build" + 0.027*"original"
topic #3 (0.250): 0.281*"back" + 0.231*"see" + 0.122*"long" + 0.082*"film" + 0.065*"come" + 0.044*"original" + 0.042*"area" + 0.035*"set" + 0.031*"short" + 0.020*"build"
topic diff=0.170666, rho=0.333333
-2.979 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.204*"area" + 0.155*"court" + 0.103*"back" + 0.095*"see" + 0.091*"build" + 0.083*"come" + 0.082*"royal" + 0.051*"school" + 0.042*"set" + 0.037*"role"
topic #1 (0.250): 0.387*"player" + 0.099*"come" + 0.088*"international" + 0.081*"short" + 0.078*"original" + 0.076*"long" + 0.069*"build" + 0.038*"set" + 0.021*"role" + 0.017*"court"
topic #2 (0.250): 0.394*"film" + 0.198*"school" + 0.076*"international" + 0.053*"set" + 0.051*"role" + 0.050*"short" + 0.038*"area" + 0.029*"original" + 0.025*"come" + 0.024*"build"
topic #3 (0.250): 0.292*"back" + 0.254*"see" + 0.115*"long" + 0.071*"come" + 0.061*"film" + 0.041*"original" + 0.040*"area" + 0.038*"set" + 0.030*"short" + 0.018*"build"
topic diff=0.165278, rho=0.316228
-2.966 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.214*"area" + 0.161*"court" + 0.096*"build" + 0.095*"back" + 0.087*"royal" + 0.082*"see" + 0.082*"come" + 0.054*"school" + 0.039*"set" + 0.037*"role"
topic #1 (0.250): 0.371*"player" + 0.119*"international" + 0.103*"come" + 0.080*"short" + 0.079*"original" + 0.071*"long" + 0.065*"build" + 0.038*"set" + 0.022*"role" + 0.013*"court"
topic #2 (0.250): 0.415*"film" + 0.202*"school" + 0.064*"international" + 0.055*"set" + 0.052*"role" + 0.051*"short" + 0.032*"area" + 0.031*"original" + 0.021*"build" + 0.020*"come"
topic #3 (0.250): 0.299*"back" + 0.271*"see" + 0.108*"long" + 0.078*"come" + 0.044*"film" + 0.041*"set" + 0.039*"area" + 0.038*"original" + 0.028*"short" + 0.017*"build"
topic diff=0.149026, rho=0.301511
-2.959 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.222*"area" + 0.166*"court" + 0.102*"build" + 0.092*"royal" + 0.087*"back" + 0.079*"come" + 0.071*"see" + 0.056*"school" + 0.038*"role" + 0.036*"set"
topic #1 (0.250): 0.353*"player" + 0.146*"international" + 0.102*"come" + 0.080*"short" + 0.078*"original" + 0.071*"long" + 0.061*"build" + 0.043*"set" + 0.021*"role" + 0.010*"court"
topic #2 (0.250): 0.434*"film" + 0.207*"school" + 0.055*"set" + 0.054*"role" + 0.052*"short" + 0.051*"international" + 0.032*"original" + 0.027*"area" + 0.018*"build" + 0.018*"long"
topic #3 (0.250): 0.307*"back" + 0.284*"see" + 0.100*"long" + 0.085*"come" + 0.043*"set" + 0.039*"area" + 0.035*"original" + 0.033*"film" + 0.025*"short" + 0.014*"build"
topic diff=0.138334, rho=0.288675
-2.961 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.229*"area" + 0.170*"court" + 0.107*"build" + 0.096*"royal" + 0.080*"back" + 0.077*"come" + 0.063*"see" + 0.059*"school" + 0.038*"role" + 0.034*"set"
topic #1 (0.250): 0.340*"player" + 0.162*"international" + 0.102*"come" + 0.080*"short" + 0.079*"original" + 0.072*"long" + 0.059*"build" + 0.046*"set" + 0.021*"role" + 0.008*"court"
topic #2 (0.250): 0.448*"film" + 0.209*"school" + 0.055*"role" + 0.055*"set" + 0.054*"short" + 0.041*"international" + 0.033*"original" + 0.023*"area" + 0.020*"long" + 0.016*"build"
topic #3 (0.250): 0.313*"back" + 0.292*"see" + 0.092*"long" + 0.092*"come" + 0.046*"set" + 0.040*"area" + 0.033*"original" + 0.025*"film" + 0.023*"short" + 0.012*"school"
topic diff=0.123266, rho=0.277350
-2.961 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=4, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:41:52.702925', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
K=4, Coherence Score: 0.550359596383059
ALL DONE!
