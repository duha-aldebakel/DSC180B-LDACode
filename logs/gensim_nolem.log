using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 1 passes over the supplied corpus of 100 documents, updating every 10 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 2 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #5/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #10/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #15/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #20/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #25/100, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #30/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #35/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.017*"page" + 0.007*"inquisivi" + 0.007*"azs" + 0.006*"works" + 0.006*"cadiz" + 0.006*"edwards" + 0.006*"la" + 0.006*"de" + 0.006*"polish" + 0.005*"named"
topic #1 (0.250): 0.011*"court" + 0.007*"mr_pammer" + 0.006*"rome" + 0.006*"regulation_ec" + 0.006*"consumer" + 0.006*"article" + 0.006*"hotel" + 0.006*"page" + 0.006*"contract" + 0.005*"mr_heller"
topic #2 (0.250): 0.015*"page" + 0.009*"works" + 0.007*"television" + 0.006*"court" + 0.006*"edwards" + 0.005*"program" + 0.005*"royal" + 0.005*"first" + 0.005*"series" + 0.004*"many"
topic #3 (0.250): 0.010*"court" + 0.008*"page" + 0.006*"article" + 0.006*"regulation_ec" + 0.006*"de" + 0.006*"consumer" + 0.006*"rome" + 0.006*"mr_pammer" + 0.005*"first" + 0.005*"cadiz"
topic diff=3.297277, rho=1.000000
PROGRESS: pass 0, dispatched chunk #7 = documents up to #40/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #8 = documents up to #45/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #9 = documents up to #50/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.034*"page" + 0.024*"lee" + 0.006*"back" + 0.004*"two" + 0.004*"shaw" + 0.004*"police" + 0.004*"area" + 0.004*"found" + 0.004*"body" + 0.003*"around"
topic #1 (0.250): 0.019*"page" + 0.010*"lee" + 0.005*"court" + 0.003*"back" + 0.003*"shaw" + 0.003*"also" + 0.003*"first" + 0.003*"two" + 0.003*"found" + 0.003*"body"
topic #2 (0.250): 0.022*"page" + 0.014*"lee" + 0.004*"art" + 0.004*"back" + 0.003*"court" + 0.003*"two" + 0.003*"works" + 0.003*"first" + 0.003*"time" + 0.003*"also"
topic #3 (0.250): 0.014*"page" + 0.009*"lee" + 0.006*"city_hall" + 0.005*"court" + 0.005*"museumsquartier" + 0.004*"phoenix" + 0.003*"art" + 0.003*"also" + 0.003*"first" + 0.003*"construction"
topic diff=0.957441, rho=0.577350
PROGRESS: pass 0, dispatched chunk #10 = documents up to #55/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.026*"page" + 0.018*"lee" + 0.005*"back" + 0.004*"club" + 0.003*"two" + 0.003*"found" + 0.003*"shaw" + 0.003*"police" + 0.003*"balls" + 0.003*"area"
topic #1 (0.250): 0.013*"page" + 0.007*"lee" + 0.004*"court" + 0.003*"back" + 0.003*"air" + 0.003*"found" + 0.002*"first" + 0.002*"balls" + 0.002*"two" + 0.002*"rubber"
topic #2 (0.250): 0.014*"page" + 0.009*"lee" + 0.006*"art" + 0.005*"balls" + 0.004*"rubber" + 0.003*"ball" + 0.003*"works" + 0.003*"two" + 0.003*"gere" + 0.003*"march"
topic #3 (0.250): 0.006*"km" + 0.006*"page" + 0.006*"ft" + 0.005*"two" + 0.004*"balls" + 0.004*"boat" + 0.004*"lee" + 0.004*"one" + 0.003*"first" + 0.003*"tower"
topic diff=0.600438, rho=0.447214
PROGRESS: pass 0, dispatched chunk #11 = documents up to #60/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #12 = documents up to #65/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.021*"page" + 0.015*"lee" + 0.004*"club" + 0.004*"back" + 0.003*"province" + 0.003*"two" + 0.003*"based" + 0.003*"county" + 0.003*"found" + 0.003*"played"
topic #1 (0.250): 0.009*"page" + 0.005*"court" + 0.005*"lee" + 0.005*"schiffer" + 0.004*"found" + 0.004*"body" + 0.003*"year_old" + 0.003*"also" + 0.003*"murder" + 0.003*"district"
topic #2 (0.250): 0.010*"page" + 0.007*"lee" + 0.004*"time" + 0.004*"art" + 0.004*"balls" + 0.004*"two" + 0.003*"march" + 0.003*"rubber" + 0.003*"ball" + 0.003*"would"
topic #3 (0.250): 0.007*"castle" + 0.004*"maria" + 0.004*"two" + 0.004*"animator" + 0.004*"nuremberg" + 0.003*"km" + 0.003*"first" + 0.003*"page" + 0.003*"ft" + 0.003*"film"
topic diff=0.504325, rho=0.377964
PROGRESS: pass 0, dispatched chunk #13 = documents up to #70/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #14 = documents up to #75/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #15 = documents up to #80/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.022*"match" + 0.012*"england" + 0.012*"page" + 0.011*"willey" + 0.010*"took" + 0.009*"first" + 0.009*"lee" + 0.007*"season" + 0.007*"played" + 0.006*"final"
topic #1 (0.250): 0.007*"page" + 0.004*"court" + 0.004*"end" + 0.003*"district" + 0.003*"lee" + 0.003*"schiffer" + 0.003*"war" + 0.003*"since" + 0.003*"found" + 0.003*"recognition"
topic #2 (0.250): 0.016*"series" + 0.016*"england" + 0.010*"willey" + 0.007*"first" + 0.007*"women" + 0.006*"page" + 0.005*"runs" + 0.005*"took" + 0.004*"odi" + 0.004*"wicket"
topic #3 (0.250): 0.005*"castle" + 0.004*"first" + 0.004*"cambridge" + 0.004*"district" + 0.004*"women" + 0.003*"two" + 0.003*"one" + 0.003*"married" + 0.003*"ft" + 0.003*"society"
topic diff=0.510723, rho=0.333333
PROGRESS: pass 0, dispatched chunk #16 = documents up to #85/100, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #17 = documents up to #90/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.018*"match" + 0.010*"england" + 0.010*"page" + 0.009*"willey" + 0.008*"took" + 0.007*"first" + 0.007*"lee" + 0.007*"played" + 0.006*"season" + 0.005*"final"
topic #1 (0.250): 0.007*"film" + 0.005*"blinatumomab" + 0.004*"cell" + 0.004*"street_trash" + 0.003*"also" + 0.003*"drug" + 0.003*"time" + 0.003*"page" + 0.003*"body" + 0.003*"court"
topic #2 (0.250): 0.013*"series" + 0.013*"england" + 0.008*"willey" + 0.007*"first" + 0.006*"women" + 0.005*"page" + 0.004*"runs" + 0.004*"took" + 0.004*"odi" + 0.004*"two"
topic #3 (0.250): 0.004*"castle" + 0.004*"first" + 0.004*"cambridge" + 0.004*"district" + 0.004*"women" + 0.003*"two" + 0.003*"one" + 0.002*"married" + 0.002*"ft" + 0.002*"society"
topic diff=0.480764, rho=0.301511
PROGRESS: pass 0, dispatched chunk #18 = documents up to #95/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.013*"match" + 0.008*"england" + 0.007*"page" + 0.007*"willey" + 0.007*"took" + 0.006*"played" + 0.006*"first" + 0.005*"lee" + 0.004*"season" + 0.004*"club"
topic #1 (0.250): 0.017*"cumbernauld" + 0.009*"town" + 0.006*"also" + 0.006*"new" + 0.004*"infantry" + 0.004*"acqui" + 0.003*"film" + 0.003*"village" + 0.003*"two" + 0.002*"north"
topic #2 (0.250): 0.010*"series" + 0.009*"england" + 0.005*"first" + 0.005*"willey" + 0.005*"cumbernauld" + 0.004*"two" + 0.004*"one" + 0.004*"also" + 0.004*"women" + 0.003*"took"
topic #3 (0.250): 0.012*"division" + 0.008*"castle" + 0.006*"cumbernauld" + 0.006*"scotland" + 0.004*"one" + 0.004*"two" + 0.003*"acqui" + 0.003*"first" + 0.003*"infantry" + 0.003*"district"
topic diff=0.961937, rho=0.277350
PROGRESS: pass 0, dispatched chunk #19 = documents up to #100/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.011*"match" + 0.007*"england" + 0.006*"page" + 0.006*"played" + 0.006*"willey" + 0.006*"took" + 0.006*"first" + 0.004*"lee" + 0.004*"season" + 0.004*"named"
topic #1 (0.250): 0.013*"cumbernauld" + 0.006*"town" + 0.006*"also" + 0.005*"new" + 0.004*"infantry" + 0.003*"village" + 0.003*"two" + 0.003*"acqui" + 0.003*"film" + 0.002*"mosque"
topic #2 (0.250): 0.007*"series" + 0.007*"first" + 0.006*"england" + 0.006*"art" + 0.004*"university" + 0.004*"also" + 0.004*"one" + 0.004*"madsen" + 0.004*"willey" + 0.003*"two"
topic #3 (0.250): 0.009*"division" + 0.007*"castle" + 0.004*"cumbernauld" + 0.004*"scotland" + 0.004*"one" + 0.004*"two" + 0.003*"cylinder" + 0.003*"first" + 0.003*"site" + 0.003*"development"
topic diff=0.523156, rho=0.258199
-10.772 per-word bound, 1748.5 perplexity estimate based on a held-out corpus of 5 documents with 1680 words
merging changes from 15 documents into a model of 100 documents
topic #0 (0.250): 0.010*"match" + 0.006*"dickinson" + 0.006*"played" + 0.006*"team" + 0.005*"england" + 0.005*"page" + 0.005*"season" + 0.005*"debut" + 0.004*"took" + 0.004*"first"
topic #1 (0.250): 0.012*"cumbernauld" + 0.006*"town" + 0.006*"also" + 0.005*"new" + 0.003*"infantry" + 0.003*"village" + 0.003*"two" + 0.003*"acqui" + 0.002*"film" + 0.002*"mosque"
topic #2 (0.250): 0.006*"series" + 0.006*"first" + 0.005*"england" + 0.005*"art" + 0.004*"university" + 0.004*"two" + 0.004*"also" + 0.004*"one" + 0.003*"married" + 0.003*"madsen"
topic #3 (0.250): 0.008*"division" + 0.006*"castle" + 0.004*"cumbernauld" + 0.004*"scotland" + 0.004*"one" + 0.003*"two" + 0.003*"cylinder" + 0.003*"first" + 0.003*"also" + 0.003*"site"
topic diff=0.339095, rho=0.242536
-10.745 per-word bound, 1716.3 perplexity estimate based on a held-out corpus of 5 documents with 1680 words
merging changes from 5 documents into a model of 100 documents
topic #0 (0.250): 0.007*"college" + 0.007*"samsung" + 0.007*"bridge" + 0.007*"first" + 0.006*"phone" + 0.005*"south" + 0.005*"match" + 0.005*"bada" + 0.004*"knox" + 0.004*"following"
topic #1 (0.250): 0.009*"cumbernauld" + 0.006*"new" + 0.006*"also" + 0.005*"town" + 0.004*"wave" + 0.003*"system" + 0.003*"published" + 0.003*"village" + 0.003*"two" + 0.003*"infantry"
topic #2 (0.250): 0.008*"bridge" + 0.007*"books" + 0.007*"first" + 0.005*"one" + 0.004*"series" + 0.004*"england" + 0.004*"play" + 0.004*"married" + 0.004*"two" + 0.003*"british"
topic #3 (0.250): 0.006*"division" + 0.005*"castle" + 0.004*"development" + 0.004*"one" + 0.004*"two" + 0.003*"city" + 0.003*"cumbernauld" + 0.003*"scotland" + 0.003*"first" + 0.003*"italian"
topic diff=0.615269, rho=0.223607
-7.752 per-word bound, 215.5 perplexity estimate based on a held-out corpus of 5 documents with 1680 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=6847, num_topics=4, decay=0.5, chunksize=5) in 0.39s', 'datetime': '2022-02-06T08:15:54.599349', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-3.10.0-1160.49.1.el7.x86_64-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 0 minutes
Note: Perplexity estimate based on a held-out corpus of 4 documents


## With Lemmatization
using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 1 passes over the supplied corpus of 100 documents, updating every 10 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 2 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #5/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #10/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #15/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #20/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #25/100, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #30/100, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #6 = documents up to #35/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.654*"court" + 0.192*"international" + 0.065*"build" + 0.065*"original" + 0.003*"player" + 0.003*"film" + 0.003*"come" + 0.002*"see" + 0.002*"short" + 0.002*"set"
topic #1 (0.250): 0.406*"come" + 0.405*"player" + 0.024*"film" + 0.021*"international" + 0.018*"court" + 0.016*"build" + 0.015*"original" + 0.011*"see" + 0.011*"short" + 0.011*"set"
topic #2 (0.250): 0.113*"international" + 0.101*"film" + 0.091*"come" + 0.089*"player" + 0.071*"court" + 0.065*"original" + 0.063*"build" + 0.045*"see" + 0.045*"short" + 0.045*"set"
topic #3 (0.250): 0.655*"film" + 0.041*"international" + 0.034*"player" + 0.034*"come" + 0.028*"court" + 0.027*"original" + 0.025*"build" + 0.017*"see" + 0.017*"short" + 0.017*"set"
topic diff=3.096141, rho=1.000000
PROGRESS: pass 0, dispatched chunk #7 = documents up to #40/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #8 = documents up to #45/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.476*"court" + 0.153*"build" + 0.124*"international" + 0.059*"see" + 0.059*"role" + 0.059*"area" + 0.043*"original" + 0.004*"royal" + 0.004*"come" + 0.004*"player"
topic #1 (0.250): 0.330*"come" + 0.326*"player" + 0.042*"build" + 0.030*"film" + 0.029*"royal" + 0.028*"international" + 0.027*"court" + 0.023*"original" + 0.021*"role" + 0.021*"back"
topic #2 (0.250): 0.512*"royal" + 0.109*"build" + 0.108*"come" + 0.106*"school" + 0.106*"back" + 0.007*"international" + 0.007*"film" + 0.006*"player" + 0.006*"court" + 0.005*"original"
topic #3 (0.250): 0.466*"film" + 0.054*"build" + 0.044*"international" + 0.042*"come" + 0.041*"royal" + 0.040*"player" + 0.038*"court" + 0.035*"original" + 0.032*"back" + 0.031*"area"
topic diff=1.491267, rho=0.577350
PROGRESS: pass 0, dispatched chunk #9 = documents up to #50/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #10 = documents up to #55/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.248*"see" + 0.186*"back" + 0.126*"area" + 0.125*"court" + 0.111*"come" + 0.046*"set" + 0.033*"build" + 0.033*"school" + 0.027*"international" + 0.017*"player"
topic #1 (0.250): 0.341*"long" + 0.174*"short" + 0.165*"school" + 0.096*"player" + 0.092*"come" + 0.018*"film" + 0.015*"build" + 0.013*"court" + 0.012*"international" + 0.012*"royal"
topic #2 (0.250): 0.427*"royal" + 0.119*"film" + 0.095*"school" + 0.093*"build" + 0.093*"come" + 0.091*"back" + 0.011*"player" + 0.010*"international" + 0.009*"court" + 0.008*"area"
topic #3 (0.250): 0.239*"see" + 0.234*"back" + 0.156*"area" + 0.133*"come" + 0.051*"court" + 0.043*"film" + 0.041*"set" + 0.035*"school" + 0.030*"international" + 0.015*"long"
topic diff=2.043001, rho=0.447214
PROGRESS: pass 0, dispatched chunk #11 = documents up to #60/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #12 = documents up to #65/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.236*"see" + 0.183*"back" + 0.129*"court" + 0.117*"come" + 0.110*"area" + 0.040*"set" + 0.030*"build" + 0.029*"original" + 0.029*"school" + 0.025*"international"
topic #1 (0.250): 0.315*"long" + 0.162*"short" + 0.154*"school" + 0.092*"player" + 0.089*"come" + 0.024*"film" + 0.019*"build" + 0.018*"international" + 0.017*"royal" + 0.017*"court"
topic #2 (0.250): 0.222*"royal" + 0.165*"film" + 0.124*"set" + 0.094*"build" + 0.066*"long" + 0.065*"short" + 0.065*"area" + 0.064*"see" + 0.037*"school" + 0.037*"come"
topic #3 (0.250): 0.220*"see" + 0.216*"back" + 0.144*"area" + 0.122*"come" + 0.062*"international" + 0.048*"long" + 0.048*"court" + 0.041*"film" + 0.039*"set" + 0.033*"school"
topic diff=0.744590, rho=0.377964
PROGRESS: pass 0, dispatched chunk #13 = documents up to #70/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #14 = documents up to #75/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #15 = documents up to #80/100, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #16 = documents up to #85/100, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #17 = documents up to #90/100, outstanding queue size 9
merging changes from 25 documents into a model of 100 documents
topic #0 (0.250): 0.205*"see" + 0.163*"back" + 0.149*"court" + 0.099*"area" + 0.096*"come" + 0.062*"international" + 0.044*"original" + 0.040*"role" + 0.034*"set" + 0.028*"build"
topic #1 (0.250): 0.302*"player" + 0.188*"come" + 0.150*"short" + 0.129*"long" + 0.065*"school" + 0.049*"set" + 0.020*"international" + 0.013*"original" + 0.013*"film" + 0.011*"role"
topic #2 (0.250): 0.240*"school" + 0.173*"royal" + 0.090*"film" + 0.071*"short" + 0.070*"area" + 0.068*"set" + 0.064*"build" + 0.063*"back" + 0.038*"role" + 0.037*"long"
topic #3 (0.250): 0.357*"film" + 0.124*"back" + 0.123*"see" + 0.087*"area" + 0.073*"come" + 0.041*"set" + 0.036*"long" + 0.034*"international" + 0.028*"original" + 0.027*"short"
topic diff=0.797132, rho=0.333333
PROGRESS: pass 0, dispatched chunk #18 = documents up to #95/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #19 = documents up to #100/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.250): 0.176*"see" + 0.147*"area" + 0.145*"back" + 0.123*"court" + 0.083*"come" + 0.067*"original" + 0.051*"international" + 0.048*"set" + 0.047*"school" + 0.040*"build"
topic #1 (0.250): 0.250*"player" + 0.200*"long" + 0.170*"come" + 0.168*"school" + 0.090*"short" + 0.031*"set" + 0.014*"international" + 0.010*"build" + 0.010*"original" + 0.009*"film"
topic #2 (0.250): 0.202*"build" + 0.173*"school" + 0.160*"area" + 0.116*"film" + 0.090*"set" + 0.085*"royal" + 0.051*"short" + 0.028*"long" + 0.026*"see" + 0.024*"come"
topic #3 (0.250): 0.283*"film" + 0.167*"area" + 0.108*"see" + 0.105*"back" + 0.071*"come" + 0.061*"international" + 0.044*"school" + 0.038*"set" + 0.032*"long" + 0.025*"court"
topic diff=0.393508, rho=0.267261
-3.036 per-word bound, 8.2 perplexity estimate based on a held-out corpus of 5 documents with 27 words
merging changes from 25 documents into a model of 100 documents
topic #0 (0.250): 0.168*"see" + 0.136*"court" + 0.134*"area" + 0.132*"back" + 0.082*"come" + 0.076*"original" + 0.051*"school" + 0.047*"international" + 0.044*"set" + 0.044*"build"
topic #1 (0.250): 0.392*"player" + 0.155*"long" + 0.139*"come" + 0.112*"school" + 0.082*"short" + 0.033*"international" + 0.021*"set" + 0.010*"original" + 0.009*"build" + 0.007*"film"
topic #2 (0.250): 0.187*"build" + 0.173*"school" + 0.148*"area" + 0.128*"film" + 0.093*"royal" + 0.084*"set" + 0.048*"short" + 0.033*"long" + 0.026*"role" + 0.025*"see"
topic #3 (0.250): 0.269*"film" + 0.148*"area" + 0.147*"back" + 0.099*"see" + 0.077*"international" + 0.059*"come" + 0.044*"long" + 0.040*"set" + 0.037*"school" + 0.022*"court"
topic diff=0.199740, rho=0.250000
-2.773 per-word bound, 6.8 perplexity estimate based on a held-out corpus of 5 documents with 27 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=4, decay=0.5, chunksize=5) in 0.09s', 'datetime': '2022-02-06T08:15:55.224744', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-3.10.0-1160.49.1.el7.x86_64-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 0 minutes
Note: Log likelihood is per-word ELBO
Note: Perplexity estimate based on a held-out corpus of 4 documents


## Finding the right value of K
Starting K=2
using symmetric alpha at 0.5
using symmetric eta at 0.5
using serial LDA version on this node
running online LDA training, 2 topics, 1 passes over the supplied corpus of 100 documents, updating every 10 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 2 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #5/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #10/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #15/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #20/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #25/100, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #30/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #35/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #7 = documents up to #40/100, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #8 = documents up to #45/100, outstanding queue size 8
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.271*"royal" + 0.151*"original" + 0.145*"build" + 0.095*"player" + 0.089*"school" + 0.086*"come" + 0.071*"film" + 0.010*"long" + 0.010*"see" + 0.010*"short"
topic #1 (0.500): 0.311*"royal" + 0.133*"film" + 0.120*"come" + 0.117*"school" + 0.111*"player" + 0.067*"build" + 0.062*"original" + 0.009*"long" + 0.009*"see" + 0.009*"short"
topic diff=1.946910, rho=1.000000
PROGRESS: pass 0, dispatched chunk #9 = documents up to #50/100, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #10 = documents up to #55/100, outstanding queue size 8
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.206*"build" + 0.202*"royal" + 0.072*"see" + 0.072*"area" + 0.070*"role" + 0.070*"court" + 0.068*"back" + 0.058*"school" + 0.046*"original" + 0.030*"player"
topic #1 (0.500): 0.183*"long" + 0.168*"royal" + 0.117*"build" + 0.112*"school" + 0.093*"short" + 0.058*"film" + 0.053*"come" + 0.050*"player" + 0.030*"original" + 0.027*"back"
topic diff=1.497898, rho=0.577350
PROGRESS: pass 0, dispatched chunk #11 = documents up to #60/100, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #12 = documents up to #65/100, outstanding queue size 8
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.317*"court" + 0.108*"royal" + 0.100*"international" + 0.077*"build" + 0.062*"see" + 0.060*"back" + 0.056*"come" + 0.052*"original" + 0.031*"film" + 0.028*"area"
topic #1 (0.500): 0.215*"court" + 0.120*"long" + 0.112*"film" + 0.094*"international" + 0.082*"royal" + 0.075*"come" + 0.052*"build" + 0.050*"school" + 0.045*"set" + 0.042*"short"
topic diff=1.017173, rho=0.447214
PROGRESS: pass 0, dispatched chunk #13 = documents up to #70/100, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #14 = documents up to #75/100, outstanding queue size 8
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.242*"court" + 0.121*"royal" + 0.105*"international" + 0.078*"see" + 0.074*"build" + 0.055*"come" + 0.049*"area" + 0.047*"back" + 0.041*"original" + 0.036*"set"
topic #1 (0.500): 0.184*"international" + 0.124*"come" + 0.100*"player" + 0.092*"court" + 0.076*"long" + 0.072*"royal" + 0.070*"short" + 0.069*"set" + 0.049*"film" + 0.045*"build"
topic diff=0.541281, rho=0.377964
PROGRESS: pass 0, dispatched chunk #15 = documents up to #80/100, outstanding queue size 8
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.197*"court" + 0.108*"see" + 0.099*"royal" + 0.087*"international" + 0.083*"area" + 0.067*"role" + 0.061*"build" + 0.049*"original" + 0.046*"come" + 0.040*"back"
topic #1 (0.500): 0.449*"film" + 0.088*"player" + 0.074*"international" + 0.062*"short" + 0.050*"come" + 0.045*"set" + 0.037*"court" + 0.034*"original" + 0.032*"role" + 0.031*"long"
topic diff=0.548134, rho=0.333333
PROGRESS: pass 0, dispatched chunk #16 = documents up to #85/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.223*"see" + 0.211*"back" + 0.132*"area" + 0.117*"come" + 0.068*"court" + 0.043*"set" + 0.038*"school" + 0.035*"royal" + 0.022*"international" + 0.021*"role"
topic #1 (0.500): 0.421*"film" + 0.090*"player" + 0.072*"international" + 0.057*"short" + 0.054*"come" + 0.043*"court" + 0.043*"set" + 0.034*"original" + 0.030*"see" + 0.030*"role"
topic diff=0.589752, rho=0.301511
PROGRESS: pass 0, dispatched chunk #17 = documents up to #90/100, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #18 = documents up to #95/100, outstanding queue size 7
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.204*"back" + 0.195*"see" + 0.138*"area" + 0.108*"come" + 0.071*"court" + 0.043*"set" + 0.040*"school" + 0.035*"role" + 0.031*"royal" + 0.030*"build"
topic #1 (0.500): 0.318*"film" + 0.176*"school" + 0.065*"short" + 0.055*"court" + 0.051*"long" + 0.045*"player" + 0.042*"original" + 0.042*"come" + 0.041*"international" + 0.039*"set"
topic diff=0.283365, rho=0.277350
PROGRESS: pass 0, dispatched chunk #19 = documents up to #100/100, outstanding queue size 6
merging changes from 10 documents into a model of 100 documents
topic #0 (0.500): 0.209*"area" + 0.181*"back" + 0.142*"see" + 0.085*"build" + 0.080*"come" + 0.057*"school" + 0.053*"court" + 0.040*"set" + 0.026*"royal" + 0.026*"film"
topic #1 (0.500): 0.278*"film" + 0.177*"school" + 0.072*"set" + 0.059*"international" + 0.059*"short" + 0.045*"build" + 0.041*"long" + 0.040*"role" + 0.039*"court" + 0.039*"player"
topic diff=0.316681, rho=0.258199
-3.264 per-word bound, 9.6 perplexity estimate based on a held-out corpus of 5 documents with 27 words
merging changes from 15 documents into a model of 100 documents
topic #0 (0.500): 0.205*"area" + 0.172*"back" + 0.140*"see" + 0.080*"build" + 0.076*"come" + 0.068*"court" + 0.060*"school" + 0.039*"set" + 0.027*"original" + 0.025*"royal"
topic #1 (0.500): 0.265*"film" + 0.170*"school" + 0.069*"set" + 0.062*"short" + 0.060*"player" + 0.057*"international" + 0.044*"long" + 0.043*"build" + 0.039*"court" + 0.038*"role"
topic diff=0.077815, rho=0.242536
-3.160 per-word bound, 8.9 perplexity estimate based on a held-out corpus of 5 documents with 27 words
merging changes from 5 documents into a model of 100 documents
topic #0 (0.500): 0.177*"back" + 0.170*"area" + 0.132*"see" + 0.093*"come" + 0.081*"build" + 0.057*"court" + 0.055*"royal" + 0.051*"school" + 0.038*"role" + 0.032*"set"
topic #1 (0.500): 0.246*"film" + 0.166*"player" + 0.129*"school" + 0.080*"long" + 0.056*"international" + 0.045*"set" + 0.045*"role" + 0.041*"short" + 0.034*"build" + 0.031*"come"
topic diff=0.256053, rho=0.223607
-2.761 per-word bound, 6.8 perplexity estimate based on a held-out corpus of 5 documents with 27 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=2, decay=0.5, chunksize=5) in 0.18s', 'datetime': '2022-02-06T08:15:55.793137', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-3.10.0-1160.49.1.el7.x86_64-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=127, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
