using symmetric alpha at 0.06666666666666667
using symmetric eta at 0.06666666666666667
using serial LDA version on this node
running online LDA training, 15 topics, 10 passes over the supplied corpus of 49835 documents, updating every 3000 documents, evaluating every ~49835 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 3 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 8
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.006*"film" + 0.005*"game" + 0.004*"player" + 0.004*"series" + 0.004*"season" + 0.003*"line" + 0.003*"station" + 0.003*"award" + 0.002*"league" + 0.002*"road"
topic #9 (0.067): 0.004*"film" + 0.004*"series" + 0.004*"song" + 0.003*"episode" + 0.003*"album" + 0.003*"band" + 0.003*"television" + 0.003*"award" + 0.003*"produce" + 0.002*"describe"
topic #6 (0.067): 0.004*"season" + 0.003*"building" + 0.003*"church" + 0.002*"study" + 0.002*"film" + 0.002*"road" + 0.002*"local" + 0.002*"specie" + 0.002*"life" + 0.002*"north"
topic #1 (0.067): 0.008*"film" + 0.003*"game" + 0.003*"player" + 0.003*"season" + 0.003*"series" + 0.003*"company" + 0.003*"championship" + 0.002*"award" + 0.002*"character" + 0.002*"design"
topic #7 (0.067): 0.006*"season" + 0.005*"film" + 0.005*"album" + 0.003*"game" + 0.003*"music" + 0.003*"song" + 0.003*"art" + 0.002*"building" + 0.002*"single" + 0.002*"church"
topic diff=6.699961, rho=1.000000
PROGRESS: pass 0, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.005*"game" + 0.004*"season" + 0.003*"series" + 0.002*"club" + 0.002*"park" + 0.002*"character" + 0.002*"player" + 0.002*"system" + 0.002*"get" + 0.002*"version"
topic #13 (0.067): 0.005*"film" + 0.004*"town" + 0.004*"game" + 0.004*"government" + 0.003*"age" + 0.003*"population" + 0.003*"season" + 0.003*"village" + 0.002*"war" + 0.002*"college"
topic #3 (0.067): 0.004*"building" + 0.003*"game" + 0.003*"system" + 0.003*"design" + 0.003*"company" + 0.002*"back" + 0.002*"engine" + 0.002*"control" + 0.002*"war" + 0.002*"station"
topic #8 (0.067): 0.009*"song" + 0.006*"band" + 0.006*"album" + 0.005*"music" + 0.004*"single" + 0.004*"say" + 0.003*"game" + 0.003*"film" + 0.003*"tour" + 0.003*"house"
topic #0 (0.067): 0.004*"game" + 0.004*"define" + 0.003*"displaystyle" + 0.003*"village" + 0.003*"nationality" + 0.003*"non_fifa" + 0.003*"rules_player" + 0.003*"building" + 0.003*"event" + 0.002*"woman"
topic diff=2.216626, rho=0.500000
PROGRESS: pass 0, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.005*"series" + 0.005*"film" + 0.004*"song" + 0.004*"album" + 0.004*"band" + 0.003*"produce" + 0.003*"episode" + 0.003*"television" + 0.003*"award" + 0.003*"book"
topic #0 (0.067): 0.008*"displaystyle" + 0.004*"define" + 0.004*"game" + 0.003*"village" + 0.003*"order" + 0.003*"building" + 0.003*"woman" + 0.003*"virus" + 0.003*"nationality" + 0.002*"event"
topic #2 (0.067): 0.016*"season" + 0.010*"club" + 0.008*"game" + 0.007*"league" + 0.007*"match" + 0.006*"goal" + 0.005*"football" + 0.005*"player" + 0.005*"score" + 0.004*"final"
topic #14 (0.067): 0.006*"population" + 0.005*"park" + 0.005*"age" + 0.004*"village" + 0.004*"river" + 0.003*"race" + 0.003*"child" + 0.003*"house" + 0.003*"building" + 0.003*"island"
topic #8 (0.067): 0.010*"song" + 0.008*"album" + 0.007*"music" + 0.007*"band" + 0.005*"single" + 0.004*"say" + 0.003*"tour" + 0.003*"company" + 0.003*"perform" + 0.003*"film"
topic diff=0.496714, rho=0.377964
PROGRESS: pass 0, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.011*"station" + 0.004*"line" + 0.004*"court" + 0.004*"public" + 0.004*"company" + 0.003*"law" + 0.003*"student" + 0.003*"railway" + 0.003*"community" + 0.003*"county"
topic #13 (0.067): 0.006*"government" + 0.005*"town" + 0.004*"college" + 0.004*"film" + 0.004*"population" + 0.003*"war" + 0.003*"age" + 0.003*"force" + 0.003*"village" + 0.003*"game"
topic #9 (0.067): 0.005*"film" + 0.005*"series" + 0.004*"song" + 0.004*"album" + 0.003*"award" + 0.003*"book" + 0.003*"korean" + 0.003*"television" + 0.003*"produce" + 0.003*"episode"
topic #6 (0.067): 0.006*"church" + 0.004*"specie" + 0.003*"road" + 0.003*"ship" + 0.003*"building" + 0.002*"describe" + 0.002*"point" + 0.002*"town" + 0.002*"island" + 0.002*"local"
topic #10 (0.067): 0.006*"government" + 0.004*"election" + 0.003*"company" + 0.003*"say" + 0.003*"party" + 0.003*"support" + 0.003*"son" + 0.002*"house" + 0.002*"order" + 0.002*"general"
topic diff=0.462717, rho=0.316228
PROGRESS: pass 0, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.006*"church" + 0.005*"specie" + 0.004*"ship" + 0.003*"road" + 0.003*"building" + 0.002*"island" + 0.002*"genus" + 0.002*"point" + 0.002*"describe" + 0.002*"site"
topic #7 (0.067): 0.008*"art" + 0.006*"season" + 0.005*"film" + 0.005*"album" + 0.005*"church" + 0.004*"music" + 0.003*"artist" + 0.003*"sign" + 0.003*"game" + 0.003*"basketball"
topic #11 (0.067): 0.006*"game" + 0.004*"season" + 0.004*"highway" + 0.003*"series" + 0.003*"system" + 0.003*"character" + 0.003*"version" + 0.003*"company" + 0.002*"car" + 0.002*"player"
topic #5 (0.067): 0.005*"woman" + 0.003*"house" + 0.003*"support" + 0.003*"say" + 0.003*"building" + 0.003*"hospital" + 0.003*"life" + 0.003*"company" + 0.002*"game" + 0.002*"program"
topic #8 (0.067): 0.014*"song" + 0.012*"album" + 0.011*"music" + 0.009*"band" + 0.006*"single" + 0.004*"say" + 0.004*"perform" + 0.004*"film" + 0.003*"tour" + 0.003*"track"
topic diff=0.419282, rho=0.277350
PROGRESS: pass 0, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.009*"game" + 0.006*"army" + 0.005*"route" + 0.005*"force" + 0.005*"player" + 0.005*"battle" + 0.004*"line" + 0.004*"war" + 0.004*"road" + 0.004*"film"
topic #7 (0.067): 0.010*"art" + 0.006*"church" + 0.006*"season" + 0.005*"film" + 0.004*"album" + 0.004*"artist" + 0.004*"music" + 0.003*"museum" + 0.003*"sign" + 0.003*"festival"
topic #3 (0.067): 0.006*"system" + 0.006*"design" + 0.005*"company" + 0.005*"model" + 0.004*"engine" + 0.004*"building" + 0.004*"vehicle" + 0.004*"aircraft" + 0.003*"car" + 0.003*"power"
topic #13 (0.067): 0.006*"government" + 0.005*"college" + 0.005*"town" + 0.004*"university" + 0.003*"population" + 0.003*"war" + 0.003*"science" + 0.003*"study" + 0.003*"student" + 0.003*"general"
topic #10 (0.067): 0.007*"government" + 0.005*"election" + 0.004*"party" + 0.003*"say" + 0.003*"support" + 0.003*"son" + 0.003*"political" + 0.003*"company" + 0.003*"force" + 0.003*"general"
topic diff=0.396344, rho=0.250000
PROGRESS: pass 0, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.011*"art" + 0.007*"church" + 0.005*"season" + 0.005*"film" + 0.004*"artist" + 0.004*"museum" + 0.003*"music" + 0.003*"album" + 0.003*"festival" + 0.003*"study"
topic #8 (0.067): 0.016*"song" + 0.015*"album" + 0.013*"music" + 0.010*"band" + 0.007*"single" + 0.005*"perform" + 0.004*"track" + 0.004*"tour" + 0.004*"say" + 0.004*"video"
topic #12 (0.067): 0.017*"station" + 0.007*"line" + 0.005*"company" + 0.005*"public" + 0.005*"court" + 0.004*"student" + 0.004*"railway" + 0.004*"law" + 0.003*"act" + 0.003*"operate"
topic #11 (0.067): 0.008*"game" + 0.004*"character" + 0.004*"series" + 0.003*"highway" + 0.003*"system" + 0.003*"version" + 0.003*"season" + 0.003*"company" + 0.003*"sell" + 0.003*"car"
topic #3 (0.067): 0.007*"system" + 0.006*"design" + 0.005*"company" + 0.005*"building" + 0.005*"engine" + 0.005*"model" + 0.004*"car" + 0.004*"aircraft" + 0.004*"vehicle" + 0.003*"power"
topic diff=0.353060, rho=0.229416
PROGRESS: pass 0, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.008*"game" + 0.007*"army" + 0.006*"battle" + 0.006*"force" + 0.006*"route" + 0.005*"war" + 0.004*"line" + 0.004*"road" + 0.004*"player" + 0.004*"attack"
topic #3 (0.067): 0.007*"design" + 0.007*"system" + 0.006*"company" + 0.005*"engine" + 0.005*"building" + 0.005*"model" + 0.004*"car" + 0.004*"aircraft" + 0.003*"vehicle" + 0.003*"power"
topic #14 (0.067): 0.008*"population" + 0.007*"village" + 0.006*"park" + 0.006*"river" + 0.005*"age" + 0.005*"town" + 0.004*"county" + 0.004*"house" + 0.004*"building" + 0.004*"land"
topic #0 (0.067): 0.007*"displaystyle" + 0.004*"define" + 0.004*"function" + 0.004*"cell" + 0.004*"example" + 0.003*"protein" + 0.003*"result" + 0.003*"point" + 0.003*"study" + 0.003*"order"
topic #6 (0.067): 0.007*"ship" + 0.007*"church" + 0.006*"specie" + 0.003*"genus" + 0.003*"road" + 0.003*"describe" + 0.003*"island" + 0.003*"water" + 0.002*"site" + 0.002*"building"
topic diff=0.331728, rho=0.213201
PROGRESS: pass 0, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.028*"season" + 0.017*"game" + 0.017*"club" + 0.014*"league" + 0.012*"football" + 0.011*"player" + 0.010*"championship" + 0.009*"match" + 0.009*"final" + 0.007*"score"
topic #10 (0.067): 0.007*"government" + 0.006*"election" + 0.006*"party" + 0.004*"say" + 0.004*"son" + 0.004*"political" + 0.003*"support" + 0.003*"vote" + 0.003*"elect" + 0.003*"general"
topic #6 (0.067): 0.009*"ship" + 0.007*"specie" + 0.006*"church" + 0.004*"genus" + 0.004*"water" + 0.003*"island" + 0.003*"describe" + 0.003*"road" + 0.002*"gun" + 0.002*"point"
topic #12 (0.067): 0.019*"station" + 0.008*"line" + 0.006*"company" + 0.005*"railway" + 0.005*"student" + 0.005*"law" + 0.005*"public" + 0.005*"court" + 0.004*"operate" + 0.003*"act"
topic #11 (0.067): 0.008*"game" + 0.005*"character" + 0.004*"system" + 0.004*"version" + 0.003*"series" + 0.003*"company" + 0.003*"software" + 0.003*"highway" + 0.003*"player" + 0.003*"sell"
topic diff=0.314571, rho=0.200000
PROGRESS: pass 0, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.008*"population" + 0.008*"village" + 0.007*"river" + 0.006*"town" + 0.006*"park" + 0.005*"age" + 0.005*"house" + 0.005*"north" + 0.005*"county" + 0.004*"building"
topic #4 (0.067): 0.008*"army" + 0.008*"game" + 0.007*"force" + 0.007*"battle" + 0.006*"war" + 0.006*"route" + 0.005*"attack" + 0.004*"line" + 0.004*"command" + 0.004*"unit"
topic #9 (0.067): 0.008*"series" + 0.007*"book" + 0.006*"film" + 0.005*"episode" + 0.005*"publish" + 0.005*"television" + 0.004*"award" + 0.004*"life" + 0.004*"novel" + 0.003*"say"
topic #7 (0.067): 0.015*"art" + 0.012*"church" + 0.006*"artist" + 0.005*"museum" + 0.005*"painting" + 0.004*"study" + 0.004*"bishop" + 0.003*"festival" + 0.003*"exhibition" + 0.003*"history"
topic #1 (0.067): 0.029*"film" + 0.006*"event" + 0.006*"series" + 0.005*"star" + 0.005*"character" + 0.005*"woman" + 0.005*"award" + 0.004*"game" + 0.004*"story" + 0.003*"direct"
topic diff=0.305244, rho=0.188982
PROGRESS: pass 0, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.008*"series" + 0.007*"book" + 0.006*"episode" + 0.005*"film" + 0.005*"publish" + 0.005*"television" + 0.004*"life" + 0.004*"award" + 0.004*"novel" + 0.004*"story"
topic #1 (0.067): 0.031*"film" + 0.007*"event" + 0.007*"star" + 0.006*"series" + 0.005*"character" + 0.005*"woman" + 0.004*"award" + 0.004*"story" + 0.004*"game" + 0.004*"direct"
topic #7 (0.067): 0.016*"art" + 0.014*"church" + 0.006*"artist" + 0.006*"museum" + 0.004*"painting" + 0.004*"bishop" + 0.004*"study" + 0.004*"design" + 0.004*"festival" + 0.003*"history"
topic #11 (0.067): 0.013*"game" + 0.004*"system" + 0.004*"character" + 0.004*"version" + 0.004*"player" + 0.004*"company" + 0.004*"software" + 0.003*"series" + 0.003*"computer" + 0.003*"sell"
topic #8 (0.067): 0.021*"album" + 0.019*"song" + 0.016*"music" + 0.013*"band" + 0.008*"single" + 0.007*"perform" + 0.006*"track" + 0.005*"tour" + 0.004*"video" + 0.004*"say"
topic diff=0.298050, rho=0.179605
PROGRESS: pass 0, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.031*"season" + 0.020*"game" + 0.018*"club" + 0.014*"league" + 0.013*"football" + 0.012*"player" + 0.011*"championship" + 0.010*"match" + 0.009*"final" + 0.008*"score"
topic #12 (0.067): 0.022*"station" + 0.009*"line" + 0.008*"company" + 0.005*"student" + 0.005*"railway" + 0.005*"public" + 0.005*"operate" + 0.004*"court" + 0.004*"law" + 0.004*"building"
topic #6 (0.067): 0.011*"ship" + 0.009*"specie" + 0.005*"church" + 0.004*"genus" + 0.004*"water" + 0.004*"island" + 0.004*"describe" + 0.003*"vessel" + 0.003*"sea" + 0.003*"cause"
topic #4 (0.067): 0.009*"army" + 0.008*"force" + 0.007*"battle" + 0.007*"war" + 0.006*"attack" + 0.006*"game" + 0.006*"command" + 0.006*"route" + 0.005*"unit" + 0.004*"line"
topic #3 (0.067): 0.008*"system" + 0.008*"design" + 0.006*"company" + 0.005*"engine" + 0.005*"aircraft" + 0.005*"model" + 0.004*"car" + 0.004*"power" + 0.004*"air" + 0.003*"building"
topic diff=0.278221, rho=0.171499
PROGRESS: pass 0, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.013*"game" + 0.005*"system" + 0.005*"version" + 0.004*"company" + 0.004*"software" + 0.004*"player" + 0.004*"character" + 0.003*"computer" + 0.003*"store" + 0.003*"sell"
topic #7 (0.067): 0.018*"church" + 0.017*"art" + 0.007*"museum" + 0.007*"artist" + 0.005*"painting" + 0.004*"study" + 0.004*"bishop" + 0.004*"exhibition" + 0.004*"history" + 0.004*"design"
topic #13 (0.067): 0.008*"university" + 0.007*"college" + 0.006*"research" + 0.006*"government" + 0.005*"study" + 0.005*"student" + 0.005*"science" + 0.005*"german" + 0.004*"department" + 0.004*"education"
topic #9 (0.067): 0.008*"book" + 0.008*"series" + 0.006*"episode" + 0.005*"publish" + 0.005*"film" + 0.005*"television" + 0.004*"life" + 0.004*"say" + 0.004*"story" + 0.004*"award"
topic #14 (0.067): 0.009*"population" + 0.009*"village" + 0.007*"river" + 0.007*"town" + 0.006*"park" + 0.005*"county" + 0.005*"north" + 0.005*"age" + 0.005*"house" + 0.005*"building"
topic diff=0.265213, rho=0.164399
PROGRESS: pass 0, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.007*"government" + 0.007*"election" + 0.007*"party" + 0.004*"vote" + 0.004*"political" + 0.004*"say" + 0.004*"son" + 0.004*"elect" + 0.004*"support" + 0.004*"law"
topic #2 (0.067): 0.031*"season" + 0.020*"game" + 0.018*"club" + 0.014*"league" + 0.013*"football" + 0.012*"player" + 0.012*"championship" + 0.010*"match" + 0.010*"final" + 0.008*"finish"
topic #6 (0.067): 0.011*"ship" + 0.010*"specie" + 0.005*"genus" + 0.004*"church" + 0.004*"describe" + 0.004*"island" + 0.004*"water" + 0.003*"vessel" + 0.003*"sea" + 0.003*"port"
topic #1 (0.067): 0.037*"film" + 0.007*"star" + 0.007*"event" + 0.006*"character" + 0.006*"series" + 0.005*"award" + 0.004*"direct" + 0.004*"story" + 0.004*"woman" + 0.004*"game"
topic #5 (0.067): 0.012*"woman" + 0.007*"hospital" + 0.005*"house" + 0.004*"medical" + 0.004*"police" + 0.003*"support" + 0.003*"building" + 0.003*"company" + 0.003*"say" + 0.003*"newspaper"
topic diff=0.248997, rho=0.158114
-8.480 per-word bound, 357.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.067): 0.039*"film" + 0.007*"star" + 0.007*"event" + 0.006*"series" + 0.006*"character" + 0.005*"award" + 0.005*"direct" + 0.004*"story" + 0.004*"woman" + 0.004*"game"
topic #7 (0.067): 0.019*"church" + 0.017*"art" + 0.008*"museum" + 0.007*"artist" + 0.006*"painting" + 0.005*"study" + 0.004*"bishop" + 0.004*"design" + 0.004*"exhibition" + 0.004*"history"
topic #14 (0.067): 0.010*"population" + 0.009*"village" + 0.007*"town" + 0.007*"river" + 0.006*"park" + 0.006*"county" + 0.006*"age" + 0.006*"north" + 0.005*"south" + 0.005*"building"
topic #12 (0.067): 0.022*"station" + 0.011*"company" + 0.009*"line" + 0.006*"public" + 0.005*"operate" + 0.005*"railway" + 0.005*"court" + 0.005*"student" + 0.004*"business" + 0.004*"provide"
topic #13 (0.067): 0.008*"university" + 0.007*"college" + 0.006*"research" + 0.006*"study" + 0.006*"student" + 0.006*"government" + 0.006*"science" + 0.005*"education" + 0.005*"department" + 0.005*"graduate"
topic diff=0.247478, rho=0.152499
-8.471 per-word bound, 354.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.067): 0.008*"university" + 0.008*"college" + 0.007*"research" + 0.006*"student" + 0.006*"study" + 0.006*"government" + 0.006*"science" + 0.005*"education" + 0.005*"department" + 0.005*"director"
topic #11 (0.067): 0.018*"game" + 0.006*"player" + 0.006*"system" + 0.006*"version" + 0.005*"code" + 0.004*"company" + 0.004*"software" + 0.004*"user" + 0.004*"computer" + 0.004*"file"
topic #8 (0.067): 0.022*"album" + 0.021*"song" + 0.019*"music" + 0.014*"band" + 0.009*"single" + 0.007*"perform" + 0.006*"track" + 0.005*"tour" + 0.005*"video" + 0.005*"chart"
topic #9 (0.067): 0.009*"book" + 0.008*"series" + 0.006*"episode" + 0.005*"publish" + 0.005*"television" + 0.005*"film" + 0.005*"life" + 0.004*"say" + 0.004*"novel" + 0.004*"story"
topic #3 (0.067): 0.009*"design" + 0.008*"system" + 0.006*"company" + 0.005*"car" + 0.005*"engine" + 0.005*"model" + 0.005*"power" + 0.004*"aircraft" + 0.004*"produce" + 0.003*"production"
topic diff=0.245127, rho=0.147442
-8.464 per-word bound, 353.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #5 (0.067): 0.014*"woman" + 0.008*"hospital" + 0.006*"house" + 0.005*"police" + 0.004*"medical" + 0.004*"child" + 0.004*"newspaper" + 0.004*"building" + 0.003*"support" + 0.003*"say"
topic #6 (0.067): 0.013*"ship" + 0.012*"specie" + 0.006*"genus" + 0.005*"describe" + 0.004*"water" + 0.004*"vessel" + 0.004*"island" + 0.004*"sea" + 0.003*"port" + 0.003*"church"
topic #14 (0.067): 0.010*"population" + 0.010*"village" + 0.008*"town" + 0.007*"river" + 0.006*"county" + 0.006*"park" + 0.006*"north" + 0.005*"building" + 0.005*"age" + 0.005*"south"
topic #11 (0.067): 0.020*"game" + 0.007*"player" + 0.006*"system" + 0.005*"version" + 0.005*"code" + 0.005*"user" + 0.004*"file" + 0.004*"company" + 0.004*"software" + 0.004*"computer"
topic #9 (0.067): 0.009*"book" + 0.008*"series" + 0.006*"episode" + 0.005*"publish" + 0.005*"television" + 0.005*"life" + 0.005*"film" + 0.004*"say" + 0.004*"story" + 0.004*"novel"
topic diff=0.229661, rho=0.142857
-8.345 per-word bound, 325.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.010*"population" + 0.010*"village" + 0.008*"town" + 0.007*"river" + 0.006*"park" + 0.006*"building" + 0.006*"county" + 0.006*"north" + 0.005*"age" + 0.005*"south"
topic #7 (0.067): 0.018*"church" + 0.018*"art" + 0.009*"museum" + 0.007*"artist" + 0.005*"painting" + 0.005*"study" + 0.005*"exhibition" + 0.005*"design" + 0.005*"history" + 0.004*"bishop"
topic #3 (0.067): 0.009*"design" + 0.008*"system" + 0.006*"company" + 0.005*"engine" + 0.005*"model" + 0.005*"car" + 0.005*"power" + 0.005*"aircraft" + 0.004*"vehicle" + 0.004*"produce"
topic #6 (0.067): 0.013*"ship" + 0.013*"specie" + 0.006*"genus" + 0.005*"island" + 0.005*"describe" + 0.004*"water" + 0.004*"vessel" + 0.004*"sea" + 0.003*"port" + 0.003*"boat"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.016*"club" + 0.013*"league" + 0.013*"player" + 0.012*"football" + 0.012*"championship" + 0.010*"match" + 0.010*"final" + 0.008*"finish"
topic diff=0.224265, rho=0.138896
PROGRESS: pass 1, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.067): 0.009*"university" + 0.008*"college" + 0.007*"study" + 0.007*"student" + 0.007*"research" + 0.006*"science" + 0.006*"education" + 0.006*"government" + 0.005*"department" + 0.005*"director"
topic #7 (0.067): 0.019*"church" + 0.018*"art" + 0.009*"museum" + 0.008*"artist" + 0.006*"painting" + 0.005*"study" + 0.005*"exhibition" + 0.005*"design" + 0.005*"history" + 0.005*"bishop"
topic #12 (0.067): 0.023*"station" + 0.013*"company" + 0.009*"line" + 0.007*"railway" + 0.006*"public" + 0.006*"operate" + 0.004*"building" + 0.004*"student" + 0.004*"business" + 0.004*"court"
topic #8 (0.067): 0.023*"album" + 0.022*"song" + 0.019*"music" + 0.016*"band" + 0.010*"single" + 0.007*"perform" + 0.006*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic #10 (0.067): 0.008*"government" + 0.008*"election" + 0.007*"party" + 0.004*"vote" + 0.004*"son" + 0.004*"law" + 0.004*"elect" + 0.004*"political" + 0.004*"say" + 0.004*"support"
topic diff=0.217561, rho=0.138896
PROGRESS: pass 1, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.067): 0.023*"album" + 0.022*"song" + 0.020*"music" + 0.015*"band" + 0.010*"single" + 0.008*"perform" + 0.006*"track" + 0.006*"tour" + 0.005*"video" + 0.005*"chart"
topic #1 (0.067): 0.046*"film" + 0.008*"star" + 0.006*"event" + 0.006*"series" + 0.006*"character" + 0.006*"direct" + 0.005*"award" + 0.005*"story" + 0.004*"woman" + 0.003*"earth"
topic #12 (0.067): 0.023*"station" + 0.013*"company" + 0.010*"line" + 0.007*"railway" + 0.006*"operate" + 0.006*"public" + 0.005*"business" + 0.004*"building" + 0.004*"court" + 0.004*"student"
topic #11 (0.067): 0.023*"game" + 0.008*"player" + 0.006*"system" + 0.005*"version" + 0.005*"computer" + 0.005*"user" + 0.005*"software" + 0.004*"company" + 0.004*"code" + 0.004*"store"
topic #13 (0.067): 0.009*"university" + 0.009*"college" + 0.007*"study" + 0.007*"student" + 0.007*"research" + 0.006*"science" + 0.006*"education" + 0.006*"government" + 0.006*"department" + 0.005*"director"
topic diff=0.209701, rho=0.138896
PROGRESS: pass 1, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.067): 0.046*"film" + 0.008*"star" + 0.006*"event" + 0.006*"series" + 0.006*"character" + 0.006*"direct" + 0.005*"award" + 0.005*"story" + 0.004*"woman" + 0.003*"game"
topic #0 (0.067): 0.008*"displaystyle" + 0.005*"example" + 0.005*"cell" + 0.005*"define" + 0.004*"function" + 0.004*"study" + 0.004*"term" + 0.004*"case" + 0.004*"result" + 0.004*"human"
topic #9 (0.067): 0.009*"book" + 0.008*"series" + 0.006*"episode" + 0.006*"publish" + 0.005*"life" + 0.005*"say" + 0.005*"television" + 0.004*"story" + 0.004*"novel" + 0.004*"appear"
topic #4 (0.067): 0.012*"army" + 0.011*"force" + 0.011*"war" + 0.009*"battle" + 0.008*"attack" + 0.007*"command" + 0.006*"military" + 0.005*"unit" + 0.005*"kill" + 0.005*"route"
topic #7 (0.067): 0.018*"church" + 0.018*"art" + 0.010*"museum" + 0.008*"artist" + 0.006*"painting" + 0.005*"study" + 0.005*"design" + 0.005*"exhibition" + 0.005*"bishop" + 0.004*"history"
topic diff=0.214019, rho=0.138896
PROGRESS: pass 1, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.018*"art" + 0.018*"church" + 0.009*"museum" + 0.008*"artist" + 0.006*"painting" + 0.005*"study" + 0.005*"design" + 0.005*"exhibition" + 0.004*"bishop" + 0.004*"history"
topic #10 (0.067): 0.009*"government" + 0.008*"election" + 0.007*"party" + 0.005*"law" + 0.005*"vote" + 0.004*"political" + 0.004*"say" + 0.004*"elect" + 0.004*"son" + 0.004*"court"
topic #12 (0.067): 0.022*"station" + 0.014*"company" + 0.010*"line" + 0.007*"railway" + 0.007*"operate" + 0.006*"public" + 0.005*"business" + 0.004*"building" + 0.004*"provide" + 0.004*"local"
topic #0 (0.067): 0.008*"displaystyle" + 0.006*"example" + 0.005*"define" + 0.004*"cell" + 0.004*"term" + 0.004*"function" + 0.004*"study" + 0.004*"case" + 0.004*"result" + 0.004*"system"
topic #8 (0.067): 0.023*"album" + 0.022*"song" + 0.021*"music" + 0.015*"band" + 0.010*"single" + 0.008*"perform" + 0.006*"track" + 0.006*"video" + 0.005*"tour" + 0.005*"chart"
topic diff=0.208083, rho=0.138896
PROGRESS: pass 1, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.067): 0.024*"album" + 0.023*"song" + 0.021*"music" + 0.015*"band" + 0.010*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.005*"video" + 0.005*"chart"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.012*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #11 (0.067): 0.025*"game" + 0.008*"player" + 0.007*"system" + 0.006*"version" + 0.005*"computer" + 0.005*"user" + 0.005*"software" + 0.005*"code" + 0.005*"company" + 0.004*"store"
topic #7 (0.067): 0.019*"church" + 0.018*"art" + 0.009*"museum" + 0.008*"artist" + 0.006*"painting" + 0.005*"design" + 0.005*"study" + 0.005*"exhibition" + 0.004*"bishop" + 0.004*"history"
topic #1 (0.067): 0.049*"film" + 0.009*"star" + 0.006*"series" + 0.006*"character" + 0.006*"direct" + 0.006*"award" + 0.006*"event" + 0.005*"story" + 0.004*"role" + 0.003*"woman"
topic diff=0.205753, rho=0.138896
PROGRESS: pass 1, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.015*"specie" + 0.014*"ship" + 0.007*"island" + 0.006*"genus" + 0.006*"water" + 0.005*"describe" + 0.004*"sea" + 0.004*"vessel" + 0.004*"boat" + 0.004*"port"
topic #11 (0.067): 0.025*"game" + 0.008*"player" + 0.006*"system" + 0.006*"version" + 0.005*"software" + 0.005*"code" + 0.005*"computer" + 0.005*"user" + 0.005*"consumer" + 0.005*"company"
topic #7 (0.067): 0.019*"church" + 0.018*"art" + 0.009*"museum" + 0.008*"artist" + 0.006*"painting" + 0.005*"design" + 0.005*"study" + 0.005*"exhibition" + 0.005*"collection" + 0.004*"history"
topic #5 (0.067): 0.014*"woman" + 0.009*"hospital" + 0.008*"house" + 0.007*"police" + 0.006*"child" + 0.004*"medical" + 0.004*"say" + 0.004*"building" + 0.004*"death" + 0.003*"newspaper"
topic #10 (0.067): 0.009*"government" + 0.008*"election" + 0.008*"party" + 0.005*"law" + 0.005*"political" + 0.005*"vote" + 0.004*"elect" + 0.004*"say" + 0.004*"son" + 0.004*"court"
topic diff=0.209975, rho=0.138896
PROGRESS: pass 1, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.011*"population" + 0.011*"village" + 0.009*"town" + 0.008*"river" + 0.007*"park" + 0.007*"county" + 0.006*"north" + 0.006*"building" + 0.006*"south" + 0.006*"age"
topic #6 (0.067): 0.016*"specie" + 0.015*"ship" + 0.007*"island" + 0.006*"genus" + 0.006*"water" + 0.005*"describe" + 0.005*"sea" + 0.004*"vessel" + 0.004*"boat" + 0.004*"port"
topic #13 (0.067): 0.010*"university" + 0.009*"college" + 0.009*"student" + 0.008*"study" + 0.008*"research" + 0.007*"science" + 0.006*"education" + 0.006*"director" + 0.005*"government" + 0.005*"graduate"
topic #3 (0.067): 0.010*"design" + 0.008*"system" + 0.006*"car" + 0.006*"engine" + 0.006*"company" + 0.006*"model" + 0.005*"power" + 0.005*"vehicle" + 0.005*"aircraft" + 0.004*"produce"
topic #11 (0.067): 0.024*"game" + 0.009*"player" + 0.007*"system" + 0.006*"version" + 0.005*"character" + 0.005*"code" + 0.005*"computer" + 0.005*"software" + 0.005*"user" + 0.004*"company"
topic diff=0.203171, rho=0.138896
PROGRESS: pass 1, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.067): 0.025*"album" + 0.024*"song" + 0.021*"music" + 0.016*"band" + 0.010*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic #7 (0.067): 0.020*"church" + 0.018*"art" + 0.009*"museum" + 0.008*"artist" + 0.006*"painting" + 0.006*"design" + 0.005*"study" + 0.004*"collection" + 0.004*"history" + 0.004*"exhibition"
topic #2 (0.067): 0.032*"season" + 0.020*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #5 (0.067): 0.016*"woman" + 0.010*"hospital" + 0.007*"house" + 0.007*"police" + 0.006*"child" + 0.004*"medical" + 0.004*"death" + 0.004*"say" + 0.004*"building" + 0.004*"care"
topic #1 (0.067): 0.049*"film" + 0.009*"star" + 0.007*"series" + 0.006*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"event" + 0.005*"story" + 0.004*"role" + 0.004*"movie"
topic diff=0.193986, rho=0.138896
PROGRESS: pass 1, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.067): 0.010*"university" + 0.010*"student" + 0.009*"college" + 0.008*"study" + 0.008*"research" + 0.007*"science" + 0.006*"education" + 0.006*"award" + 0.006*"director" + 0.006*"department"
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.004*"cell" + 0.004*"study" + 0.004*"term" + 0.004*"result" + 0.004*"case" + 0.004*"theory" + 0.004*"function" + 0.004*"type"
topic #3 (0.067): 0.010*"design" + 0.008*"system" + 0.006*"engine" + 0.006*"car" + 0.006*"power" + 0.006*"company" + 0.006*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.004*"produce"
topic #7 (0.067): 0.020*"church" + 0.018*"art" + 0.009*"museum" + 0.008*"artist" + 0.007*"painting" + 0.006*"design" + 0.005*"study" + 0.004*"collection" + 0.004*"history" + 0.004*"exhibition"
topic #1 (0.067): 0.049*"film" + 0.010*"star" + 0.007*"series" + 0.006*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"event" + 0.005*"story" + 0.004*"role" + 0.004*"movie"
topic diff=0.195253, rho=0.138896
PROGRESS: pass 1, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.067): 0.026*"album" + 0.024*"song" + 0.022*"music" + 0.017*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic #6 (0.067): 0.017*"specie" + 0.015*"ship" + 0.008*"island" + 0.007*"water" + 0.006*"genus" + 0.005*"describe" + 0.005*"sea" + 0.005*"vessel" + 0.004*"port" + 0.004*"boat"
topic #12 (0.067): 0.023*"station" + 0.016*"company" + 0.011*"line" + 0.007*"railway" + 0.007*"operate" + 0.006*"public" + 0.005*"business" + 0.005*"building" + 0.005*"provide" + 0.005*"train"
topic #1 (0.067): 0.049*"film" + 0.011*"star" + 0.007*"series" + 0.006*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"event" + 0.005*"story" + 0.004*"role" + 0.004*"movie"
topic #4 (0.067): 0.012*"army" + 0.012*"force" + 0.012*"war" + 0.009*"battle" + 0.008*"attack" + 0.007*"command" + 0.007*"military" + 0.006*"kill" + 0.006*"unit" + 0.005*"fight"
topic diff=0.196409, rho=0.138896
PROGRESS: pass 1, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.016*"woman" + 0.009*"hospital" + 0.007*"house" + 0.007*"police" + 0.007*"child" + 0.005*"medical" + 0.004*"black" + 0.004*"death" + 0.004*"newspaper" + 0.004*"say"
topic #8 (0.067): 0.026*"album" + 0.024*"song" + 0.022*"music" + 0.017*"band" + 0.010*"single" + 0.009*"perform" + 0.007*"track" + 0.006*"tour" + 0.005*"video" + 0.005*"chart"
topic #12 (0.067): 0.024*"station" + 0.016*"company" + 0.011*"line" + 0.007*"railway" + 0.007*"operate" + 0.006*"public" + 0.006*"business" + 0.005*"building" + 0.005*"local" + 0.005*"provide"
topic #11 (0.067): 0.027*"game" + 0.009*"player" + 0.008*"system" + 0.006*"version" + 0.005*"computer" + 0.005*"software" + 0.005*"user" + 0.005*"code" + 0.004*"character" + 0.004*"product"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic diff=0.187710, rho=0.138896
PROGRESS: pass 1, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.009*"government" + 0.009*"election" + 0.008*"party" + 0.005*"law" + 0.005*"vote" + 0.005*"political" + 0.005*"court" + 0.005*"elect" + 0.004*"say" + 0.004*"support"
topic #13 (0.067): 0.011*"university" + 0.010*"student" + 0.009*"college" + 0.009*"study" + 0.009*"research" + 0.007*"science" + 0.007*"education" + 0.006*"award" + 0.006*"director" + 0.006*"department"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"system" + 0.005*"study" + 0.004*"term" + 0.004*"cell" + 0.004*"result" + 0.004*"protein" + 0.004*"case" + 0.004*"function"
topic #9 (0.067): 0.011*"book" + 0.008*"series" + 0.007*"publish" + 0.006*"episode" + 0.006*"life" + 0.006*"say" + 0.005*"story" + 0.005*"television" + 0.005*"novel" + 0.004*"appear"
topic #12 (0.067): 0.023*"station" + 0.017*"company" + 0.011*"line" + 0.007*"operate" + 0.007*"railway" + 0.006*"public" + 0.006*"business" + 0.005*"building" + 0.005*"local" + 0.004*"provide"
topic diff=0.182573, rho=0.138896
PROGRESS: pass 1, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.017*"woman" + 0.009*"hospital" + 0.007*"house" + 0.007*"police" + 0.007*"child" + 0.005*"medical" + 0.004*"death" + 0.004*"black" + 0.004*"newspaper" + 0.004*"say"
topic #10 (0.067): 0.009*"government" + 0.009*"election" + 0.008*"party" + 0.005*"law" + 0.005*"vote" + 0.005*"court" + 0.005*"political" + 0.005*"elect" + 0.004*"say" + 0.004*"support"
topic #14 (0.067): 0.012*"village" + 0.011*"population" + 0.010*"town" + 0.008*"river" + 0.007*"county" + 0.007*"north" + 0.007*"park" + 0.006*"south" + 0.006*"building" + 0.005*"road"
topic #1 (0.067): 0.051*"film" + 0.011*"star" + 0.007*"series" + 0.007*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"event" + 0.005*"role" + 0.004*"story" + 0.004*"movie"
topic #13 (0.067): 0.011*"university" + 0.010*"student" + 0.009*"college" + 0.009*"study" + 0.008*"research" + 0.007*"science" + 0.007*"education" + 0.007*"award" + 0.006*"director" + 0.006*"graduate"
topic diff=0.174252, rho=0.138896
-8.365 per-word bound, 329.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #4 (0.067): 0.014*"army" + 0.013*"war" + 0.012*"force" + 0.010*"battle" + 0.009*"attack" + 0.008*"military" + 0.008*"command" + 0.006*"unit" + 0.006*"german" + 0.006*"division"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"cell" + 0.005*"term" + 0.004*"study" + 0.004*"function" + 0.004*"result" + 0.004*"system" + 0.004*"case" + 0.004*"define"
topic #1 (0.067): 0.052*"film" + 0.011*"star" + 0.007*"series" + 0.007*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"role" + 0.004*"movie" + 0.004*"event" + 0.004*"story"
topic #10 (0.067): 0.010*"government" + 0.009*"election" + 0.008*"party" + 0.006*"law" + 0.005*"vote" + 0.005*"court" + 0.005*"political" + 0.005*"elect" + 0.004*"say" + 0.004*"support"
topic #7 (0.067): 0.023*"church" + 0.018*"art" + 0.010*"museum" + 0.008*"artist" + 0.006*"painting" + 0.006*"design" + 0.005*"study" + 0.004*"bishop" + 0.004*"history" + 0.004*"collection"
topic diff=0.173079, rho=0.138896
-8.366 per-word bound, 329.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #11 (0.067): 0.027*"game" + 0.010*"player" + 0.009*"system" + 0.007*"version" + 0.006*"user" + 0.006*"code" + 0.006*"computer" + 0.005*"software" + 0.005*"file" + 0.004*"datum"
topic #6 (0.067): 0.019*"specie" + 0.015*"ship" + 0.008*"island" + 0.007*"genus" + 0.007*"water" + 0.006*"describe" + 0.005*"sea" + 0.005*"vessel" + 0.004*"port" + 0.004*"boat"
topic #1 (0.067): 0.054*"film" + 0.011*"star" + 0.007*"series" + 0.007*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"role" + 0.005*"movie" + 0.004*"story" + 0.004*"event"
topic #3 (0.067): 0.010*"design" + 0.008*"system" + 0.006*"car" + 0.006*"power" + 0.006*"engine" + 0.006*"model" + 0.005*"company" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"vehicle"
topic #13 (0.067): 0.011*"university" + 0.011*"student" + 0.010*"college" + 0.009*"study" + 0.008*"research" + 0.007*"science" + 0.007*"education" + 0.007*"award" + 0.006*"director" + 0.006*"department"
topic diff=0.171658, rho=0.138896
-8.315 per-word bound, 318.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.022*"church" + 0.018*"art" + 0.010*"museum" + 0.008*"artist" + 0.006*"painting" + 0.006*"design" + 0.005*"study" + 0.005*"history" + 0.005*"exhibition" + 0.004*"bishop"
topic #9 (0.067): 0.010*"book" + 0.008*"series" + 0.007*"episode" + 0.007*"publish" + 0.006*"life" + 0.006*"say" + 0.005*"story" + 0.005*"novel" + 0.005*"television" + 0.004*"appear"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.006*"car" + 0.006*"power" + 0.006*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"company" + 0.005*"vehicle" + 0.005*"produce"
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"study" + 0.005*"system" + 0.004*"cell" + 0.004*"term" + 0.004*"result" + 0.004*"define" + 0.004*"function" + 0.004*"case"
topic #14 (0.067): 0.012*"village" + 0.012*"population" + 0.010*"town" + 0.008*"river" + 0.007*"county" + 0.007*"north" + 0.007*"building" + 0.007*"park" + 0.007*"south" + 0.006*"road"
topic diff=0.167921, rho=0.137575
PROGRESS: pass 2, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.028*"game" + 0.011*"player" + 0.008*"system" + 0.006*"version" + 0.006*"computer" + 0.006*"user" + 0.005*"code" + 0.005*"software" + 0.004*"information" + 0.004*"datum"
topic #9 (0.067): 0.010*"book" + 0.008*"series" + 0.007*"publish" + 0.007*"episode" + 0.006*"life" + 0.006*"say" + 0.005*"story" + 0.005*"novel" + 0.004*"television" + 0.004*"appear"
topic #6 (0.067): 0.019*"specie" + 0.015*"ship" + 0.010*"island" + 0.007*"genus" + 0.007*"water" + 0.006*"describe" + 0.005*"sea" + 0.004*"vessel" + 0.004*"port" + 0.004*"boat"
topic #14 (0.067): 0.012*"village" + 0.012*"population" + 0.010*"town" + 0.008*"river" + 0.007*"county" + 0.007*"north" + 0.007*"park" + 0.007*"building" + 0.007*"south" + 0.006*"age"
topic #4 (0.067): 0.014*"war" + 0.014*"army" + 0.012*"force" + 0.010*"battle" + 0.009*"attack" + 0.008*"military" + 0.007*"command" + 0.006*"unit" + 0.005*"kill" + 0.005*"order"
topic diff=0.160675, rho=0.137575
PROGRESS: pass 2, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.012*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #14 (0.067): 0.012*"village" + 0.012*"population" + 0.010*"town" + 0.008*"river" + 0.008*"county" + 0.007*"north" + 0.007*"park" + 0.007*"building" + 0.007*"south" + 0.006*"age"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.006*"engine" + 0.006*"power" + 0.006*"model" + 0.005*"aircraft" + 0.005*"company" + 0.005*"produce" + 0.005*"vehicle"
topic #5 (0.067): 0.016*"woman" + 0.009*"hospital" + 0.009*"child" + 0.008*"house" + 0.008*"police" + 0.004*"death" + 0.004*"medical" + 0.004*"black" + 0.004*"murder" + 0.004*"say"
topic #1 (0.067): 0.053*"film" + 0.011*"star" + 0.008*"series" + 0.007*"direct" + 0.006*"character" + 0.006*"award" + 0.005*"movie" + 0.005*"role" + 0.004*"story" + 0.004*"appear"
topic diff=0.152252, rho=0.137575
PROGRESS: pass 2, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.012*"population" + 0.012*"village" + 0.010*"town" + 0.008*"river" + 0.008*"county" + 0.007*"north" + 0.007*"park" + 0.007*"building" + 0.007*"south" + 0.006*"age"
topic #4 (0.067): 0.014*"army" + 0.014*"war" + 0.013*"force" + 0.010*"battle" + 0.009*"military" + 0.009*"attack" + 0.007*"command" + 0.006*"unit" + 0.006*"kill" + 0.005*"german"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.012*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #5 (0.067): 0.016*"woman" + 0.009*"hospital" + 0.008*"child" + 0.008*"police" + 0.008*"house" + 0.005*"murder" + 0.005*"death" + 0.004*"say" + 0.004*"medical" + 0.004*"black"
topic #3 (0.067): 0.010*"design" + 0.008*"system" + 0.007*"car" + 0.006*"engine" + 0.006*"power" + 0.006*"model" + 0.006*"aircraft" + 0.005*"vehicle" + 0.005*"company" + 0.005*"produce"
topic diff=0.155112, rho=0.137575
PROGRESS: pass 2, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.011*"book" + 0.007*"series" + 0.007*"publish" + 0.006*"life" + 0.006*"say" + 0.006*"episode" + 0.005*"story" + 0.005*"novel" + 0.004*"television" + 0.004*"appear"
topic #12 (0.067): 0.022*"station" + 0.019*"company" + 0.011*"line" + 0.007*"operate" + 0.007*"railway" + 0.006*"public" + 0.006*"business" + 0.005*"market" + 0.005*"building" + 0.005*"network"
topic #11 (0.067): 0.030*"game" + 0.011*"player" + 0.009*"system" + 0.007*"version" + 0.006*"computer" + 0.006*"user" + 0.005*"code" + 0.005*"software" + 0.005*"product" + 0.005*"information"
topic #5 (0.067): 0.016*"woman" + 0.009*"hospital" + 0.009*"police" + 0.009*"child" + 0.008*"house" + 0.005*"medical" + 0.005*"death" + 0.005*"murder" + 0.004*"black" + 0.004*"say"
topic #8 (0.067): 0.027*"album" + 0.026*"song" + 0.024*"music" + 0.018*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic diff=0.147122, rho=0.137575
PROGRESS: pass 2, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.028*"game" + 0.010*"player" + 0.008*"system" + 0.006*"version" + 0.006*"computer" + 0.006*"user" + 0.005*"software" + 0.005*"consumer" + 0.005*"product" + 0.005*"information"
topic #5 (0.067): 0.015*"woman" + 0.009*"hospital" + 0.009*"police" + 0.009*"child" + 0.008*"house" + 0.005*"death" + 0.005*"medical" + 0.005*"murder" + 0.004*"black" + 0.004*"say"
topic #8 (0.067): 0.026*"album" + 0.026*"song" + 0.024*"music" + 0.017*"band" + 0.011*"single" + 0.009*"perform" + 0.008*"track" + 0.006*"tour" + 0.006*"video" + 0.006*"chart"
topic #12 (0.067): 0.024*"station" + 0.018*"company" + 0.011*"line" + 0.007*"operate" + 0.007*"railway" + 0.006*"public" + 0.006*"business" + 0.005*"market" + 0.005*"local" + 0.005*"building"
topic #10 (0.067): 0.010*"government" + 0.009*"election" + 0.008*"party" + 0.006*"law" + 0.006*"court" + 0.005*"vote" + 0.005*"political" + 0.005*"elect" + 0.004*"say" + 0.004*"act"
topic diff=0.150346, rho=0.137575
PROGRESS: pass 2, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.067): 0.012*"student" + 0.011*"university" + 0.011*"college" + 0.010*"study" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.007*"award" + 0.007*"director" + 0.006*"program"
topic #12 (0.067): 0.023*"station" + 0.019*"company" + 0.011*"line" + 0.007*"operate" + 0.007*"railway" + 0.006*"public" + 0.006*"business" + 0.005*"market" + 0.005*"building" + 0.005*"local"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.004*"study" + 0.004*"result" + 0.004*"term" + 0.004*"cell" + 0.004*"system" + 0.004*"case" + 0.004*"function" + 0.004*"type"
topic #14 (0.067): 0.012*"village" + 0.012*"population" + 0.011*"town" + 0.009*"river" + 0.008*"county" + 0.008*"park" + 0.008*"north" + 0.007*"building" + 0.007*"south" + 0.007*"road"
topic #5 (0.067): 0.015*"woman" + 0.010*"hospital" + 0.009*"child" + 0.008*"police" + 0.008*"house" + 0.005*"death" + 0.005*"medical" + 0.004*"murder" + 0.004*"say" + 0.004*"black"
topic diff=0.145982, rho=0.137575
PROGRESS: pass 2, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.010*"government" + 0.009*"election" + 0.008*"party" + 0.006*"law" + 0.006*"court" + 0.005*"political" + 0.005*"vote" + 0.005*"elect" + 0.004*"act" + 0.004*"say"
topic #6 (0.067): 0.020*"specie" + 0.015*"ship" + 0.010*"island" + 0.007*"water" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"boat" + 0.004*"vessel" + 0.004*"plant"
topic #9 (0.067): 0.011*"book" + 0.008*"series" + 0.007*"publish" + 0.006*"life" + 0.006*"episode" + 0.006*"say" + 0.005*"story" + 0.005*"novel" + 0.004*"appear" + 0.004*"television"
topic #2 (0.067): 0.031*"season" + 0.020*"game" + 0.018*"club" + 0.014*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.010*"match" + 0.010*"final" + 0.009*"finish"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.007*"engine" + 0.006*"power" + 0.006*"model" + 0.005*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic diff=0.135670, rho=0.137575
PROGRESS: pass 2, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.027*"game" + 0.010*"player" + 0.009*"system" + 0.007*"version" + 0.006*"computer" + 0.006*"code" + 0.005*"user" + 0.005*"software" + 0.005*"character" + 0.005*"product"
topic #4 (0.067): 0.015*"war" + 0.014*"army" + 0.013*"force" + 0.010*"battle" + 0.009*"military" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"kill" + 0.006*"unit"
topic #5 (0.067): 0.016*"woman" + 0.010*"hospital" + 0.009*"child" + 0.009*"police" + 0.008*"house" + 0.005*"death" + 0.005*"medical" + 0.004*"murder" + 0.004*"say" + 0.004*"prison"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.007*"engine" + 0.006*"power" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.004*"production"
topic #1 (0.067): 0.053*"film" + 0.011*"star" + 0.009*"series" + 0.007*"direct" + 0.007*"award" + 0.007*"character" + 0.006*"role" + 0.005*"movie" + 0.005*"appear" + 0.004*"story"
topic diff=0.132633, rho=0.137575
PROGRESS: pass 2, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.012*"village" + 0.012*"population" + 0.011*"town" + 0.010*"river" + 0.008*"county" + 0.008*"north" + 0.007*"park" + 0.007*"south" + 0.007*"building" + 0.006*"road"
topic #6 (0.067): 0.020*"specie" + 0.015*"ship" + 0.011*"island" + 0.008*"water" + 0.006*"genus" + 0.006*"describe" + 0.006*"sea" + 0.004*"plant" + 0.004*"boat" + 0.004*"vessel"
topic #12 (0.067): 0.023*"station" + 0.019*"company" + 0.011*"line" + 0.007*"operate" + 0.007*"railway" + 0.006*"business" + 0.006*"public" + 0.005*"building" + 0.005*"train" + 0.005*"radio"
topic #11 (0.067): 0.028*"game" + 0.011*"player" + 0.008*"system" + 0.006*"version" + 0.005*"computer" + 0.005*"code" + 0.005*"user" + 0.005*"software" + 0.005*"product" + 0.005*"character"
topic #8 (0.067): 0.028*"album" + 0.026*"song" + 0.025*"music" + 0.018*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.006*"tour" + 0.006*"video" + 0.006*"chart"
topic diff=0.132533, rho=0.137575
PROGRESS: pass 2, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.006*"law" + 0.006*"vote" + 0.005*"elect" + 0.005*"political" + 0.005*"court" + 0.004*"say" + 0.004*"support"
topic #4 (0.067): 0.015*"war" + 0.014*"army" + 0.014*"force" + 0.010*"battle" + 0.009*"attack" + 0.009*"military" + 0.008*"command" + 0.006*"german" + 0.006*"kill" + 0.006*"unit"
topic #5 (0.067): 0.016*"woman" + 0.010*"hospital" + 0.009*"child" + 0.008*"police" + 0.008*"house" + 0.005*"medical" + 0.005*"death" + 0.004*"murder" + 0.004*"say" + 0.004*"care"
topic #1 (0.067): 0.052*"film" + 0.012*"star" + 0.010*"series" + 0.007*"direct" + 0.006*"character" + 0.006*"award" + 0.006*"role" + 0.005*"movie" + 0.005*"appear" + 0.004*"story"
topic #6 (0.067): 0.020*"specie" + 0.015*"ship" + 0.011*"island" + 0.008*"water" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"vessel" + 0.004*"plant" + 0.004*"boat"
topic diff=0.134228, rho=0.137575
PROGRESS: pass 2, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.067): 0.013*"student" + 0.012*"university" + 0.011*"college" + 0.010*"study" + 0.009*"research" + 0.008*"award" + 0.008*"education" + 0.008*"science" + 0.007*"director" + 0.007*"program"
topic #6 (0.067): 0.021*"specie" + 0.015*"ship" + 0.011*"island" + 0.008*"water" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"vessel" + 0.004*"plant" + 0.004*"boat"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.019*"club" + 0.015*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #8 (0.067): 0.028*"album" + 0.027*"song" + 0.024*"music" + 0.018*"band" + 0.011*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #14 (0.067): 0.013*"village" + 0.013*"population" + 0.012*"town" + 0.009*"river" + 0.008*"county" + 0.008*"north" + 0.007*"park" + 0.007*"south" + 0.007*"road" + 0.007*"building"
topic diff=0.127281, rho=0.137575
PROGRESS: pass 2, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #8 (0.067): 0.029*"album" + 0.026*"song" + 0.024*"music" + 0.018*"band" + 0.011*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #7 (0.067): 0.023*"church" + 0.018*"art" + 0.010*"museum" + 0.008*"artist" + 0.007*"design" + 0.006*"painting" + 0.006*"german" + 0.005*"building" + 0.005*"study" + 0.004*"collection"
topic #1 (0.067): 0.052*"film" + 0.012*"star" + 0.009*"series" + 0.007*"direct" + 0.007*"character" + 0.006*"award" + 0.006*"role" + 0.005*"movie" + 0.005*"appear" + 0.005*"actor"
topic #11 (0.067): 0.027*"game" + 0.010*"player" + 0.009*"system" + 0.007*"version" + 0.006*"computer" + 0.006*"user" + 0.005*"software" + 0.005*"code" + 0.005*"datum" + 0.005*"information"
topic diff=0.122577, rho=0.137575
PROGRESS: pass 2, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"study" + 0.004*"cell" + 0.004*"system" + 0.004*"case" + 0.004*"function" + 0.004*"human"
topic #8 (0.067): 0.028*"album" + 0.027*"song" + 0.024*"music" + 0.018*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.006*"tour" + 0.006*"video" + 0.006*"chart"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"power" + 0.007*"car" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production" + 0.004*"vehicle"
topic #9 (0.067): 0.012*"book" + 0.008*"publish" + 0.007*"series" + 0.007*"say" + 0.006*"life" + 0.006*"episode" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"appear"
topic #10 (0.067): 0.010*"government" + 0.009*"election" + 0.009*"party" + 0.006*"law" + 0.006*"court" + 0.006*"vote" + 0.005*"elect" + 0.005*"political" + 0.004*"act" + 0.004*"support"
topic diff=0.116003, rho=0.137575
-8.345 per-word bound, 325.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #12 (0.067): 0.023*"station" + 0.020*"company" + 0.011*"line" + 0.008*"operate" + 0.007*"railway" + 0.006*"business" + 0.006*"public" + 0.005*"building" + 0.005*"market" + 0.005*"local"
topic #8 (0.067): 0.029*"album" + 0.027*"song" + 0.025*"music" + 0.018*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.006*"tour" + 0.006*"video" + 0.006*"chart"
topic #1 (0.067): 0.053*"film" + 0.012*"star" + 0.010*"series" + 0.008*"direct" + 0.007*"character" + 0.007*"award" + 0.006*"role" + 0.006*"movie" + 0.005*"appear" + 0.005*"actor"
topic #7 (0.067): 0.023*"church" + 0.017*"art" + 0.010*"museum" + 0.008*"artist" + 0.007*"design" + 0.006*"painting" + 0.005*"german" + 0.005*"building" + 0.005*"study" + 0.004*"collection"
topic #9 (0.067): 0.012*"book" + 0.008*"publish" + 0.007*"series" + 0.007*"say" + 0.006*"life" + 0.006*"episode" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"appear"
topic diff=0.115006, rho=0.137575
-8.346 per-word bound, 325.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #5 (0.067): 0.016*"woman" + 0.010*"child" + 0.009*"police" + 0.009*"hospital" + 0.007*"house" + 0.005*"death" + 0.005*"black" + 0.005*"medical" + 0.004*"murder" + 0.004*"say"
topic #14 (0.067): 0.013*"village" + 0.013*"population" + 0.011*"town" + 0.009*"county" + 0.009*"river" + 0.008*"north" + 0.008*"south" + 0.007*"park" + 0.007*"road" + 0.007*"building"
topic #9 (0.067): 0.012*"book" + 0.007*"publish" + 0.007*"series" + 0.007*"say" + 0.006*"life" + 0.006*"episode" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"appear"
topic #1 (0.067): 0.054*"film" + 0.012*"star" + 0.010*"series" + 0.008*"direct" + 0.007*"character" + 0.006*"award" + 0.006*"role" + 0.006*"movie" + 0.005*"actor" + 0.005*"appear"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic diff=0.115306, rho=0.137575
-8.300 per-word bound, 315.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.021*"specie" + 0.014*"ship" + 0.011*"island" + 0.007*"water" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"plant" + 0.004*"vessel" + 0.004*"boat"
topic #9 (0.067): 0.011*"book" + 0.007*"publish" + 0.007*"say" + 0.007*"series" + 0.006*"life" + 0.006*"episode" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"appear"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"vote" + 0.006*"court" + 0.006*"elect" + 0.005*"political" + 0.004*"act" + 0.004*"support"
topic #12 (0.067): 0.023*"station" + 0.021*"company" + 0.011*"line" + 0.008*"operate" + 0.007*"railway" + 0.006*"business" + 0.006*"public" + 0.005*"market" + 0.005*"building" + 0.005*"local"
topic #8 (0.067): 0.028*"album" + 0.027*"song" + 0.025*"music" + 0.018*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.006*"tour" + 0.006*"video" + 0.006*"chart"
topic diff=0.111635, rho=0.136291
PROGRESS: pass 3, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"production"
topic #5 (0.067): 0.015*"woman" + 0.010*"child" + 0.009*"hospital" + 0.009*"police" + 0.008*"house" + 0.005*"death" + 0.005*"black" + 0.005*"medical" + 0.004*"mother" + 0.004*"murder"
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.012*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #8 (0.067): 0.028*"album" + 0.027*"song" + 0.025*"music" + 0.019*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"elect" + 0.006*"vote" + 0.005*"political" + 0.004*"support" + 0.004*"act"
topic diff=0.107202, rho=0.136291
PROGRESS: pass 3, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.008*"car" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"production"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.012*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"elect" + 0.006*"vote" + 0.005*"political" + 0.004*"act" + 0.004*"support"
topic #1 (0.067): 0.053*"film" + 0.011*"star" + 0.010*"series" + 0.008*"direct" + 0.007*"character" + 0.006*"award" + 0.006*"role" + 0.006*"movie" + 0.005*"appear" + 0.005*"actor"
topic #6 (0.067): 0.021*"specie" + 0.014*"ship" + 0.012*"island" + 0.007*"water" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"plant" + 0.004*"vessel" + 0.004*"boat"
topic diff=0.100448, rho=0.136291
PROGRESS: pass 3, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.016*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.006*"german" + 0.006*"unit" + 0.006*"kill"
topic #5 (0.067): 0.015*"woman" + 0.010*"child" + 0.009*"police" + 0.008*"hospital" + 0.008*"house" + 0.005*"death" + 0.005*"murder" + 0.005*"black" + 0.004*"say" + 0.004*"prison"
topic #6 (0.067): 0.021*"specie" + 0.014*"ship" + 0.012*"island" + 0.008*"water" + 0.007*"genus" + 0.006*"describe" + 0.005*"plant" + 0.005*"sea" + 0.004*"boat" + 0.004*"vessel"
topic #11 (0.067): 0.030*"game" + 0.011*"player" + 0.009*"system" + 0.007*"version" + 0.007*"computer" + 0.006*"user" + 0.006*"information" + 0.005*"datum" + 0.005*"software" + 0.005*"code"
topic #7 (0.067): 0.020*"church" + 0.017*"art" + 0.010*"museum" + 0.008*"artist" + 0.007*"design" + 0.007*"painting" + 0.005*"building" + 0.005*"german" + 0.005*"collection" + 0.005*"exhibition"
topic diff=0.104054, rho=0.136291
PROGRESS: pass 3, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.011*"book" + 0.008*"publish" + 0.007*"say" + 0.007*"life" + 0.006*"series" + 0.006*"story" + 0.005*"episode" + 0.005*"novel" + 0.004*"writer" + 0.004*"appear"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"vote" + 0.006*"elect" + 0.005*"political" + 0.004*"act" + 0.004*"support"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #4 (0.067): 0.016*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.006*"unit" + 0.006*"german" + 0.006*"kill"
topic #1 (0.067): 0.053*"film" + 0.011*"star" + 0.011*"series" + 0.008*"direct" + 0.007*"character" + 0.007*"award" + 0.007*"role" + 0.006*"movie" + 0.006*"appear" + 0.005*"actor"
topic diff=0.098729, rho=0.136291
PROGRESS: pass 3, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.014*"woman" + 0.010*"child" + 0.009*"police" + 0.009*"hospital" + 0.007*"house" + 0.005*"death" + 0.005*"murder" + 0.005*"black" + 0.004*"say" + 0.004*"medical"
topic #13 (0.067): 0.014*"student" + 0.012*"college" + 0.012*"university" + 0.011*"study" + 0.009*"research" + 0.009*"award" + 0.008*"education" + 0.008*"science" + 0.007*"program" + 0.007*"director"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"vote" + 0.006*"elect" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #9 (0.067): 0.012*"book" + 0.008*"publish" + 0.007*"say" + 0.007*"life" + 0.006*"series" + 0.006*"story" + 0.005*"novel" + 0.005*"episode" + 0.004*"writer" + 0.004*"appear"
topic diff=0.105542, rho=0.136291
PROGRESS: pass 3, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.014*"woman" + 0.010*"child" + 0.009*"hospital" + 0.009*"police" + 0.008*"house" + 0.005*"death" + 0.005*"murder" + 0.004*"say" + 0.004*"black" + 0.004*"medical"
topic #13 (0.067): 0.013*"student" + 0.011*"college" + 0.011*"university" + 0.011*"study" + 0.009*"research" + 0.009*"award" + 0.008*"education" + 0.008*"science" + 0.007*"program" + 0.007*"director"
topic #11 (0.067): 0.029*"game" + 0.010*"player" + 0.009*"system" + 0.007*"version" + 0.006*"information" + 0.006*"computer" + 0.006*"user" + 0.005*"software" + 0.005*"datum" + 0.005*"product"
topic #7 (0.067): 0.020*"church" + 0.016*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"painting" + 0.005*"building" + 0.005*"german" + 0.005*"collection" + 0.004*"exhibition"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic diff=0.094361, rho=0.136291
PROGRESS: pass 3, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #6 (0.067): 0.021*"specie" + 0.013*"ship" + 0.012*"island" + 0.008*"water" + 0.006*"genus" + 0.006*"describe" + 0.006*"plant" + 0.005*"sea" + 0.005*"boat" + 0.004*"vessel"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"elect" + 0.006*"political" + 0.006*"vote" + 0.005*"act" + 0.004*"support"
topic #13 (0.067): 0.013*"student" + 0.012*"university" + 0.011*"college" + 0.011*"study" + 0.009*"award" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.007*"program" + 0.007*"international"
topic diff=0.090375, rho=0.136291
PROGRESS: pass 3, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic #7 (0.067): 0.021*"church" + 0.016*"art" + 0.010*"museum" + 0.008*"artist" + 0.007*"design" + 0.006*"painting" + 0.005*"building" + 0.005*"german" + 0.005*"th_century" + 0.005*"collection"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"term" + 0.005*"result" + 0.004*"case" + 0.004*"cell" + 0.004*"system" + 0.004*"study" + 0.004*"type" + 0.004*"human"
topic #6 (0.067): 0.021*"specie" + 0.014*"ship" + 0.012*"island" + 0.009*"water" + 0.006*"genus" + 0.006*"describe" + 0.006*"plant" + 0.006*"sea" + 0.005*"boat" + 0.004*"vessel"
topic #12 (0.067): 0.023*"station" + 0.021*"company" + 0.011*"line" + 0.008*"operate" + 0.007*"railway" + 0.006*"business" + 0.005*"public" + 0.005*"radio" + 0.005*"market" + 0.005*"building"
topic diff=0.089876, rho=0.136291
PROGRESS: pass 3, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.067): 0.051*"film" + 0.013*"series" + 0.011*"star" + 0.008*"direct" + 0.007*"character" + 0.007*"role" + 0.007*"award" + 0.006*"appear" + 0.006*"movie" + 0.005*"actor"
topic #12 (0.067): 0.024*"station" + 0.021*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.005*"building" + 0.005*"public" + 0.005*"market" + 0.005*"radio"
topic #4 (0.067): 0.016*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"order"
topic #14 (0.067): 0.014*"village" + 0.013*"population" + 0.012*"town" + 0.010*"river" + 0.009*"county" + 0.009*"north" + 0.008*"park" + 0.008*"south" + 0.007*"building" + 0.007*"road"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.019*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic diff=0.089865, rho=0.136291
PROGRESS: pass 3, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.011*"government" + 0.011*"election" + 0.010*"party" + 0.007*"law" + 0.006*"vote" + 0.006*"elect" + 0.006*"political" + 0.006*"court" + 0.005*"act" + 0.004*"support"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"term" + 0.004*"system" + 0.004*"case" + 0.004*"cell" + 0.004*"study" + 0.004*"human" + 0.004*"increase"
topic #5 (0.067): 0.015*"woman" + 0.010*"child" + 0.009*"hospital" + 0.009*"police" + 0.007*"house" + 0.005*"death" + 0.005*"medical" + 0.004*"murder" + 0.004*"say" + 0.004*"mother"
topic #4 (0.067): 0.016*"war" + 0.014*"army" + 0.014*"force" + 0.010*"battle" + 0.009*"military" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"kill"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.015*"league" + 0.013*"football" + 0.013*"championship" + 0.012*"player" + 0.011*"final" + 0.011*"match" + 0.009*"finish"
topic diff=0.092456, rho=0.136291
PROGRESS: pass 3, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.012*"book" + 0.008*"publish" + 0.007*"say" + 0.007*"life" + 0.006*"story" + 0.005*"series" + 0.005*"novel" + 0.005*"episode" + 0.004*"writer" + 0.004*"tell"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.006*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"production"
topic #5 (0.067): 0.015*"woman" + 0.011*"child" + 0.009*"hospital" + 0.008*"police" + 0.007*"house" + 0.006*"black" + 0.005*"death" + 0.004*"medical" + 0.004*"say" + 0.004*"mother"
topic #1 (0.067): 0.051*"film" + 0.013*"series" + 0.013*"star" + 0.008*"direct" + 0.007*"character" + 0.007*"role" + 0.006*"award" + 0.006*"movie" + 0.006*"appear" + 0.006*"episode"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.011*"college" + 0.011*"study" + 0.010*"research" + 0.010*"award" + 0.008*"education" + 0.008*"science" + 0.008*"program" + 0.007*"international"
topic diff=0.089333, rho=0.136291
PROGRESS: pass 3, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.014*"village" + 0.014*"population" + 0.012*"town" + 0.010*"river" + 0.009*"county" + 0.009*"north" + 0.008*"park" + 0.008*"south" + 0.007*"road" + 0.007*"building"
topic #4 (0.067): 0.016*"war" + 0.015*"army" + 0.013*"force" + 0.010*"battle" + 0.010*"military" + 0.009*"attack" + 0.008*"german" + 0.008*"command" + 0.006*"unit" + 0.006*"order"
topic #5 (0.067): 0.015*"woman" + 0.011*"child" + 0.008*"police" + 0.008*"hospital" + 0.007*"house" + 0.006*"death" + 0.005*"black" + 0.004*"medical" + 0.004*"mother" + 0.004*"say"
topic #11 (0.067): 0.027*"game" + 0.010*"system" + 0.009*"player" + 0.007*"version" + 0.006*"computer" + 0.006*"information" + 0.006*"datum" + 0.006*"user" + 0.005*"software" + 0.005*"code"
topic #13 (0.067): 0.013*"student" + 0.012*"university" + 0.011*"college" + 0.011*"study" + 0.010*"award" + 0.010*"research" + 0.008*"education" + 0.008*"science" + 0.008*"program" + 0.007*"international"
topic diff=0.085199, rho=0.136291
PROGRESS: pass 3, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"power" + 0.008*"system" + 0.007*"car" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production" + 0.005*"vehicle"
topic #0 (0.067): 0.006*"example" + 0.005*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.004*"study" + 0.004*"case" + 0.004*"cell" + 0.004*"human" + 0.004*"function"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"elect" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic #9 (0.067): 0.012*"book" + 0.008*"publish" + 0.007*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.005*"series" + 0.004*"episode" + 0.004*"writer" + 0.004*"tell"
topic #13 (0.067): 0.013*"student" + 0.012*"university" + 0.011*"college" + 0.011*"study" + 0.010*"award" + 0.009*"research" + 0.008*"science" + 0.008*"education" + 0.008*"program" + 0.007*"international"
topic diff=0.081248, rho=0.136291
-8.334 per-word bound, 322.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.023*"station" + 0.022*"company" + 0.011*"line" + 0.008*"operate" + 0.007*"business" + 0.007*"railway" + 0.005*"market" + 0.005*"public" + 0.005*"building" + 0.005*"sell"
topic #9 (0.067): 0.013*"book" + 0.008*"publish" + 0.007*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.005*"series" + 0.004*"episode" + 0.004*"writer" + 0.003*"tell"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.008*"car" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production" + 0.005*"vehicle"
topic #1 (0.067): 0.051*"film" + 0.013*"series" + 0.012*"star" + 0.008*"direct" + 0.007*"character" + 0.007*"role" + 0.007*"award" + 0.006*"episode" + 0.006*"appear" + 0.006*"movie"
topic #5 (0.067): 0.015*"woman" + 0.010*"child" + 0.009*"police" + 0.009*"hospital" + 0.007*"house" + 0.006*"death" + 0.005*"black" + 0.005*"medical" + 0.004*"say" + 0.004*"mother"
topic diff=0.081949, rho=0.136291
-8.336 per-word bound, 323.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #7 (0.067): 0.022*"church" + 0.016*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"painting" + 0.006*"german" + 0.005*"building" + 0.005*"son" + 0.005*"th_century"
topic #10 (0.067): 0.011*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"elect" + 0.006*"vote" + 0.006*"political" + 0.004*"act" + 0.004*"support"
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"term" + 0.005*"cell" + 0.005*"result" + 0.005*"system" + 0.004*"case" + 0.004*"study" + 0.004*"function" + 0.004*"method"
topic #13 (0.067): 0.014*"student" + 0.012*"university" + 0.012*"college" + 0.011*"study" + 0.010*"award" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.008*"program" + 0.007*"international"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic diff=0.084179, rho=0.136291
-8.291 per-word bound, 313.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1000 documents into a model of 49835 documents
topic #9 (0.067): 0.013*"book" + 0.008*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.005*"series" + 0.004*"writer" + 0.004*"episode" + 0.004*"tell"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"court" + 0.006*"vote" + 0.006*"elect" + 0.006*"political" + 0.005*"act" + 0.004*"general"
topic #11 (0.067): 0.029*"game" + 0.010*"system" + 0.010*"player" + 0.007*"version" + 0.006*"user" + 0.006*"computer" + 0.006*"code" + 0.006*"information" + 0.006*"datum" + 0.005*"design"
topic #14 (0.067): 0.014*"village" + 0.014*"population" + 0.012*"town" + 0.010*"county" + 0.009*"north" + 0.009*"river" + 0.008*"south" + 0.008*"park" + 0.008*"road" + 0.007*"building"
topic #1 (0.067): 0.053*"film" + 0.013*"series" + 0.011*"star" + 0.008*"direct" + 0.007*"role" + 0.007*"character" + 0.006*"award" + 0.006*"episode" + 0.006*"movie" + 0.006*"appear"
topic diff=0.092634, rho=0.136291
-8.300 per-word bound, 315.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.007*"power" + 0.007*"engine" + 0.006*"vehicle" + 0.006*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #1 (0.067): 0.052*"film" + 0.013*"series" + 0.011*"star" + 0.008*"direct" + 0.007*"character" + 0.007*"role" + 0.007*"episode" + 0.006*"movie" + 0.006*"award" + 0.006*"appear"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"vote" + 0.006*"elect" + 0.006*"court" + 0.006*"political" + 0.005*"act" + 0.004*"general"
topic #14 (0.067): 0.014*"village" + 0.014*"population" + 0.012*"town" + 0.009*"county" + 0.009*"river" + 0.009*"north" + 0.009*"building" + 0.008*"park" + 0.008*"south" + 0.008*"road"
topic diff=0.079432, rho=0.135043
PROGRESS: pass 4, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"elect" + 0.006*"court" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"general"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"order"
topic #12 (0.067): 0.023*"station" + 0.022*"company" + 0.011*"line" + 0.008*"railway" + 0.008*"operate" + 0.007*"business" + 0.006*"market" + 0.006*"public" + 0.005*"network" + 0.005*"building"
topic #11 (0.067): 0.029*"game" + 0.011*"player" + 0.010*"system" + 0.007*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"user" + 0.006*"datum" + 0.006*"code" + 0.005*"video"
topic #7 (0.067): 0.021*"church" + 0.016*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"painting" + 0.006*"german" + 0.005*"building" + 0.005*"son" + 0.005*"french"
topic diff=0.077393, rho=0.135043
PROGRESS: pass 4, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.023*"station" + 0.023*"company" + 0.012*"line" + 0.008*"railway" + 0.008*"operate" + 0.007*"business" + 0.006*"market" + 0.006*"public" + 0.005*"network" + 0.005*"sell"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"engine" + 0.007*"power" + 0.006*"vehicle" + 0.006*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic #8 (0.067): 0.029*"album" + 0.028*"song" + 0.026*"music" + 0.020*"band" + 0.012*"single" + 0.010*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #11 (0.067): 0.029*"game" + 0.011*"player" + 0.010*"system" + 0.007*"computer" + 0.007*"information" + 0.007*"version" + 0.006*"user" + 0.006*"datum" + 0.005*"design" + 0.005*"code"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.009*"party" + 0.008*"law" + 0.007*"court" + 0.006*"elect" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"general"
topic diff=0.071722, rho=0.135043
PROGRESS: pass 4, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"term" + 0.005*"result" + 0.004*"case" + 0.004*"system" + 0.004*"human" + 0.004*"cell" + 0.004*"study" + 0.004*"define"
topic #12 (0.067): 0.023*"company" + 0.023*"station" + 0.011*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.005*"public" + 0.005*"sell" + 0.005*"network"
topic #1 (0.067): 0.051*"film" + 0.013*"series" + 0.011*"star" + 0.008*"direct" + 0.007*"character" + 0.007*"role" + 0.007*"episode" + 0.006*"appear" + 0.006*"award" + 0.006*"movie"
topic #13 (0.067): 0.013*"student" + 0.012*"college" + 0.012*"university" + 0.012*"study" + 0.010*"award" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.008*"program" + 0.008*"international"
topic #7 (0.067): 0.020*"church" + 0.016*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"painting" + 0.006*"german" + 0.005*"building" + 0.005*"son" + 0.005*"french"
topic diff=0.075236, rho=0.135043
PROGRESS: pass 4, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"term" + 0.005*"result" + 0.004*"system" + 0.004*"case" + 0.004*"human" + 0.004*"study" + 0.004*"define" + 0.004*"cell"
topic #8 (0.067): 0.029*"album" + 0.028*"song" + 0.027*"music" + 0.020*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #7 (0.067): 0.019*"church" + 0.016*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"painting" + 0.005*"building" + 0.005*"german" + 0.005*"son" + 0.005*"th_century"
topic #13 (0.067): 0.013*"student" + 0.012*"college" + 0.012*"university" + 0.011*"study" + 0.010*"award" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.008*"program" + 0.007*"international"
topic #14 (0.067): 0.014*"population" + 0.014*"village" + 0.012*"town" + 0.010*"county" + 0.009*"river" + 0.009*"north" + 0.009*"park" + 0.008*"building" + 0.008*"south" + 0.008*"road"
topic diff=0.071823, rho=0.135043
PROGRESS: pass 4, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.012*"ship" + 0.008*"water" + 0.006*"genus" + 0.006*"plant" + 0.006*"describe" + 0.005*"sea" + 0.004*"boat" + 0.004*"fish"
topic #9 (0.067): 0.012*"book" + 0.008*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"series" + 0.004*"describe" + 0.004*"tell"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"power" + 0.007*"vehicle" + 0.007*"engine" + 0.007*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #1 (0.067): 0.051*"film" + 0.014*"series" + 0.011*"star" + 0.008*"direct" + 0.007*"episode" + 0.007*"character" + 0.007*"role" + 0.006*"appear" + 0.006*"award" + 0.006*"television"
topic diff=0.077989, rho=0.135043
PROGRESS: pass 4, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.029*"game" + 0.010*"system" + 0.010*"player" + 0.007*"version" + 0.007*"information" + 0.006*"computer" + 0.006*"user" + 0.006*"datum" + 0.005*"software" + 0.005*"code"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.009*"party" + 0.007*"law" + 0.006*"elect" + 0.006*"court" + 0.006*"political" + 0.006*"vote" + 0.005*"act" + 0.004*"general"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.012*"ship" + 0.008*"water" + 0.006*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"boat" + 0.004*"fish"
topic #7 (0.067): 0.020*"church" + 0.015*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"painting" + 0.006*"building" + 0.005*"german" + 0.005*"son" + 0.005*"th_century"
topic #14 (0.067): 0.014*"village" + 0.014*"population" + 0.013*"town" + 0.010*"county" + 0.010*"river" + 0.009*"north" + 0.009*"park" + 0.008*"south" + 0.008*"building" + 0.008*"road"
topic diff=0.069160, rho=0.135043
PROGRESS: pass 4, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"vehicle" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.009*"party" + 0.008*"law" + 0.006*"elect" + 0.006*"court" + 0.006*"political" + 0.006*"vote" + 0.005*"act" + 0.004*"general"
topic #1 (0.067): 0.049*"film" + 0.015*"series" + 0.011*"star" + 0.008*"episode" + 0.007*"direct" + 0.007*"character" + 0.007*"role" + 0.007*"award" + 0.006*"television" + 0.006*"appear"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"order"
topic #7 (0.067): 0.020*"church" + 0.015*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"building" + 0.006*"painting" + 0.006*"german" + 0.005*"son" + 0.005*"th_century"
topic diff=0.065680, rho=0.135043
PROGRESS: pass 4, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"series" + 0.004*"describe" + 0.003*"tell"
topic #8 (0.067): 0.029*"album" + 0.028*"song" + 0.027*"music" + 0.020*"band" + 0.012*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #11 (0.067): 0.029*"game" + 0.010*"system" + 0.010*"player" + 0.007*"version" + 0.006*"information" + 0.006*"computer" + 0.006*"datum" + 0.006*"code" + 0.005*"user" + 0.005*"software"
topic #14 (0.067): 0.014*"village" + 0.014*"population" + 0.013*"town" + 0.011*"river" + 0.010*"county" + 0.009*"north" + 0.009*"park" + 0.008*"south" + 0.008*"road" + 0.008*"building"
topic #6 (0.067): 0.021*"specie" + 0.013*"island" + 0.012*"ship" + 0.009*"water" + 0.006*"genus" + 0.006*"plant" + 0.006*"describe" + 0.006*"sea" + 0.004*"boat" + 0.004*"fish"
topic diff=0.067157, rho=0.135043
PROGRESS: pass 4, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.019*"club" + 0.015*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #5 (0.067): 0.014*"woman" + 0.011*"child" + 0.009*"hospital" + 0.009*"police" + 0.007*"house" + 0.006*"death" + 0.005*"mother" + 0.005*"say" + 0.005*"murder" + 0.004*"wife"
topic #1 (0.067): 0.050*"film" + 0.015*"series" + 0.012*"star" + 0.008*"episode" + 0.008*"direct" + 0.007*"character" + 0.007*"role" + 0.006*"award" + 0.006*"appear" + 0.006*"television"
topic #13 (0.067): 0.014*"student" + 0.012*"university" + 0.012*"college" + 0.012*"study" + 0.011*"award" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.008*"international" + 0.008*"director"
topic #11 (0.067): 0.028*"game" + 0.010*"player" + 0.010*"system" + 0.007*"version" + 0.006*"information" + 0.006*"computer" + 0.006*"datum" + 0.006*"user" + 0.006*"software" + 0.005*"code"
topic diff=0.063874, rho=0.135043
PROGRESS: pass 4, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.007*"car" + 0.007*"system" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.004*"production"
topic #1 (0.067): 0.049*"film" + 0.015*"series" + 0.012*"star" + 0.008*"episode" + 0.007*"character" + 0.007*"direct" + 0.007*"role" + 0.006*"award" + 0.006*"appear" + 0.006*"television"
topic #6 (0.067): 0.021*"specie" + 0.012*"island" + 0.012*"ship" + 0.009*"water" + 0.006*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"boat" + 0.004*"fish"
topic #8 (0.067): 0.031*"album" + 0.029*"song" + 0.027*"music" + 0.020*"band" + 0.012*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #5 (0.067): 0.014*"woman" + 0.011*"child" + 0.009*"police" + 0.008*"hospital" + 0.007*"house" + 0.006*"death" + 0.005*"say" + 0.005*"mother" + 0.005*"murder" + 0.004*"wife"
topic diff=0.069076, rho=0.135043
PROGRESS: pass 4, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.015*"village" + 0.014*"population" + 0.013*"town" + 0.010*"river" + 0.010*"county" + 0.010*"north" + 0.009*"south" + 0.008*"park" + 0.008*"road" + 0.008*"building"
topic #12 (0.067): 0.024*"station" + 0.022*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"radio" + 0.005*"sell" + 0.005*"public"
topic #10 (0.067): 0.012*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.006*"vote" + 0.006*"elect" + 0.006*"political" + 0.006*"court" + 0.005*"act" + 0.004*"support"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.015*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.012*"study" + 0.012*"college" + 0.011*"award" + 0.010*"research" + 0.008*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic diff=0.068454, rho=0.135043
PROGRESS: pass 4, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"series" + 0.004*"describe" + 0.003*"author"
topic #5 (0.067): 0.014*"woman" + 0.011*"child" + 0.008*"police" + 0.008*"hospital" + 0.007*"house" + 0.006*"death" + 0.005*"black" + 0.005*"mother" + 0.005*"say" + 0.004*"murder"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"battle" + 0.010*"military" + 0.009*"attack" + 0.008*"german" + 0.008*"command" + 0.006*"unit" + 0.006*"order"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.018*"club" + 0.015*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #0 (0.067): 0.006*"example" + 0.005*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.004*"case" + 0.004*"study" + 0.004*"cell" + 0.004*"human" + 0.004*"effect"
topic diff=0.065184, rho=0.135043
PROGRESS: pass 4, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"example" + 0.005*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.004*"study" + 0.004*"cell" + 0.004*"human" + 0.004*"function"
topic #3 (0.067): 0.011*"design" + 0.008*"power" + 0.008*"system" + 0.008*"car" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"vehicle" + 0.005*"production"
topic #12 (0.067): 0.023*"station" + 0.023*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"public"
topic #11 (0.067): 0.029*"game" + 0.011*"system" + 0.010*"player" + 0.007*"version" + 0.006*"information" + 0.006*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"design" + 0.005*"code"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.010*"party" + 0.008*"law" + 0.007*"court" + 0.006*"elect" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic diff=0.062772, rho=0.135043
-8.329 per-word bound, 321.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.014*"woman" + 0.011*"child" + 0.009*"police" + 0.008*"hospital" + 0.006*"house" + 0.006*"death" + 0.005*"black" + 0.005*"mother" + 0.005*"say" + 0.004*"wife"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.008*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"vehicle" + 0.005*"production"
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.013*"town" + 0.010*"county" + 0.010*"river" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.008*"road" + 0.008*"building"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.009*"player" + 0.007*"version" + 0.007*"information" + 0.007*"computer" + 0.006*"code" + 0.006*"user" + 0.006*"datum" + 0.005*"software"
topic #13 (0.067): 0.014*"student" + 0.012*"university" + 0.012*"college" + 0.012*"study" + 0.011*"award" + 0.009*"research" + 0.008*"education" + 0.008*"science" + 0.008*"program" + 0.007*"director"
topic diff=0.062252, rho=0.135043
-8.330 per-word bound, 321.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"cell" + 0.005*"system" + 0.005*"case" + 0.004*"study" + 0.004*"function" + 0.004*"human"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.003*"author" + 0.003*"magazine"
topic #8 (0.067): 0.030*"album" + 0.029*"song" + 0.027*"music" + 0.019*"band" + 0.012*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"german" + 0.008*"command" + 0.006*"unit" + 0.006*"order"
topic diff=0.059391, rho=0.135043
-8.296 per-word bound, 314.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.015*"village" + 0.014*"population" + 0.012*"town" + 0.010*"county" + 0.010*"river" + 0.009*"north" + 0.009*"building" + 0.009*"south" + 0.009*"park" + 0.008*"road"
topic #8 (0.067): 0.030*"album" + 0.029*"song" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.004*"case" + 0.004*"study" + 0.004*"cell" + 0.004*"function" + 0.004*"increase"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.012*"study" + 0.012*"college" + 0.011*"award" + 0.009*"research" + 0.008*"education" + 0.008*"program" + 0.008*"science" + 0.007*"international"
topic #7 (0.067): 0.021*"church" + 0.015*"art" + 0.010*"museum" + 0.007*"artist" + 0.007*"design" + 0.006*"german" + 0.006*"son" + 0.006*"painting" + 0.005*"building" + 0.005*"th_century"
topic diff=0.061679, rho=0.133828
PROGRESS: pass 5, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"order"
topic #14 (0.067): 0.015*"village" + 0.014*"population" + 0.013*"town" + 0.010*"river" + 0.010*"county" + 0.009*"north" + 0.009*"building" + 0.009*"park" + 0.009*"south" + 0.008*"road"
topic #8 (0.067): 0.030*"album" + 0.029*"song" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #10 (0.067): 0.012*"government" + 0.011*"election" + 0.009*"party" + 0.008*"law" + 0.007*"elect" + 0.006*"court" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic #13 (0.067): 0.014*"student" + 0.012*"university" + 0.012*"study" + 0.012*"college" + 0.011*"award" + 0.009*"research" + 0.009*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic diff=0.062168, rho=0.133828
PROGRESS: pass 5, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"production"
topic #6 (0.067): 0.022*"specie" + 0.013*"island" + 0.011*"ship" + 0.008*"water" + 0.007*"genus" + 0.006*"plant" + 0.006*"describe" + 0.005*"sea" + 0.004*"boat" + 0.004*"fish"
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.013*"town" + 0.010*"county" + 0.010*"river" + 0.009*"north" + 0.009*"building" + 0.009*"park" + 0.009*"south" + 0.008*"road"
topic #1 (0.067): 0.050*"film" + 0.015*"series" + 0.011*"star" + 0.010*"episode" + 0.008*"direct" + 0.008*"character" + 0.007*"role" + 0.007*"television" + 0.007*"appear" + 0.006*"award"
topic #13 (0.067): 0.014*"student" + 0.012*"college" + 0.012*"university" + 0.012*"study" + 0.011*"award" + 0.009*"research" + 0.009*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic diff=0.057253, rho=0.133828
PROGRESS: pass 5, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.024*"company" + 0.023*"station" + 0.011*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"network"
topic #6 (0.067): 0.022*"specie" + 0.013*"island" + 0.010*"ship" + 0.008*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"bird" + 0.004*"boat"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"system" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.006*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"production"
topic #1 (0.067): 0.049*"film" + 0.015*"series" + 0.011*"star" + 0.009*"episode" + 0.008*"character" + 0.007*"direct" + 0.007*"role" + 0.007*"television" + 0.007*"appear" + 0.006*"award"
topic #4 (0.067): 0.018*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"order"
topic diff=0.060340, rho=0.133828
PROGRESS: pass 5, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"term" + 0.005*"result" + 0.005*"case" + 0.005*"system" + 0.004*"human" + 0.004*"increase" + 0.004*"study" + 0.004*"function"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"operation"
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.009*"police" + 0.008*"hospital" + 0.006*"house" + 0.006*"death" + 0.005*"say" + 0.005*"murder" + 0.005*"mother" + 0.005*"black"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.009*"party" + 0.008*"law" + 0.007*"court" + 0.007*"elect" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"system" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.006*"aircraft" + 0.006*"vehicle" + 0.005*"produce" + 0.005*"production"
topic diff=0.058679, rho=0.133828
PROGRESS: pass 5, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.024*"station" + 0.023*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"public"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"order"
topic #5 (0.067): 0.012*"woman" + 0.012*"child" + 0.009*"police" + 0.008*"hospital" + 0.006*"house" + 0.006*"death" + 0.005*"say" + 0.005*"murder" + 0.005*"mother" + 0.005*"wife"
topic #8 (0.067): 0.029*"album" + 0.029*"song" + 0.028*"music" + 0.019*"band" + 0.012*"single" + 0.010*"perform" + 0.008*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic diff=0.063205, rho=0.133828
PROGRESS: pass 5, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 10
PROGRESS: pass 5, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.024*"station" + 0.023*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"public"
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.014*"town" + 0.010*"county" + 0.010*"river" + 0.009*"north" + 0.009*"park" + 0.009*"south" + 0.009*"building" + 0.009*"road"
topic #5 (0.067): 0.012*"woman" + 0.012*"child" + 0.008*"police" + 0.008*"hospital" + 0.007*"house" + 0.006*"death" + 0.005*"mother" + 0.005*"say" + 0.005*"wife" + 0.005*"murder"
topic #7 (0.067): 0.020*"church" + 0.015*"art" + 0.010*"museum" + 0.007*"design" + 0.007*"artist" + 0.006*"son" + 0.006*"german" + 0.006*"building" + 0.006*"painting" + 0.005*"th_century"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"term" + 0.005*"system" + 0.005*"case" + 0.004*"cell" + 0.004*"study" + 0.004*"function" + 0.004*"method"
topic diff=0.056080, rho=0.133828
PROGRESS: pass 5, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"result" + 0.005*"term" + 0.005*"case" + 0.004*"system" + 0.004*"cell" + 0.004*"human" + 0.004*"study" + 0.004*"function"
topic #8 (0.067): 0.029*"song" + 0.029*"album" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.008*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #14 (0.067): 0.015*"village" + 0.014*"population" + 0.013*"town" + 0.010*"county" + 0.010*"river" + 0.010*"north" + 0.009*"park" + 0.009*"south" + 0.009*"building" + 0.008*"road"
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #13 (0.067): 0.014*"student" + 0.012*"university" + 0.012*"college" + 0.012*"study" + 0.011*"award" + 0.009*"research" + 0.008*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic diff=0.053334, rho=0.133828
PROGRESS: pass 5, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"system" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.006*"vote" + 0.006*"political" + 0.006*"court" + 0.005*"act" + 0.004*"support"
topic #13 (0.067): 0.014*"student" + 0.012*"college" + 0.012*"university" + 0.012*"study" + 0.011*"award" + 0.009*"research" + 0.008*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"term" + 0.005*"case" + 0.004*"system" + 0.004*"cell" + 0.004*"human" + 0.004*"study" + 0.004*"type"
topic #6 (0.067): 0.021*"specie" + 0.013*"island" + 0.010*"ship" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"fish" + 0.005*"bird"
topic diff=0.055232, rho=0.133828
PROGRESS: pass 5, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.008*"german" + 0.007*"command" + 0.006*"unit" + 0.006*"order"
topic #1 (0.067): 0.048*"film" + 0.017*"series" + 0.011*"star" + 0.010*"episode" + 0.008*"character" + 0.007*"direct" + 0.007*"role" + 0.007*"television" + 0.007*"appear" + 0.006*"award"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.006*"political" + 0.006*"vote" + 0.006*"court" + 0.005*"act" + 0.004*"support"
topic #5 (0.067): 0.013*"woman" + 0.011*"child" + 0.009*"police" + 0.008*"hospital" + 0.006*"house" + 0.006*"death" + 0.005*"say" + 0.005*"mother" + 0.005*"wife" + 0.005*"father"
topic #14 (0.067): 0.015*"village" + 0.014*"population" + 0.013*"town" + 0.011*"river" + 0.010*"county" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.008*"building" + 0.008*"road"
topic diff=0.054523, rho=0.133828
PROGRESS: pass 5, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.024*"station" + 0.023*"company" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"train"
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.008*"police" + 0.008*"hospital" + 0.007*"house" + 0.006*"death" + 0.005*"mother" + 0.005*"say" + 0.005*"wife" + 0.005*"father"
topic #8 (0.067): 0.031*"album" + 0.029*"song" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #3 (0.067): 0.011*"design" + 0.008*"power" + 0.007*"car" + 0.007*"system" + 0.007*"engine" + 0.006*"model" + 0.006*"vehicle" + 0.006*"aircraft" + 0.005*"produce" + 0.005*"space"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.003*"author" + 0.003*"magazine"
topic diff=0.057732, rho=0.133828
PROGRESS: pass 5, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"ship" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"bird" + 0.004*"fish"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.009*"german" + 0.008*"command" + 0.006*"unit" + 0.006*"order"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.004*"author" + 0.003*"magazine"
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.004*"case" + 0.004*"study" + 0.004*"increase" + 0.004*"human" + 0.004*"cell"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.015*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic diff=0.055440, rho=0.133828
PROGRESS: pass 5, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"battle" + 0.010*"military" + 0.009*"attack" + 0.008*"german" + 0.008*"command" + 0.006*"order" + 0.006*"unit"
topic #12 (0.067): 0.024*"station" + 0.023*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"business" + 0.007*"railway" + 0.006*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"building"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.012*"study" + 0.012*"college" + 0.011*"award" + 0.010*"research" + 0.008*"program" + 0.008*"education" + 0.008*"international" + 0.008*"science"
topic #0 (0.067): 0.006*"example" + 0.005*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.004*"case" + 0.004*"study" + 0.004*"cell" + 0.004*"human" + 0.004*"increase"
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.009*"player" + 0.007*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"code" + 0.005*"design"
topic diff=0.055515, rho=0.133828
PROGRESS: pass 5, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.013*"town" + 0.011*"county" + 0.010*"river" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.009*"road" + 0.008*"building"
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.008*"police" + 0.008*"hospital" + 0.006*"death" + 0.006*"house" + 0.005*"mother" + 0.005*"black" + 0.005*"say" + 0.005*"wife"
topic #11 (0.067): 0.028*"game" + 0.011*"system" + 0.009*"player" + 0.007*"information" + 0.007*"version" + 0.006*"datum" + 0.006*"computer" + 0.006*"user" + 0.005*"design" + 0.005*"code"
topic #8 (0.067): 0.031*"album" + 0.029*"song" + 0.027*"music" + 0.019*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #10 (0.067): 0.012*"government" + 0.010*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.007*"court" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic diff=0.052926, rho=0.133828
-8.325 per-word bound, 320.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.009*"police" + 0.008*"hospital" + 0.007*"death" + 0.006*"house" + 0.005*"mother" + 0.005*"say" + 0.005*"black" + 0.005*"wife"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.006*"court" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.004*"support"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.012*"college" + 0.012*"study" + 0.011*"award" + 0.009*"research" + 0.008*"program" + 0.008*"education" + 0.008*"science" + 0.008*"director"
topic #2 (0.067): 0.032*"season" + 0.021*"game" + 0.019*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.008*"power" + 0.008*"car" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"light"
topic diff=0.054986, rho=0.133828
-8.327 per-word bound, 321.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #6 (0.067): 0.021*"specie" + 0.012*"island" + 0.009*"ship" + 0.008*"water" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"fish" + 0.004*"tree"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.009*"player" + 0.007*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"user" + 0.006*"code" + 0.006*"datum" + 0.005*"design"
topic #7 (0.067): 0.021*"church" + 0.015*"art" + 0.010*"museum" + 0.007*"german" + 0.007*"artist" + 0.007*"son" + 0.007*"design" + 0.006*"painting" + 0.006*"building" + 0.005*"th_century"
topic #1 (0.067): 0.050*"film" + 0.016*"series" + 0.011*"star" + 0.011*"episode" + 0.008*"character" + 0.007*"direct" + 0.007*"role" + 0.007*"television" + 0.007*"appear" + 0.006*"award"
topic diff=0.047393, rho=0.133828
-8.284 per-word bound, 311.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1000 documents into a model of 49835 documents
topic #1 (0.067): 0.050*"film" + 0.016*"series" + 0.011*"star" + 0.011*"episode" + 0.008*"character" + 0.008*"direct" + 0.007*"role" + 0.007*"television" + 0.006*"appear" + 0.006*"award"
topic #0 (0.067): 0.006*"cell" + 0.006*"example" + 0.005*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.004*"study" + 0.004*"increase" + 0.004*"function"
topic #12 (0.067): 0.024*"company" + 0.023*"station" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"local"
topic #7 (0.067): 0.021*"church" + 0.014*"art" + 0.010*"museum" + 0.007*"son" + 0.007*"artist" + 0.007*"design" + 0.007*"german" + 0.006*"building" + 0.005*"painting" + 0.005*"th_century"
topic #6 (0.067): 0.022*"specie" + 0.011*"island" + 0.008*"ship" + 0.008*"water" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"boat" + 0.004*"fish"
topic diff=0.077738, rho=0.133828
-8.296 per-word bound, 314.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"court" + 0.006*"political" + 0.005*"act" + 0.005*"general"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.006*"cell" + 0.005*"term" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.004*"study" + 0.004*"increase" + 0.004*"function"
topic #5 (0.067): 0.013*"woman" + 0.013*"child" + 0.009*"police" + 0.008*"hospital" + 0.007*"house" + 0.007*"death" + 0.005*"mother" + 0.005*"black" + 0.005*"marry" + 0.005*"wife"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"unit" + 0.006*"ship"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"player" + 0.013*"football" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic diff=0.051669, rho=0.132645
PROGRESS: pass 6, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.012*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.003*"author" + 0.003*"magazine"
topic #8 (0.067): 0.030*"album" + 0.030*"song" + 0.028*"music" + 0.019*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.008*"water" + 0.008*"ship" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"boat" + 0.004*"fish"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.007*"vote" + 0.007*"court" + 0.006*"political" + 0.005*"act" + 0.004*"general"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.011*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"design" + 0.006*"code" + 0.005*"user"
topic diff=0.050837, rho=0.132645
PROGRESS: pass 6, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.012*"player" + 0.012*"football" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.008*"ship" + 0.008*"water" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.004*"bird" + 0.004*"boat"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.011*"player" + 0.008*"information" + 0.007*"version" + 0.007*"computer" + 0.006*"datum" + 0.006*"user" + 0.006*"design" + 0.005*"code"
topic #8 (0.067): 0.030*"album" + 0.030*"song" + 0.028*"music" + 0.019*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"court" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"act" + 0.005*"general"
topic diff=0.052057, rho=0.132645
PROGRESS: pass 6, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.012*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.003*"great" + 0.003*"author"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"court" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"act" + 0.005*"support"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"power" + 0.007*"system" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"production"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.008*"water" + 0.008*"ship" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"bird" + 0.005*"boat"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.006*"german" + 0.006*"ship" + 0.006*"unit"
topic diff=0.051378, rho=0.132645
PROGRESS: pass 6, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.009*"museum" + 0.007*"son" + 0.007*"artist" + 0.006*"design" + 0.006*"german" + 0.006*"painting" + 0.006*"building" + 0.005*"th_century"
topic #14 (0.067): 0.015*"population" + 0.015*"village" + 0.013*"town" + 0.010*"county" + 0.010*"north" + 0.010*"river" + 0.009*"park" + 0.009*"south" + 0.009*"building" + 0.008*"road"
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.009*"police" + 0.008*"hospital" + 0.007*"death" + 0.006*"house" + 0.005*"say" + 0.005*"mother" + 0.005*"wife" + 0.005*"marry"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"battle" + 0.008*"attack" + 0.008*"command" + 0.006*"unit" + 0.006*"german" + 0.006*"ship"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.003*"author" + 0.003*"magazine"
topic diff=0.050310, rho=0.132645
PROGRESS: pass 6, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"code" + 0.005*"consumer"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"hospital" + 0.006*"death" + 0.006*"house" + 0.005*"say" + 0.005*"mother" + 0.005*"father" + 0.005*"marry"
topic #12 (0.067): 0.024*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"network"
topic #13 (0.067): 0.015*"student" + 0.013*"college" + 0.012*"study" + 0.012*"university" + 0.011*"award" + 0.009*"research" + 0.009*"education" + 0.008*"program" + 0.008*"international" + 0.008*"science"
topic diff=0.053939, rho=0.132645
PROGRESS: pass 6, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.009*"museum" + 0.007*"son" + 0.007*"design" + 0.007*"artist" + 0.006*"german" + 0.006*"building" + 0.006*"painting" + 0.005*"th_century"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.007*"ship" + 0.006*"german" + 0.006*"unit"
topic #13 (0.067): 0.014*"student" + 0.012*"college" + 0.012*"study" + 0.012*"university" + 0.011*"award" + 0.009*"research" + 0.008*"education" + 0.008*"program" + 0.008*"international" + 0.008*"science"
topic #8 (0.067): 0.030*"song" + 0.029*"album" + 0.028*"music" + 0.019*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"cell" + 0.005*"result" + 0.005*"term" + 0.005*"system" + 0.005*"case" + 0.004*"study" + 0.004*"function" + 0.004*"increase"
topic diff=0.048073, rho=0.132645
PROGRESS: pass 6, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.020*"church" + 0.013*"art" + 0.009*"museum" + 0.007*"son" + 0.007*"design" + 0.007*"artist" + 0.006*"german" + 0.006*"building" + 0.005*"painting" + 0.005*"th_century"
topic #1 (0.067): 0.047*"film" + 0.017*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"role" + 0.007*"direct" + 0.007*"appear" + 0.007*"award"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.007*"command" + 0.007*"ship" + 0.006*"german" + 0.006*"unit"
topic #12 (0.067): 0.024*"station" + 0.023*"company" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"radio" + 0.006*"sell" + 0.005*"train"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.006*"court" + 0.006*"vote" + 0.006*"political" + 0.006*"act" + 0.004*"support"
topic diff=0.045826, rho=0.132645
PROGRESS: pass 6, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"cell" + 0.005*"term" + 0.005*"case" + 0.004*"system" + 0.004*"human" + 0.004*"increase" + 0.004*"study"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.008*"attack" + 0.007*"german" + 0.007*"command" + 0.007*"ship" + 0.006*"unit"
topic #1 (0.067): 0.048*"film" + 0.017*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"direct" + 0.007*"role" + 0.007*"appear" + 0.007*"award"
topic #3 (0.067): 0.012*"design" + 0.008*"car" + 0.007*"power" + 0.007*"engine" + 0.007*"system" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.018*"club" + 0.015*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic diff=0.047913, rho=0.132645
PROGRESS: pass 6, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.010*"player" + 0.007*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"code" + 0.005*"design" + 0.005*"provide"
topic #3 (0.067): 0.011*"design" + 0.007*"engine" + 0.007*"power" + 0.007*"car" + 0.007*"system" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"light"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.003*"author" + 0.003*"magazine"
topic #13 (0.067): 0.015*"student" + 0.013*"university" + 0.012*"study" + 0.012*"college" + 0.012*"award" + 0.010*"research" + 0.008*"program" + 0.008*"education" + 0.008*"international" + 0.008*"science"
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.008*"police" + 0.008*"hospital" + 0.007*"death" + 0.006*"house" + 0.005*"say" + 0.005*"wife" + 0.005*"mother" + 0.005*"father"
topic diff=0.047325, rho=0.132645
PROGRESS: pass 6, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.007*"ship" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"bird" + 0.005*"fish"
topic #12 (0.067): 0.024*"station" + 0.024*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"train"
topic #4 (0.067): 0.016*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.008*"command" + 0.008*"german" + 0.007*"ship" + 0.006*"unit"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.005*"cell" + 0.004*"case" + 0.004*"increase" + 0.004*"study" + 0.004*"human"
topic #13 (0.067): 0.015*"student" + 0.013*"university" + 0.012*"study" + 0.012*"college" + 0.012*"award" + 0.010*"research" + 0.008*"education" + 0.008*"program" + 0.008*"international" + 0.008*"director"
topic diff=0.052546, rho=0.132645
PROGRESS: pass 6, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.008*"german" + 0.008*"command" + 0.007*"ship" + 0.006*"unit"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.006*"court" + 0.005*"act" + 0.005*"support"
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.014*"town" + 0.011*"river" + 0.010*"county" + 0.010*"north" + 0.009*"south" + 0.009*"road" + 0.009*"park" + 0.008*"building"
topic #3 (0.067): 0.011*"design" + 0.008*"system" + 0.007*"car" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.004*"space"
topic #11 (0.067): 0.027*"game" + 0.012*"system" + 0.010*"player" + 0.007*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.005*"code" + 0.005*"user" + 0.005*"design"
topic diff=0.048481, rho=0.132645
PROGRESS: pass 6, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.008*"german" + 0.008*"ship" + 0.006*"order"
topic #12 (0.067): 0.025*"station" + 0.024*"company" + 0.012*"line" + 0.008*"operate" + 0.007*"business" + 0.007*"railway" + 0.006*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"network"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.004*"author" + 0.003*"way"
topic #7 (0.067): 0.022*"church" + 0.015*"art" + 0.009*"museum" + 0.008*"german" + 0.007*"son" + 0.007*"artist" + 0.006*"design" + 0.006*"painting" + 0.006*"building" + 0.006*"th_century"
topic #0 (0.067): 0.006*"example" + 0.005*"displaystyle" + 0.005*"result" + 0.005*"term" + 0.005*"system" + 0.005*"cell" + 0.004*"case" + 0.004*"study" + 0.004*"increase" + 0.004*"human"
topic diff=0.046314, rho=0.132645
PROGRESS: pass 6, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #11 (0.067): 0.028*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"datum" + 0.006*"computer" + 0.006*"user" + 0.006*"design" + 0.005*"code"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.013*"study" + 0.012*"college" + 0.012*"award" + 0.010*"research" + 0.008*"program" + 0.008*"education" + 0.008*"international" + 0.008*"science"
topic #5 (0.067): 0.013*"woman" + 0.012*"child" + 0.008*"police" + 0.007*"hospital" + 0.007*"death" + 0.006*"house" + 0.005*"mother" + 0.005*"say" + 0.005*"marry" + 0.005*"wife"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.008*"german" + 0.007*"ship" + 0.006*"order"
topic diff=0.045460, rho=0.132645
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"court" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.005*"general"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"code" + 0.006*"user" + 0.005*"design"
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"term" + 0.005*"result" + 0.005*"cell" + 0.005*"system" + 0.005*"case" + 0.004*"function" + 0.004*"study" + 0.004*"increase"
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #9 (0.067): 0.014*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.004*"author" + 0.003*"great"
topic diff=0.048787, rho=0.132645
-8.324 per-word bound, 320.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"cell" + 0.005*"result" + 0.005*"term" + 0.005*"case" + 0.005*"system" + 0.004*"increase" + 0.004*"function" + 0.004*"study"
topic #13 (0.067): 0.015*"student" + 0.013*"college" + 0.012*"university" + 0.012*"study" + 0.012*"award" + 0.009*"research" + 0.009*"education" + 0.008*"program" + 0.008*"international" + 0.008*"science"
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.013*"town" + 0.011*"county" + 0.010*"river" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.009*"road" + 0.008*"building"
topic #1 (0.067): 0.049*"film" + 0.017*"series" + 0.011*"star" + 0.011*"episode" + 0.008*"character" + 0.008*"direct" + 0.007*"television" + 0.007*"role" + 0.007*"appear" + 0.006*"award"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic diff=0.044938, rho=0.132645
-8.283 per-word bound, 311.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"ship" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"system" + 0.007*"power" + 0.007*"engine" + 0.007*"vehicle" + 0.006*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic #7 (0.067): 0.020*"church" + 0.014*"art" + 0.010*"museum" + 0.008*"son" + 0.007*"german" + 0.007*"king" + 0.006*"design" + 0.006*"artist" + 0.006*"building" + 0.006*"house"
topic #11 (0.067): 0.028*"game" + 0.012*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"user" + 0.006*"computer" + 0.006*"code" + 0.006*"datum" + 0.006*"design"
topic #9 (0.067): 0.014*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"magazine"
topic diff=0.060487, rho=0.132645
-8.291 per-word bound, 313.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"court" + 0.006*"political" + 0.005*"act" + 0.005*"general"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.006*"version" + 0.006*"computer" + 0.006*"user" + 0.006*"datum" + 0.006*"code" + 0.005*"design"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.003*"magazine" + 0.003*"author"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"ship" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.005*"cell" + 0.004*"study" + 0.004*"case" + 0.004*"increase" + 0.004*"effect"
topic diff=0.047461, rho=0.131494
PROGRESS: pass 7, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
PROGRESS: pass 7, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 11
PROGRESS: pass 7, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 12
PROGRESS: pass 7, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 13
PROGRESS: pass 7, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 14
PROGRESS: pass 7, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 15
merging changes from 4000 documents into a model of 49835 documents
topic #12 (0.067): 0.025*"company" + 0.023*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.007*"market" + 0.006*"sell" + 0.006*"network" + 0.005*"radio"
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.003*"author" + 0.003*"accord"
topic #1 (0.067): 0.049*"film" + 0.016*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"direct" + 0.007*"role" + 0.007*"appear" + 0.006*"movie"
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.010*"museum" + 0.008*"son" + 0.007*"german" + 0.007*"king" + 0.006*"artist" + 0.006*"design" + 0.006*"house" + 0.006*"building"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.006*"ship" + 0.005*"sea" + 0.005*"bird" + 0.005*"fish"
topic diff=0.043124, rho=0.131494
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.018*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"ship" + 0.008*"command" + 0.007*"german" + 0.006*"order"
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.010*"museum" + 0.008*"son" + 0.007*"german" + 0.006*"design" + 0.006*"artist" + 0.006*"king" + 0.006*"painting" + 0.006*"house"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.013*"town" + 0.010*"county" + 0.010*"north" + 0.010*"river" + 0.009*"building" + 0.009*"south" + 0.009*"park" + 0.009*"road"
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.008*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"court" + 0.006*"political" + 0.005*"act" + 0.005*"general"
topic #1 (0.067): 0.049*"film" + 0.016*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.007*"direct" + 0.007*"television" + 0.007*"role" + 0.007*"appear" + 0.006*"movie"
topic diff=0.045445, rho=0.131494
PROGRESS: pass 7, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"hospital" + 0.007*"death" + 0.006*"house" + 0.006*"say" + 0.006*"mother" + 0.005*"wife" + 0.005*"marry"
topic #11 (0.067): 0.028*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.007*"computer" + 0.007*"version" + 0.006*"user" + 0.006*"datum" + 0.005*"design" + 0.005*"code"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"ship" + 0.005*"bird" + 0.005*"sea" + 0.004*"fish"
topic #12 (0.067): 0.025*"company" + 0.023*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"network" + 0.005*"radio"
topic #14 (0.067): 0.015*"population" + 0.015*"village" + 0.014*"town" + 0.011*"county" + 0.010*"north" + 0.010*"river" + 0.010*"building" + 0.009*"park" + 0.009*"south" + 0.009*"road"
topic diff=0.045025, rho=0.131494
PROGRESS: pass 7, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"battle" + 0.008*"attack" + 0.008*"ship" + 0.008*"command" + 0.007*"german" + 0.006*"unit"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.010*"player" + 0.009*"information" + 0.007*"version" + 0.007*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"design" + 0.005*"code"
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"artist" + 0.006*"design" + 0.006*"german" + 0.006*"king" + 0.006*"painting" + 0.006*"building"
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.013*"line" + 0.009*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"network" + 0.005*"radio"
topic diff=0.043583, rho=0.131494
PROGRESS: pass 7, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.067): 0.022*"specie" + 0.013*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"ship" + 0.005*"bird" + 0.005*"sea" + 0.005*"fish"
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"consumer" + 0.005*"code"
topic #12 (0.067): 0.025*"company" + 0.025*"station" + 0.013*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"network"
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"artist" + 0.006*"design" + 0.006*"king" + 0.006*"german" + 0.006*"th_century" + 0.006*"house"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"system" + 0.007*"power" + 0.007*"engine" + 0.007*"model" + 0.007*"vehicle" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"production"
topic diff=0.049176, rho=0.131494
PROGRESS: pass 7, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.008*"ship" + 0.007*"command" + 0.006*"german" + 0.006*"unit"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"county" + 0.010*"river" + 0.010*"north" + 0.010*"park" + 0.009*"road" + 0.009*"building" + 0.009*"south"
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.003*"accord"
topic #7 (0.067): 0.019*"church" + 0.013*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"design" + 0.006*"artist" + 0.006*"german" + 0.006*"king" + 0.006*"building" + 0.006*"house"
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"result" + 0.005*"term" + 0.005*"case" + 0.005*"system" + 0.005*"cell" + 0.004*"increase" + 0.004*"study" + 0.004*"function"
topic diff=0.043517, rho=0.131494
PROGRESS: pass 7, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.012*"design" + 0.008*"car" + 0.007*"power" + 0.007*"system" + 0.007*"engine" + 0.007*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"datum" + 0.006*"computer" + 0.006*"code" + 0.005*"user" + 0.005*"design"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"term" + 0.005*"case" + 0.005*"cell" + 0.005*"system" + 0.004*"increase" + 0.004*"human" + 0.004*"effect"
topic #6 (0.067): 0.022*"specie" + 0.013*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"bird" + 0.005*"ship" + 0.005*"fish"
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic diff=0.041074, rho=0.131494
PROGRESS: pass 7, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
merging changes from 4000 documents into a model of 49835 documents
topic #8 (0.067): 0.031*"album" + 0.029*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.006*"video"
topic #1 (0.067): 0.047*"film" + 0.018*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"role" + 0.007*"direct" + 0.007*"appear" + 0.006*"award"
topic #12 (0.067): 0.025*"station" + 0.024*"company" + 0.013*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"radio" + 0.006*"market" + 0.006*"sell" + 0.006*"train"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.019*"club" + 0.015*"league" + 0.014*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"engine" + 0.007*"power" + 0.007*"system" + 0.006*"vehicle" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"light"
topic diff=0.037088, rho=0.131494
PROGRESS: pass 7, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.006*"court" + 0.006*"act" + 0.005*"support"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"river" + 0.011*"county" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.009*"road" + 0.009*"building"
topic #13 (0.067): 0.015*"student" + 0.013*"university" + 0.013*"award" + 0.012*"study" + 0.012*"college" + 0.010*"research" + 0.008*"education" + 0.008*"program" + 0.008*"director" + 0.008*"science"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.009*"player" + 0.007*"information" + 0.006*"version" + 0.006*"computer" + 0.006*"datum" + 0.005*"design" + 0.005*"code" + 0.005*"provide"
topic #8 (0.067): 0.032*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic diff=0.045299, rho=0.131494
PROGRESS: pass 7, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.009*"ship" + 0.008*"command" + 0.007*"german" + 0.006*"unit"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.006*"court" + 0.006*"act" + 0.005*"support"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"river" + 0.011*"county" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.009*"road" + 0.009*"building"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.015*"league" + 0.014*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #5 (0.067): 0.012*"woman" + 0.012*"child" + 0.008*"police" + 0.007*"hospital" + 0.006*"death" + 0.006*"house" + 0.006*"say" + 0.006*"mother" + 0.006*"father" + 0.005*"wife"
topic diff=0.047221, rho=0.131494
PROGRESS: pass 7, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.009*"ship" + 0.008*"german" + 0.008*"command" + 0.006*"order"
topic #8 (0.067): 0.032*"album" + 0.030*"song" + 0.028*"music" + 0.020*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"chart" + 0.006*"video"
topic #0 (0.067): 0.006*"example" + 0.005*"displaystyle" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.004*"case" + 0.004*"cell" + 0.004*"increase" + 0.004*"study" + 0.004*"effect"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"county" + 0.011*"river" + 0.010*"north" + 0.009*"south" + 0.009*"park" + 0.009*"road" + 0.009*"building"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"bird" + 0.005*"sea" + 0.005*"ship" + 0.005*"fish"
topic diff=0.039288, rho=0.131494
PROGRESS: pass 7, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"writer" + 0.004*"describe" + 0.004*"author" + 0.003*"way"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.013*"study" + 0.012*"award" + 0.012*"college" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"director" + 0.008*"international"
topic #3 (0.067): 0.011*"design" + 0.008*"power" + 0.008*"car" + 0.008*"system" + 0.007*"engine" + 0.006*"model" + 0.005*"produce" + 0.005*"vehicle" + 0.005*"aircraft" + 0.005*"light"
topic #7 (0.067): 0.022*"church" + 0.014*"art" + 0.010*"museum" + 0.008*"german" + 0.008*"son" + 0.007*"artist" + 0.007*"king" + 0.006*"design" + 0.006*"house" + 0.006*"th_century"
topic #6 (0.067): 0.021*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"tree" + 0.005*"bird" + 0.004*"fish"
topic diff=0.041999, rho=0.131494
PROGRESS: pass 7, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.067): 0.048*"film" + 0.018*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"direct" + 0.007*"role" + 0.007*"appear" + 0.006*"award"
topic #3 (0.067): 0.011*"design" + 0.008*"power" + 0.008*"system" + 0.007*"car" + 0.007*"engine" + 0.006*"model" + 0.005*"produce" + 0.005*"vehicle" + 0.005*"aircraft" + 0.005*"light"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #5 (0.067): 0.012*"woman" + 0.012*"child" + 0.008*"police" + 0.007*"hospital" + 0.007*"death" + 0.006*"house" + 0.006*"say" + 0.006*"mother" + 0.006*"marry" + 0.006*"wife"
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.003*"great"
topic diff=0.041212, rho=0.131494
-8.319 per-word bound, 319.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"result" + 0.005*"cell" + 0.005*"term" + 0.005*"system" + 0.005*"case" + 0.004*"increase" + 0.004*"function" + 0.004*"study"
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.007*"railway" + 0.007*"business" + 0.006*"sell" + 0.006*"market" + 0.005*"radio" + 0.005*"train"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"court" + 0.006*"vote" + 0.006*"political" + 0.005*"act" + 0.005*"general"
topic #4 (0.067): 0.016*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.009*"ship" + 0.007*"command" + 0.007*"german" + 0.006*"unit"
topic #5 (0.067): 0.012*"woman" + 0.012*"child" + 0.008*"police" + 0.007*"death" + 0.007*"hospital" + 0.006*"house" + 0.006*"mother" + 0.006*"say" + 0.006*"marry" + 0.006*"wife"
topic diff=0.043712, rho=0.131494
-8.321 per-word bound, 319.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #9 (0.067): 0.014*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"accord"
topic #6 (0.067): 0.021*"specie" + 0.011*"island" + 0.009*"water" + 0.007*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"fish" + 0.005*"tree" + 0.004*"ship"
topic #7 (0.067): 0.020*"church" + 0.013*"art" + 0.010*"museum" + 0.009*"son" + 0.008*"king" + 0.007*"german" + 0.006*"design" + 0.006*"artist" + 0.006*"house" + 0.006*"building"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"ship" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #13 (0.067): 0.014*"student" + 0.013*"study" + 0.013*"university" + 0.013*"college" + 0.012*"award" + 0.009*"research" + 0.008*"program" + 0.008*"education" + 0.008*"science" + 0.008*"international"
topic diff=0.044907, rho=0.131494
-8.248 per-word bound, 304.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"network" + 0.005*"radio"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.009*"ship" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.013*"town" + 0.011*"county" + 0.010*"building" + 0.010*"north" + 0.010*"river" + 0.009*"park" + 0.009*"south" + 0.009*"road"
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.005*"case" + 0.004*"cell" + 0.004*"study" + 0.004*"increase" + 0.004*"function"
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic diff=0.042946, rho=0.130371
PROGRESS: pass 8, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.013*"book" + 0.009*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"accord"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.009*"battle" + 0.009*"ship" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"death" + 0.007*"hospital" + 0.006*"house" + 0.006*"mother" + 0.006*"marry" + 0.006*"say" + 0.006*"wife"
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.007*"court" + 0.006*"political" + 0.006*"act" + 0.005*"general"
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"network"
topic diff=0.044409, rho=0.130371
PROGRESS: pass 8, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #7 (0.067): 0.019*"church" + 0.013*"art" + 0.010*"museum" + 0.008*"son" + 0.007*"king" + 0.007*"german" + 0.006*"design" + 0.006*"artist" + 0.006*"house" + 0.006*"building"
topic #1 (0.067): 0.048*"film" + 0.017*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.008*"direct" + 0.007*"role" + 0.007*"appear" + 0.006*"movie"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.017*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"court" + 0.007*"vote" + 0.006*"political" + 0.006*"act" + 0.005*"general"
topic diff=0.039892, rho=0.130371
PROGRESS: pass 8, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.067): 0.026*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.007*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"network"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"county" + 0.010*"north" + 0.010*"river" + 0.010*"building" + 0.010*"park" + 0.009*"south" + 0.009*"road"
topic #11 (0.067): 0.029*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.007*"computer" + 0.007*"version" + 0.006*"datum" + 0.006*"user" + 0.005*"design" + 0.005*"code"
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"term" + 0.005*"case" + 0.005*"system" + 0.005*"cell" + 0.004*"increase" + 0.004*"method" + 0.004*"human"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.014*"championship" + 0.013*"player" + 0.012*"football" + 0.012*"match" + 0.011*"final" + 0.009*"finish"
topic diff=0.042601, rho=0.130371
PROGRESS: pass 8, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"battle" + 0.009*"ship" + 0.008*"attack" + 0.008*"command" + 0.006*"german" + 0.006*"unit"
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.012*"line" + 0.009*"operate" + 0.008*"railway" + 0.007*"business" + 0.007*"market" + 0.006*"sell" + 0.005*"radio" + 0.005*"network"
topic #2 (0.067): 0.031*"season" + 0.021*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.012*"match" + 0.010*"final" + 0.009*"finish"
topic #9 (0.067): 0.013*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"accord"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"power" + 0.007*"engine" + 0.007*"system" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"production"
topic diff=0.041620, rho=0.130371
PROGRESS: pass 8, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.007*"power" + 0.007*"system" + 0.007*"engine" + 0.007*"vehicle" + 0.007*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"light"
topic #5 (0.067): 0.012*"child" + 0.011*"woman" + 0.008*"police" + 0.007*"death" + 0.006*"hospital" + 0.006*"say" + 0.006*"father" + 0.006*"house" + 0.006*"marry" + 0.006*"mother"
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.007*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"train"
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"court" + 0.007*"vote" + 0.006*"political" + 0.006*"act" + 0.005*"general"
topic #14 (0.067): 0.015*"village" + 0.015*"population" + 0.014*"town" + 0.011*"county" + 0.010*"river" + 0.010*"park" + 0.010*"north" + 0.010*"building" + 0.009*"south" + 0.009*"road"
topic diff=0.045675, rho=0.130371
PROGRESS: pass 8, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"design" + 0.005*"code"
topic #1 (0.067): 0.047*"film" + 0.018*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"role" + 0.007*"appear" + 0.007*"direct" + 0.006*"award"
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.006*"novel" + 0.005*"story" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.003*"great"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.005*"term" + 0.005*"cell" + 0.004*"increase" + 0.004*"function" + 0.004*"study"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.011*"military" + 0.009*"ship" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.006*"german" + 0.006*"unit"
topic diff=0.039753, rho=0.130371
PROGRESS: pass 8, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.067): 0.046*"film" + 0.018*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"role" + 0.007*"direct" + 0.007*"appear" + 0.007*"award"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"battle" + 0.009*"ship" + 0.009*"attack" + 0.007*"command" + 0.006*"german" + 0.006*"unit"
topic #8 (0.067): 0.030*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"hospital" + 0.007*"death" + 0.006*"say" + 0.006*"house" + 0.006*"father" + 0.006*"mother" + 0.006*"wife"
topic #13 (0.067): 0.014*"student" + 0.013*"study" + 0.013*"university" + 0.013*"college" + 0.013*"award" + 0.009*"research" + 0.008*"program" + 0.008*"education" + 0.008*"science" + 0.008*"international"
topic diff=0.037341, rho=0.130371
PROGRESS: pass 8, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"ship" + 0.009*"battle" + 0.008*"attack" + 0.007*"german" + 0.007*"command" + 0.006*"unit"
topic #6 (0.067): 0.021*"specie" + 0.013*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"bird" + 0.005*"sea" + 0.005*"fish" + 0.004*"tree"
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.007*"political" + 0.006*"court" + 0.006*"act" + 0.005*"general"
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"datum" + 0.006*"computer" + 0.006*"code" + 0.005*"user" + 0.005*"provide"
topic diff=0.040393, rho=0.130371
PROGRESS: pass 8, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"case" + 0.005*"term" + 0.005*"system" + 0.004*"cell" + 0.004*"increase" + 0.004*"effect" + 0.004*"study"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"ship" + 0.009*"battle" + 0.009*"attack" + 0.007*"german" + 0.007*"command" + 0.006*"order"
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.010*"player" + 0.007*"information" + 0.006*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"code" + 0.005*"provide" + 0.005*"design"
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.003*"accord"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"death" + 0.007*"hospital" + 0.006*"say" + 0.006*"father" + 0.006*"house" + 0.006*"wife" + 0.006*"mother"
topic diff=0.039516, rho=0.130371
PROGRESS: pass 8, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.020*"church" + 0.014*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"king" + 0.007*"design" + 0.007*"artist" + 0.007*"german" + 0.006*"building" + 0.006*"house"
topic #3 (0.067): 0.012*"design" + 0.008*"power" + 0.007*"car" + 0.007*"engine" + 0.007*"system" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"space"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"increase" + 0.005*"system" + 0.005*"case" + 0.005*"term" + 0.004*"cell" + 0.004*"effect" + 0.004*"study"
topic #13 (0.067): 0.014*"student" + 0.013*"university" + 0.013*"award" + 0.013*"study" + 0.012*"college" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"director" + 0.008*"international"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"death" + 0.007*"hospital" + 0.006*"say" + 0.006*"house" + 0.006*"wife" + 0.006*"father" + 0.006*"mother"
topic diff=0.042321, rho=0.130371
PROGRESS: pass 8, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.022*"church" + 0.014*"art" + 0.009*"museum" + 0.009*"german" + 0.008*"son" + 0.007*"king" + 0.006*"design" + 0.006*"artist" + 0.006*"painting" + 0.006*"house"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.015*"league" + 0.014*"championship" + 0.014*"football" + 0.013*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.006*"court" + 0.006*"act" + 0.005*"support"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.005*"increase" + 0.005*"term" + 0.004*"effect" + 0.004*"cell" + 0.004*"human"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.007*"police" + 0.007*"death" + 0.006*"hospital" + 0.006*"say" + 0.006*"father" + 0.006*"mother" + 0.006*"wife" + 0.006*"marry"
topic diff=0.043551, rho=0.130371
PROGRESS: pass 8, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.007*"police" + 0.007*"death" + 0.006*"hospital" + 0.006*"say" + 0.006*"father" + 0.006*"mother" + 0.006*"marry" + 0.006*"wife"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"bird" + 0.005*"tree" + 0.005*"sea" + 0.005*"fish"
topic #10 (0.067): 0.013*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.007*"political" + 0.006*"court" + 0.006*"act" + 0.005*"general"
topic #7 (0.067): 0.021*"church" + 0.014*"art" + 0.009*"museum" + 0.008*"german" + 0.008*"son" + 0.007*"king" + 0.007*"artist" + 0.006*"design" + 0.006*"painting" + 0.006*"house"
topic #13 (0.067): 0.015*"student" + 0.013*"university" + 0.013*"study" + 0.013*"award" + 0.012*"college" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"director" + 0.008*"science"
topic diff=0.040272, rho=0.130371
PROGRESS: pass 8, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.067): 0.022*"church" + 0.013*"art" + 0.009*"museum" + 0.008*"son" + 0.008*"german" + 0.007*"king" + 0.006*"artist" + 0.006*"design" + 0.006*"house" + 0.006*"th_century"
topic #8 (0.067): 0.032*"album" + 0.030*"song" + 0.028*"music" + 0.020*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"county" + 0.011*"river" + 0.011*"north" + 0.010*"south" + 0.009*"park" + 0.009*"road" + 0.009*"building"
topic #13 (0.067): 0.014*"student" + 0.013*"study" + 0.013*"university" + 0.013*"college" + 0.013*"award" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"science" + 0.008*"director"
topic #12 (0.067): 0.025*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"business" + 0.008*"railway" + 0.007*"sell" + 0.006*"market" + 0.006*"radio" + 0.005*"train"
topic diff=0.039991, rho=0.130371
-8.303 per-word bound, 315.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"court" + 0.007*"political" + 0.006*"vote" + 0.006*"act" + 0.005*"general"
topic #11 (0.067): 0.026*"game" + 0.012*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.007*"computer" + 0.006*"datum" + 0.006*"code" + 0.006*"user" + 0.006*"provide"
topic #13 (0.067): 0.015*"student" + 0.013*"study" + 0.013*"college" + 0.013*"university" + 0.012*"award" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"science" + 0.008*"director"
topic #0 (0.067): 0.006*"example" + 0.006*"displaystyle" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.005*"case" + 0.005*"cell" + 0.004*"increase" + 0.004*"function" + 0.004*"study"
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.014*"league" + 0.013*"football" + 0.013*"championship" + 0.013*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic diff=0.037694, rho=0.130371
-8.305 per-word bound, 316.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"death" + 0.006*"hospital" + 0.006*"say" + 0.006*"marry" + 0.006*"father" + 0.006*"mother" + 0.006*"wife"
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"language"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.019*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #1 (0.067): 0.049*"film" + 0.018*"series" + 0.011*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"direct" + 0.007*"role" + 0.007*"appear" + 0.006*"award"
topic #12 (0.067): 0.026*"company" + 0.024*"station" + 0.012*"line" + 0.008*"operate" + 0.008*"railway" + 0.007*"business" + 0.006*"sell" + 0.006*"market" + 0.005*"radio" + 0.005*"train"
topic diff=0.036374, rho=0.130371
-8.277 per-word bound, 310.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.006*"court" + 0.006*"political" + 0.006*"act" + 0.005*"general"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.013*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #9 (0.067): 0.013*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"language" + 0.004*"author"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.013*"town" + 0.011*"county" + 0.010*"north" + 0.010*"river" + 0.010*"building" + 0.009*"south" + 0.009*"park" + 0.009*"road"
topic diff=0.039635, rho=0.129277
PROGRESS: pass 9, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
PROGRESS: pass 9, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"ship" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #1 (0.067): 0.048*"film" + 0.017*"series" + 0.012*"episode" + 0.011*"star" + 0.008*"character" + 0.008*"television" + 0.007*"direct" + 0.007*"role" + 0.007*"appear" + 0.006*"award"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.013*"championship" + 0.013*"football" + 0.012*"player" + 0.011*"match" + 0.010*"final" + 0.009*"finish"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"county" + 0.010*"river" + 0.010*"north" + 0.010*"building" + 0.010*"park" + 0.009*"south" + 0.009*"road"
topic #3 (0.067): 0.012*"design" + 0.008*"power" + 0.007*"system" + 0.007*"car" + 0.007*"engine" + 0.006*"model" + 0.005*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"light"
topic diff=0.041433, rho=0.129277
PROGRESS: pass 9, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.067): 0.014*"student" + 0.013*"college" + 0.013*"study" + 0.013*"university" + 0.012*"award" + 0.009*"research" + 0.009*"education" + 0.009*"program" + 0.008*"science" + 0.008*"international"
topic #8 (0.067): 0.031*"album" + 0.030*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #12 (0.067): 0.026*"company" + 0.025*"station" + 0.012*"line" + 0.009*"operate" + 0.008*"railway" + 0.007*"business" + 0.007*"sell" + 0.006*"market" + 0.005*"radio" + 0.005*"train"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"military" + 0.010*"ship" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.006*"german" + 0.006*"order"
topic #14 (0.067): 0.016*"village" + 0.016*"population" + 0.014*"town" + 0.011*"county" + 0.010*"river" + 0.010*"north" + 0.010*"building" + 0.010*"park" + 0.009*"south" + 0.009*"road"
topic diff=0.037160, rho=0.129277
PROGRESS: pass 9, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.028*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.007*"computer" + 0.007*"version" + 0.006*"datum" + 0.006*"user" + 0.005*"design" + 0.005*"code"
topic #2 (0.067): 0.031*"season" + 0.022*"game" + 0.018*"club" + 0.014*"league" + 0.014*"championship" + 0.012*"football" + 0.012*"player" + 0.012*"match" + 0.011*"final" + 0.009*"finish"
topic #5 (0.067): 0.012*"child" + 0.011*"woman" + 0.008*"police" + 0.007*"death" + 0.006*"say" + 0.006*"wife" + 0.006*"father" + 0.006*"hospital" + 0.006*"mother" + 0.006*"marry"
topic #13 (0.067): 0.014*"student" + 0.013*"college" + 0.013*"study" + 0.012*"university" + 0.012*"award" + 0.009*"research" + 0.009*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic #6 (0.067): 0.022*"specie" + 0.012*"island" + 0.009*"water" + 0.007*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"bird" + 0.005*"fish" + 0.005*"tree" + 0.005*"white"
topic diff=0.039794, rho=0.129277
PROGRESS: pass 9, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.005*"term" + 0.005*"increase" + 0.004*"cell" + 0.004*"function" + 0.004*"method"
topic #11 (0.067): 0.028*"game" + 0.012*"system" + 0.010*"player" + 0.009*"information" + 0.007*"version" + 0.007*"computer" + 0.006*"datum" + 0.006*"user" + 0.005*"code" + 0.005*"provide"
topic #12 (0.067): 0.026*"company" + 0.024*"station" + 0.012*"line" + 0.009*"operate" + 0.007*"railway" + 0.007*"business" + 0.007*"market" + 0.006*"sell" + 0.006*"radio" + 0.005*"network"
topic #6 (0.067): 0.022*"specie" + 0.013*"island" + 0.009*"water" + 0.008*"plant" + 0.006*"genus" + 0.006*"describe" + 0.005*"bird" + 0.005*"fish" + 0.005*"animal" + 0.005*"tree"
topic #14 (0.067): 0.016*"population" + 0.016*"village" + 0.014*"town" + 0.011*"county" + 0.010*"north" + 0.010*"park" + 0.010*"river" + 0.010*"building" + 0.010*"south" + 0.009*"road"
topic diff=0.039412, rho=0.129277
PROGRESS: pass 9, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 4000 documents into a model of 49835 documents
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.008*"say" + 0.007*"life" + 0.005*"novel" + 0.005*"story" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"language"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"ship" + 0.009*"battle" + 0.008*"attack" + 0.008*"command" + 0.006*"german" + 0.006*"unit"
topic #5 (0.067): 0.012*"child" + 0.011*"woman" + 0.008*"police" + 0.007*"death" + 0.006*"say" + 0.006*"father" + 0.006*"hospital" + 0.006*"wife" + 0.006*"marry" + 0.006*"mother"
topic #13 (0.067): 0.015*"student" + 0.013*"college" + 0.013*"study" + 0.012*"university" + 0.012*"award" + 0.009*"research" + 0.009*"education" + 0.008*"program" + 0.008*"science" + 0.008*"director"
topic #7 (0.067): 0.019*"church" + 0.013*"art" + 0.009*"museum" + 0.008*"son" + 0.008*"king" + 0.007*"german" + 0.006*"artist" + 0.006*"design" + 0.006*"house" + 0.006*"th_century"
topic diff=0.037669, rho=0.129277
PROGRESS: pass 9, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.067): 0.016*"village" + 0.016*"population" + 0.014*"town" + 0.011*"county" + 0.011*"river" + 0.010*"north" + 0.010*"park" + 0.010*"building" + 0.010*"road" + 0.009*"south"
topic #13 (0.067): 0.015*"student" + 0.013*"college" + 0.013*"study" + 0.013*"university" + 0.012*"award" + 0.009*"research" + 0.008*"education" + 0.008*"program" + 0.008*"science" + 0.008*"director"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.014*"force" + 0.010*"military" + 0.010*"ship" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.006*"german" + 0.006*"unit"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"case" + 0.005*"cell" + 0.005*"term" + 0.005*"increase" + 0.004*"function" + 0.004*"effect"
topic #7 (0.067): 0.019*"church" + 0.013*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"king" + 0.007*"german" + 0.006*"design" + 0.006*"artist" + 0.006*"house" + 0.006*"building"
topic diff=0.037534, rho=0.129277
PROGRESS: pass 9, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.007*"say" + 0.007*"life" + 0.005*"novel" + 0.005*"story" + 0.004*"describe" + 0.004*"writer" + 0.004*"author" + 0.004*"language"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.012*"river" + 0.012*"county" + 0.011*"north" + 0.010*"park" + 0.010*"building" + 0.010*"south" + 0.009*"road"
topic #5 (0.067): 0.012*"child" + 0.011*"woman" + 0.008*"police" + 0.007*"death" + 0.006*"say" + 0.006*"hospital" + 0.006*"father" + 0.006*"wife" + 0.006*"mother" + 0.006*"marry"
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"datum" + 0.006*"computer" + 0.006*"code" + 0.005*"user" + 0.005*"provide"
topic #7 (0.067): 0.020*"church" + 0.012*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"king" + 0.007*"german" + 0.006*"design" + 0.006*"artist" + 0.006*"house" + 0.006*"building"
topic diff=0.035549, rho=0.129277
PROGRESS: pass 9, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.067): 0.027*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"code" + 0.006*"user" + 0.005*"provide"
topic #3 (0.067): 0.012*"design" + 0.008*"power" + 0.008*"car" + 0.007*"engine" + 0.007*"system" + 0.006*"model" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"produce" + 0.004*"light"
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"vote" + 0.007*"political" + 0.006*"court" + 0.006*"act" + 0.005*"country"
topic #7 (0.067): 0.019*"church" + 0.014*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"king" + 0.007*"artist" + 0.007*"german" + 0.007*"painting" + 0.006*"design" + 0.006*"house"
topic #13 (0.067): 0.015*"student" + 0.013*"study" + 0.013*"college" + 0.013*"award" + 0.012*"university" + 0.009*"research" + 0.008*"education" + 0.008*"program" + 0.008*"science" + 0.008*"international"
topic diff=0.034784, rho=0.129277
PROGRESS: pass 9, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.007*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"case" + 0.005*"system" + 0.005*"term" + 0.004*"increase" + 0.004*"cell" + 0.004*"effect" + 0.004*"study"
topic #11 (0.067): 0.026*"game" + 0.011*"system" + 0.010*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"provide" + 0.005*"code" + 0.005*"design"
topic #5 (0.067): 0.012*"child" + 0.012*"woman" + 0.008*"police" + 0.007*"death" + 0.006*"hospital" + 0.006*"say" + 0.006*"father" + 0.006*"wife" + 0.006*"mother" + 0.006*"marry"
topic #8 (0.067): 0.032*"album" + 0.030*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.007*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"language" + 0.004*"word"
topic diff=0.036347, rho=0.129277
PROGRESS: pass 9, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.067): 0.014*"book" + 0.010*"publish" + 0.007*"say" + 0.007*"life" + 0.005*"story" + 0.005*"novel" + 0.004*"describe" + 0.004*"writer" + 0.004*"language" + 0.004*"word"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.011*"river" + 0.011*"county" + 0.011*"north" + 0.010*"road" + 0.010*"south" + 0.010*"park" + 0.009*"building"
topic #7 (0.067): 0.020*"church" + 0.013*"art" + 0.009*"museum" + 0.008*"son" + 0.007*"king" + 0.007*"german" + 0.007*"design" + 0.006*"artist" + 0.006*"house" + 0.006*"building"
topic #3 (0.067): 0.012*"design" + 0.008*"car" + 0.008*"power" + 0.007*"system" + 0.007*"engine" + 0.006*"model" + 0.006*"aircraft" + 0.006*"vehicle" + 0.005*"space" + 0.005*"light"
topic #8 (0.067): 0.032*"album" + 0.031*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.008*"tour" + 0.006*"chart" + 0.006*"video"
topic diff=0.040089, rho=0.129277
PROGRESS: pass 9, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.067): 0.032*"season" + 0.022*"game" + 0.019*"club" + 0.015*"league" + 0.014*"championship" + 0.014*"football" + 0.012*"player" + 0.011*"match" + 0.011*"final" + 0.009*"finish"
topic #5 (0.067): 0.012*"child" + 0.011*"woman" + 0.007*"police" + 0.007*"death" + 0.006*"say" + 0.006*"father" + 0.006*"wife" + 0.006*"mother" + 0.006*"hospital" + 0.006*"marry"
topic #14 (0.067): 0.016*"village" + 0.016*"population" + 0.015*"town" + 0.011*"county" + 0.011*"river" + 0.011*"north" + 0.010*"south" + 0.010*"road" + 0.010*"park" + 0.009*"building"
topic #13 (0.067): 0.015*"student" + 0.013*"university" + 0.013*"study" + 0.013*"college" + 0.013*"award" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"director" + 0.008*"science"
topic #8 (0.067): 0.032*"album" + 0.030*"song" + 0.029*"music" + 0.020*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.008*"tour" + 0.006*"video" + 0.006*"chart"
topic diff=0.038098, rho=0.129277
PROGRESS: pass 9, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.067): 0.017*"war" + 0.014*"army" + 0.013*"force" + 0.010*"ship" + 0.010*"military" + 0.010*"battle" + 0.009*"attack" + 0.008*"command" + 0.008*"german" + 0.006*"order"
topic #13 (0.067): 0.015*"student" + 0.013*"university" + 0.013*"study" + 0.013*"college" + 0.013*"award" + 0.010*"research" + 0.009*"program" + 0.008*"education" + 0.008*"director" + 0.008*"science"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.005*"case" + 0.004*"cell" + 0.004*"increase" + 0.004*"study" + 0.004*"effect"
topic #7 (0.067): 0.022*"church" + 0.013*"art" + 0.009*"museum" + 0.009*"german" + 0.008*"son" + 0.008*"king" + 0.007*"artist" + 0.006*"design" + 0.006*"house" + 0.006*"th_century"
topic #8 (0.067): 0.032*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic diff=0.037448, rho=0.129277
PROGRESS: pass 9, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.067): 0.014*"government" + 0.011*"election" + 0.010*"party" + 0.009*"law" + 0.007*"elect" + 0.007*"court" + 0.007*"vote" + 0.007*"political" + 0.006*"act" + 0.005*"general"
topic #14 (0.067): 0.016*"village" + 0.015*"population" + 0.014*"town" + 0.012*"county" + 0.011*"north" + 0.011*"river" + 0.010*"south" + 0.010*"road" + 0.010*"park" + 0.009*"building"
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"system" + 0.005*"term" + 0.005*"cell" + 0.005*"case" + 0.004*"function" + 0.004*"increase" + 0.004*"study"
topic #3 (0.067): 0.011*"design" + 0.008*"power" + 0.008*"car" + 0.007*"system" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"light" + 0.005*"vehicle"
topic #5 (0.067): 0.012*"child" + 0.011*"woman" + 0.007*"police" + 0.007*"death" + 0.006*"say" + 0.006*"father" + 0.006*"hospital" + 0.006*"wife" + 0.006*"marry" + 0.006*"mother"
topic diff=0.037066, rho=0.129277
-8.311 per-word bound, 317.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.067): 0.006*"displaystyle" + 0.006*"example" + 0.005*"result" + 0.005*"case" + 0.005*"term" + 0.005*"system" + 0.005*"cell" + 0.004*"function" + 0.004*"increase" + 0.004*"study"
topic #1 (0.067): 0.048*"film" + 0.018*"series" + 0.011*"episode" + 0.011*"star" + 0.009*"character" + 0.008*"television" + 0.007*"role" + 0.007*"direct" + 0.007*"appear" + 0.006*"award"
topic #3 (0.067): 0.011*"design" + 0.008*"car" + 0.008*"power" + 0.007*"system" + 0.007*"engine" + 0.005*"model" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"light"
topic #11 (0.067): 0.026*"game" + 0.012*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.007*"computer" + 0.006*"datum" + 0.006*"code" + 0.006*"user" + 0.005*"provide"
topic #6 (0.067): 0.021*"specie" + 0.012*"island" + 0.009*"water" + 0.008*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"tree" + 0.005*"fish" + 0.005*"white" + 0.005*"sea"
topic diff=0.038201, rho=0.129277
-8.313 per-word bound, 318.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #7 (0.067): 0.020*"church" + 0.013*"art" + 0.009*"museum" + 0.009*"son" + 0.008*"king" + 0.008*"german" + 0.006*"design" + 0.006*"artist" + 0.006*"house" + 0.006*"th_century"
topic #4 (0.067): 0.017*"war" + 0.015*"army" + 0.013*"force" + 0.010*"ship" + 0.010*"military" + 0.009*"battle" + 0.009*"attack" + 0.007*"command" + 0.007*"german" + 0.006*"order"
topic #14 (0.067): 0.016*"village" + 0.016*"population" + 0.014*"town" + 0.012*"county" + 0.011*"north" + 0.010*"river" + 0.010*"south" + 0.010*"road" + 0.009*"park" + 0.009*"building"
topic #6 (0.067): 0.021*"specie" + 0.011*"island" + 0.009*"water" + 0.008*"plant" + 0.007*"genus" + 0.006*"describe" + 0.005*"tree" + 0.005*"fish" + 0.005*"white" + 0.004*"sea"
topic #11 (0.067): 0.026*"game" + 0.012*"system" + 0.009*"player" + 0.008*"information" + 0.007*"version" + 0.006*"computer" + 0.006*"datum" + 0.006*"code" + 0.006*"user" + 0.005*"provide"
topic diff=0.037289, rho=0.129277
-8.275 per-word bound, 309.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=26262, num_topics=15, decay=0.5, chunksize=1000) in 552.83s', 'datetime': '2022-02-06T23:14:25.854298', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows
1 batches submitted to accumulate stats from 64 documents (2187 virtual)
2 batches submitted to accumulate stats from 128 documents (6627 virtual)
3 batches submitted to accumulate stats from 192 documents (7985 virtual)
4 batches submitted to accumulate stats from 256 documents (8080 virtual)
5 batches submitted to accumulate stats from 320 documents (9095 virtual)
6 batches submitted to accumulate stats from 384 documents (13100 virtual)
9 batches submitted to accumulate stats from 576 documents (14514 virtual)
10 batches submitted to accumulate stats from 640 documents (16851 virtual)
11 batches submitted to accumulate stats from 704 documents (18047 virtual)
12 batches submitted to accumulate stats from 768 documents (22827 virtual)
13 batches submitted to accumulate stats from 832 documents (25611 virtual)
16 batches submitted to accumulate stats from 1024 documents (27698 virtual)
17 batches submitted to accumulate stats from 1088 documents (27914 virtual)
18 batches submitted to accumulate stats from 1152 documents (32268 virtual)
19 batches submitted to accumulate stats from 1216 documents (34117 virtual)
20 batches submitted to accumulate stats from 1280 documents (37099 virtual)
21 batches submitted to accumulate stats from 1344 documents (38534 virtual)
22 batches submitted to accumulate stats from 1408 documents (38702 virtual)
23 batches submitted to accumulate stats from 1472 documents (42613 virtual)
24 batches submitted to accumulate stats from 1536 documents (45234 virtual)
25 batches submitted to accumulate stats from 1600 documents (47399 virtual)
27 batches submitted to accumulate stats from 1728 documents (49160 virtual)
28 batches submitted to accumulate stats from 1792 documents (59407 virtual)
29 batches submitted to accumulate stats from 1856 documents (60547 virtual)
31 batches submitted to accumulate stats from 1984 documents (60697 virtual)
33 batches submitted to accumulate stats from 2112 documents (61484 virtual)
34 batches submitted to accumulate stats from 2176 documents (63884 virtual)
35 batches submitted to accumulate stats from 2240 documents (66263 virtual)
36 batches submitted to accumulate stats from 2304 documents (69403 virtual)
39 batches submitted to accumulate stats from 2496 documents (73342 virtual)
40 batches submitted to accumulate stats from 2560 documents (74596 virtual)
41 batches submitted to accumulate stats from 2624 documents (74916 virtual)
42 batches submitted to accumulate stats from 2688 documents (80383 virtual)
44 batches submitted to accumulate stats from 2816 documents (83035 virtual)
45 batches submitted to accumulate stats from 2880 documents (86208 virtual)
46 batches submitted to accumulate stats from 2944 documents (87718 virtual)
47 batches submitted to accumulate stats from 3008 documents (93613 virtual)
49 batches submitted to accumulate stats from 3136 documents (92434 virtual)
50 batches submitted to accumulate stats from 3200 documents (95280 virtual)
51 batches submitted to accumulate stats from 3264 documents (98530 virtual)
52 batches submitted to accumulate stats from 3328 documents (99772 virtual)
53 batches submitted to accumulate stats from 3392 documents (102766 virtual)
54 batches submitted to accumulate stats from 3456 documents (103475 virtual)
55 batches submitted to accumulate stats from 3520 documents (106996 virtual)
56 batches submitted to accumulate stats from 3584 documents (110156 virtual)
57 batches submitted to accumulate stats from 3648 documents (112489 virtual)
58 batches submitted to accumulate stats from 3712 documents (114440 virtual)
59 batches submitted to accumulate stats from 3776 documents (115010 virtual)
61 batches submitted to accumulate stats from 3904 documents (115832 virtual)
63 batches submitted to accumulate stats from 4032 documents (118125 virtual)
64 batches submitted to accumulate stats from 4096 documents (118841 virtual)
67 batches submitted to accumulate stats from 4288 documents (125470 virtual)
68 batches submitted to accumulate stats from 4352 documents (132480 virtual)
69 batches submitted to accumulate stats from 4416 documents (134661 virtual)
70 batches submitted to accumulate stats from 4480 documents (138744 virtual)
71 batches submitted to accumulate stats from 4544 documents (142278 virtual)
74 batches submitted to accumulate stats from 4736 documents (141497 virtual)
75 batches submitted to accumulate stats from 4800 documents (145541 virtual)
76 batches submitted to accumulate stats from 4864 documents (149878 virtual)
77 batches submitted to accumulate stats from 4928 documents (150552 virtual)
78 batches submitted to accumulate stats from 4992 documents (152791 virtual)
80 batches submitted to accumulate stats from 5120 documents (154421 virtual)
81 batches submitted to accumulate stats from 5184 documents (154740 virtual)
83 batches submitted to accumulate stats from 5312 documents (155774 virtual)
84 batches submitted to accumulate stats from 5376 documents (156999 virtual)
85 batches submitted to accumulate stats from 5440 documents (158023 virtual)
87 batches submitted to accumulate stats from 5568 documents (157181 virtual)
88 batches submitted to accumulate stats from 5632 documents (159347 virtual)
89 batches submitted to accumulate stats from 5696 documents (162759 virtual)
90 batches submitted to accumulate stats from 5760 documents (163583 virtual)
91 batches submitted to accumulate stats from 5824 documents (164304 virtual)
92 batches submitted to accumulate stats from 5888 documents (170599 virtual)
93 batches submitted to accumulate stats from 5952 documents (173498 virtual)
94 batches submitted to accumulate stats from 6016 documents (174174 virtual)
95 batches submitted to accumulate stats from 6080 documents (176779 virtual)
96 batches submitted to accumulate stats from 6144 documents (179303 virtual)
97 batches submitted to accumulate stats from 6208 documents (181794 virtual)
98 batches submitted to accumulate stats from 6272 documents (182914 virtual)
100 batches submitted to accumulate stats from 6400 documents (183553 virtual)
101 batches submitted to accumulate stats from 6464 documents (185697 virtual)
103 batches submitted to accumulate stats from 6592 documents (185739 virtual)
104 batches submitted to accumulate stats from 6656 documents (188382 virtual)
106 batches submitted to accumulate stats from 6784 documents (190956 virtual)
108 batches submitted to accumulate stats from 6912 documents (191968 virtual)
110 batches submitted to accumulate stats from 7040 documents (190943 virtual)
111 batches submitted to accumulate stats from 7104 documents (194459 virtual)
113 batches submitted to accumulate stats from 7232 documents (197104 virtual)
114 batches submitted to accumulate stats from 7296 documents (200103 virtual)
115 batches submitted to accumulate stats from 7360 documents (201496 virtual)
116 batches submitted to accumulate stats from 7424 documents (203757 virtual)
117 batches submitted to accumulate stats from 7488 documents (206845 virtual)
120 batches submitted to accumulate stats from 7680 documents (205722 virtual)
121 batches submitted to accumulate stats from 7744 documents (208265 virtual)
123 batches submitted to accumulate stats from 7872 documents (210389 virtual)
124 batches submitted to accumulate stats from 7936 documents (212735 virtual)
125 batches submitted to accumulate stats from 8000 documents (213569 virtual)
127 batches submitted to accumulate stats from 8128 documents (217990 virtual)
128 batches submitted to accumulate stats from 8192 documents (222214 virtual)
129 batches submitted to accumulate stats from 8256 documents (222665 virtual)
131 batches submitted to accumulate stats from 8384 documents (222596 virtual)
132 batches submitted to accumulate stats from 8448 documents (225798 virtual)
133 batches submitted to accumulate stats from 8512 documents (226731 virtual)
136 batches submitted to accumulate stats from 8704 documents (224021 virtual)
137 batches submitted to accumulate stats from 8768 documents (236379 virtual)
138 batches submitted to accumulate stats from 8832 documents (239724 virtual)
140 batches submitted to accumulate stats from 8960 documents (242954 virtual)
141 batches submitted to accumulate stats from 9024 documents (246764 virtual)
142 batches submitted to accumulate stats from 9088 documents (253103 virtual)
143 batches submitted to accumulate stats from 9152 documents (254386 virtual)
144 batches submitted to accumulate stats from 9216 documents (254598 virtual)
146 batches submitted to accumulate stats from 9344 documents (257466 virtual)
147 batches submitted to accumulate stats from 9408 documents (259512 virtual)
148 batches submitted to accumulate stats from 9472 documents (261228 virtual)
149 batches submitted to accumulate stats from 9536 documents (264054 virtual)
151 batches submitted to accumulate stats from 9664 documents (267268 virtual)
152 batches submitted to accumulate stats from 9728 documents (267348 virtual)
153 batches submitted to accumulate stats from 9792 documents (269316 virtual)
154 batches submitted to accumulate stats from 9856 documents (275437 virtual)
157 batches submitted to accumulate stats from 10048 documents (274207 virtual)
158 batches submitted to accumulate stats from 10112 documents (275165 virtual)
159 batches submitted to accumulate stats from 10176 documents (276705 virtual)
160 batches submitted to accumulate stats from 10240 documents (282821 virtual)
163 batches submitted to accumulate stats from 10432 documents (281633 virtual)
165 batches submitted to accumulate stats from 10560 documents (283293 virtual)
168 batches submitted to accumulate stats from 10752 documents (286491 virtual)
169 batches submitted to accumulate stats from 10816 documents (287328 virtual)
170 batches submitted to accumulate stats from 10880 documents (290806 virtual)
171 batches submitted to accumulate stats from 10944 documents (296214 virtual)
172 batches submitted to accumulate stats from 11008 documents (298679 virtual)
173 batches submitted to accumulate stats from 11072 documents (301277 virtual)
175 batches submitted to accumulate stats from 11200 documents (305035 virtual)
176 batches submitted to accumulate stats from 11264 documents (308957 virtual)
177 batches submitted to accumulate stats from 11328 documents (312511 virtual)
178 batches submitted to accumulate stats from 11392 documents (312803 virtual)
179 batches submitted to accumulate stats from 11456 documents (313626 virtual)
181 batches submitted to accumulate stats from 11584 documents (314872 virtual)
182 batches submitted to accumulate stats from 11648 documents (315915 virtual)
183 batches submitted to accumulate stats from 11712 documents (319360 virtual)
186 batches submitted to accumulate stats from 11904 documents (318932 virtual)
188 batches submitted to accumulate stats from 12032 documents (319628 virtual)
189 batches submitted to accumulate stats from 12096 documents (322496 virtual)
190 batches submitted to accumulate stats from 12160 documents (324105 virtual)
191 batches submitted to accumulate stats from 12224 documents (326341 virtual)
192 batches submitted to accumulate stats from 12288 documents (328441 virtual)
193 batches submitted to accumulate stats from 12352 documents (330972 virtual)
194 batches submitted to accumulate stats from 12416 documents (336286 virtual)
195 batches submitted to accumulate stats from 12480 documents (337040 virtual)
196 batches submitted to accumulate stats from 12544 documents (342184 virtual)
197 batches submitted to accumulate stats from 12608 documents (342196 virtual)
198 batches submitted to accumulate stats from 12672 documents (342528 virtual)
199 batches submitted to accumulate stats from 12736 documents (342975 virtual)
200 batches submitted to accumulate stats from 12800 documents (344503 virtual)
202 batches submitted to accumulate stats from 12928 documents (345349 virtual)
204 batches submitted to accumulate stats from 13056 documents (348125 virtual)
205 batches submitted to accumulate stats from 13120 documents (348907 virtual)
207 batches submitted to accumulate stats from 13248 documents (350934 virtual)
209 batches submitted to accumulate stats from 13376 documents (355832 virtual)
210 batches submitted to accumulate stats from 13440 documents (356937 virtual)
211 batches submitted to accumulate stats from 13504 documents (360293 virtual)
212 batches submitted to accumulate stats from 13568 documents (363073 virtual)
213 batches submitted to accumulate stats from 13632 documents (366775 virtual)
214 batches submitted to accumulate stats from 13696 documents (367849 virtual)
215 batches submitted to accumulate stats from 13760 documents (368995 virtual)
216 batches submitted to accumulate stats from 13824 documents (373253 virtual)
217 batches submitted to accumulate stats from 13888 documents (374647 virtual)
218 batches submitted to accumulate stats from 13952 documents (378251 virtual)
221 batches submitted to accumulate stats from 14144 documents (383188 virtual)
223 batches submitted to accumulate stats from 14272 documents (386415 virtual)
224 batches submitted to accumulate stats from 14336 documents (386638 virtual)
226 batches submitted to accumulate stats from 14464 documents (388107 virtual)
227 batches submitted to accumulate stats from 14528 documents (390303 virtual)
228 batches submitted to accumulate stats from 14592 documents (392292 virtual)
229 batches submitted to accumulate stats from 14656 documents (393425 virtual)
230 batches submitted to accumulate stats from 14720 documents (396076 virtual)
233 batches submitted to accumulate stats from 14912 documents (396663 virtual)
234 batches submitted to accumulate stats from 14976 documents (397499 virtual)
235 batches submitted to accumulate stats from 15040 documents (399009 virtual)
236 batches submitted to accumulate stats from 15104 documents (400688 virtual)
237 batches submitted to accumulate stats from 15168 documents (401034 virtual)
238 batches submitted to accumulate stats from 15232 documents (401549 virtual)
239 batches submitted to accumulate stats from 15296 documents (403452 virtual)
240 batches submitted to accumulate stats from 15360 documents (403557 virtual)
241 batches submitted to accumulate stats from 15424 documents (404055 virtual)
242 batches submitted to accumulate stats from 15488 documents (408633 virtual)
243 batches submitted to accumulate stats from 15552 documents (409227 virtual)
244 batches submitted to accumulate stats from 15616 documents (413288 virtual)
246 batches submitted to accumulate stats from 15744 documents (413519 virtual)
247 batches submitted to accumulate stats from 15808 documents (417452 virtual)
248 batches submitted to accumulate stats from 15872 documents (422414 virtual)
249 batches submitted to accumulate stats from 15936 documents (430605 virtual)
250 batches submitted to accumulate stats from 16000 documents (435801 virtual)
251 batches submitted to accumulate stats from 16064 documents (437543 virtual)
252 batches submitted to accumulate stats from 16128 documents (447728 virtual)
253 batches submitted to accumulate stats from 16192 documents (455173 virtual)
254 batches submitted to accumulate stats from 16256 documents (456381 virtual)
255 batches submitted to accumulate stats from 16320 documents (459262 virtual)
256 batches submitted to accumulate stats from 16384 documents (462336 virtual)
257 batches submitted to accumulate stats from 16448 documents (462741 virtual)
258 batches submitted to accumulate stats from 16512 documents (463069 virtual)
259 batches submitted to accumulate stats from 16576 documents (464006 virtual)
260 batches submitted to accumulate stats from 16640 documents (467200 virtual)
261 batches submitted to accumulate stats from 16704 documents (470405 virtual)
262 batches submitted to accumulate stats from 16768 documents (473624 virtual)
263 batches submitted to accumulate stats from 16832 documents (475512 virtual)
264 batches submitted to accumulate stats from 16896 documents (479198 virtual)
265 batches submitted to accumulate stats from 16960 documents (481892 virtual)
266 batches submitted to accumulate stats from 17024 documents (483960 virtual)
267 batches submitted to accumulate stats from 17088 documents (491249 virtual)
268 batches submitted to accumulate stats from 17152 documents (494164 virtual)
270 batches submitted to accumulate stats from 17280 documents (495343 virtual)
271 batches submitted to accumulate stats from 17344 documents (499120 virtual)
272 batches submitted to accumulate stats from 17408 documents (500435 virtual)
273 batches submitted to accumulate stats from 17472 documents (501241 virtual)
274 batches submitted to accumulate stats from 17536 documents (502285 virtual)
275 batches submitted to accumulate stats from 17600 documents (503690 virtual)
279 batches submitted to accumulate stats from 17856 documents (500758 virtual)
280 batches submitted to accumulate stats from 17920 documents (501258 virtual)
281 batches submitted to accumulate stats from 17984 documents (504457 virtual)
282 batches submitted to accumulate stats from 18048 documents (505085 virtual)
283 batches submitted to accumulate stats from 18112 documents (509250 virtual)
284 batches submitted to accumulate stats from 18176 documents (511309 virtual)
285 batches submitted to accumulate stats from 18240 documents (513486 virtual)
286 batches submitted to accumulate stats from 18304 documents (514004 virtual)
287 batches submitted to accumulate stats from 18368 documents (514895 virtual)
288 batches submitted to accumulate stats from 18432 documents (515822 virtual)
289 batches submitted to accumulate stats from 18496 documents (517432 virtual)
290 batches submitted to accumulate stats from 18560 documents (518546 virtual)
291 batches submitted to accumulate stats from 18624 documents (519656 virtual)
293 batches submitted to accumulate stats from 18752 documents (518876 virtual)
294 batches submitted to accumulate stats from 18816 documents (519958 virtual)
295 batches submitted to accumulate stats from 18880 documents (521336 virtual)
296 batches submitted to accumulate stats from 18944 documents (526382 virtual)
297 batches submitted to accumulate stats from 19008 documents (528072 virtual)
298 batches submitted to accumulate stats from 19072 documents (529195 virtual)
299 batches submitted to accumulate stats from 19136 documents (531960 virtual)
300 batches submitted to accumulate stats from 19200 documents (539883 virtual)
301 batches submitted to accumulate stats from 19264 documents (540320 virtual)
302 batches submitted to accumulate stats from 19328 documents (540431 virtual)
304 batches submitted to accumulate stats from 19456 documents (539179 virtual)
306 batches submitted to accumulate stats from 19584 documents (541300 virtual)
307 batches submitted to accumulate stats from 19648 documents (542094 virtual)
308 batches submitted to accumulate stats from 19712 documents (542842 virtual)
309 batches submitted to accumulate stats from 19776 documents (544341 virtual)
310 batches submitted to accumulate stats from 19840 documents (545581 virtual)
311 batches submitted to accumulate stats from 19904 documents (556699 virtual)
312 batches submitted to accumulate stats from 19968 documents (558855 virtual)
313 batches submitted to accumulate stats from 20032 documents (560684 virtual)
314 batches submitted to accumulate stats from 20096 documents (563077 virtual)
315 batches submitted to accumulate stats from 20160 documents (569595 virtual)
316 batches submitted to accumulate stats from 20224 documents (570678 virtual)
317 batches submitted to accumulate stats from 20288 documents (572044 virtual)
318 batches submitted to accumulate stats from 20352 documents (573480 virtual)
319 batches submitted to accumulate stats from 20416 documents (575888 virtual)
320 batches submitted to accumulate stats from 20480 documents (576784 virtual)
321 batches submitted to accumulate stats from 20544 documents (579089 virtual)
322 batches submitted to accumulate stats from 20608 documents (580494 virtual)
324 batches submitted to accumulate stats from 20736 documents (580283 virtual)
325 batches submitted to accumulate stats from 20800 documents (581932 virtual)
326 batches submitted to accumulate stats from 20864 documents (582503 virtual)
327 batches submitted to accumulate stats from 20928 documents (582713 virtual)
328 batches submitted to accumulate stats from 20992 documents (585792 virtual)
329 batches submitted to accumulate stats from 21056 documents (587966 virtual)
330 batches submitted to accumulate stats from 21120 documents (592030 virtual)
331 batches submitted to accumulate stats from 21184 documents (596980 virtual)
333 batches submitted to accumulate stats from 21312 documents (597231 virtual)
334 batches submitted to accumulate stats from 21376 documents (597363 virtual)
337 batches submitted to accumulate stats from 21568 documents (600364 virtual)
338 batches submitted to accumulate stats from 21632 documents (601754 virtual)
340 batches submitted to accumulate stats from 21760 documents (601332 virtual)
342 batches submitted to accumulate stats from 21888 documents (603727 virtual)
343 batches submitted to accumulate stats from 21952 documents (606793 virtual)
344 batches submitted to accumulate stats from 22016 documents (608202 virtual)
345 batches submitted to accumulate stats from 22080 documents (609032 virtual)
346 batches submitted to accumulate stats from 22144 documents (611881 virtual)
347 batches submitted to accumulate stats from 22208 documents (614004 virtual)
348 batches submitted to accumulate stats from 22272 documents (616061 virtual)
349 batches submitted to accumulate stats from 22336 documents (617667 virtual)
350 batches submitted to accumulate stats from 22400 documents (619184 virtual)
351 batches submitted to accumulate stats from 22464 documents (621708 virtual)
353 batches submitted to accumulate stats from 22592 documents (624562 virtual)
354 batches submitted to accumulate stats from 22656 documents (629203 virtual)
355 batches submitted to accumulate stats from 22720 documents (632011 virtual)
357 batches submitted to accumulate stats from 22848 documents (632026 virtual)
358 batches submitted to accumulate stats from 22912 documents (633171 virtual)
359 batches submitted to accumulate stats from 22976 documents (635460 virtual)
360 batches submitted to accumulate stats from 23040 documents (636297 virtual)
362 batches submitted to accumulate stats from 23168 documents (636875 virtual)
363 batches submitted to accumulate stats from 23232 documents (637374 virtual)
364 batches submitted to accumulate stats from 23296 documents (640008 virtual)
365 batches submitted to accumulate stats from 23360 documents (642613 virtual)
366 batches submitted to accumulate stats from 23424 documents (643810 virtual)
367 batches submitted to accumulate stats from 23488 documents (644066 virtual)
368 batches submitted to accumulate stats from 23552 documents (644116 virtual)
369 batches submitted to accumulate stats from 23616 documents (645113 virtual)
371 batches submitted to accumulate stats from 23744 documents (645136 virtual)
372 batches submitted to accumulate stats from 23808 documents (650165 virtual)
373 batches submitted to accumulate stats from 23872 documents (653812 virtual)
374 batches submitted to accumulate stats from 23936 documents (655841 virtual)
375 batches submitted to accumulate stats from 24000 documents (658313 virtual)
376 batches submitted to accumulate stats from 24064 documents (660725 virtual)
378 batches submitted to accumulate stats from 24192 documents (670671 virtual)
379 batches submitted to accumulate stats from 24256 documents (672382 virtual)
380 batches submitted to accumulate stats from 24320 documents (673324 virtual)
381 batches submitted to accumulate stats from 24384 documents (674383 virtual)
382 batches submitted to accumulate stats from 24448 documents (675365 virtual)
383 batches submitted to accumulate stats from 24512 documents (682126 virtual)
384 batches submitted to accumulate stats from 24576 documents (689091 virtual)
386 batches submitted to accumulate stats from 24704 documents (690880 virtual)
387 batches submitted to accumulate stats from 24768 documents (695213 virtual)
390 batches submitted to accumulate stats from 24960 documents (693585 virtual)
391 batches submitted to accumulate stats from 25024 documents (694682 virtual)
392 batches submitted to accumulate stats from 25088 documents (696115 virtual)
393 batches submitted to accumulate stats from 25152 documents (700140 virtual)
395 batches submitted to accumulate stats from 25280 documents (700782 virtual)
396 batches submitted to accumulate stats from 25344 documents (703351 virtual)
398 batches submitted to accumulate stats from 25472 documents (702845 virtual)
400 batches submitted to accumulate stats from 25600 documents (704842 virtual)
402 batches submitted to accumulate stats from 25728 documents (706418 virtual)
403 batches submitted to accumulate stats from 25792 documents (707069 virtual)
404 batches submitted to accumulate stats from 25856 documents (708076 virtual)
406 batches submitted to accumulate stats from 25984 documents (709296 virtual)
407 batches submitted to accumulate stats from 26048 documents (709524 virtual)
408 batches submitted to accumulate stats from 26112 documents (710436 virtual)
409 batches submitted to accumulate stats from 26176 documents (712618 virtual)
410 batches submitted to accumulate stats from 26240 documents (718232 virtual)
412 batches submitted to accumulate stats from 26368 documents (720141 virtual)
413 batches submitted to accumulate stats from 26432 documents (730613 virtual)
414 batches submitted to accumulate stats from 26496 documents (735199 virtual)
416 batches submitted to accumulate stats from 26624 documents (736480 virtual)
417 batches submitted to accumulate stats from 26688 documents (737746 virtual)
418 batches submitted to accumulate stats from 26752 documents (740395 virtual)
419 batches submitted to accumulate stats from 26816 documents (741073 virtual)
420 batches submitted to accumulate stats from 26880 documents (742760 virtual)
422 batches submitted to accumulate stats from 27008 documents (747792 virtual)
423 batches submitted to accumulate stats from 27072 documents (749386 virtual)
424 batches submitted to accumulate stats from 27136 documents (754356 virtual)
427 batches submitted to accumulate stats from 27328 documents (751778 virtual)
428 batches submitted to accumulate stats from 27392 documents (752052 virtual)
429 batches submitted to accumulate stats from 27456 documents (753538 virtual)
430 batches submitted to accumulate stats from 27520 documents (754380 virtual)
431 batches submitted to accumulate stats from 27584 documents (756063 virtual)
433 batches submitted to accumulate stats from 27712 documents (756831 virtual)
434 batches submitted to accumulate stats from 27776 documents (757278 virtual)
435 batches submitted to accumulate stats from 27840 documents (762100 virtual)
436 batches submitted to accumulate stats from 27904 documents (764494 virtual)
437 batches submitted to accumulate stats from 27968 documents (767429 virtual)
438 batches submitted to accumulate stats from 28032 documents (768888 virtual)
439 batches submitted to accumulate stats from 28096 documents (770750 virtual)
441 batches submitted to accumulate stats from 28224 documents (770456 virtual)
442 batches submitted to accumulate stats from 28288 documents (775098 virtual)
444 batches submitted to accumulate stats from 28416 documents (774618 virtual)
446 batches submitted to accumulate stats from 28544 documents (779480 virtual)
447 batches submitted to accumulate stats from 28608 documents (781664 virtual)
448 batches submitted to accumulate stats from 28672 documents (784514 virtual)
449 batches submitted to accumulate stats from 28736 documents (788893 virtual)
450 batches submitted to accumulate stats from 28800 documents (790476 virtual)
451 batches submitted to accumulate stats from 28864 documents (793036 virtual)
452 batches submitted to accumulate stats from 28928 documents (794900 virtual)
453 batches submitted to accumulate stats from 28992 documents (799934 virtual)
454 batches submitted to accumulate stats from 29056 documents (808064 virtual)
456 batches submitted to accumulate stats from 29184 documents (809737 virtual)
457 batches submitted to accumulate stats from 29248 documents (813076 virtual)
458 batches submitted to accumulate stats from 29312 documents (816342 virtual)
459 batches submitted to accumulate stats from 29376 documents (818990 virtual)
460 batches submitted to accumulate stats from 29440 documents (827465 virtual)
461 batches submitted to accumulate stats from 29504 documents (827875 virtual)
462 batches submitted to accumulate stats from 29568 documents (829266 virtual)
463 batches submitted to accumulate stats from 29632 documents (829396 virtual)
466 batches submitted to accumulate stats from 29824 documents (829633 virtual)
468 batches submitted to accumulate stats from 29952 documents (830335 virtual)
469 batches submitted to accumulate stats from 30016 documents (833660 virtual)
470 batches submitted to accumulate stats from 30080 documents (834731 virtual)
472 batches submitted to accumulate stats from 30208 documents (834906 virtual)
473 batches submitted to accumulate stats from 30272 documents (837084 virtual)
474 batches submitted to accumulate stats from 30336 documents (843236 virtual)
475 batches submitted to accumulate stats from 30400 documents (844007 virtual)
476 batches submitted to accumulate stats from 30464 documents (848296 virtual)
477 batches submitted to accumulate stats from 30528 documents (851954 virtual)
479 batches submitted to accumulate stats from 30656 documents (859539 virtual)
480 batches submitted to accumulate stats from 30720 documents (862737 virtual)
481 batches submitted to accumulate stats from 30784 documents (866004 virtual)
482 batches submitted to accumulate stats from 30848 documents (866401 virtual)
483 batches submitted to accumulate stats from 30912 documents (869601 virtual)
484 batches submitted to accumulate stats from 30976 documents (871852 virtual)
485 batches submitted to accumulate stats from 31040 documents (873722 virtual)
486 batches submitted to accumulate stats from 31104 documents (878860 virtual)
487 batches submitted to accumulate stats from 31168 documents (881730 virtual)
488 batches submitted to accumulate stats from 31232 documents (882478 virtual)
490 batches submitted to accumulate stats from 31360 documents (885663 virtual)
491 batches submitted to accumulate stats from 31424 documents (888375 virtual)
492 batches submitted to accumulate stats from 31488 documents (892384 virtual)
493 batches submitted to accumulate stats from 31552 documents (893008 virtual)
494 batches submitted to accumulate stats from 31616 documents (894061 virtual)
495 batches submitted to accumulate stats from 31680 documents (899222 virtual)
496 batches submitted to accumulate stats from 31744 documents (906554 virtual)
497 batches submitted to accumulate stats from 31808 documents (909458 virtual)
498 batches submitted to accumulate stats from 31872 documents (914946 virtual)
499 batches submitted to accumulate stats from 31936 documents (916887 virtual)
500 batches submitted to accumulate stats from 32000 documents (918432 virtual)
502 batches submitted to accumulate stats from 32128 documents (918910 virtual)
503 batches submitted to accumulate stats from 32192 documents (920242 virtual)
504 batches submitted to accumulate stats from 32256 documents (922183 virtual)
505 batches submitted to accumulate stats from 32320 documents (924132 virtual)
506 batches submitted to accumulate stats from 32384 documents (926900 virtual)
507 batches submitted to accumulate stats from 32448 documents (927904 virtual)
508 batches submitted to accumulate stats from 32512 documents (929150 virtual)
509 batches submitted to accumulate stats from 32576 documents (931291 virtual)
510 batches submitted to accumulate stats from 32640 documents (932510 virtual)
511 batches submitted to accumulate stats from 32704 documents (932558 virtual)
513 batches submitted to accumulate stats from 32832 documents (934365 virtual)
514 batches submitted to accumulate stats from 32896 documents (941093 virtual)
515 batches submitted to accumulate stats from 32960 documents (941145 virtual)
518 batches submitted to accumulate stats from 33152 documents (946987 virtual)
519 batches submitted to accumulate stats from 33216 documents (952404 virtual)
520 batches submitted to accumulate stats from 33280 documents (953586 virtual)
521 batches submitted to accumulate stats from 33344 documents (954321 virtual)
522 batches submitted to accumulate stats from 33408 documents (960100 virtual)
523 batches submitted to accumulate stats from 33472 documents (963023 virtual)
524 batches submitted to accumulate stats from 33536 documents (964262 virtual)
525 batches submitted to accumulate stats from 33600 documents (966101 virtual)
526 batches submitted to accumulate stats from 33664 documents (967593 virtual)
527 batches submitted to accumulate stats from 33728 documents (968673 virtual)
528 batches submitted to accumulate stats from 33792 documents (971627 virtual)
530 batches submitted to accumulate stats from 33920 documents (977171 virtual)
531 batches submitted to accumulate stats from 33984 documents (980199 virtual)
532 batches submitted to accumulate stats from 34048 documents (980282 virtual)
534 batches submitted to accumulate stats from 34176 documents (980903 virtual)
536 batches submitted to accumulate stats from 34304 documents (985124 virtual)
537 batches submitted to accumulate stats from 34368 documents (987016 virtual)
540 batches submitted to accumulate stats from 34560 documents (987018 virtual)
541 batches submitted to accumulate stats from 34624 documents (988325 virtual)
544 batches submitted to accumulate stats from 34816 documents (987386 virtual)
545 batches submitted to accumulate stats from 34880 documents (991580 virtual)
546 batches submitted to accumulate stats from 34944 documents (993347 virtual)
547 batches submitted to accumulate stats from 35008 documents (996465 virtual)
548 batches submitted to accumulate stats from 35072 documents (1001425 virtual)
549 batches submitted to accumulate stats from 35136 documents (1004553 virtual)
550 batches submitted to accumulate stats from 35200 documents (1005485 virtual)
551 batches submitted to accumulate stats from 35264 documents (1006308 virtual)
552 batches submitted to accumulate stats from 35328 documents (1008902 virtual)
554 batches submitted to accumulate stats from 35456 documents (1011015 virtual)
556 batches submitted to accumulate stats from 35584 documents (1011316 virtual)
557 batches submitted to accumulate stats from 35648 documents (1013086 virtual)
558 batches submitted to accumulate stats from 35712 documents (1017646 virtual)
560 batches submitted to accumulate stats from 35840 documents (1018595 virtual)
561 batches submitted to accumulate stats from 35904 documents (1019444 virtual)
562 batches submitted to accumulate stats from 35968 documents (1022857 virtual)
564 batches submitted to accumulate stats from 36096 documents (1024965 virtual)
565 batches submitted to accumulate stats from 36160 documents (1030059 virtual)
567 batches submitted to accumulate stats from 36288 documents (1038959 virtual)
568 batches submitted to accumulate stats from 36352 documents (1041416 virtual)
569 batches submitted to accumulate stats from 36416 documents (1046088 virtual)
570 batches submitted to accumulate stats from 36480 documents (1049378 virtual)
571 batches submitted to accumulate stats from 36544 documents (1057494 virtual)
574 batches submitted to accumulate stats from 36736 documents (1057261 virtual)
576 batches submitted to accumulate stats from 36864 documents (1058540 virtual)
578 batches submitted to accumulate stats from 36992 documents (1060224 virtual)
579 batches submitted to accumulate stats from 37056 documents (1062218 virtual)
580 batches submitted to accumulate stats from 37120 documents (1065267 virtual)
581 batches submitted to accumulate stats from 37184 documents (1067956 virtual)
584 batches submitted to accumulate stats from 37376 documents (1070716 virtual)
585 batches submitted to accumulate stats from 37440 documents (1072788 virtual)
586 batches submitted to accumulate stats from 37504 documents (1078813 virtual)
588 batches submitted to accumulate stats from 37632 documents (1083492 virtual)
590 batches submitted to accumulate stats from 37760 documents (1084229 virtual)
592 batches submitted to accumulate stats from 37888 documents (1089038 virtual)
593 batches submitted to accumulate stats from 37952 documents (1092197 virtual)
595 batches submitted to accumulate stats from 38080 documents (1091372 virtual)
598 batches submitted to accumulate stats from 38272 documents (1091505 virtual)
599 batches submitted to accumulate stats from 38336 documents (1094275 virtual)
600 batches submitted to accumulate stats from 38400 documents (1095219 virtual)
601 batches submitted to accumulate stats from 38464 documents (1099053 virtual)
602 batches submitted to accumulate stats from 38528 documents (1103122 virtual)
603 batches submitted to accumulate stats from 38592 documents (1105639 virtual)
604 batches submitted to accumulate stats from 38656 documents (1109400 virtual)
605 batches submitted to accumulate stats from 38720 documents (1111276 virtual)
606 batches submitted to accumulate stats from 38784 documents (1113453 virtual)
609 batches submitted to accumulate stats from 38976 documents (1111627 virtual)
610 batches submitted to accumulate stats from 39040 documents (1114804 virtual)
611 batches submitted to accumulate stats from 39104 documents (1118082 virtual)
612 batches submitted to accumulate stats from 39168 documents (1118622 virtual)
613 batches submitted to accumulate stats from 39232 documents (1122367 virtual)
614 batches submitted to accumulate stats from 39296 documents (1122993 virtual)
615 batches submitted to accumulate stats from 39360 documents (1125367 virtual)
616 batches submitted to accumulate stats from 39424 documents (1126964 virtual)
618 batches submitted to accumulate stats from 39552 documents (1127766 virtual)
619 batches submitted to accumulate stats from 39616 documents (1128868 virtual)
622 batches submitted to accumulate stats from 39808 documents (1131066 virtual)
623 batches submitted to accumulate stats from 39872 documents (1132876 virtual)
624 batches submitted to accumulate stats from 39936 documents (1134050 virtual)
626 batches submitted to accumulate stats from 40064 documents (1134314 virtual)
627 batches submitted to accumulate stats from 40128 documents (1136088 virtual)
629 batches submitted to accumulate stats from 40256 documents (1136191 virtual)
630 batches submitted to accumulate stats from 40320 documents (1144906 virtual)
631 batches submitted to accumulate stats from 40384 documents (1147410 virtual)
632 batches submitted to accumulate stats from 40448 documents (1159071 virtual)
634 batches submitted to accumulate stats from 40576 documents (1159845 virtual)
635 batches submitted to accumulate stats from 40640 documents (1160038 virtual)
636 batches submitted to accumulate stats from 40704 documents (1161645 virtual)
637 batches submitted to accumulate stats from 40768 documents (1164458 virtual)
638 batches submitted to accumulate stats from 40832 documents (1167466 virtual)
639 batches submitted to accumulate stats from 40896 documents (1172626 virtual)
640 batches submitted to accumulate stats from 40960 documents (1177524 virtual)
641 batches submitted to accumulate stats from 41024 documents (1177581 virtual)
642 batches submitted to accumulate stats from 41088 documents (1177809 virtual)
644 batches submitted to accumulate stats from 41216 documents (1180869 virtual)
645 batches submitted to accumulate stats from 41280 documents (1182635 virtual)
647 batches submitted to accumulate stats from 41408 documents (1183947 virtual)
648 batches submitted to accumulate stats from 41472 documents (1188483 virtual)
650 batches submitted to accumulate stats from 41600 documents (1188672 virtual)
653 batches submitted to accumulate stats from 41792 documents (1190401 virtual)
654 batches submitted to accumulate stats from 41856 documents (1193424 virtual)
655 batches submitted to accumulate stats from 41920 documents (1197646 virtual)
656 batches submitted to accumulate stats from 41984 documents (1198659 virtual)
658 batches submitted to accumulate stats from 42112 documents (1201659 virtual)
660 batches submitted to accumulate stats from 42240 documents (1205970 virtual)
661 batches submitted to accumulate stats from 42304 documents (1208678 virtual)
662 batches submitted to accumulate stats from 42368 documents (1209894 virtual)
663 batches submitted to accumulate stats from 42432 documents (1213366 virtual)
665 batches submitted to accumulate stats from 42560 documents (1217991 virtual)
666 batches submitted to accumulate stats from 42624 documents (1218602 virtual)
668 batches submitted to accumulate stats from 42752 documents (1218771 virtual)
669 batches submitted to accumulate stats from 42816 documents (1219908 virtual)
671 batches submitted to accumulate stats from 42944 documents (1221672 virtual)
672 batches submitted to accumulate stats from 43008 documents (1222317 virtual)
673 batches submitted to accumulate stats from 43072 documents (1224743 virtual)
674 batches submitted to accumulate stats from 43136 documents (1225016 virtual)
675 batches submitted to accumulate stats from 43200 documents (1227108 virtual)
678 batches submitted to accumulate stats from 43392 documents (1226389 virtual)
679 batches submitted to accumulate stats from 43456 documents (1228341 virtual)
680 batches submitted to accumulate stats from 43520 documents (1233775 virtual)
681 batches submitted to accumulate stats from 43584 documents (1235276 virtual)
682 batches submitted to accumulate stats from 43648 documents (1243282 virtual)
683 batches submitted to accumulate stats from 43712 documents (1245734 virtual)
684 batches submitted to accumulate stats from 43776 documents (1246335 virtual)
685 batches submitted to accumulate stats from 43840 documents (1247016 virtual)
686 batches submitted to accumulate stats from 43904 documents (1250831 virtual)
687 batches submitted to accumulate stats from 43968 documents (1256584 virtual)
689 batches submitted to accumulate stats from 44096 documents (1259137 virtual)
690 batches submitted to accumulate stats from 44160 documents (1261909 virtual)
693 batches submitted to accumulate stats from 44352 documents (1261683 virtual)
694 batches submitted to accumulate stats from 44416 documents (1268662 virtual)
695 batches submitted to accumulate stats from 44480 documents (1269528 virtual)
696 batches submitted to accumulate stats from 44544 documents (1269950 virtual)
697 batches submitted to accumulate stats from 44608 documents (1273304 virtual)
698 batches submitted to accumulate stats from 44672 documents (1274984 virtual)
699 batches submitted to accumulate stats from 44736 documents (1277926 virtual)
700 batches submitted to accumulate stats from 44800 documents (1291299 virtual)
701 batches submitted to accumulate stats from 44864 documents (1292081 virtual)
702 batches submitted to accumulate stats from 44928 documents (1293351 virtual)
703 batches submitted to accumulate stats from 44992 documents (1294758 virtual)
704 batches submitted to accumulate stats from 45056 documents (1298763 virtual)
705 batches submitted to accumulate stats from 45120 documents (1314030 virtual)
706 batches submitted to accumulate stats from 45184 documents (1315214 virtual)
707 batches submitted to accumulate stats from 45248 documents (1318621 virtual)
708 batches submitted to accumulate stats from 45312 documents (1319094 virtual)
709 batches submitted to accumulate stats from 45376 documents (1319354 virtual)
711 batches submitted to accumulate stats from 45504 documents (1321719 virtual)
712 batches submitted to accumulate stats from 45568 documents (1325835 virtual)
713 batches submitted to accumulate stats from 45632 documents (1328198 virtual)
714 batches submitted to accumulate stats from 45696 documents (1331012 virtual)
715 batches submitted to accumulate stats from 45760 documents (1332869 virtual)
716 batches submitted to accumulate stats from 45824 documents (1335866 virtual)
717 batches submitted to accumulate stats from 45888 documents (1344548 virtual)
718 batches submitted to accumulate stats from 45952 documents (1346203 virtual)
719 batches submitted to accumulate stats from 46016 documents (1352993 virtual)
720 batches submitted to accumulate stats from 46080 documents (1356530 virtual)
721 batches submitted to accumulate stats from 46144 documents (1358775 virtual)
722 batches submitted to accumulate stats from 46208 documents (1363234 virtual)
723 batches submitted to accumulate stats from 46272 documents (1364770 virtual)
724 batches submitted to accumulate stats from 46336 documents (1365502 virtual)
725 batches submitted to accumulate stats from 46400 documents (1365737 virtual)
727 batches submitted to accumulate stats from 46528 documents (1365694 virtual)
728 batches submitted to accumulate stats from 46592 documents (1367467 virtual)
729 batches submitted to accumulate stats from 46656 documents (1367763 virtual)
730 batches submitted to accumulate stats from 46720 documents (1371495 virtual)
734 batches submitted to accumulate stats from 46976 documents (1377258 virtual)
735 batches submitted to accumulate stats from 47040 documents (1381346 virtual)
736 batches submitted to accumulate stats from 47104 documents (1384938 virtual)
737 batches submitted to accumulate stats from 47168 documents (1388022 virtual)
739 batches submitted to accumulate stats from 47296 documents (1390341 virtual)
740 batches submitted to accumulate stats from 47360 documents (1394699 virtual)
742 batches submitted to accumulate stats from 47488 documents (1393560 virtual)
743 batches submitted to accumulate stats from 47552 documents (1396986 virtual)
745 batches submitted to accumulate stats from 47680 documents (1396544 virtual)
746 batches submitted to accumulate stats from 47744 documents (1399846 virtual)
747 batches submitted to accumulate stats from 47808 documents (1403543 virtual)
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
3 accumulators retrieved from output queue
accumulated word occurrence stats for 3847441 virtual documents
K=15, Coherence Score: 0.624953968141419
Starting K=20
using symmetric alpha at 0.05
using symmetric eta at 0.05
using serial LDA version on this node
running online LDA training, 20 topics, 10 passes over the supplied corpus of 49835 documents, updating every 3000 documents, evaluating every ~49835 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 3 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 10
PROGRESS: pass 0, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.004*"building" + 0.003*"game" + 0.003*"system" + 0.003*"design" + 0.002*"level" + 0.002*"control" + 0.002*"engine" + 0.002*"war" + 0.002*"study" + 0.002*"company"
topic #11 (0.050): 0.004*"game" + 0.004*"season" + 0.004*"series" + 0.003*"get" + 0.003*"station" + 0.002*"character" + 0.002*"episode" + 0.002*"park" + 0.002*"samantha" + 0.002*"system"
topic #13 (0.050): 0.007*"film" + 0.004*"game" + 0.004*"age" + 0.003*"government" + 0.003*"town" + 0.003*"season" + 0.003*"war" + 0.003*"village" + 0.003*"engineering" + 0.003*"population"
topic #0 (0.050): 0.005*"define" + 0.005*"nationality" + 0.005*"rules_player" + 0.005*"non_fifa" + 0.003*"game" + 0.003*"building" + 0.003*"displaystyle" + 0.002*"event" + 0.002*"note_flag" + 0.002*"site"
topic #5 (0.050): 0.004*"game" + 0.004*"support" + 0.003*"building" + 0.002*"season" + 0.002*"award" + 0.002*"series" + 0.002*"life" + 0.002*"say" + 0.002*"woman" + 0.002*"set"
topic diff=8.815159, rho=1.000000
PROGRESS: pass 0, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.006*"song" + 0.005*"band" + 0.004*"series" + 0.004*"film" + 0.004*"album" + 0.003*"award" + 0.003*"episode" + 0.003*"produce" + 0.003*"say" + 0.003*"television"
topic #11 (0.050): 0.004*"game" + 0.004*"season" + 0.003*"series" + 0.002*"station" + 0.002*"episode" + 0.002*"get" + 0.002*"character" + 0.002*"version" + 0.002*"park" + 0.002*"player"
topic #8 (0.050): 0.008*"song" + 0.005*"band" + 0.005*"music" + 0.004*"album" + 0.003*"say" + 0.003*"single" + 0.003*"game" + 0.003*"film" + 0.003*"war" + 0.003*"life"
topic #15 (0.050): 0.006*"game" + 0.006*"season" + 0.004*"player" + 0.003*"finish" + 0.003*"company" + 0.003*"building" + 0.002*"club" + 0.002*"series" + 0.002*"final" + 0.002*"championship"
topic #19 (0.050): 0.003*"house" + 0.003*"woman" + 0.003*"company" + 0.003*"design" + 0.002*"building" + 0.002*"town" + 0.002*"river" + 0.002*"album" + 0.002*"public" + 0.002*"season"
topic diff=3.553367, rho=0.500000
PROGRESS: pass 0, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.050): 0.009*"station" + 0.004*"season" + 0.003*"court" + 0.003*"line" + 0.003*"law" + 0.003*"public" + 0.003*"company" + 0.003*"community" + 0.003*"child" + 0.002*"railway"
topic #6 (0.050): 0.004*"season" + 0.004*"specie" + 0.003*"island" + 0.003*"church" + 0.003*"ship" + 0.003*"study" + 0.002*"describe" + 0.002*"point" + 0.002*"building" + 0.002*"village"
topic #5 (0.050): 0.004*"game" + 0.003*"woman" + 0.003*"support" + 0.003*"say" + 0.002*"life" + 0.002*"event" + 0.002*"displaystyle" + 0.002*"award" + 0.002*"season" + 0.002*"program"
topic #4 (0.050): 0.007*"film" + 0.006*"game" + 0.004*"line" + 0.004*"route" + 0.004*"series" + 0.004*"station" + 0.003*"road" + 0.003*"player" + 0.003*"season" + 0.003*"football"
topic #8 (0.050): 0.010*"song" + 0.007*"music" + 0.006*"album" + 0.005*"band" + 0.004*"single" + 0.003*"say" + 0.003*"film" + 0.003*"game" + 0.003*"video" + 0.003*"tour"
topic diff=1.120006, rho=0.377964
PROGRESS: pass 0, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.012*"song" + 0.009*"music" + 0.008*"album" + 0.007*"band" + 0.005*"single" + 0.004*"say" + 0.003*"video" + 0.003*"film" + 0.003*"tour" + 0.003*"life"
topic #1 (0.050): 0.015*"film" + 0.004*"game" + 0.004*"series" + 0.004*"final" + 0.003*"character" + 0.003*"story" + 0.003*"title" + 0.003*"event" + 0.003*"championship" + 0.003*"star"
topic #17 (0.050): 0.005*"county" + 0.003*"company" + 0.003*"study" + 0.003*"society" + 0.002*"art" + 0.002*"say" + 0.002*"building" + 0.002*"change" + 0.002*"community" + 0.002*"system"
topic #0 (0.050): 0.005*"displaystyle" + 0.004*"define" + 0.004*"game" + 0.003*"order" + 0.003*"rules_player" + 0.003*"village" + 0.003*"title" + 0.003*"non_fifa" + 0.003*"event" + 0.002*"county"
topic #11 (0.050): 0.006*"game" + 0.005*"season" + 0.005*"series" + 0.003*"episode" + 0.003*"character" + 0.003*"version" + 0.003*"station" + 0.003*"get" + 0.002*"system" + 0.002*"car"
topic diff=0.552729, rho=0.316228
PROGRESS: pass 0, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.010*"album" + 0.009*"art" + 0.008*"film" + 0.006*"music" + 0.005*"season" + 0.004*"artist" + 0.004*"festival" + 0.004*"church" + 0.003*"song" + 0.003*"single"
topic #14 (0.050): 0.007*"park" + 0.007*"population" + 0.005*"age" + 0.005*"village" + 0.003*"land" + 0.003*"race" + 0.003*"male" + 0.003*"specie" + 0.003*"river" + 0.003*"island"
topic #13 (0.050): 0.006*"government" + 0.005*"film" + 0.004*"college" + 0.004*"town" + 0.004*"age" + 0.004*"war" + 0.003*"population" + 0.003*"force" + 0.003*"program" + 0.003*"student"
topic #17 (0.050): 0.004*"county" + 0.003*"society" + 0.003*"company" + 0.003*"study" + 0.003*"information" + 0.002*"change" + 0.002*"art" + 0.002*"building" + 0.002*"system" + 0.002*"law"
topic #9 (0.050): 0.007*"series" + 0.006*"film" + 0.005*"award" + 0.005*"television" + 0.004*"episode" + 0.004*"song" + 0.004*"book" + 0.003*"produce" + 0.003*"band" + 0.003*"say"
topic diff=0.516442, rho=0.277350
PROGRESS: pass 0, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 10
PROGRESS: pass 0, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.006*"system" + 0.005*"design" + 0.005*"model" + 0.005*"engine" + 0.004*"vehicle" + 0.003*"building" + 0.003*"aircraft" + 0.003*"power" + 0.003*"line" + 0.003*"company"
topic #11 (0.050): 0.006*"game" + 0.005*"series" + 0.005*"season" + 0.004*"character" + 0.004*"episode" + 0.004*"get" + 0.003*"version" + 0.003*"car" + 0.003*"station" + 0.002*"air"
topic #8 (0.050): 0.016*"song" + 0.013*"music" + 0.012*"album" + 0.008*"band" + 0.007*"single" + 0.004*"video" + 0.004*"perform" + 0.004*"track" + 0.004*"say" + 0.004*"tour"
topic #6 (0.050): 0.007*"ship" + 0.006*"specie" + 0.004*"island" + 0.003*"genus" + 0.003*"describe" + 0.003*"point" + 0.003*"church" + 0.003*"study" + 0.003*"gun" + 0.002*"life"
topic #13 (0.050): 0.006*"government" + 0.005*"college" + 0.005*"town" + 0.004*"force" + 0.004*"war" + 0.004*"film" + 0.003*"university" + 0.003*"student" + 0.003*"age" + 0.003*"military"
topic diff=0.489966, rho=0.250000
PROGRESS: pass 0, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.009*"ship" + 0.008*"specie" + 0.005*"island" + 0.004*"genus" + 0.003*"describe" + 0.003*"gun" + 0.003*"point" + 0.003*"church" + 0.002*"study" + 0.002*"class"
topic #15 (0.050): 0.025*"game" + 0.014*"season" + 0.012*"player" + 0.006*"finish" + 0.006*"championship" + 0.005*"basketball" + 0.005*"tournament" + 0.004*"point" + 0.004*"conference" + 0.004*"round"
topic #8 (0.050): 0.018*"song" + 0.014*"album" + 0.013*"music" + 0.010*"band" + 0.008*"single" + 0.005*"perform" + 0.004*"tour" + 0.004*"track" + 0.004*"video" + 0.004*"say"
topic #16 (0.050): 0.006*"war" + 0.005*"army" + 0.004*"event" + 0.004*"unit" + 0.004*"german" + 0.004*"squadron" + 0.004*"championship" + 0.004*"compete" + 0.004*"force" + 0.003*"air_force"
topic #17 (0.050): 0.004*"study" + 0.003*"consumer" + 0.003*"county" + 0.003*"company" + 0.003*"information" + 0.003*"society" + 0.003*"product" + 0.003*"research" + 0.003*"process" + 0.003*"system"
topic diff=0.451785, rho=0.229416
PROGRESS: pass 0, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.050): 0.026*"game" + 0.014*"season" + 0.013*"player" + 0.007*"finish" + 0.007*"championship" + 0.005*"basketball" + 0.005*"tournament" + 0.005*"race" + 0.004*"point" + 0.004*"round"
topic #14 (0.050): 0.008*"population" + 0.007*"park" + 0.007*"village" + 0.006*"age" + 0.005*"river" + 0.004*"specie" + 0.004*"county" + 0.004*"land" + 0.004*"water" + 0.003*"town"
topic #13 (0.050): 0.006*"government" + 0.005*"college" + 0.005*"war" + 0.005*"university" + 0.004*"town" + 0.004*"force" + 0.004*"student" + 0.003*"military" + 0.003*"general" + 0.003*"program"
topic #11 (0.050): 0.007*"game" + 0.005*"character" + 0.005*"series" + 0.005*"episode" + 0.004*"get" + 0.004*"season" + 0.004*"car" + 0.003*"version" + 0.003*"magic" + 0.003*"station"
topic #8 (0.050): 0.018*"song" + 0.016*"album" + 0.014*"music" + 0.012*"band" + 0.009*"single" + 0.006*"perform" + 0.005*"track" + 0.005*"video" + 0.005*"tour" + 0.004*"chart"
topic diff=0.428730, rho=0.213201
PROGRESS: pass 0, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.012*"ship" + 0.009*"specie" + 0.007*"island" + 0.005*"genus" + 0.004*"describe" + 0.003*"vessel" + 0.003*"water" + 0.003*"sea" + 0.003*"gun" + 0.003*"navy"
topic #8 (0.050): 0.019*"song" + 0.018*"album" + 0.015*"music" + 0.013*"band" + 0.009*"single" + 0.006*"perform" + 0.006*"track" + 0.005*"tour" + 0.005*"video" + 0.004*"chart"
topic #19 (0.050): 0.011*"building" + 0.008*"house" + 0.007*"church" + 0.006*"design" + 0.004*"town" + 0.004*"street" + 0.004*"river" + 0.003*"th_century" + 0.003*"style" + 0.003*"main"
topic #10 (0.050): 0.008*"election" + 0.007*"party" + 0.006*"government" + 0.004*"elect" + 0.004*"vote" + 0.004*"say" + 0.004*"political" + 0.004*"support" + 0.003*"son" + 0.003*"general"
topic #16 (0.050): 0.007*"war" + 0.006*"german" + 0.006*"army" + 0.005*"event" + 0.005*"squadron" + 0.005*"unit" + 0.004*"compete" + 0.004*"force" + 0.004*"protein" + 0.004*"air_force"
topic diff=0.410684, rho=0.200000
PROGRESS: pass 0, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.050): 0.015*"company" + 0.006*"business" + 0.005*"government" + 0.005*"found" + 0.004*"club" + 0.004*"public" + 0.004*"international" + 0.004*"law" + 0.004*"establish" + 0.004*"population"
topic #7 (0.050): 0.017*"art" + 0.008*"church" + 0.008*"artist" + 0.006*"painting" + 0.006*"music" + 0.006*"museum" + 0.005*"film" + 0.005*"album" + 0.005*"festival" + 0.005*"study"
topic #14 (0.050): 0.009*"population" + 0.008*"village" + 0.007*"river" + 0.006*"park" + 0.006*"age" + 0.005*"specie" + 0.004*"water" + 0.004*"town" + 0.004*"county" + 0.004*"north"
topic #11 (0.050): 0.009*"game" + 0.005*"character" + 0.005*"get" + 0.005*"episode" + 0.005*"series" + 0.004*"kill" + 0.004*"season" + 0.003*"car" + 0.003*"try" + 0.003*"tell"
topic #3 (0.050): 0.007*"system" + 0.007*"design" + 0.006*"engine" + 0.005*"model" + 0.004*"aircraft" + 0.004*"power" + 0.003*"vehicle" + 0.003*"control" + 0.003*"type" + 0.003*"car"
topic diff=0.406387, rho=0.188982
PROGRESS: pass 0, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.029*"season" + 0.021*"club" + 0.017*"league" + 0.016*"football" + 0.012*"game" + 0.012*"match" + 0.011*"final" + 0.010*"championship" + 0.009*"player" + 0.008*"score"
topic #6 (0.050): 0.014*"ship" + 0.010*"specie" + 0.008*"island" + 0.005*"genus" + 0.004*"describe" + 0.004*"vessel" + 0.004*"sea" + 0.003*"navy" + 0.003*"water" + 0.003*"port"
topic #13 (0.050): 0.007*"government" + 0.006*"college" + 0.006*"university" + 0.006*"war" + 0.005*"student" + 0.004*"german" + 0.004*"force" + 0.004*"military" + 0.004*"program" + 0.004*"graduate"
topic #1 (0.050): 0.035*"film" + 0.007*"star" + 0.006*"character" + 0.005*"series" + 0.005*"story" + 0.004*"game" + 0.004*"direct" + 0.004*"title" + 0.004*"event" + 0.003*"earth"
topic #17 (0.050): 0.006*"research" + 0.005*"study" + 0.004*"process" + 0.004*"product" + 0.004*"increase" + 0.004*"technology" + 0.004*"information" + 0.004*"datum" + 0.003*"system" + 0.003*"provide"
topic diff=0.395560, rho=0.179605
PROGRESS: pass 0, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.014*"building" + 0.010*"church" + 0.010*"house" + 0.006*"design" + 0.006*"street" + 0.004*"town" + 0.004*"th_century" + 0.004*"main" + 0.004*"site" + 0.003*"style"
topic #6 (0.050): 0.015*"ship" + 0.010*"specie" + 0.008*"island" + 0.006*"genus" + 0.005*"describe" + 0.004*"vessel" + 0.004*"sea" + 0.004*"navy" + 0.003*"coast" + 0.003*"port"
topic #11 (0.050): 0.009*"game" + 0.007*"episode" + 0.007*"get" + 0.006*"character" + 0.005*"series" + 0.004*"try" + 0.004*"tell" + 0.004*"kill" + 0.004*"back" + 0.003*"help"
topic #17 (0.050): 0.006*"research" + 0.006*"study" + 0.004*"process" + 0.004*"product" + 0.004*"technology" + 0.004*"increase" + 0.004*"system" + 0.004*"information" + 0.004*"datum" + 0.003*"human"
topic #1 (0.050): 0.038*"film" + 0.008*"star" + 0.006*"character" + 0.005*"story" + 0.005*"series" + 0.004*"direct" + 0.004*"game" + 0.004*"title" + 0.003*"award" + 0.003*"earth"
topic diff=0.372839, rho=0.171499
PROGRESS: pass 0, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.050): 0.008*"game" + 0.007*"get" + 0.007*"episode" + 0.006*"character" + 0.005*"kill" + 0.005*"back" + 0.005*"series" + 0.004*"tell" + 0.004*"try" + 0.004*"help"
topic #16 (0.050): 0.009*"german" + 0.008*"army" + 0.008*"protein" + 0.007*"war" + 0.007*"unit" + 0.006*"event" + 0.006*"air" + 0.005*"squadron" + 0.005*"force" + 0.005*"command"
topic #15 (0.050): 0.036*"game" + 0.020*"season" + 0.014*"player" + 0.010*"race" + 0.008*"finish" + 0.008*"championship" + 0.007*"basketball" + 0.006*"point" + 0.006*"conference" + 0.006*"tournament"
topic #14 (0.050): 0.011*"population" + 0.010*"village" + 0.007*"river" + 0.007*"park" + 0.006*"age" + 0.006*"specie" + 0.005*"town" + 0.005*"county" + 0.005*"water" + 0.004*"north"
topic #19 (0.050): 0.015*"building" + 0.010*"church" + 0.010*"house" + 0.006*"design" + 0.006*"street" + 0.005*"th_century" + 0.004*"town" + 0.004*"site" + 0.004*"main" + 0.003*"style"
topic diff=0.360421, rho=0.164399
PROGRESS: pass 0, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.050): 0.008*"game" + 0.007*"get" + 0.007*"episode" + 0.006*"character" + 0.005*"kill" + 0.005*"back" + 0.005*"series" + 0.005*"tell" + 0.005*"try" + 0.004*"help"
topic #9 (0.050): 0.009*"series" + 0.009*"book" + 0.007*"film" + 0.006*"television" + 0.006*"award" + 0.006*"publish" + 0.005*"episode" + 0.005*"novel" + 0.004*"story" + 0.004*"life"
topic #3 (0.050): 0.009*"system" + 0.007*"design" + 0.005*"engine" + 0.005*"model" + 0.005*"power" + 0.004*"car" + 0.004*"aircraft" + 0.003*"type" + 0.003*"produce" + 0.003*"control"
topic #1 (0.050): 0.043*"film" + 0.008*"star" + 0.006*"character" + 0.006*"story" + 0.005*"series" + 0.005*"direct" + 0.004*"game" + 0.004*"award" + 0.003*"event" + 0.003*"title"
topic #13 (0.050): 0.007*"college" + 0.007*"university" + 0.007*"government" + 0.006*"german" + 0.006*"student" + 0.006*"war" + 0.004*"military" + 0.004*"study" + 0.004*"graduate" + 0.004*"department"
topic diff=0.338494, rho=0.158114
-8.429 per-word bound, 344.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.010*"series" + 0.009*"book" + 0.007*"film" + 0.006*"award" + 0.006*"television" + 0.006*"publish" + 0.005*"episode" + 0.005*"novel" + 0.005*"story" + 0.004*"life"
topic #16 (0.050): 0.009*"army" + 0.009*"german" + 0.009*"unit" + 0.008*"war" + 0.007*"protein" + 0.006*"air" + 0.006*"event" + 0.006*"force" + 0.006*"command" + 0.005*"squadron"
topic #3 (0.050): 0.009*"system" + 0.007*"design" + 0.005*"engine" + 0.005*"model" + 0.005*"power" + 0.004*"car" + 0.004*"aircraft" + 0.003*"produce" + 0.003*"type" + 0.003*"control"
topic #2 (0.050): 0.031*"season" + 0.024*"club" + 0.017*"league" + 0.016*"football" + 0.013*"match" + 0.012*"game" + 0.012*"championship" + 0.011*"final" + 0.010*"player" + 0.009*"score"
topic #13 (0.050): 0.008*"college" + 0.007*"university" + 0.007*"student" + 0.007*"government" + 0.006*"war" + 0.006*"german" + 0.004*"military" + 0.004*"education" + 0.004*"study" + 0.004*"department"
topic diff=0.335222, rho=0.152499
-8.419 per-word bound, 342.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #9 (0.050): 0.010*"series" + 0.009*"book" + 0.007*"film" + 0.006*"television" + 0.006*"award" + 0.006*"publish" + 0.005*"episode" + 0.005*"novel" + 0.005*"story" + 0.005*"life"
topic #1 (0.050): 0.049*"film" + 0.008*"star" + 0.007*"character" + 0.006*"story" + 0.006*"direct" + 0.005*"series" + 0.004*"game" + 0.004*"award" + 0.004*"earth" + 0.003*"title"
topic #3 (0.050): 0.010*"system" + 0.008*"design" + 0.005*"model" + 0.005*"engine" + 0.004*"power" + 0.004*"car" + 0.004*"produce" + 0.004*"aircraft" + 0.004*"line" + 0.003*"type"
topic #15 (0.050): 0.040*"game" + 0.021*"season" + 0.017*"player" + 0.013*"race" + 0.009*"finish" + 0.008*"basketball" + 0.008*"championship" + 0.007*"point" + 0.006*"tournament" + 0.006*"conference"
topic #13 (0.050): 0.008*"college" + 0.008*"student" + 0.008*"university" + 0.007*"government" + 0.006*"war" + 0.006*"german" + 0.005*"education" + 0.005*"study" + 0.004*"military" + 0.004*"department"
topic diff=0.325598, rho=0.147442
-8.330 per-word bound, 321.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1000 documents into a model of 49835 documents
topic #19 (0.050): 0.017*"building" + 0.011*"house" + 0.010*"church" + 0.007*"design" + 0.006*"street" + 0.005*"th_century" + 0.005*"site" + 0.005*"town" + 0.004*"main" + 0.004*"style"
topic #7 (0.050): 0.022*"art" + 0.011*"church" + 0.010*"artist" + 0.009*"museum" + 0.007*"study" + 0.006*"painting" + 0.006*"festival" + 0.005*"exhibition" + 0.005*"bishop" + 0.004*"award"
topic #4 (0.050): 0.010*"line" + 0.010*"road" + 0.009*"route" + 0.008*"battle" + 0.008*"army" + 0.006*"force" + 0.006*"north" + 0.006*"attack" + 0.006*"war" + 0.006*"highway"
topic #11 (0.050): 0.007*"get" + 0.007*"episode" + 0.007*"game" + 0.006*"kill" + 0.006*"tell" + 0.005*"back" + 0.005*"character" + 0.005*"try" + 0.004*"series" + 0.004*"help"
topic #15 (0.050): 0.042*"game" + 0.022*"season" + 0.017*"player" + 0.012*"race" + 0.009*"finish" + 0.008*"basketball" + 0.007*"championship" + 0.007*"point" + 0.006*"tournament" + 0.006*"conference"
topic diff=0.314522, rho=0.141655
-8.333 per-word bound, 322.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.022*"art" + 0.011*"church" + 0.010*"artist" + 0.010*"museum" + 0.007*"study" + 0.007*"painting" + 0.006*"festival" + 0.005*"exhibition" + 0.005*"bishop" + 0.004*"history"
topic #4 (0.050): 0.010*"line" + 0.010*"road" + 0.009*"route" + 0.009*"battle" + 0.008*"army" + 0.006*"force" + 0.006*"war" + 0.006*"north" + 0.006*"attack" + 0.006*"highway"
topic #10 (0.050): 0.011*"election" + 0.009*"party" + 0.008*"government" + 0.006*"vote" + 0.006*"elect" + 0.005*"political" + 0.004*"son" + 0.004*"general" + 0.004*"support" + 0.004*"candidate"
topic #14 (0.050): 0.012*"population" + 0.012*"village" + 0.007*"age" + 0.007*"river" + 0.007*"park" + 0.006*"town" + 0.006*"county" + 0.006*"specie" + 0.005*"census" + 0.005*"water"
topic #3 (0.050): 0.010*"system" + 0.008*"design" + 0.005*"model" + 0.005*"engine" + 0.005*"power" + 0.005*"vehicle" + 0.004*"car" + 0.004*"produce" + 0.003*"aircraft" + 0.003*"type"
topic diff=0.305794, rho=0.138896
PROGRESS: pass 1, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.050): 0.054*"film" + 0.009*"star" + 0.007*"character" + 0.006*"direct" + 0.006*"story" + 0.005*"series" + 0.004*"earth" + 0.004*"award" + 0.004*"game" + 0.003*"director"
topic #3 (0.050): 0.010*"system" + 0.008*"design" + 0.005*"model" + 0.005*"engine" + 0.005*"power" + 0.004*"vehicle" + 0.004*"car" + 0.004*"produce" + 0.004*"aircraft" + 0.003*"control"
topic #8 (0.050): 0.026*"album" + 0.025*"song" + 0.021*"music" + 0.018*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.006*"chart"
topic #7 (0.050): 0.022*"art" + 0.011*"church" + 0.010*"artist" + 0.010*"museum" + 0.007*"study" + 0.007*"painting" + 0.006*"festival" + 0.006*"exhibition" + 0.005*"bishop" + 0.004*"history"
topic #0 (0.050): 0.012*"displaystyle" + 0.009*"language" + 0.008*"define" + 0.006*"example" + 0.006*"word" + 0.005*"point" + 0.005*"text" + 0.005*"term" + 0.004*"function" + 0.004*"theory"
topic diff=0.301252, rho=0.138896
PROGRESS: pass 1, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.050): 0.008*"get" + 0.007*"episode" + 0.006*"kill" + 0.006*"game" + 0.006*"tell" + 0.006*"back" + 0.005*"try" + 0.005*"character" + 0.004*"help" + 0.004*"series"
topic #5 (0.050): 0.022*"woman" + 0.010*"hospital" + 0.008*"medical" + 0.006*"patient" + 0.006*"child" + 0.005*"health" + 0.004*"life" + 0.004*"care" + 0.004*"say" + 0.003*"support"
topic #6 (0.050): 0.020*"ship" + 0.013*"island" + 0.013*"specie" + 0.008*"genus" + 0.006*"describe" + 0.005*"sea" + 0.005*"vessel" + 0.005*"navy" + 0.005*"port" + 0.004*"fleet"
topic #4 (0.050): 0.010*"road" + 0.010*"route" + 0.010*"line" + 0.009*"battle" + 0.008*"army" + 0.007*"force" + 0.006*"war" + 0.006*"north" + 0.006*"attack" + 0.006*"highway"
topic #8 (0.050): 0.027*"album" + 0.025*"song" + 0.021*"music" + 0.017*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic diff=0.291272, rho=0.138896
PROGRESS: pass 1, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.005*"information" + 0.004*"increase" + 0.004*"system" + 0.004*"datum" + 0.004*"process" + 0.004*"human" + 0.004*"provide" + 0.004*"social"
topic #12 (0.050): 0.034*"station" + 0.015*"court" + 0.011*"law" + 0.009*"line" + 0.008*"railway" + 0.007*"case" + 0.006*"public" + 0.006*"operate" + 0.005*"judge" + 0.005*"act"
topic #15 (0.050): 0.046*"game" + 0.024*"season" + 0.019*"player" + 0.013*"race" + 0.009*"finish" + 0.009*"basketball" + 0.007*"championship" + 0.007*"point" + 0.005*"round" + 0.005*"yard"
topic #14 (0.050): 0.013*"population" + 0.012*"village" + 0.008*"age" + 0.007*"river" + 0.007*"town" + 0.007*"county" + 0.007*"park" + 0.006*"specie" + 0.005*"census" + 0.005*"water"
topic #7 (0.050): 0.023*"art" + 0.010*"church" + 0.010*"artist" + 0.010*"museum" + 0.008*"study" + 0.008*"painting" + 0.006*"exhibition" + 0.005*"festival" + 0.005*"bishop" + 0.005*"collection"
topic diff=0.293371, rho=0.138896
PROGRESS: pass 1, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.027*"album" + 0.026*"song" + 0.022*"music" + 0.018*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic #9 (0.050): 0.011*"book" + 0.010*"series" + 0.008*"publish" + 0.007*"television" + 0.007*"award" + 0.006*"film" + 0.005*"novel" + 0.005*"story" + 0.005*"life" + 0.005*"say"
topic #5 (0.050): 0.024*"woman" + 0.011*"hospital" + 0.009*"medical" + 0.006*"patient" + 0.006*"child" + 0.006*"health" + 0.005*"life" + 0.004*"care" + 0.004*"say" + 0.004*"treatment"
topic #18 (0.050): 0.024*"company" + 0.009*"business" + 0.007*"found" + 0.006*"government" + 0.006*"public" + 0.005*"industry" + 0.005*"country" + 0.005*"establish" + 0.005*"international" + 0.004*"development"
topic #6 (0.050): 0.019*"ship" + 0.014*"island" + 0.013*"specie" + 0.008*"genus" + 0.006*"sea" + 0.006*"describe" + 0.005*"vessel" + 0.005*"port" + 0.005*"boat" + 0.005*"navy"
topic diff=0.286939, rho=0.138896
PROGRESS: pass 1, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.023*"art" + 0.011*"church" + 0.011*"artist" + 0.010*"museum" + 0.008*"study" + 0.008*"painting" + 0.006*"exhibition" + 0.005*"festival" + 0.005*"bishop" + 0.005*"collection"
topic #4 (0.050): 0.011*"road" + 0.010*"route" + 0.010*"line" + 0.009*"army" + 0.009*"battle" + 0.008*"force" + 0.007*"war" + 0.007*"highway" + 0.006*"attack" + 0.006*"north"
topic #18 (0.050): 0.025*"company" + 0.010*"business" + 0.007*"found" + 0.006*"government" + 0.006*"public" + 0.005*"industry" + 0.005*"country" + 0.005*"international" + 0.005*"establish" + 0.004*"sell"
topic #15 (0.050): 0.048*"game" + 0.025*"season" + 0.018*"player" + 0.013*"race" + 0.009*"finish" + 0.008*"basketball" + 0.007*"point" + 0.007*"championship" + 0.006*"conference" + 0.006*"tournament"
topic #13 (0.050): 0.011*"student" + 0.010*"college" + 0.009*"university" + 0.008*"government" + 0.007*"war" + 0.006*"education" + 0.006*"study" + 0.005*"department" + 0.005*"science" + 0.005*"german"
topic diff=0.291028, rho=0.138896
PROGRESS: pass 1, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.020*"building" + 0.012*"house" + 0.011*"church" + 0.007*"design" + 0.006*"street" + 0.006*"site" + 0.006*"th_century" + 0.005*"town" + 0.004*"main" + 0.004*"wall"
topic #13 (0.050): 0.011*"student" + 0.010*"college" + 0.009*"university" + 0.008*"government" + 0.007*"war" + 0.006*"education" + 0.006*"study" + 0.005*"department" + 0.005*"science" + 0.005*"graduate"
topic #10 (0.050): 0.012*"election" + 0.010*"party" + 0.009*"government" + 0.006*"elect" + 0.006*"vote" + 0.006*"political" + 0.004*"son" + 0.004*"candidate" + 0.004*"seat" + 0.004*"support"
topic #16 (0.050): 0.012*"unit" + 0.011*"army" + 0.009*"war" + 0.009*"german" + 0.008*"air" + 0.008*"force" + 0.008*"operation" + 0.008*"command" + 0.007*"squadron" + 0.007*"aircraft"
topic #15 (0.050): 0.049*"game" + 0.025*"season" + 0.019*"player" + 0.014*"race" + 0.009*"finish" + 0.008*"basketball" + 0.008*"point" + 0.007*"championship" + 0.006*"conference" + 0.005*"tournament"
topic diff=0.277215, rho=0.138896
PROGRESS: pass 1, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.021*"ship" + 0.015*"island" + 0.013*"specie" + 0.008*"genus" + 0.007*"sea" + 0.006*"vessel" + 0.006*"boat" + 0.006*"describe" + 0.005*"navy" + 0.005*"port"
topic #1 (0.050): 0.057*"film" + 0.010*"star" + 0.008*"character" + 0.007*"direct" + 0.007*"story" + 0.006*"series" + 0.004*"earth" + 0.004*"game" + 0.004*"role" + 0.004*"award"
topic #19 (0.050): 0.021*"building" + 0.012*"house" + 0.012*"church" + 0.007*"design" + 0.006*"street" + 0.006*"site" + 0.006*"th_century" + 0.005*"town" + 0.004*"main" + 0.004*"wall"
topic #10 (0.050): 0.012*"election" + 0.010*"party" + 0.009*"government" + 0.007*"elect" + 0.006*"vote" + 0.006*"political" + 0.004*"son" + 0.004*"seat" + 0.004*"support" + 0.004*"candidate"
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.005*"information" + 0.005*"increase" + 0.005*"system" + 0.005*"process" + 0.004*"datum" + 0.004*"human" + 0.004*"provide" + 0.004*"individual"
topic diff=0.272065, rho=0.138896
PROGRESS: pass 1, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.011*"student" + 0.010*"college" + 0.010*"university" + 0.008*"government" + 0.007*"war" + 0.006*"education" + 0.006*"german" + 0.006*"study" + 0.006*"department" + 0.005*"science"
topic #12 (0.050): 0.037*"station" + 0.014*"court" + 0.012*"law" + 0.011*"line" + 0.008*"railway" + 0.008*"case" + 0.006*"radio" + 0.006*"operate" + 0.006*"public" + 0.006*"act"
topic #16 (0.050): 0.012*"unit" + 0.011*"army" + 0.010*"german" + 0.010*"war" + 0.009*"air" + 0.008*"force" + 0.008*"operation" + 0.008*"command" + 0.008*"squadron" + 0.008*"aircraft"
topic #10 (0.050): 0.012*"election" + 0.011*"party" + 0.010*"government" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"candidate" + 0.005*"son" + 0.005*"say" + 0.004*"support"
topic #1 (0.050): 0.060*"film" + 0.010*"star" + 0.008*"character" + 0.007*"direct" + 0.007*"series" + 0.007*"story" + 0.004*"earth" + 0.004*"role" + 0.004*"award" + 0.004*"game"
topic diff=0.267762, rho=0.138896
PROGRESS: pass 1, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.012*"election" + 0.011*"party" + 0.010*"government" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"candidate" + 0.005*"support" + 0.004*"son" + 0.004*"say"
topic #12 (0.050): 0.038*"station" + 0.014*"court" + 0.012*"law" + 0.011*"line" + 0.008*"railway" + 0.007*"case" + 0.006*"operate" + 0.006*"radio" + 0.006*"act" + 0.006*"public"
topic #18 (0.050): 0.027*"company" + 0.011*"business" + 0.007*"found" + 0.006*"government" + 0.006*"public" + 0.005*"industry" + 0.005*"country" + 0.005*"international" + 0.005*"establish" + 0.005*"development"
topic #0 (0.050): 0.011*"displaystyle" + 0.009*"language" + 0.007*"example" + 0.006*"word" + 0.006*"define" + 0.005*"text" + 0.005*"term" + 0.005*"theory" + 0.005*"point" + 0.004*"mean"
topic #1 (0.050): 0.059*"film" + 0.011*"star" + 0.008*"character" + 0.007*"direct" + 0.007*"series" + 0.007*"story" + 0.004*"game" + 0.004*"role" + 0.004*"earth" + 0.004*"award"
topic diff=0.267632, rho=0.138896
PROGRESS: pass 1, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.029*"album" + 0.027*"song" + 0.023*"music" + 0.019*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #1 (0.050): 0.060*"film" + 0.012*"star" + 0.008*"character" + 0.008*"direct" + 0.007*"series" + 0.007*"story" + 0.005*"earth" + 0.004*"game" + 0.004*"role" + 0.004*"movie"
topic #0 (0.050): 0.011*"displaystyle" + 0.010*"language" + 0.007*"example" + 0.007*"word" + 0.005*"text" + 0.005*"define" + 0.005*"term" + 0.005*"theory" + 0.005*"point" + 0.004*"mean"
topic #6 (0.050): 0.022*"ship" + 0.016*"island" + 0.012*"specie" + 0.008*"genus" + 0.007*"sea" + 0.006*"vessel" + 0.006*"navy" + 0.006*"port" + 0.005*"boat" + 0.005*"describe"
topic #4 (0.050): 0.011*"road" + 0.011*"route" + 0.010*"battle" + 0.009*"line" + 0.009*"army" + 0.009*"force" + 0.008*"war" + 0.007*"attack" + 0.007*"highway" + 0.007*"north"
topic diff=0.266805, rho=0.138896
PROGRESS: pass 1, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.012*"election" + 0.011*"party" + 0.010*"government" + 0.007*"vote" + 0.007*"elect" + 0.006*"political" + 0.005*"support" + 0.005*"candidate" + 0.004*"general" + 0.004*"son"
topic #5 (0.050): 0.028*"woman" + 0.012*"hospital" + 0.011*"medical" + 0.008*"child" + 0.007*"health" + 0.006*"patient" + 0.005*"care" + 0.005*"life" + 0.005*"medicine" + 0.005*"treatment"
topic #18 (0.050): 0.027*"company" + 0.012*"business" + 0.007*"found" + 0.006*"public" + 0.006*"government" + 0.005*"industry" + 0.005*"country" + 0.005*"sell" + 0.005*"establish" + 0.005*"international"
topic #6 (0.050): 0.023*"ship" + 0.016*"island" + 0.013*"specie" + 0.008*"genus" + 0.007*"sea" + 0.006*"navy" + 0.006*"vessel" + 0.006*"port" + 0.006*"describe" + 0.006*"coast"
topic #14 (0.050): 0.014*"population" + 0.013*"village" + 0.009*"river" + 0.008*"town" + 0.007*"county" + 0.007*"age" + 0.007*"specie" + 0.007*"park" + 0.006*"region" + 0.006*"north"
topic diff=0.255545, rho=0.138896
PROGRESS: pass 1, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.029*"season" + 0.027*"club" + 0.017*"league" + 0.016*"football" + 0.016*"match" + 0.015*"championship" + 0.014*"final" + 0.011*"game" + 0.010*"player" + 0.010*"goal"
topic #15 (0.050): 0.047*"game" + 0.029*"season" + 0.018*"player" + 0.016*"race" + 0.010*"finish" + 0.008*"point" + 0.008*"basketball" + 0.008*"championship" + 0.006*"conference" + 0.006*"league"
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.005*"system" + 0.005*"information" + 0.005*"increase" + 0.005*"datum" + 0.005*"process" + 0.004*"human" + 0.004*"provide" + 0.004*"social"
topic #0 (0.050): 0.010*"language" + 0.010*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.005*"define" + 0.005*"text" + 0.005*"point" + 0.005*"theory" + 0.005*"mean"
topic #7 (0.050): 0.024*"art" + 0.014*"church" + 0.011*"artist" + 0.011*"museum" + 0.008*"study" + 0.008*"painting" + 0.005*"exhibition" + 0.005*"bishop" + 0.005*"collection" + 0.005*"history"
topic diff=0.246518, rho=0.138896
PROGRESS: pass 1, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.014*"population" + 0.014*"village" + 0.009*"river" + 0.008*"town" + 0.008*"county" + 0.007*"age" + 0.007*"specie" + 0.006*"park" + 0.006*"region" + 0.006*"north"
topic #16 (0.050): 0.013*"unit" + 0.012*"army" + 0.010*"air" + 0.010*"german" + 0.010*"war" + 0.009*"command" + 0.009*"force" + 0.008*"protein" + 0.008*"operation" + 0.008*"aircraft"
topic #3 (0.050): 0.010*"system" + 0.008*"design" + 0.006*"power" + 0.006*"engine" + 0.005*"model" + 0.005*"car" + 0.004*"produce" + 0.004*"type" + 0.004*"low" + 0.004*"control"
topic #19 (0.050): 0.020*"building" + 0.013*"house" + 0.013*"church" + 0.008*"design" + 0.007*"street" + 0.007*"site" + 0.006*"th_century" + 0.005*"town" + 0.005*"main" + 0.004*"construction"
topic #18 (0.050): 0.028*"company" + 0.012*"business" + 0.007*"found" + 0.006*"public" + 0.006*"industry" + 0.006*"government" + 0.005*"sell" + 0.005*"development" + 0.005*"country" + 0.005*"international"
topic diff=0.235450, rho=0.138896
-8.306 per-word bound, 316.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.029*"album" + 0.028*"song" + 0.024*"music" + 0.019*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.006*"video" + 0.006*"chart"
topic #2 (0.050): 0.028*"season" + 0.028*"club" + 0.017*"league" + 0.016*"match" + 0.015*"football" + 0.015*"championship" + 0.013*"final" + 0.011*"game" + 0.010*"player" + 0.010*"goal"
topic #19 (0.050): 0.021*"building" + 0.013*"house" + 0.012*"church" + 0.008*"design" + 0.007*"site" + 0.007*"street" + 0.006*"th_century" + 0.005*"town" + 0.005*"main" + 0.005*"side"
topic #5 (0.050): 0.029*"woman" + 0.014*"hospital" + 0.012*"medical" + 0.008*"health" + 0.008*"child" + 0.008*"patient" + 0.005*"medicine" + 0.005*"treatment" + 0.005*"care" + 0.005*"life"
topic #6 (0.050): 0.022*"ship" + 0.017*"island" + 0.013*"specie" + 0.009*"genus" + 0.007*"sea" + 0.006*"port" + 0.006*"vessel" + 0.006*"navy" + 0.006*"describe" + 0.006*"boat"
topic diff=0.236507, rho=0.138896
-8.306 per-word bound, 316.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.024*"art" + 0.014*"church" + 0.011*"artist" + 0.011*"museum" + 0.009*"study" + 0.008*"painting" + 0.005*"exhibition" + 0.005*"collection" + 0.005*"bishop" + 0.005*"history"
topic #5 (0.050): 0.031*"woman" + 0.014*"hospital" + 0.012*"medical" + 0.009*"child" + 0.008*"health" + 0.007*"patient" + 0.005*"medicine" + 0.005*"care" + 0.005*"treatment" + 0.005*"life"
topic #15 (0.050): 0.049*"game" + 0.029*"season" + 0.019*"player" + 0.017*"race" + 0.010*"finish" + 0.009*"point" + 0.009*"basketball" + 0.007*"championship" + 0.006*"football" + 0.006*"league"
topic #18 (0.050): 0.029*"company" + 0.011*"business" + 0.007*"found" + 0.006*"public" + 0.006*"industry" + 0.006*"government" + 0.005*"sell" + 0.005*"development" + 0.005*"country" + 0.005*"establish"
topic #11 (0.050): 0.008*"get" + 0.007*"kill" + 0.007*"tell" + 0.007*"episode" + 0.006*"back" + 0.006*"try" + 0.005*"say" + 0.004*"help" + 0.004*"want" + 0.004*"character"
topic diff=0.238022, rho=0.138896
-8.306 per-word bound, 316.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #13 (0.050): 0.013*"student" + 0.012*"university" + 0.011*"college" + 0.007*"education" + 0.007*"government" + 0.007*"study" + 0.007*"department" + 0.006*"war" + 0.006*"german" + 0.006*"science"
topic #4 (0.050): 0.011*"road" + 0.010*"battle" + 0.010*"army" + 0.010*"route" + 0.010*"line" + 0.009*"force" + 0.008*"war" + 0.007*"attack" + 0.007*"north" + 0.006*"highway"
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.005*"system" + 0.005*"increase" + 0.005*"information" + 0.005*"datum" + 0.005*"process" + 0.004*"human" + 0.004*"develop" + 0.004*"individual"
topic #6 (0.050): 0.025*"ship" + 0.016*"island" + 0.012*"specie" + 0.009*"genus" + 0.008*"sea" + 0.007*"vessel" + 0.007*"navy" + 0.007*"port" + 0.006*"boat" + 0.006*"describe"
topic #10 (0.050): 0.012*"election" + 0.011*"party" + 0.010*"government" + 0.007*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"son" + 0.005*"general" + 0.005*"support" + 0.004*"candidate"
topic diff=0.224788, rho=0.138896
-8.202 per-word bound, 294.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.015*"population" + 0.014*"village" + 0.009*"river" + 0.008*"town" + 0.008*"age" + 0.008*"county" + 0.007*"specie" + 0.006*"park" + 0.006*"census" + 0.006*"region"
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.005*"system" + 0.005*"increase" + 0.005*"information" + 0.005*"process" + 0.005*"datum" + 0.004*"level" + 0.004*"human" + 0.004*"individual"
topic #15 (0.050): 0.050*"game" + 0.030*"season" + 0.020*"player" + 0.016*"race" + 0.010*"finish" + 0.008*"point" + 0.008*"basketball" + 0.007*"league" + 0.007*"championship" + 0.007*"football"
topic #6 (0.050): 0.024*"ship" + 0.017*"island" + 0.013*"specie" + 0.009*"genus" + 0.008*"sea" + 0.007*"vessel" + 0.007*"navy" + 0.007*"port" + 0.006*"boat" + 0.006*"describe"
topic #18 (0.050): 0.029*"company" + 0.012*"business" + 0.008*"found" + 0.006*"public" + 0.006*"government" + 0.006*"industry" + 0.005*"sell" + 0.005*"development" + 0.005*"country" + 0.005*"establish"
topic diff=0.216197, rho=0.137575
PROGRESS: pass 2, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 10
PROGRESS: pass 2, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.030*"album" + 0.028*"song" + 0.025*"music" + 0.020*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.006*"information" + 0.005*"system" + 0.005*"increase" + 0.005*"datum" + 0.005*"process" + 0.004*"human" + 0.004*"level" + 0.004*"provide"
topic #4 (0.050): 0.011*"road" + 0.011*"army" + 0.010*"battle" + 0.010*"route" + 0.009*"war" + 0.009*"force" + 0.009*"line" + 0.007*"attack" + 0.007*"north" + 0.006*"troop"
topic #1 (0.050): 0.070*"film" + 0.013*"star" + 0.009*"direct" + 0.009*"character" + 0.007*"series" + 0.007*"story" + 0.006*"movie" + 0.005*"role" + 0.005*"award" + 0.005*"earth"
topic #19 (0.050): 0.023*"building" + 0.013*"house" + 0.011*"church" + 0.008*"site" + 0.008*"design" + 0.007*"street" + 0.006*"th_century" + 0.005*"town" + 0.005*"stone" + 0.005*"construction"
topic diff=0.209248, rho=0.137575
PROGRESS: pass 2, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 10
PROGRESS: pass 2, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.014*"student" + 0.012*"college" + 0.012*"university" + 0.008*"education" + 0.007*"study" + 0.007*"government" + 0.007*"department" + 0.007*"science" + 0.006*"graduate" + 0.006*"war"
topic #8 (0.050): 0.030*"album" + 0.028*"song" + 0.025*"music" + 0.020*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #6 (0.050): 0.025*"ship" + 0.019*"island" + 0.013*"specie" + 0.009*"genus" + 0.008*"sea" + 0.007*"port" + 0.007*"vessel" + 0.007*"navy" + 0.006*"boat" + 0.006*"describe"
topic #18 (0.050): 0.030*"company" + 0.012*"business" + 0.007*"found" + 0.006*"public" + 0.006*"industry" + 0.006*"government" + 0.006*"sell" + 0.005*"development" + 0.005*"country" + 0.005*"establish"
topic #0 (0.050): 0.012*"displaystyle" + 0.010*"language" + 0.007*"example" + 0.007*"word" + 0.006*"define" + 0.006*"text" + 0.005*"term" + 0.005*"point" + 0.005*"theory" + 0.005*"mean"
topic diff=0.197663, rho=0.137575
PROGRESS: pass 2, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.050): 0.032*"woman" + 0.014*"hospital" + 0.012*"medical" + 0.011*"child" + 0.009*"health" + 0.008*"patient" + 0.005*"care" + 0.005*"life" + 0.005*"treatment" + 0.005*"medicine"
topic #1 (0.050): 0.069*"film" + 0.012*"star" + 0.009*"character" + 0.009*"direct" + 0.007*"series" + 0.007*"story" + 0.006*"movie" + 0.005*"role" + 0.005*"game" + 0.005*"award"
topic #3 (0.050): 0.011*"system" + 0.008*"design" + 0.006*"model" + 0.006*"engine" + 0.005*"power" + 0.005*"car" + 0.004*"produce" + 0.004*"vehicle" + 0.004*"low" + 0.004*"type"
topic #12 (0.050): 0.038*"station" + 0.017*"court" + 0.012*"law" + 0.012*"line" + 0.009*"railway" + 0.008*"case" + 0.007*"operate" + 0.007*"radio" + 0.007*"public" + 0.006*"act"
topic #2 (0.050): 0.027*"club" + 0.026*"season" + 0.017*"match" + 0.016*"championship" + 0.016*"league" + 0.014*"final" + 0.014*"football" + 0.010*"game" + 0.010*"player" + 0.009*"event"
topic diff=0.197152, rho=0.137575
PROGRESS: pass 2, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.016*"population" + 0.015*"village" + 0.009*"town" + 0.009*"river" + 0.008*"age" + 0.008*"county" + 0.008*"specie" + 0.007*"park" + 0.006*"region" + 0.006*"census"
topic #2 (0.050): 0.028*"club" + 0.026*"season" + 0.017*"match" + 0.016*"league" + 0.016*"championship" + 0.014*"football" + 0.013*"final" + 0.010*"player" + 0.010*"game" + 0.009*"event"
topic #6 (0.050): 0.023*"ship" + 0.020*"island" + 0.013*"specie" + 0.008*"genus" + 0.008*"sea" + 0.007*"boat" + 0.007*"port" + 0.007*"vessel" + 0.006*"navy" + 0.006*"describe"
topic #7 (0.050): 0.023*"art" + 0.012*"church" + 0.011*"museum" + 0.011*"artist" + 0.009*"study" + 0.008*"painting" + 0.006*"exhibition" + 0.005*"collection" + 0.005*"history" + 0.005*"bishop"
topic #8 (0.050): 0.030*"album" + 0.029*"song" + 0.026*"music" + 0.020*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"video" + 0.007*"tour" + 0.006*"chart"
topic diff=0.189335, rho=0.137575
PROGRESS: pass 2, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.011*"government" + 0.011*"party" + 0.008*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"general" + 0.005*"seat"
topic #5 (0.050): 0.031*"woman" + 0.015*"hospital" + 0.013*"medical" + 0.012*"child" + 0.009*"health" + 0.008*"patient" + 0.006*"treatment" + 0.005*"care" + 0.005*"medicine" + 0.005*"life"
topic #11 (0.050): 0.008*"get" + 0.008*"tell" + 0.007*"kill" + 0.006*"say" + 0.006*"back" + 0.006*"episode" + 0.006*"try" + 0.005*"help" + 0.004*"want" + 0.004*"murder"
topic #14 (0.050): 0.016*"population" + 0.015*"village" + 0.009*"town" + 0.009*"river" + 0.008*"county" + 0.008*"age" + 0.008*"specie" + 0.007*"park" + 0.007*"region" + 0.007*"census"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"model" + 0.006*"engine" + 0.005*"power" + 0.005*"vehicle" + 0.005*"car" + 0.004*"produce" + 0.004*"low" + 0.004*"standard"
topic diff=0.191212, rho=0.137575
PROGRESS: pass 2, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.023*"building" + 0.014*"house" + 0.011*"church" + 0.008*"site" + 0.008*"design" + 0.007*"street" + 0.006*"th_century" + 0.005*"town" + 0.005*"main" + 0.005*"stone"
topic #7 (0.050): 0.023*"art" + 0.013*"church" + 0.011*"museum" + 0.010*"artist" + 0.009*"study" + 0.008*"painting" + 0.006*"collection" + 0.006*"exhibition" + 0.005*"history" + 0.005*"father"
topic #9 (0.050): 0.015*"book" + 0.012*"series" + 0.010*"publish" + 0.009*"television" + 0.008*"award" + 0.007*"novel" + 0.006*"story" + 0.006*"life" + 0.005*"appear" + 0.005*"writer"
topic #8 (0.050): 0.029*"album" + 0.029*"song" + 0.026*"music" + 0.020*"band" + 0.013*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #14 (0.050): 0.016*"population" + 0.015*"village" + 0.009*"river" + 0.009*"town" + 0.008*"county" + 0.008*"age" + 0.008*"specie" + 0.007*"park" + 0.007*"region" + 0.006*"census"
topic diff=0.176833, rho=0.137575
PROGRESS: pass 2, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.050): 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"say" + 0.006*"back" + 0.006*"episode" + 0.006*"try" + 0.005*"help" + 0.004*"want" + 0.004*"murder"
topic #8 (0.050): 0.030*"album" + 0.029*"song" + 0.026*"music" + 0.020*"band" + 0.013*"single" + 0.009*"perform" + 0.008*"track" + 0.007*"video" + 0.007*"tour" + 0.007*"chart"
topic #19 (0.050): 0.023*"building" + 0.014*"house" + 0.011*"church" + 0.008*"site" + 0.008*"design" + 0.007*"street" + 0.006*"th_century" + 0.005*"town" + 0.005*"main" + 0.005*"stone"
topic #6 (0.050): 0.025*"ship" + 0.019*"island" + 0.012*"specie" + 0.009*"sea" + 0.008*"genus" + 0.007*"boat" + 0.007*"port" + 0.007*"vessel" + 0.006*"navy" + 0.006*"crew"
topic #16 (0.050): 0.015*"unit" + 0.013*"army" + 0.011*"aircraft" + 0.011*"operation" + 0.011*"war" + 0.011*"air" + 0.010*"force" + 0.010*"german" + 0.010*"military" + 0.009*"command"
topic diff=0.170870, rho=0.137575
PROGRESS: pass 2, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.050): 0.010*"displaystyle" + 0.010*"language" + 0.007*"example" + 0.006*"word" + 0.006*"text" + 0.006*"term" + 0.005*"define" + 0.005*"point" + 0.005*"mean" + 0.005*"theory"
topic #12 (0.050): 0.039*"station" + 0.015*"court" + 0.013*"line" + 0.013*"law" + 0.010*"railway" + 0.009*"case" + 0.008*"radio" + 0.007*"operate" + 0.007*"train" + 0.007*"act"
topic #4 (0.050): 0.012*"road" + 0.011*"route" + 0.011*"army" + 0.011*"battle" + 0.010*"war" + 0.010*"force" + 0.008*"line" + 0.007*"highway" + 0.007*"attack" + 0.007*"north"
topic #8 (0.050): 0.030*"album" + 0.029*"song" + 0.026*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #10 (0.050): 0.013*"election" + 0.011*"party" + 0.011*"government" + 0.008*"elect" + 0.007*"vote" + 0.007*"political" + 0.005*"candidate" + 0.005*"support" + 0.005*"seat" + 0.005*"son"
topic diff=0.167082, rho=0.137575
PROGRESS: pass 2, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.023*"building" + 0.014*"house" + 0.011*"church" + 0.009*"site" + 0.008*"design" + 0.008*"street" + 0.006*"th_century" + 0.005*"town" + 0.005*"stone" + 0.005*"main"
topic #10 (0.050): 0.013*"election" + 0.012*"party" + 0.011*"government" + 0.008*"elect" + 0.007*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"seat" + 0.005*"law"
topic #17 (0.050): 0.008*"research" + 0.007*"study" + 0.005*"increase" + 0.005*"system" + 0.005*"information" + 0.005*"process" + 0.005*"human" + 0.005*"datum" + 0.004*"provide" + 0.004*"result"
topic #18 (0.050): 0.031*"company" + 0.012*"business" + 0.007*"found" + 0.007*"industry" + 0.006*"public" + 0.006*"sell" + 0.006*"government" + 0.005*"development" + 0.005*"country" + 0.005*"market"
topic #0 (0.050): 0.010*"displaystyle" + 0.009*"language" + 0.007*"example" + 0.007*"word" + 0.006*"text" + 0.005*"term" + 0.005*"theory" + 0.005*"define" + 0.005*"mean" + 0.005*"point"
topic diff=0.165592, rho=0.137575
PROGRESS: pass 2, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.012*"party" + 0.011*"government" + 0.008*"elect" + 0.008*"vote" + 0.006*"political" + 0.005*"candidate" + 0.005*"support" + 0.005*"seat" + 0.005*"law"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"control" + 0.004*"low"
topic #4 (0.050): 0.012*"road" + 0.011*"route" + 0.011*"battle" + 0.010*"army" + 0.010*"force" + 0.010*"war" + 0.008*"line" + 0.008*"attack" + 0.007*"highway" + 0.007*"north"
topic #2 (0.050): 0.029*"club" + 0.025*"season" + 0.017*"league" + 0.016*"match" + 0.016*"championship" + 0.014*"football" + 0.014*"final" + 0.010*"player" + 0.010*"event" + 0.010*"game"
topic #11 (0.050): 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"say" + 0.006*"try" + 0.006*"back" + 0.006*"episode" + 0.005*"help" + 0.004*"want" + 0.004*"ask"
topic diff=0.166366, rho=0.137575
PROGRESS: pass 2, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.050): 0.041*"station" + 0.014*"court" + 0.014*"line" + 0.012*"law" + 0.009*"railway" + 0.008*"radio" + 0.008*"case" + 0.007*"operate" + 0.007*"act" + 0.007*"train"
topic #8 (0.050): 0.032*"album" + 0.030*"song" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #10 (0.050): 0.013*"election" + 0.011*"party" + 0.011*"government" + 0.008*"elect" + 0.008*"vote" + 0.006*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"general" + 0.005*"law"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"engine" + 0.006*"model" + 0.005*"power" + 0.005*"car" + 0.004*"produce" + 0.004*"vehicle" + 0.004*"low" + 0.004*"type"
topic #9 (0.050): 0.015*"book" + 0.011*"series" + 0.011*"publish" + 0.009*"television" + 0.008*"award" + 0.007*"story" + 0.007*"novel" + 0.006*"life" + 0.005*"writer" + 0.005*"magazine"
topic diff=0.157993, rho=0.137575
PROGRESS: pass 2, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.011*"party" + 0.011*"government" + 0.008*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"general" + 0.005*"law"
topic #19 (0.050): 0.023*"building" + 0.014*"house" + 0.011*"church" + 0.009*"site" + 0.008*"street" + 0.008*"design" + 0.006*"th_century" + 0.005*"town" + 0.005*"park" + 0.005*"stone"
topic #18 (0.050): 0.031*"company" + 0.013*"business" + 0.007*"found" + 0.007*"industry" + 0.006*"sell" + 0.006*"public" + 0.006*"government" + 0.005*"country" + 0.005*"development" + 0.005*"market"
topic #0 (0.050): 0.010*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.006*"text" + 0.005*"mean" + 0.005*"point" + 0.005*"theory" + 0.005*"refer"
topic #9 (0.050): 0.016*"book" + 0.011*"series" + 0.011*"publish" + 0.008*"television" + 0.008*"award" + 0.007*"story" + 0.007*"novel" + 0.006*"life" + 0.006*"writer" + 0.005*"magazine"
topic diff=0.148974, rho=0.137575
PROGRESS: pass 2, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.050): 0.051*"game" + 0.035*"season" + 0.020*"player" + 0.016*"race" + 0.010*"finish" + 0.010*"football" + 0.009*"point" + 0.009*"league" + 0.009*"basketball" + 0.007*"championship"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"power" + 0.006*"engine" + 0.006*"model" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"low" + 0.004*"control"
topic #18 (0.050): 0.031*"company" + 0.013*"business" + 0.007*"found" + 0.006*"sell" + 0.006*"industry" + 0.006*"public" + 0.006*"market" + 0.005*"government" + 0.005*"development" + 0.005*"country"
topic #9 (0.050): 0.016*"book" + 0.011*"publish" + 0.011*"series" + 0.009*"television" + 0.008*"award" + 0.007*"story" + 0.007*"novel" + 0.006*"life" + 0.006*"writer" + 0.005*"magazine"
topic #19 (0.050): 0.022*"building" + 0.014*"house" + 0.012*"church" + 0.009*"site" + 0.008*"street" + 0.008*"design" + 0.006*"th_century" + 0.005*"town" + 0.005*"construction" + 0.005*"main"
topic diff=0.141177, rho=0.137575
-8.251 per-word bound, 304.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.050): 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.007*"say" + 0.006*"back" + 0.006*"episode" + 0.006*"try" + 0.005*"help" + 0.004*"want" + 0.004*"ask"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"found" + 0.007*"industry" + 0.006*"sell" + 0.006*"public" + 0.005*"development" + 0.005*"market" + 0.005*"government" + 0.005*"country"
topic #9 (0.050): 0.016*"book" + 0.012*"series" + 0.011*"publish" + 0.009*"television" + 0.008*"award" + 0.007*"novel" + 0.007*"story" + 0.006*"life" + 0.006*"writer" + 0.005*"magazine"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"power" + 0.006*"engine" + 0.005*"model" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"control" + 0.004*"low"
topic #0 (0.050): 0.010*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.005*"mean" + 0.005*"text" + 0.005*"point" + 0.005*"theory" + 0.005*"refer"
topic diff=0.142616, rho=0.137575
-8.253 per-word bound, 305.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.012*"party" + 0.011*"government" + 0.008*"elect" + 0.007*"vote" + 0.006*"political" + 0.005*"support" + 0.005*"general" + 0.005*"law" + 0.005*"candidate"
topic #8 (0.050): 0.031*"album" + 0.030*"song" + 0.028*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #19 (0.050): 0.023*"building" + 0.015*"house" + 0.011*"church" + 0.010*"site" + 0.008*"design" + 0.008*"street" + 0.006*"th_century" + 0.005*"town" + 0.005*"stone" + 0.005*"construction"
topic #14 (0.050): 0.016*"population" + 0.015*"village" + 0.009*"river" + 0.009*"town" + 0.008*"specie" + 0.008*"county" + 0.008*"age" + 0.007*"region" + 0.006*"north" + 0.006*"south"
topic #5 (0.050): 0.035*"woman" + 0.015*"hospital" + 0.014*"medical" + 0.013*"child" + 0.011*"health" + 0.008*"patient" + 0.006*"treatment" + 0.006*"medicine" + 0.006*"black" + 0.006*"care"
topic diff=0.145957, rho=0.137575
-8.256 per-word bound, 305.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #17 (0.050): 0.007*"research" + 0.007*"study" + 0.006*"increase" + 0.006*"system" + 0.005*"information" + 0.005*"human" + 0.005*"process" + 0.005*"datum" + 0.005*"level" + 0.004*"result"
topic #2 (0.050): 0.029*"club" + 0.024*"season" + 0.017*"match" + 0.016*"championship" + 0.016*"league" + 0.014*"final" + 0.014*"football" + 0.010*"event" + 0.010*"player" + 0.010*"game"
topic #13 (0.050): 0.016*"student" + 0.014*"university" + 0.013*"college" + 0.009*"study" + 0.009*"education" + 0.008*"science" + 0.008*"department" + 0.007*"graduate" + 0.007*"award" + 0.006*"director"
topic #11 (0.050): 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.007*"say" + 0.006*"back" + 0.006*"try" + 0.006*"episode" + 0.005*"help" + 0.004*"want" + 0.004*"reveal"
topic #8 (0.050): 0.032*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic diff=0.136728, rho=0.137575
-8.166 per-word bound, 287.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.016*"student" + 0.014*"university" + 0.013*"college" + 0.009*"study" + 0.009*"education" + 0.008*"science" + 0.008*"department" + 0.007*"graduate" + 0.007*"award" + 0.006*"research"
topic #0 (0.050): 0.010*"language" + 0.010*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"define" + 0.006*"text" + 0.005*"term" + 0.005*"point" + 0.005*"mean" + 0.005*"theory"
topic #14 (0.050): 0.016*"population" + 0.016*"village" + 0.009*"town" + 0.009*"river" + 0.008*"specie" + 0.008*"county" + 0.008*"age" + 0.007*"region" + 0.007*"census" + 0.006*"north"
topic #12 (0.050): 0.040*"station" + 0.016*"court" + 0.014*"line" + 0.012*"law" + 0.011*"railway" + 0.008*"case" + 0.008*"operate" + 0.007*"radio" + 0.007*"train" + 0.006*"public"
topic #9 (0.050): 0.016*"book" + 0.012*"series" + 0.011*"publish" + 0.009*"television" + 0.008*"award" + 0.007*"story" + 0.007*"novel" + 0.006*"life" + 0.006*"writer" + 0.006*"episode"
topic diff=0.130098, rho=0.136291
PROGRESS: pass 3, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.015*"book" + 0.012*"series" + 0.011*"publish" + 0.009*"television" + 0.008*"award" + 0.007*"novel" + 0.007*"story" + 0.006*"life" + 0.006*"writer" + 0.005*"magazine"
topic #19 (0.050): 0.025*"building" + 0.014*"house" + 0.010*"church" + 0.009*"site" + 0.008*"design" + 0.008*"street" + 0.006*"park" + 0.006*"th_century" + 0.005*"stone" + 0.005*"town"
topic #1 (0.050): 0.075*"film" + 0.014*"star" + 0.011*"character" + 0.010*"direct" + 0.009*"series" + 0.008*"movie" + 0.007*"story" + 0.006*"role" + 0.005*"game" + 0.005*"production"
topic #8 (0.050): 0.031*"album" + 0.030*"song" + 0.027*"music" + 0.022*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"control"
topic diff=0.126636, rho=0.136291
PROGRESS: pass 3, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.031*"album" + 0.030*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #10 (0.050): 0.013*"election" + 0.012*"government" + 0.011*"party" + 0.008*"elect" + 0.008*"vote" + 0.006*"political" + 0.005*"support" + 0.005*"general" + 0.005*"law" + 0.005*"candidate"
topic #19 (0.050): 0.024*"building" + 0.014*"house" + 0.010*"church" + 0.009*"site" + 0.008*"design" + 0.008*"street" + 0.006*"park" + 0.006*"th_century" + 0.005*"stone" + 0.005*"town"
topic #1 (0.050): 0.074*"film" + 0.014*"star" + 0.010*"character" + 0.010*"direct" + 0.009*"series" + 0.008*"movie" + 0.007*"story" + 0.006*"role" + 0.006*"game" + 0.005*"production"
topic #5 (0.050): 0.033*"woman" + 0.016*"child" + 0.015*"hospital" + 0.014*"medical" + 0.011*"health" + 0.009*"patient" + 0.006*"treatment" + 0.006*"black" + 0.006*"medicine" + 0.006*"care"
topic diff=0.117987, rho=0.136291
PROGRESS: pass 3, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.050): 0.012*"road" + 0.011*"army" + 0.011*"war" + 0.011*"battle" + 0.011*"route" + 0.010*"force" + 0.008*"line" + 0.007*"attack" + 0.007*"north" + 0.006*"highway"
topic #5 (0.050): 0.035*"woman" + 0.015*"hospital" + 0.015*"child" + 0.014*"medical" + 0.011*"health" + 0.009*"patient" + 0.006*"treatment" + 0.006*"black" + 0.006*"care" + 0.006*"medicine"
topic #7 (0.050): 0.022*"art" + 0.014*"church" + 0.011*"museum" + 0.010*"artist" + 0.009*"study" + 0.008*"painting" + 0.006*"collection" + 0.006*"exhibition" + 0.005*"history" + 0.005*"father"
topic #11 (0.050): 0.008*"get" + 0.008*"say" + 0.008*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.005*"murder" + 0.004*"want"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.005*"vehicle" + 0.004*"type" + 0.004*"low"
topic diff=0.119540, rho=0.136291
PROGRESS: pass 3, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.023*"art" + 0.014*"church" + 0.011*"museum" + 0.010*"artist" + 0.009*"study" + 0.008*"painting" + 0.006*"exhibition" + 0.006*"collection" + 0.005*"father" + 0.005*"history"
topic #18 (0.050): 0.031*"company" + 0.012*"business" + 0.007*"industry" + 0.007*"found" + 0.007*"sell" + 0.006*"public" + 0.006*"market" + 0.006*"government" + 0.006*"country" + 0.005*"development"
topic #16 (0.050): 0.016*"unit" + 0.014*"army" + 0.014*"aircraft" + 0.012*"military" + 0.012*"force" + 0.012*"operation" + 0.012*"air" + 0.012*"war" + 0.011*"command" + 0.010*"fly"
topic #17 (0.050): 0.007*"research" + 0.007*"study" + 0.006*"information" + 0.006*"increase" + 0.005*"system" + 0.005*"human" + 0.005*"process" + 0.005*"datum" + 0.005*"level" + 0.004*"result"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.006*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"low" + 0.004*"standard"
topic diff=0.114769, rho=0.136291
PROGRESS: pass 3, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"car" + 0.006*"power" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"low" + 0.004*"type"
topic #6 (0.050): 0.026*"ship" + 0.023*"island" + 0.011*"specie" + 0.009*"sea" + 0.008*"boat" + 0.008*"port" + 0.007*"genus" + 0.007*"navy" + 0.007*"vessel" + 0.006*"crew"
topic #2 (0.050): 0.029*"club" + 0.024*"season" + 0.017*"match" + 0.016*"championship" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"game"
topic #7 (0.050): 0.023*"art" + 0.014*"church" + 0.011*"museum" + 0.010*"artist" + 0.009*"study" + 0.008*"painting" + 0.006*"exhibition" + 0.006*"collection" + 0.006*"father" + 0.005*"history"
topic #8 (0.050): 0.031*"album" + 0.030*"song" + 0.028*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"video" + 0.007*"tour" + 0.007*"chart"
topic diff=0.115491, rho=0.136291
PROGRESS: pass 3, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.006*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"low" + 0.004*"type"
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.006*"word" + 0.006*"text" + 0.005*"term" + 0.005*"mean" + 0.005*"point" + 0.005*"refer" + 0.005*"theory"
topic #15 (0.050): 0.053*"game" + 0.038*"season" + 0.021*"player" + 0.016*"race" + 0.010*"finish" + 0.009*"league" + 0.009*"football" + 0.009*"point" + 0.009*"basketball" + 0.007*"career"
topic #5 (0.050): 0.033*"woman" + 0.017*"hospital" + 0.015*"child" + 0.015*"medical" + 0.011*"health" + 0.009*"patient" + 0.007*"treatment" + 0.007*"medicine" + 0.006*"care" + 0.006*"black"
topic #13 (0.050): 0.017*"student" + 0.014*"college" + 0.014*"university" + 0.009*"study" + 0.009*"education" + 0.008*"science" + 0.008*"graduate" + 0.008*"department" + 0.007*"award" + 0.007*"director"
topic diff=0.111601, rho=0.136291
PROGRESS: pass 3, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.021*"art" + 0.015*"church" + 0.011*"museum" + 0.010*"artist" + 0.009*"study" + 0.008*"painting" + 0.006*"collection" + 0.006*"father" + 0.006*"german" + 0.005*"history"
topic #4 (0.050): 0.012*"road" + 0.012*"war" + 0.011*"route" + 0.011*"army" + 0.011*"battle" + 0.010*"force" + 0.007*"highway" + 0.007*"attack" + 0.007*"line" + 0.007*"north"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.010*"river" + 0.010*"town" + 0.009*"specie" + 0.009*"county" + 0.008*"age" + 0.007*"region" + 0.007*"census" + 0.006*"north"
topic #1 (0.050): 0.072*"film" + 0.014*"star" + 0.011*"character" + 0.011*"series" + 0.010*"direct" + 0.007*"movie" + 0.007*"story" + 0.006*"role" + 0.006*"game" + 0.005*"production"
topic diff=0.103618, rho=0.136291
PROGRESS: pass 3, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.027*"ship" + 0.022*"island" + 0.010*"specie" + 0.010*"sea" + 0.008*"boat" + 0.008*"port" + 0.007*"genus" + 0.007*"navy" + 0.007*"water" + 0.007*"vessel"
topic #10 (0.050): 0.013*"election" + 0.012*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"seat" + 0.005*"general"
topic #11 (0.050): 0.008*"get" + 0.008*"say" + 0.008*"tell" + 0.008*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.005*"want" + 0.004*"ask"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"found" + 0.007*"sell" + 0.006*"public" + 0.006*"market" + 0.006*"government" + 0.005*"development" + 0.005*"country"
topic diff=0.103433, rho=0.136291
PROGRESS: pass 3, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"found" + 0.007*"sell" + 0.006*"market" + 0.006*"public" + 0.006*"government" + 0.006*"development" + 0.005*"country"
topic #5 (0.050): 0.035*"woman" + 0.017*"hospital" + 0.015*"child" + 0.015*"medical" + 0.011*"health" + 0.009*"patient" + 0.008*"treatment" + 0.007*"care" + 0.006*"medicine" + 0.006*"black"
topic #4 (0.050): 0.012*"road" + 0.012*"war" + 0.011*"route" + 0.011*"battle" + 0.011*"army" + 0.010*"force" + 0.007*"attack" + 0.007*"line" + 0.007*"highway" + 0.007*"north"
topic #16 (0.050): 0.016*"unit" + 0.015*"army" + 0.014*"aircraft" + 0.012*"air" + 0.012*"war" + 0.012*"military" + 0.012*"operation" + 0.012*"force" + 0.011*"german" + 0.011*"command"
topic #9 (0.050): 0.017*"book" + 0.013*"publish" + 0.012*"series" + 0.009*"award" + 0.009*"television" + 0.008*"novel" + 0.007*"story" + 0.006*"life" + 0.006*"writer" + 0.006*"magazine"
topic diff=0.102278, rho=0.136291
PROGRESS: pass 3, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.014*"election" + 0.012*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"seat" + 0.005*"law"
topic #18 (0.050): 0.031*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"found" + 0.007*"sell" + 0.006*"market" + 0.006*"public" + 0.006*"government" + 0.005*"development" + 0.005*"country"
topic #16 (0.050): 0.015*"unit" + 0.014*"army" + 0.014*"aircraft" + 0.013*"air" + 0.012*"war" + 0.011*"force" + 0.011*"operation" + 0.011*"military" + 0.011*"command" + 0.011*"german"
topic #2 (0.050): 0.030*"club" + 0.023*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.015*"final" + 0.014*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #7 (0.050): 0.023*"art" + 0.016*"church" + 0.010*"artist" + 0.010*"museum" + 0.008*"study" + 0.008*"painting" + 0.006*"collection" + 0.006*"german" + 0.005*"father" + 0.005*"son"
topic diff=0.105201, rho=0.136291
PROGRESS: pass 3, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.010*"church" + 0.010*"site" + 0.009*"street" + 0.008*"design" + 0.006*"park" + 0.006*"th_century" + 0.006*"town" + 0.005*"stone"
topic #10 (0.050): 0.013*"election" + 0.012*"government" + 0.012*"party" + 0.008*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"general" + 0.005*"law"
topic #4 (0.050): 0.012*"road" + 0.012*"war" + 0.011*"route" + 0.011*"battle" + 0.011*"army" + 0.010*"force" + 0.008*"attack" + 0.007*"line" + 0.007*"north" + 0.007*"highway"
topic #8 (0.050): 0.033*"album" + 0.031*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #6 (0.050): 0.027*"ship" + 0.022*"island" + 0.010*"sea" + 0.010*"specie" + 0.008*"port" + 0.008*"coast" + 0.007*"boat" + 0.007*"navy" + 0.007*"vessel" + 0.007*"crew"
topic diff=0.100997, rho=0.136291
PROGRESS: pass 3, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.050): 0.075*"film" + 0.016*"star" + 0.012*"character" + 0.011*"series" + 0.010*"direct" + 0.008*"movie" + 0.007*"game" + 0.007*"role" + 0.007*"story" + 0.005*"production"
topic #16 (0.050): 0.015*"army" + 0.015*"unit" + 0.014*"aircraft" + 0.013*"air" + 0.012*"military" + 0.012*"war" + 0.012*"german" + 0.011*"command" + 0.011*"force" + 0.011*"operation"
topic #8 (0.050): 0.033*"album" + 0.031*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic #3 (0.050): 0.012*"system" + 0.009*"design" + 0.006*"power" + 0.006*"engine" + 0.006*"model" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"low"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.010*"river" + 0.010*"specie" + 0.010*"town" + 0.008*"county" + 0.008*"age" + 0.007*"region" + 0.006*"north" + 0.006*"census"
topic diff=0.093970, rho=0.136291
PROGRESS: pass 3, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 10
PROGRESS: pass 3, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.012*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"general" + 0.005*"law" + 0.005*"candidate"
topic #6 (0.050): 0.026*"ship" + 0.022*"island" + 0.010*"sea" + 0.010*"specie" + 0.008*"port" + 0.008*"coast" + 0.008*"boat" + 0.007*"vessel" + 0.007*"navy" + 0.007*"crew"
topic #11 (0.050): 0.008*"get" + 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.004*"want" + 0.004*"ask"
topic #2 (0.050): 0.030*"club" + 0.024*"season" + 0.017*"match" + 0.016*"championship" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"sell" + 0.007*"industry" + 0.007*"market" + 0.007*"found" + 0.006*"public" + 0.006*"development" + 0.005*"government" + 0.005*"country"
topic diff=0.089704, rho=0.136291
-8.230 per-word bound, 300.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.050): 0.040*"station" + 0.016*"court" + 0.015*"line" + 0.011*"law" + 0.011*"railway" + 0.008*"case" + 0.008*"operate" + 0.008*"train" + 0.008*"radio" + 0.007*"act"
topic #8 (0.050): 0.033*"album" + 0.031*"song" + 0.028*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #11 (0.050): 0.008*"get" + 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.004*"want" + 0.004*"ask"
topic #5 (0.050): 0.035*"woman" + 0.016*"hospital" + 0.015*"medical" + 0.015*"child" + 0.013*"health" + 0.009*"patient" + 0.008*"treatment" + 0.007*"black" + 0.007*"medicine" + 0.006*"care"
topic #10 (0.050): 0.013*"election" + 0.012*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"general" + 0.005*"law" + 0.005*"candidate"
topic diff=0.088769, rho=0.136291
-8.232 per-word bound, 300.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"sell" + 0.007*"found" + 0.006*"market" + 0.006*"public" + 0.006*"development" + 0.006*"government" + 0.005*"country"
topic #6 (0.050): 0.027*"ship" + 0.022*"island" + 0.010*"sea" + 0.009*"specie" + 0.008*"port" + 0.008*"boat" + 0.008*"coast" + 0.008*"vessel" + 0.007*"navy" + 0.007*"genus"
topic #1 (0.050): 0.077*"film" + 0.015*"star" + 0.011*"series" + 0.011*"character" + 0.011*"direct" + 0.008*"movie" + 0.007*"role" + 0.007*"game" + 0.007*"story" + 0.006*"production"
topic #7 (0.050): 0.022*"art" + 0.016*"church" + 0.011*"museum" + 0.010*"artist" + 0.008*"study" + 0.008*"painting" + 0.007*"german" + 0.006*"son" + 0.006*"father" + 0.005*"collection"
topic #4 (0.050): 0.012*"road" + 0.012*"war" + 0.011*"battle" + 0.011*"army" + 0.010*"route" + 0.010*"force" + 0.008*"attack" + 0.008*"line" + 0.007*"north" + 0.006*"highway"
topic diff=0.088921, rho=0.136291
-8.195 per-word bound, 293.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.010*"specie" + 0.010*"river" + 0.009*"town" + 0.009*"county" + 0.008*"age" + 0.007*"region" + 0.007*"census" + 0.007*"north"
topic #11 (0.050): 0.009*"get" + 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.004*"want" + 0.004*"death"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.009*"church" + 0.009*"street" + 0.008*"design" + 0.007*"park" + 0.005*"th_century" + 0.005*"construction" + 0.005*"stone"
topic #2 (0.050): 0.029*"club" + 0.023*"season" + 0.017*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #3 (0.050): 0.012*"system" + 0.009*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"control"
topic diff=0.087417, rho=0.135043
PROGRESS: pass 4, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.010*"site" + 0.010*"church" + 0.009*"street" + 0.008*"design" + 0.007*"park" + 0.006*"town" + 0.006*"th_century" + 0.005*"construction"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.010*"specie" + 0.010*"river" + 0.009*"town" + 0.009*"age" + 0.008*"county" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #11 (0.050): 0.009*"get" + 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.004*"want" + 0.004*"death"
topic #13 (0.050): 0.017*"student" + 0.015*"university" + 0.014*"college" + 0.011*"study" + 0.010*"education" + 0.009*"science" + 0.008*"award" + 0.008*"graduate" + 0.008*"department" + 0.007*"research"
topic #2 (0.050): 0.028*"club" + 0.022*"season" + 0.018*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"goal" + 0.010*"game"
topic diff=0.086970, rho=0.135043
PROGRESS: pass 4, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.050): 0.011*"displaystyle" + 0.010*"language" + 0.007*"example" + 0.007*"word" + 0.006*"text" + 0.005*"term" + 0.005*"point" + 0.005*"mean" + 0.005*"refer" + 0.005*"theory"
topic #4 (0.050): 0.012*"war" + 0.012*"road" + 0.011*"route" + 0.011*"battle" + 0.011*"army" + 0.010*"force" + 0.007*"attack" + 0.007*"line" + 0.007*"north" + 0.007*"bridge"
topic #16 (0.050): 0.015*"unit" + 0.015*"army" + 0.014*"aircraft" + 0.013*"military" + 0.013*"war" + 0.012*"force" + 0.012*"operation" + 0.012*"air" + 0.011*"command" + 0.010*"german"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.010*"site" + 0.010*"church" + 0.009*"street" + 0.008*"design" + 0.007*"park" + 0.006*"th_century" + 0.006*"town" + 0.005*"construction"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.010*"specie" + 0.010*"town" + 0.009*"river" + 0.009*"county" + 0.009*"age" + 0.007*"region" + 0.007*"census" + 0.006*"north"
topic diff=0.080126, rho=0.135043
PROGRESS: pass 4, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.050): 0.075*"film" + 0.015*"star" + 0.012*"character" + 0.011*"series" + 0.011*"direct" + 0.008*"game" + 0.008*"movie" + 0.007*"role" + 0.007*"story" + 0.006*"production"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"low"
topic #16 (0.050): 0.015*"army" + 0.015*"unit" + 0.015*"aircraft" + 0.013*"military" + 0.013*"force" + 0.013*"war" + 0.012*"operation" + 0.012*"air" + 0.011*"command" + 0.010*"german"
topic #17 (0.050): 0.007*"research" + 0.007*"study" + 0.006*"increase" + 0.006*"information" + 0.005*"human" + 0.005*"system" + 0.005*"process" + 0.005*"datum" + 0.005*"cell" + 0.005*"result"
topic #2 (0.050): 0.029*"club" + 0.022*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.012*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic diff=0.082267, rho=0.135043
PROGRESS: pass 4, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.006*"text" + 0.005*"refer" + 0.005*"mean" + 0.005*"point" + 0.005*"theory"
topic #3 (0.050): 0.011*"system" + 0.009*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.006*"car" + 0.005*"produce" + 0.005*"vehicle" + 0.004*"low" + 0.004*"type"
topic #2 (0.050): 0.029*"club" + 0.022*"season" + 0.018*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #17 (0.050): 0.007*"research" + 0.007*"study" + 0.006*"information" + 0.006*"increase" + 0.005*"human" + 0.005*"system" + 0.005*"process" + 0.005*"level" + 0.005*"datum" + 0.005*"result"
topic #10 (0.050): 0.013*"election" + 0.013*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.007*"political" + 0.005*"support" + 0.005*"candidate" + 0.005*"general" + 0.005*"seat"
topic diff=0.078797, rho=0.135043
PROGRESS: pass 4, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.032*"album" + 0.031*"song" + 0.030*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"video" + 0.007*"tour" + 0.007*"chart"
topic #2 (0.050): 0.030*"club" + 0.022*"season" + 0.017*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"market" + 0.007*"sell" + 0.006*"found" + 0.006*"public" + 0.006*"government" + 0.005*"country" + 0.005*"development"
topic #5 (0.050): 0.035*"woman" + 0.017*"child" + 0.017*"hospital" + 0.015*"medical" + 0.013*"health" + 0.009*"patient" + 0.008*"treatment" + 0.007*"black" + 0.006*"medicine" + 0.006*"care"
topic #17 (0.050): 0.007*"research" + 0.007*"study" + 0.006*"information" + 0.006*"increase" + 0.005*"system" + 0.005*"process" + 0.005*"human" + 0.005*"result" + 0.005*"level" + 0.005*"datum"
topic diff=0.082842, rho=0.135043
PROGRESS: pass 4, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.013*"government" + 0.011*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.005*"support" + 0.005*"seat" + 0.005*"general" + 0.005*"candidate"
topic #7 (0.050): 0.021*"art" + 0.015*"church" + 0.011*"museum" + 0.010*"artist" + 0.008*"study" + 0.008*"painting" + 0.007*"son" + 0.007*"father" + 0.006*"german" + 0.006*"french"
topic #6 (0.050): 0.027*"ship" + 0.024*"island" + 0.010*"sea" + 0.009*"specie" + 0.008*"boat" + 0.008*"port" + 0.007*"navy" + 0.007*"crew" + 0.007*"coast" + 0.007*"vessel"
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.006*"word" + 0.005*"term" + 0.005*"text" + 0.005*"mean" + 0.005*"refer" + 0.005*"point" + 0.005*"theory"
topic #2 (0.050): 0.030*"club" + 0.022*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic diff=0.074173, rho=0.135043
PROGRESS: pass 4, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.050): 0.051*"game" + 0.039*"season" + 0.021*"player" + 0.018*"race" + 0.010*"football" + 0.010*"finish" + 0.010*"league" + 0.009*"point" + 0.009*"basketball" + 0.007*"coach"
topic #7 (0.050): 0.021*"art" + 0.016*"church" + 0.011*"museum" + 0.010*"artist" + 0.008*"study" + 0.008*"painting" + 0.007*"german" + 0.007*"son" + 0.006*"father" + 0.006*"french"
topic #16 (0.050): 0.016*"unit" + 0.016*"army" + 0.014*"aircraft" + 0.014*"military" + 0.013*"war" + 0.013*"operation" + 0.012*"force" + 0.012*"air" + 0.011*"command" + 0.011*"german"
topic #3 (0.050): 0.011*"system" + 0.010*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.006*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.010*"river" + 0.010*"specie" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic diff=0.072237, rho=0.135043
PROGRESS: pass 4, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.029*"club" + 0.023*"season" + 0.017*"championship" + 0.017*"match" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"sell" + 0.007*"market" + 0.007*"found" + 0.006*"public" + 0.006*"government" + 0.005*"development" + 0.005*"country"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.011*"river" + 0.010*"specie" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"water" + 0.007*"north"
topic #3 (0.050): 0.011*"system" + 0.010*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #13 (0.050): 0.018*"student" + 0.015*"university" + 0.014*"college" + 0.011*"study" + 0.009*"education" + 0.009*"science" + 0.009*"award" + 0.008*"graduate" + 0.008*"department" + 0.008*"director"
topic diff=0.073693, rho=0.135043
PROGRESS: pass 4, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.018*"student" + 0.015*"university" + 0.014*"college" + 0.011*"study" + 0.009*"education" + 0.009*"award" + 0.009*"science" + 0.008*"graduate" + 0.008*"research" + 0.008*"department"
topic #7 (0.050): 0.022*"art" + 0.016*"church" + 0.010*"artist" + 0.010*"museum" + 0.009*"painting" + 0.008*"study" + 0.007*"son" + 0.007*"german" + 0.006*"father" + 0.006*"french"
topic #2 (0.050): 0.030*"club" + 0.023*"season" + 0.017*"championship" + 0.017*"match" + 0.016*"league" + 0.014*"final" + 0.014*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #10 (0.050): 0.014*"election" + 0.013*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"support" + 0.005*"law" + 0.005*"candidate" + 0.005*"seat"
topic #16 (0.050): 0.016*"army" + 0.016*"unit" + 0.015*"aircraft" + 0.013*"war" + 0.013*"military" + 0.012*"air" + 0.012*"force" + 0.012*"german" + 0.012*"operation" + 0.011*"command"
topic diff=0.072508, rho=0.135043
PROGRESS: pass 4, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"industry" + 0.007*"sell" + 0.007*"market" + 0.006*"found" + 0.006*"public" + 0.006*"country" + 0.005*"development" + 0.005*"government"
topic #15 (0.050): 0.051*"game" + 0.039*"season" + 0.021*"player" + 0.016*"race" + 0.010*"football" + 0.010*"league" + 0.010*"finish" + 0.009*"point" + 0.008*"basketball" + 0.007*"coach"
topic #0 (0.050): 0.010*"language" + 0.009*"displaystyle" + 0.007*"word" + 0.007*"example" + 0.005*"text" + 0.005*"term" + 0.005*"refer" + 0.005*"mean" + 0.005*"theory" + 0.005*"point"
topic #12 (0.050): 0.042*"station" + 0.017*"line" + 0.014*"court" + 0.012*"railway" + 0.011*"law" + 0.009*"radio" + 0.009*"case" + 0.009*"train" + 0.008*"operate" + 0.007*"act"
topic #10 (0.050): 0.013*"election" + 0.013*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.007*"political" + 0.006*"support" + 0.005*"law" + 0.005*"candidate" + 0.005*"general"
topic diff=0.073539, rho=0.135043
PROGRESS: pass 4, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.013*"election" + 0.013*"government" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"support" + 0.005*"law" + 0.005*"candidate" + 0.005*"general"
topic #8 (0.050): 0.034*"album" + 0.032*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #16 (0.050): 0.016*"army" + 0.015*"unit" + 0.015*"aircraft" + 0.013*"war" + 0.013*"military" + 0.013*"air" + 0.012*"german" + 0.012*"command" + 0.012*"force" + 0.011*"operation"
topic #1 (0.050): 0.075*"film" + 0.016*"star" + 0.013*"series" + 0.012*"character" + 0.011*"direct" + 0.009*"game" + 0.008*"movie" + 0.008*"role" + 0.007*"story" + 0.006*"production"
topic #2 (0.050): 0.031*"club" + 0.023*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.015*"final" + 0.014*"football" + 0.011*"event" + 0.010*"compete" + 0.010*"player"
topic diff=0.074193, rho=0.135043
PROGRESS: pass 4, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 10
PROGRESS: pass 4, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.011*"site" + 0.010*"church" + 0.009*"street" + 0.008*"design" + 0.008*"park" + 0.006*"th_century" + 0.006*"town" + 0.005*"construction"
topic #4 (0.050): 0.012*"war" + 0.012*"battle" + 0.012*"road" + 0.011*"route" + 0.010*"army" + 0.010*"force" + 0.008*"attack" + 0.007*"highway" + 0.007*"bridge" + 0.007*"north"
topic #3 (0.050): 0.012*"system" + 0.009*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"vehicle" + 0.004*"low"
topic #16 (0.050): 0.016*"army" + 0.015*"unit" + 0.014*"aircraft" + 0.013*"war" + 0.013*"military" + 0.013*"german" + 0.013*"air" + 0.012*"force" + 0.012*"command" + 0.011*"operation"
topic #8 (0.050): 0.034*"album" + 0.031*"song" + 0.029*"music" + 0.022*"band" + 0.013*"single" + 0.011*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic diff=0.066883, rho=0.135043
PROGRESS: pass 4, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.011*"site" + 0.010*"church" + 0.009*"street" + 0.008*"design" + 0.007*"park" + 0.006*"th_century" + 0.006*"town" + 0.006*"construction"
topic #16 (0.050): 0.017*"army" + 0.015*"unit" + 0.014*"aircraft" + 0.014*"military" + 0.013*"war" + 0.013*"german" + 0.013*"air" + 0.012*"command" + 0.012*"force" + 0.011*"operation"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.010*"specie" + 0.010*"river" + 0.009*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"north" + 0.007*"water"
topic #0 (0.050): 0.010*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.006*"mean" + 0.005*"text" + 0.005*"point" + 0.005*"refer" + 0.005*"theory"
topic #17 (0.050): 0.007*"study" + 0.006*"research" + 0.006*"increase" + 0.005*"human" + 0.005*"system" + 0.005*"process" + 0.005*"cell" + 0.005*"result" + 0.005*"information" + 0.005*"datum"
topic diff=0.065679, rho=0.135043
-8.235 per-word bound, 301.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 5000 documents into a model of 49835 documents
topic #2 (0.050): 0.030*"club" + 0.022*"season" + 0.017*"match" + 0.016*"championship" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #4 (0.050): 0.012*"war" + 0.012*"road" + 0.012*"battle" + 0.010*"route" + 0.010*"army" + 0.010*"force" + 0.008*"attack" + 0.007*"line" + 0.007*"north" + 0.007*"highway"
topic #12 (0.050): 0.041*"station" + 0.016*"line" + 0.016*"court" + 0.011*"law" + 0.011*"railway" + 0.008*"case" + 0.008*"train" + 0.008*"operate" + 0.008*"radio" + 0.007*"act"
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.011*"site" + 0.010*"church" + 0.009*"street" + 0.008*"design" + 0.007*"park" + 0.006*"th_century" + 0.006*"town" + 0.006*"construction"
topic #8 (0.050): 0.033*"album" + 0.032*"song" + 0.029*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic diff=0.063147, rho=0.135043
-8.236 per-word bound, 301.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 2835 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.011*"site" + 0.009*"church" + 0.009*"street" + 0.008*"design" + 0.007*"park" + 0.006*"construction" + 0.006*"town" + 0.006*"stone"
topic #9 (0.050): 0.018*"book" + 0.014*"publish" + 0.011*"series" + 0.009*"award" + 0.009*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"episode" + 0.005*"help" + 0.004*"death" + 0.004*"want"
topic #1 (0.050): 0.078*"film" + 0.016*"star" + 0.013*"series" + 0.012*"character" + 0.011*"direct" + 0.009*"game" + 0.009*"movie" + 0.008*"role" + 0.006*"story" + 0.006*"production"
topic #14 (0.050): 0.017*"population" + 0.016*"village" + 0.011*"specie" + 0.010*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.007*"region" + 0.007*"north" + 0.006*"south"
topic diff=0.068804, rho=0.135043
-8.176 per-word bound, 289.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.033*"album" + 0.032*"song" + 0.029*"music" + 0.021*"band" + 0.014*"single" + 0.011*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"market" + 0.007*"sell" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.006*"development" + 0.005*"government" + 0.005*"country"
topic #12 (0.050): 0.041*"station" + 0.016*"line" + 0.016*"court" + 0.012*"railway" + 0.011*"law" + 0.008*"train" + 0.008*"operate" + 0.008*"case" + 0.008*"radio" + 0.007*"network"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"control"
topic #17 (0.050): 0.007*"study" + 0.006*"research" + 0.006*"increase" + 0.006*"system" + 0.005*"information" + 0.005*"cell" + 0.005*"human" + 0.005*"process" + 0.005*"result" + 0.005*"level"
topic diff=0.065698, rho=0.133828
PROGRESS: pass 5, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.033*"album" + 0.032*"song" + 0.029*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #16 (0.050): 0.016*"army" + 0.015*"unit" + 0.014*"military" + 0.014*"war" + 0.014*"aircraft" + 0.012*"force" + 0.012*"operation" + 0.011*"command" + 0.011*"air" + 0.011*"german"
topic #7 (0.050): 0.021*"art" + 0.017*"church" + 0.011*"museum" + 0.010*"artist" + 0.008*"son" + 0.008*"german" + 0.008*"painting" + 0.007*"study" + 0.007*"father" + 0.006*"french"
topic #5 (0.050): 0.036*"woman" + 0.018*"hospital" + 0.017*"child" + 0.016*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"treatment" + 0.008*"black" + 0.006*"medicine" + 0.006*"care"
topic #2 (0.050): 0.028*"club" + 0.022*"season" + 0.018*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"goal"
topic diff=0.067439, rho=0.133828
PROGRESS: pass 5, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.028*"club" + 0.022*"season" + 0.018*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"score"
topic #4 (0.050): 0.013*"war" + 0.011*"battle" + 0.011*"road" + 0.011*"route" + 0.010*"army" + 0.010*"force" + 0.007*"attack" + 0.007*"bridge" + 0.007*"north" + 0.007*"line"
topic #16 (0.050): 0.016*"army" + 0.015*"unit" + 0.014*"aircraft" + 0.014*"military" + 0.014*"war" + 0.012*"force" + 0.012*"operation" + 0.011*"air" + 0.011*"command" + 0.011*"german"
topic #5 (0.050): 0.036*"woman" + 0.018*"child" + 0.017*"hospital" + 0.015*"medical" + 0.013*"health" + 0.010*"patient" + 0.008*"black" + 0.008*"treatment" + 0.007*"medicine" + 0.006*"care"
topic #15 (0.050): 0.051*"game" + 0.040*"season" + 0.022*"player" + 0.016*"race" + 0.010*"football" + 0.010*"league" + 0.010*"finish" + 0.009*"basketball" + 0.009*"point" + 0.008*"coach"
topic diff=0.061255, rho=0.133828
PROGRESS: pass 5, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.050): 0.007*"study" + 0.006*"research" + 0.006*"increase" + 0.006*"human" + 0.006*"information" + 0.005*"cell" + 0.005*"system" + 0.005*"process" + 0.005*"datum" + 0.005*"result"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.011*"specie" + 0.010*"town" + 0.010*"river" + 0.009*"county" + 0.009*"age" + 0.007*"region" + 0.007*"census" + 0.007*"water"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.008*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"episode" + 0.004*"want"
topic #4 (0.050): 0.013*"war" + 0.011*"battle" + 0.011*"road" + 0.011*"army" + 0.010*"route" + 0.010*"force" + 0.007*"attack" + 0.007*"north" + 0.006*"highway" + 0.006*"line"
topic #7 (0.050): 0.020*"art" + 0.016*"church" + 0.011*"museum" + 0.009*"artist" + 0.008*"son" + 0.008*"painting" + 0.008*"german" + 0.007*"study" + 0.007*"father" + 0.006*"marry"
topic diff=0.063511, rho=0.133828
PROGRESS: pass 5, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.010*"site" + 0.009*"street" + 0.009*"church" + 0.008*"design" + 0.008*"park" + 0.006*"town" + 0.005*"th_century" + 0.005*"main"
topic #1 (0.050): 0.076*"film" + 0.015*"star" + 0.013*"series" + 0.012*"character" + 0.011*"direct" + 0.011*"game" + 0.009*"movie" + 0.008*"role" + 0.007*"story" + 0.006*"production"
topic #5 (0.050): 0.037*"woman" + 0.018*"child" + 0.017*"hospital" + 0.016*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"treatment" + 0.008*"black" + 0.007*"medicine" + 0.006*"care"
topic #2 (0.050): 0.029*"club" + 0.022*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"player" + 0.010*"compete"
topic #6 (0.050): 0.027*"ship" + 0.026*"island" + 0.010*"sea" + 0.009*"boat" + 0.009*"port" + 0.008*"coast" + 0.008*"specie" + 0.007*"crew" + 0.007*"vessel" + 0.007*"water"
topic diff=0.060748, rho=0.133828
PROGRESS: pass 5, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.011*"specie" + 0.010*"river" + 0.010*"town" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"water"
topic #7 (0.050): 0.021*"art" + 0.016*"church" + 0.010*"museum" + 0.010*"artist" + 0.009*"son" + 0.008*"painting" + 0.007*"german" + 0.007*"father" + 0.007*"study" + 0.006*"french"
topic #5 (0.050): 0.036*"woman" + 0.018*"child" + 0.018*"hospital" + 0.016*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"treatment" + 0.008*"black" + 0.007*"medicine" + 0.006*"age"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.008*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"want" + 0.004*"death" + 0.004*"episode"
topic #1 (0.050): 0.076*"film" + 0.015*"star" + 0.013*"series" + 0.012*"character" + 0.011*"direct" + 0.010*"game" + 0.009*"movie" + 0.008*"role" + 0.007*"story" + 0.006*"production"
topic diff=0.065614, rho=0.133828
PROGRESS: pass 5, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 10
PROGRESS: pass 5, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.006*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.008*"market" + 0.008*"industry" + 0.007*"sell" + 0.006*"public" + 0.006*"found" + 0.006*"government" + 0.006*"country" + 0.005*"development"
topic #2 (0.050): 0.030*"club" + 0.022*"season" + 0.017*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #13 (0.050): 0.018*"student" + 0.015*"college" + 0.015*"university" + 0.012*"study" + 0.010*"education" + 0.009*"science" + 0.009*"award" + 0.009*"graduate" + 0.009*"research" + 0.008*"director"
topic #15 (0.050): 0.052*"game" + 0.042*"season" + 0.021*"player" + 0.017*"race" + 0.010*"league" + 0.010*"football" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic diff=0.057731, rho=0.133828
PROGRESS: pass 5, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.020*"art" + 0.017*"church" + 0.010*"museum" + 0.009*"artist" + 0.009*"son" + 0.008*"german" + 0.007*"painting" + 0.007*"father" + 0.007*"study" + 0.007*"french"
topic #9 (0.050): 0.019*"book" + 0.014*"publish" + 0.012*"series" + 0.010*"award" + 0.009*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"writer" + 0.007*"life" + 0.006*"magazine"
topic #5 (0.050): 0.038*"woman" + 0.019*"hospital" + 0.017*"child" + 0.017*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"treatment" + 0.007*"medicine" + 0.007*"black" + 0.007*"care"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"want" + 0.004*"death" + 0.004*"episode"
topic #13 (0.050): 0.018*"student" + 0.016*"university" + 0.015*"college" + 0.012*"study" + 0.010*"education" + 0.009*"science" + 0.009*"award" + 0.009*"graduate" + 0.008*"research" + 0.008*"director"
topic diff=0.056670, rho=0.133828
PROGRESS: pass 5, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.011*"system" + 0.010*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #7 (0.050): 0.020*"art" + 0.016*"church" + 0.010*"museum" + 0.010*"artist" + 0.009*"son" + 0.008*"painting" + 0.008*"german" + 0.007*"father" + 0.007*"study" + 0.006*"french"
topic #13 (0.050): 0.018*"student" + 0.015*"university" + 0.015*"college" + 0.012*"study" + 0.010*"education" + 0.009*"award" + 0.009*"science" + 0.009*"graduate" + 0.008*"research" + 0.008*"director"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.008*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"want" + 0.004*"death" + 0.004*"episode"
topic #16 (0.050): 0.016*"army" + 0.015*"unit" + 0.015*"aircraft" + 0.015*"war" + 0.014*"military" + 0.013*"german" + 0.012*"force" + 0.012*"operation" + 0.012*"command" + 0.011*"air"
topic diff=0.058704, rho=0.133828
PROGRESS: pass 5, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.019*"book" + 0.015*"publish" + 0.011*"series" + 0.010*"award" + 0.009*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic #6 (0.050): 0.028*"ship" + 0.025*"island" + 0.011*"sea" + 0.009*"port" + 0.008*"boat" + 0.008*"coast" + 0.008*"water" + 0.007*"crew" + 0.007*"vessel" + 0.007*"navy"
topic #16 (0.050): 0.017*"army" + 0.015*"unit" + 0.015*"aircraft" + 0.014*"war" + 0.014*"military" + 0.013*"german" + 0.013*"force" + 0.012*"air" + 0.012*"operation" + 0.011*"command"
topic #15 (0.050): 0.051*"game" + 0.041*"season" + 0.021*"player" + 0.016*"race" + 0.010*"football" + 0.010*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.012*"site" + 0.009*"street" + 0.009*"church" + 0.008*"design" + 0.008*"park" + 0.006*"town" + 0.006*"main" + 0.006*"th_century"
topic diff=0.057648, rho=0.133828
PROGRESS: pass 5, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.007*"sell" + 0.007*"market" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.005*"development" + 0.005*"provide" + 0.005*"government"
topic #2 (0.050): 0.030*"club" + 0.022*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.015*"final" + 0.014*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #13 (0.050): 0.018*"student" + 0.016*"university" + 0.015*"college" + 0.012*"study" + 0.010*"education" + 0.010*"award" + 0.009*"research" + 0.009*"science" + 0.009*"graduate" + 0.008*"director"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.008*"tell" + 0.007*"kill" + 0.006*"try" + 0.006*"back" + 0.005*"help" + 0.005*"want" + 0.004*"ask" + 0.004*"episode"
topic #6 (0.050): 0.028*"ship" + 0.024*"island" + 0.011*"sea" + 0.008*"coast" + 0.008*"port" + 0.008*"boat" + 0.008*"crew" + 0.007*"water" + 0.007*"vessel" + 0.007*"navy"
topic diff=0.061789, rho=0.133828
PROGRESS: pass 5, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.050): 0.042*"station" + 0.017*"line" + 0.014*"court" + 0.011*"railway" + 0.011*"law" + 0.009*"radio" + 0.009*"train" + 0.009*"case" + 0.008*"operate" + 0.007*"act"
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.011*"site" + 0.010*"street" + 0.009*"church" + 0.009*"design" + 0.008*"park" + 0.006*"town" + 0.006*"main" + 0.006*"construction"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"power" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"vehicle" + 0.004*"low"
topic #5 (0.050): 0.037*"woman" + 0.017*"child" + 0.017*"hospital" + 0.016*"medical" + 0.013*"health" + 0.009*"black" + 0.009*"patient" + 0.008*"treatment" + 0.007*"medicine" + 0.006*"care"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.011*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"north" + 0.007*"water"
topic diff=0.060696, rho=0.133828
PROGRESS: pass 5, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.031*"club" + 0.022*"season" + 0.018*"match" + 0.017*"championship" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"compete" + 0.010*"player"
topic #16 (0.050): 0.017*"army" + 0.015*"unit" + 0.014*"war" + 0.014*"german" + 0.014*"aircraft" + 0.014*"military" + 0.013*"air" + 0.012*"force" + 0.012*"command" + 0.011*"operation"
topic #10 (0.050): 0.014*"government" + 0.013*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"support" + 0.006*"law" + 0.005*"general" + 0.005*"candidate"
topic #1 (0.050): 0.077*"film" + 0.017*"star" + 0.014*"series" + 0.013*"character" + 0.011*"direct" + 0.011*"game" + 0.009*"movie" + 0.008*"role" + 0.006*"story" + 0.006*"actor"
topic #5 (0.050): 0.037*"woman" + 0.016*"child" + 0.016*"hospital" + 0.016*"medical" + 0.014*"health" + 0.009*"black" + 0.009*"patient" + 0.009*"treatment" + 0.007*"medicine" + 0.006*"care"
topic diff=0.055407, rho=0.133828
PROGRESS: pass 5, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 10
PROGRESS: pass 5, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.020*"art" + 0.019*"church" + 0.010*"museum" + 0.010*"german" + 0.009*"artist" + 0.009*"son" + 0.007*"painting" + 0.007*"father" + 0.007*"study" + 0.006*"marry"
topic #12 (0.050): 0.041*"station" + 0.017*"line" + 0.017*"court" + 0.012*"railway" + 0.011*"law" + 0.009*"train" + 0.008*"radio" + 0.008*"case" + 0.008*"operate" + 0.007*"act"
topic #9 (0.050): 0.019*"book" + 0.015*"publish" + 0.011*"series" + 0.009*"award" + 0.009*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"sell" + 0.008*"market" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.006*"development" + 0.005*"country" + 0.005*"provide"
topic #13 (0.050): 0.018*"student" + 0.016*"university" + 0.015*"college" + 0.012*"study" + 0.010*"education" + 0.010*"award" + 0.009*"science" + 0.009*"research" + 0.009*"graduate" + 0.008*"director"
topic diff=0.053195, rho=0.133828
-8.225 per-word bound, 299.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"sell" + 0.008*"market" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.005*"development" + 0.005*"country" + 0.005*"provide"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"episode" + 0.004*"death" + 0.004*"ask"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.030*"music" + 0.021*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.007*"tour" + 0.007*"video" + 0.007*"chart"
topic #7 (0.050): 0.020*"art" + 0.018*"church" + 0.010*"museum" + 0.010*"german" + 0.010*"artist" + 0.010*"son" + 0.008*"painting" + 0.007*"father" + 0.007*"study" + 0.006*"daughter"
topic #9 (0.050): 0.019*"book" + 0.015*"publish" + 0.011*"series" + 0.009*"award" + 0.009*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic diff=0.054398, rho=0.133828
-8.228 per-word bound, 299.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #10 (0.050): 0.014*"government" + 0.013*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"support" + 0.005*"general" + 0.005*"candidate"
topic #15 (0.050): 0.050*"game" + 0.042*"season" + 0.021*"player" + 0.018*"race" + 0.011*"football" + 0.010*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic #5 (0.050): 0.037*"woman" + 0.017*"hospital" + 0.017*"child" + 0.016*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"black" + 0.008*"treatment" + 0.007*"medicine" + 0.006*"care"
topic #17 (0.050): 0.007*"study" + 0.006*"research" + 0.006*"increase" + 0.006*"cell" + 0.006*"human" + 0.006*"system" + 0.005*"process" + 0.005*"result" + 0.005*"information" + 0.005*"level"
topic #2 (0.050): 0.030*"club" + 0.022*"season" + 0.017*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"player" + 0.010*"compete"
topic diff=0.051196, rho=0.133828
-8.192 per-word bound, 292.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.009*"street" + 0.008*"park" + 0.008*"church" + 0.008*"design" + 0.006*"construction" + 0.006*"town" + 0.005*"stone"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.006*"development" + 0.005*"country" + 0.005*"government"
topic #15 (0.050): 0.050*"game" + 0.041*"season" + 0.021*"player" + 0.017*"race" + 0.011*"football" + 0.010*"finish" + 0.010*"league" + 0.009*"point" + 0.009*"basketball" + 0.008*"coach"
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.011*"specie" + 0.010*"river" + 0.009*"town" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #2 (0.050): 0.029*"club" + 0.022*"season" + 0.017*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.011*"event" + 0.010*"compete" + 0.010*"player"
topic diff=0.054156, rho=0.132645
PROGRESS: pass 6, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.012*"system" + 0.009*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"control"
topic #7 (0.050): 0.020*"art" + 0.017*"church" + 0.010*"museum" + 0.010*"son" + 0.009*"artist" + 0.009*"german" + 0.008*"painting" + 0.007*"father" + 0.007*"daughter" + 0.007*"marry"
topic #11 (0.050): 0.009*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"episode" + 0.005*"death" + 0.004*"want"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.008*"market" + 0.007*"sell" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.006*"development" + 0.005*"country" + 0.005*"government"
topic #12 (0.050): 0.042*"station" + 0.016*"line" + 0.016*"court" + 0.012*"railway" + 0.010*"law" + 0.009*"operate" + 0.008*"train" + 0.008*"radio" + 0.008*"case" + 0.007*"network"
topic diff=0.056780, rho=0.132645
PROGRESS: pass 6, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.020*"art" + 0.017*"church" + 0.010*"museum" + 0.010*"son" + 0.009*"artist" + 0.009*"german" + 0.007*"painting" + 0.007*"father" + 0.007*"marry" + 0.007*"daughter"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"episode" + 0.004*"want"
topic #4 (0.050): 0.012*"war" + 0.011*"battle" + 0.011*"road" + 0.011*"route" + 0.010*"force" + 0.010*"army" + 0.007*"attack" + 0.007*"north" + 0.006*"line" + 0.006*"bridge"
topic #5 (0.050): 0.037*"woman" + 0.019*"child" + 0.017*"hospital" + 0.016*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"black" + 0.008*"treatment" + 0.007*"medicine" + 0.006*"age"
topic #12 (0.050): 0.042*"station" + 0.017*"court" + 0.017*"line" + 0.012*"railway" + 0.011*"law" + 0.009*"train" + 0.009*"operate" + 0.008*"case" + 0.008*"radio" + 0.007*"public"
topic diff=0.051035, rho=0.132645
PROGRESS: pass 6, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.029*"club" + 0.021*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.009*"street" + 0.009*"park" + 0.008*"church" + 0.008*"design" + 0.006*"town" + 0.006*"construction" + 0.005*"main"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.012*"specie" + 0.010*"river" + 0.010*"town" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"water"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"found" + 0.006*"country" + 0.006*"public" + 0.005*"development" + 0.005*"government"
topic #4 (0.050): 0.013*"war" + 0.011*"battle" + 0.011*"road" + 0.010*"route" + 0.010*"army" + 0.010*"force" + 0.007*"attack" + 0.007*"north" + 0.006*"highway" + 0.006*"line"
topic diff=0.053300, rho=0.132645
PROGRESS: pass 6, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.020*"art" + 0.016*"church" + 0.010*"son" + 0.010*"museum" + 0.009*"artist" + 0.008*"german" + 0.008*"painting" + 0.008*"father" + 0.007*"marry" + 0.007*"daughter"
topic #16 (0.050): 0.017*"army" + 0.015*"unit" + 0.015*"war" + 0.015*"military" + 0.014*"aircraft" + 0.013*"force" + 0.012*"operation" + 0.012*"command" + 0.012*"air" + 0.011*"german"
topic #18 (0.050): 0.032*"company" + 0.012*"business" + 0.008*"market" + 0.008*"industry" + 0.008*"sell" + 0.006*"found" + 0.006*"public" + 0.006*"country" + 0.005*"development" + 0.005*"government"
topic #3 (0.050): 0.012*"system" + 0.009*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.006*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"low" + 0.004*"type"
topic #14 (0.050): 0.019*"population" + 0.016*"village" + 0.012*"specie" + 0.010*"town" + 0.010*"river" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"water"
topic diff=0.050429, rho=0.132645
PROGRESS: pass 6, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.018*"match" + 0.016*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #1 (0.050): 0.076*"film" + 0.016*"star" + 0.015*"series" + 0.013*"character" + 0.012*"game" + 0.011*"direct" + 0.009*"movie" + 0.008*"role" + 0.007*"story" + 0.007*"production"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.012*"specie" + 0.010*"river" + 0.010*"town" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"water"
topic #4 (0.050): 0.012*"war" + 0.011*"road" + 0.011*"battle" + 0.010*"force" + 0.010*"route" + 0.010*"army" + 0.007*"attack" + 0.007*"highway" + 0.006*"north" + 0.006*"troop"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"low" + 0.004*"standard"
topic diff=0.055605, rho=0.132645
PROGRESS: pass 6, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.028*"ship" + 0.026*"island" + 0.010*"sea" + 0.009*"boat" + 0.009*"port" + 0.008*"coast" + 0.007*"crew" + 0.007*"water" + 0.007*"navy" + 0.007*"vessel"
topic #10 (0.050): 0.015*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"political" + 0.008*"vote" + 0.006*"support" + 0.005*"general" + 0.005*"law" + 0.005*"seat"
topic #12 (0.050): 0.042*"station" + 0.017*"line" + 0.015*"court" + 0.012*"railway" + 0.010*"law" + 0.009*"operate" + 0.008*"train" + 0.008*"radio" + 0.008*"case" + 0.007*"act"
topic #7 (0.050): 0.019*"art" + 0.016*"church" + 0.010*"son" + 0.010*"museum" + 0.009*"artist" + 0.008*"german" + 0.008*"father" + 0.007*"painting" + 0.007*"marry" + 0.007*"daughter"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.006*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic diff=0.048304, rho=0.132645
PROGRESS: pass 6, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 10
merging changes from 4000 documents into a model of 49835 documents
topic #8 (0.050): 0.033*"album" + 0.032*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #12 (0.050): 0.042*"station" + 0.017*"line" + 0.015*"court" + 0.012*"railway" + 0.010*"law" + 0.009*"radio" + 0.009*"train" + 0.009*"case" + 0.009*"operate" + 0.007*"act"
topic #1 (0.050): 0.074*"film" + 0.016*"star" + 0.015*"series" + 0.013*"character" + 0.012*"game" + 0.011*"direct" + 0.009*"role" + 0.008*"movie" + 0.007*"production" + 0.006*"story"
topic #5 (0.050): 0.037*"woman" + 0.019*"hospital" + 0.017*"child" + 0.017*"medical" + 0.014*"health" + 0.010*"patient" + 0.009*"treatment" + 0.008*"black" + 0.008*"medicine" + 0.007*"care"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"industry" + 0.007*"sell" + 0.006*"found" + 0.006*"public" + 0.005*"country" + 0.005*"provide" + 0.005*"development"
topic diff=0.044570, rho=0.132645
PROGRESS: pass 6, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.050): 0.050*"game" + 0.043*"season" + 0.021*"player" + 0.017*"race" + 0.011*"football" + 0.010*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic #5 (0.050): 0.037*"woman" + 0.019*"hospital" + 0.018*"child" + 0.016*"medical" + 0.014*"health" + 0.010*"patient" + 0.008*"treatment" + 0.007*"black" + 0.007*"care" + 0.007*"medicine"
topic #9 (0.050): 0.020*"book" + 0.016*"publish" + 0.011*"series" + 0.010*"award" + 0.008*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.007*"life"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.012*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"water"
topic #16 (0.050): 0.017*"army" + 0.015*"war" + 0.015*"unit" + 0.014*"military" + 0.014*"german" + 0.014*"aircraft" + 0.013*"force" + 0.012*"operation" + 0.012*"command" + 0.011*"air"
topic diff=0.047351, rho=0.132645
PROGRESS: pass 6, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.011*"system" + 0.010*"design" + 0.006*"engine" + 0.006*"model" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic #15 (0.050): 0.049*"game" + 0.042*"season" + 0.021*"player" + 0.016*"race" + 0.010*"football" + 0.010*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic #2 (0.050): 0.031*"club" + 0.022*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #1 (0.050): 0.074*"film" + 0.016*"series" + 0.016*"star" + 0.013*"character" + 0.013*"game" + 0.011*"direct" + 0.008*"role" + 0.008*"movie" + 0.006*"story" + 0.006*"production"
topic #19 (0.050): 0.025*"building" + 0.016*"house" + 0.012*"site" + 0.009*"street" + 0.009*"design" + 0.009*"park" + 0.008*"church" + 0.006*"town" + 0.006*"main" + 0.006*"construction"
topic diff=0.050974, rho=0.132645
PROGRESS: pass 6, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.020*"book" + 0.015*"publish" + 0.011*"series" + 0.009*"award" + 0.008*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.007*"life"
topic #19 (0.050): 0.025*"building" + 0.016*"house" + 0.012*"site" + 0.010*"street" + 0.009*"design" + 0.009*"park" + 0.008*"church" + 0.006*"town" + 0.006*"main" + 0.006*"construction"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.012*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"water" + 0.007*"north"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.008*"sell" + 0.008*"market" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.005*"provide" + 0.005*"development" + 0.005*"country"
topic #1 (0.050): 0.074*"film" + 0.017*"star" + 0.016*"series" + 0.013*"game" + 0.013*"character" + 0.011*"direct" + 0.009*"movie" + 0.008*"role" + 0.007*"production" + 0.006*"story"
topic diff=0.053271, rho=0.132645
PROGRESS: pass 6, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 11
PROGRESS: pass 6, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 12
merging changes from 4000 documents into a model of 49835 documents
topic #7 (0.050): 0.020*"art" + 0.019*"church" + 0.011*"german" + 0.010*"son" + 0.009*"museum" + 0.009*"artist" + 0.008*"painting" + 0.008*"father" + 0.007*"marry" + 0.007*"daughter"
topic #0 (0.050): 0.010*"language" + 0.008*"displaystyle" + 0.007*"word" + 0.007*"example" + 0.006*"term" + 0.005*"mean" + 0.005*"refer" + 0.005*"text" + 0.005*"point" + 0.005*"theory"
topic #12 (0.050): 0.043*"station" + 0.018*"line" + 0.015*"court" + 0.012*"railway" + 0.010*"law" + 0.009*"radio" + 0.009*"train" + 0.008*"case" + 0.008*"operate" + 0.007*"network"
topic #5 (0.050): 0.037*"woman" + 0.017*"hospital" + 0.017*"child" + 0.016*"medical" + 0.014*"health" + 0.009*"patient" + 0.009*"black" + 0.009*"treatment" + 0.007*"medicine" + 0.007*"care"
topic #1 (0.050): 0.076*"film" + 0.017*"star" + 0.016*"series" + 0.013*"character" + 0.013*"game" + 0.011*"direct" + 0.009*"movie" + 0.009*"role" + 0.007*"production" + 0.007*"actor"
topic diff=0.045501, rho=0.132645
PROGRESS: pass 6, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.020*"book" + 0.015*"publish" + 0.011*"series" + 0.009*"award" + 0.008*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic #10 (0.050): 0.014*"government" + 0.013*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"support" + 0.005*"general" + 0.005*"candidate"
topic #1 (0.050): 0.076*"film" + 0.017*"star" + 0.015*"series" + 0.013*"character" + 0.013*"game" + 0.011*"direct" + 0.009*"movie" + 0.008*"role" + 0.007*"production" + 0.006*"actor"
topic #7 (0.050): 0.020*"art" + 0.019*"church" + 0.011*"son" + 0.010*"german" + 0.010*"museum" + 0.009*"artist" + 0.008*"father" + 0.007*"painting" + 0.007*"marry" + 0.007*"daughter"
topic #5 (0.050): 0.037*"woman" + 0.017*"hospital" + 0.016*"medical" + 0.016*"child" + 0.014*"health" + 0.009*"patient" + 0.009*"treatment" + 0.008*"black" + 0.007*"medicine" + 0.007*"age"
topic diff=0.048482, rho=0.132645
PROGRESS: pass 6, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.011*"site" + 0.009*"street" + 0.009*"church" + 0.009*"park" + 0.009*"design" + 0.006*"town" + 0.006*"construction" + 0.006*"main"
topic #15 (0.050): 0.048*"game" + 0.044*"season" + 0.020*"player" + 0.017*"race" + 0.011*"football" + 0.010*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.012*"specie" + 0.010*"river" + 0.009*"town" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #10 (0.050): 0.015*"government" + 0.014*"election" + 0.013*"party" + 0.009*"elect" + 0.008*"political" + 0.008*"vote" + 0.006*"law" + 0.006*"support" + 0.005*"general" + 0.005*"candidate"
topic diff=0.044579, rho=0.132645
-8.230 per-word bound, 300.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #6 (0.050): 0.028*"ship" + 0.024*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"coast" + 0.008*"boat" + 0.008*"water" + 0.008*"vessel" + 0.007*"crew" + 0.007*"fleet"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"want" + 0.004*"ask"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.031*"music" + 0.021*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #5 (0.050): 0.037*"woman" + 0.017*"hospital" + 0.017*"child" + 0.016*"medical" + 0.015*"health" + 0.010*"patient" + 0.009*"treatment" + 0.008*"black" + 0.007*"medicine" + 0.006*"age"
topic #4 (0.050): 0.011*"war" + 0.011*"battle" + 0.011*"road" + 0.010*"route" + 0.010*"force" + 0.009*"army" + 0.007*"attack" + 0.007*"north" + 0.006*"line" + 0.006*"highway"
topic diff=0.047567, rho=0.132645
-8.231 per-word bound, 300.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #1 (0.050): 0.078*"film" + 0.016*"star" + 0.016*"series" + 0.012*"character" + 0.012*"game" + 0.011*"direct" + 0.009*"movie" + 0.009*"role" + 0.007*"production" + 0.007*"actor"
topic #4 (0.050): 0.012*"war" + 0.011*"battle" + 0.010*"road" + 0.010*"force" + 0.010*"route" + 0.009*"army" + 0.007*"attack" + 0.007*"king" + 0.007*"north" + 0.007*"line"
topic #9 (0.050): 0.020*"book" + 0.015*"publish" + 0.011*"series" + 0.009*"award" + 0.008*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic #16 (0.050): 0.018*"army" + 0.015*"war" + 0.014*"unit" + 0.014*"military" + 0.014*"german" + 0.013*"aircraft" + 0.012*"force" + 0.012*"operation" + 0.012*"command" + 0.011*"division"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"found" + 0.006*"public" + 0.006*"country" + 0.006*"development" + 0.005*"provide"
topic diff=0.051405, rho=0.132645
-8.149 per-word bound, 283.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.019*"art" + 0.017*"church" + 0.012*"son" + 0.010*"museum" + 0.009*"german" + 0.009*"artist" + 0.008*"father" + 0.008*"daughter" + 0.007*"painting" + 0.007*"marry"
topic #0 (0.050): 0.010*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"text" + 0.006*"term" + 0.005*"mean" + 0.005*"refer" + 0.005*"point" + 0.005*"theory"
topic #10 (0.050): 0.015*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"general" + 0.006*"support" + 0.005*"candidate"
topic #2 (0.050): 0.029*"club" + 0.021*"season" + 0.018*"match" + 0.017*"championship" + 0.014*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"player" + 0.010*"compete"
topic #16 (0.050): 0.018*"army" + 0.016*"war" + 0.014*"military" + 0.014*"unit" + 0.013*"german" + 0.013*"aircraft" + 0.012*"force" + 0.012*"command" + 0.011*"operation" + 0.011*"division"
topic diff=0.046953, rho=0.131494
PROGRESS: pass 7, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.050): 0.036*"woman" + 0.018*"hospital" + 0.017*"child" + 0.016*"medical" + 0.015*"health" + 0.011*"patient" + 0.009*"treatment" + 0.008*"black" + 0.007*"medicine" + 0.007*"age"
topic #13 (0.050): 0.019*"student" + 0.016*"university" + 0.015*"college" + 0.013*"study" + 0.011*"education" + 0.010*"award" + 0.010*"science" + 0.010*"research" + 0.009*"graduate" + 0.008*"program"
topic #17 (0.050): 0.007*"study" + 0.006*"increase" + 0.006*"human" + 0.006*"research" + 0.006*"information" + 0.006*"result" + 0.006*"process" + 0.005*"cell" + 0.005*"system" + 0.005*"level"
topic #2 (0.050): 0.029*"club" + 0.021*"season" + 0.018*"match" + 0.016*"championship" + 0.014*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"score" + 0.010*"compete"
topic #19 (0.050): 0.026*"building" + 0.015*"house" + 0.011*"site" + 0.009*"park" + 0.009*"street" + 0.008*"design" + 0.008*"church" + 0.006*"construction" + 0.006*"town" + 0.006*"stone"
topic diff=0.050382, rho=0.131494
PROGRESS: pass 7, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 10
PROGRESS: pass 7, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.050): 0.033*"album" + 0.032*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.011*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #2 (0.050): 0.028*"club" + 0.021*"season" + 0.018*"match" + 0.016*"championship" + 0.014*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"score" + 0.010*"round"
topic #1 (0.050): 0.076*"film" + 0.016*"star" + 0.016*"series" + 0.013*"game" + 0.013*"character" + 0.011*"direct" + 0.009*"movie" + 0.008*"role" + 0.007*"production" + 0.006*"story"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"want" + 0.004*"episode"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"low"
topic diff=0.044641, rho=0.131494
PROGRESS: pass 7, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"country" + 0.006*"found" + 0.006*"public" + 0.005*"development" + 0.005*"government"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.009*"street" + 0.009*"park" + 0.008*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.005*"stone"
topic #1 (0.050): 0.076*"film" + 0.016*"star" + 0.015*"series" + 0.014*"game" + 0.013*"character" + 0.011*"direct" + 0.009*"movie" + 0.009*"role" + 0.007*"production" + 0.007*"story"
topic #12 (0.050): 0.041*"station" + 0.018*"line" + 0.017*"court" + 0.013*"railway" + 0.010*"law" + 0.009*"operate" + 0.009*"train" + 0.008*"case" + 0.008*"radio" + 0.007*"network"
topic #9 (0.050): 0.020*"book" + 0.015*"publish" + 0.010*"series" + 0.009*"award" + 0.008*"television" + 0.008*"novel" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic diff=0.046661, rho=0.131494
PROGRESS: pass 7, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.019*"population" + 0.017*"village" + 0.013*"specie" + 0.010*"town" + 0.010*"river" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"km"
topic #18 (0.050): 0.032*"company" + 0.012*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"country" + 0.006*"public" + 0.006*"found" + 0.005*"government" + 0.005*"development"
topic #8 (0.050): 0.033*"album" + 0.033*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #4 (0.050): 0.012*"war" + 0.011*"route" + 0.011*"battle" + 0.010*"road" + 0.010*"force" + 0.009*"army" + 0.007*"highway" + 0.007*"north" + 0.007*"attack" + 0.006*"king"
topic #10 (0.050): 0.015*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.005*"seat"
topic diff=0.044540, rho=0.131494
PROGRESS: pass 7, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.050): 0.018*"army" + 0.016*"war" + 0.015*"military" + 0.014*"unit" + 0.013*"aircraft" + 0.013*"force" + 0.012*"operation" + 0.012*"german" + 0.012*"command" + 0.011*"air"
topic #18 (0.050): 0.033*"company" + 0.012*"business" + 0.008*"market" + 0.008*"industry" + 0.008*"sell" + 0.006*"public" + 0.006*"country" + 0.006*"purchase" + 0.006*"found" + 0.005*"product"
topic #4 (0.050): 0.012*"war" + 0.011*"battle" + 0.010*"road" + 0.010*"force" + 0.010*"route" + 0.009*"army" + 0.007*"highway" + 0.007*"attack" + 0.006*"north" + 0.006*"king"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.010*"park" + 0.009*"street" + 0.008*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.005*"main"
topic #15 (0.050): 0.050*"game" + 0.044*"season" + 0.020*"player" + 0.016*"race" + 0.011*"league" + 0.011*"football" + 0.010*"finish" + 0.009*"basketball" + 0.009*"point" + 0.008*"career"
topic diff=0.049812, rho=0.131494
PROGRESS: pass 7, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.050): 0.007*"study" + 0.006*"increase" + 0.006*"process" + 0.006*"research" + 0.006*"information" + 0.006*"human" + 0.005*"cell" + 0.005*"result" + 0.005*"system" + 0.005*"datum"
topic #13 (0.050): 0.019*"student" + 0.016*"college" + 0.016*"university" + 0.013*"study" + 0.011*"education" + 0.010*"award" + 0.010*"science" + 0.010*"research" + 0.009*"graduate" + 0.009*"director"
topic #8 (0.050): 0.033*"song" + 0.032*"album" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #0 (0.050): 0.009*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.005*"term" + 0.005*"text" + 0.005*"mean" + 0.005*"refer" + 0.005*"point" + 0.005*"theory"
topic #7 (0.050): 0.019*"art" + 0.016*"church" + 0.012*"son" + 0.010*"museum" + 0.009*"artist" + 0.009*"father" + 0.008*"german" + 0.008*"marry" + 0.008*"daughter" + 0.007*"painting"
topic diff=0.042453, rho=0.131494
PROGRESS: pass 7, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.018*"championship" + 0.017*"match" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #6 (0.050): 0.029*"ship" + 0.026*"island" + 0.011*"sea" + 0.009*"boat" + 0.009*"port" + 0.008*"coast" + 0.007*"crew" + 0.007*"vessel" + 0.007*"water" + 0.007*"navy"
topic #1 (0.050): 0.074*"film" + 0.017*"series" + 0.016*"star" + 0.013*"game" + 0.013*"character" + 0.011*"direct" + 0.009*"role" + 0.009*"movie" + 0.007*"production" + 0.006*"story"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.009*"park" + 0.009*"street" + 0.008*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.006*"main"
topic #13 (0.050): 0.019*"student" + 0.016*"university" + 0.016*"college" + 0.013*"study" + 0.010*"education" + 0.010*"award" + 0.010*"science" + 0.009*"research" + 0.009*"graduate" + 0.009*"director"
topic diff=0.042420, rho=0.131494
PROGRESS: pass 7, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.009*"park" + 0.009*"street" + 0.008*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.006*"main"
topic #13 (0.050): 0.019*"student" + 0.016*"university" + 0.016*"college" + 0.013*"study" + 0.010*"education" + 0.010*"award" + 0.010*"science" + 0.009*"research" + 0.009*"graduate" + 0.009*"director"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"industry" + 0.008*"sell" + 0.006*"found" + 0.006*"public" + 0.006*"provide" + 0.005*"country" + 0.005*"product"
topic #1 (0.050): 0.075*"film" + 0.017*"series" + 0.016*"star" + 0.013*"character" + 0.013*"game" + 0.011*"direct" + 0.009*"role" + 0.009*"movie" + 0.007*"production" + 0.006*"story"
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.017*"championship" + 0.017*"match" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic diff=0.044679, rho=0.131494
PROGRESS: pass 7, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.050): 0.009*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"text" + 0.005*"term" + 0.005*"mean" + 0.005*"refer" + 0.005*"theory" + 0.005*"point"
topic #2 (0.050): 0.031*"club" + 0.021*"season" + 0.017*"championship" + 0.017*"match" + 0.016*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"player" + 0.010*"compete"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.012*"site" + 0.009*"street" + 0.009*"park" + 0.008*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.006*"main"
topic #4 (0.050): 0.012*"war" + 0.011*"battle" + 0.011*"road" + 0.010*"force" + 0.010*"route" + 0.009*"army" + 0.007*"highway" + 0.007*"attack" + 0.006*"north" + 0.006*"king"
topic #9 (0.050): 0.021*"book" + 0.016*"publish" + 0.010*"series" + 0.010*"award" + 0.009*"novel" + 0.008*"television" + 0.008*"story" + 0.007*"magazine" + 0.007*"writer" + 0.007*"life"
topic diff=0.043149, rho=0.131494
PROGRESS: pass 7, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.013*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"region" + 0.008*"age" + 0.007*"water" + 0.007*"north"
topic #17 (0.050): 0.006*"study" + 0.006*"increase" + 0.006*"research" + 0.006*"process" + 0.006*"human" + 0.006*"result" + 0.006*"cell" + 0.005*"datum" + 0.005*"information" + 0.005*"effect"
topic #15 (0.050): 0.049*"game" + 0.043*"season" + 0.020*"player" + 0.017*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.009*"point" + 0.008*"basketball" + 0.008*"coach"
topic #18 (0.050): 0.032*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"found" + 0.006*"provide" + 0.005*"country" + 0.005*"public" + 0.005*"product"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"try" + 0.006*"back" + 0.005*"help" + 0.005*"want" + 0.004*"ask" + 0.004*"death"
topic diff=0.047706, rho=0.131494
PROGRESS: pass 7, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.050): 0.075*"film" + 0.017*"star" + 0.017*"series" + 0.014*"game" + 0.014*"character" + 0.011*"direct" + 0.009*"role" + 0.009*"movie" + 0.007*"production" + 0.007*"actor"
topic #8 (0.050): 0.035*"album" + 0.033*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #2 (0.050): 0.032*"club" + 0.022*"season" + 0.018*"match" + 0.017*"championship" + 0.016*"league" + 0.015*"final" + 0.013*"football" + 0.011*"event" + 0.010*"compete" + 0.010*"player"
topic #17 (0.050): 0.006*"study" + 0.006*"increase" + 0.006*"research" + 0.006*"human" + 0.006*"process" + 0.006*"result" + 0.005*"cell" + 0.005*"system" + 0.005*"datum" + 0.005*"information"
topic #13 (0.050): 0.020*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.010*"education" + 0.010*"research" + 0.010*"award" + 0.010*"science" + 0.009*"graduate" + 0.009*"director"
topic diff=0.047170, rho=0.131494
PROGRESS: pass 7, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.020*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.010*"award" + 0.010*"education" + 0.010*"research" + 0.010*"science" + 0.009*"graduate" + 0.009*"director"
topic #2 (0.050): 0.031*"club" + 0.022*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.015*"final" + 0.013*"football" + 0.011*"event" + 0.010*"compete" + 0.010*"player"
topic #7 (0.050): 0.019*"church" + 0.019*"art" + 0.012*"son" + 0.011*"german" + 0.010*"museum" + 0.009*"artist" + 0.008*"father" + 0.008*"painting" + 0.008*"marry" + 0.007*"daughter"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"country" + 0.006*"found" + 0.005*"public" + 0.005*"provide" + 0.005*"development"
topic #5 (0.050): 0.037*"woman" + 0.016*"hospital" + 0.016*"child" + 0.016*"medical" + 0.014*"health" + 0.009*"patient" + 0.009*"black" + 0.009*"treatment" + 0.007*"medicine" + 0.007*"age"
topic diff=0.042594, rho=0.131494
PROGRESS: pass 7, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.013*"specie" + 0.010*"river" + 0.009*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"water"
topic #10 (0.050): 0.015*"government" + 0.013*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"political" + 0.008*"vote" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.005*"candidate"
topic #13 (0.050): 0.019*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.011*"award" + 0.010*"education" + 0.010*"science" + 0.010*"research" + 0.010*"graduate" + 0.009*"director"
topic #15 (0.050): 0.047*"game" + 0.044*"season" + 0.019*"player" + 0.017*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.009*"point" + 0.009*"basketball" + 0.008*"coach"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.004*"want" + 0.004*"death" + 0.004*"ask"
topic diff=0.040971, rho=0.131494
-8.210 per-word bound, 296.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.050): 0.018*"army" + 0.015*"war" + 0.015*"german" + 0.014*"unit" + 0.014*"military" + 0.012*"force" + 0.012*"aircraft" + 0.012*"division" + 0.012*"command" + 0.011*"operation"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"found" + 0.006*"country" + 0.005*"development" + 0.005*"public" + 0.005*"provide"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #4 (0.050): 0.011*"war" + 0.011*"battle" + 0.010*"road" + 0.010*"force" + 0.009*"route" + 0.009*"army" + 0.007*"attack" + 0.007*"king" + 0.007*"north" + 0.006*"highway"
topic #7 (0.050): 0.019*"church" + 0.019*"art" + 0.012*"son" + 0.011*"german" + 0.009*"museum" + 0.009*"artist" + 0.008*"father" + 0.008*"daughter" + 0.008*"painting" + 0.008*"marry"
topic diff=0.041626, rho=0.131494
-8.215 per-word bound, 297.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #12 (0.050): 0.042*"station" + 0.019*"line" + 0.016*"court" + 0.013*"railway" + 0.009*"law" + 0.009*"train" + 0.009*"operate" + 0.008*"radio" + 0.008*"case" + 0.007*"network"
topic #13 (0.050): 0.020*"student" + 0.016*"college" + 0.016*"university" + 0.014*"study" + 0.011*"education" + 0.010*"award" + 0.010*"science" + 0.010*"research" + 0.009*"graduate" + 0.009*"program"
topic #10 (0.050): 0.015*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.005*"candidate"
topic #17 (0.050): 0.007*"study" + 0.006*"cell" + 0.006*"increase" + 0.006*"human" + 0.006*"result" + 0.006*"research" + 0.006*"process" + 0.005*"system" + 0.005*"information" + 0.005*"level"
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.017*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"player" + 0.010*"compete"
topic diff=0.041399, rho=0.131494
-8.217 per-word bound, 297.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #7 (0.050): 0.018*"art" + 0.017*"church" + 0.014*"son" + 0.010*"museum" + 0.010*"german" + 0.009*"father" + 0.008*"artist" + 0.008*"marry" + 0.008*"daughter" + 0.007*"death"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.013*"specie" + 0.010*"river" + 0.010*"town" + 0.010*"county" + 0.009*"age" + 0.008*"region" + 0.007*"km" + 0.007*"census"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.009*"track" + 0.008*"tour" + 0.008*"video" + 0.007*"chart"
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.005*"mean" + 0.005*"term" + 0.005*"refer" + 0.005*"text" + 0.005*"theory" + 0.005*"point"
topic #6 (0.050): 0.030*"ship" + 0.023*"island" + 0.010*"sea" + 0.009*"vessel" + 0.009*"port" + 0.008*"coast" + 0.008*"boat" + 0.008*"water" + 0.007*"navy" + 0.007*"crew"
topic diff=0.055111, rho=0.131494
-8.075 per-word bound, 269.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.020*"book" + 0.016*"publish" + 0.011*"series" + 0.009*"award" + 0.008*"novel" + 0.008*"story" + 0.008*"television" + 0.007*"magazine" + 0.007*"writer" + 0.006*"life"
topic #1 (0.050): 0.078*"film" + 0.017*"star" + 0.016*"series" + 0.013*"character" + 0.013*"game" + 0.012*"direct" + 0.010*"movie" + 0.009*"role" + 0.007*"production" + 0.007*"actor"
topic #8 (0.050): 0.034*"album" + 0.032*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"video" + 0.008*"tour" + 0.007*"chart"
topic #18 (0.050): 0.033*"company" + 0.012*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"found" + 0.006*"development" + 0.006*"country" + 0.006*"provide" + 0.005*"purchase"
topic #13 (0.050): 0.019*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.011*"education" + 0.010*"award" + 0.010*"science" + 0.010*"research" + 0.009*"graduate" + 0.009*"program"
topic diff=0.042385, rho=0.130371
PROGRESS: pass 8, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.015*"government" + 0.013*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.005*"president"
topic #16 (0.050): 0.017*"army" + 0.017*"war" + 0.015*"military" + 0.014*"unit" + 0.013*"aircraft" + 0.013*"german" + 0.012*"force" + 0.012*"operation" + 0.011*"command" + 0.010*"division"
topic #9 (0.050): 0.020*"book" + 0.016*"publish" + 0.010*"series" + 0.009*"award" + 0.009*"novel" + 0.008*"television" + 0.008*"story" + 0.007*"writer" + 0.007*"magazine" + 0.006*"life"
topic #2 (0.050): 0.029*"club" + 0.021*"season" + 0.018*"match" + 0.016*"championship" + 0.014*"final" + 0.014*"league" + 0.012*"football" + 0.012*"event" + 0.010*"score" + 0.010*"compete"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"vehicle" + 0.004*"standard"
topic diff=0.046298, rho=0.130371
PROGRESS: pass 8, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.050): 0.015*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.007*"law" + 0.006*"support" + 0.006*"general" + 0.005*"candidate"
topic #13 (0.050): 0.019*"student" + 0.016*"university" + 0.016*"college" + 0.014*"study" + 0.011*"education" + 0.010*"award" + 0.010*"science" + 0.010*"research" + 0.009*"program" + 0.009*"graduate"
topic #1 (0.050): 0.076*"film" + 0.016*"series" + 0.016*"star" + 0.013*"game" + 0.013*"character" + 0.012*"direct" + 0.009*"movie" + 0.009*"role" + 0.007*"production" + 0.006*"story"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"want" + 0.004*"friend"
topic #7 (0.050): 0.018*"art" + 0.016*"church" + 0.013*"son" + 0.010*"museum" + 0.009*"german" + 0.009*"father" + 0.009*"artist" + 0.008*"marry" + 0.008*"daughter" + 0.007*"death"
topic diff=0.041159, rho=0.130371
PROGRESS: pass 8, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.050): 0.049*"game" + 0.044*"season" + 0.021*"player" + 0.016*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.010*"basketball" + 0.009*"point" + 0.008*"coach"
topic #2 (0.050): 0.029*"club" + 0.021*"season" + 0.019*"match" + 0.017*"championship" + 0.015*"final" + 0.014*"league" + 0.012*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"power" + 0.006*"engine" + 0.005*"car" + 0.005*"type" + 0.004*"produce" + 0.004*"vehicle" + 0.004*"standard"
topic #12 (0.050): 0.042*"station" + 0.018*"line" + 0.018*"court" + 0.013*"railway" + 0.009*"law" + 0.009*"train" + 0.009*"operate" + 0.008*"case" + 0.008*"radio" + 0.007*"network"
topic #18 (0.050): 0.033*"company" + 0.012*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"country" + 0.006*"product" + 0.006*"found" + 0.005*"provide" + 0.005*"development"
topic diff=0.043000, rho=0.130371
PROGRESS: pass 8, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.028*"ship" + 0.027*"island" + 0.010*"sea" + 0.009*"port" + 0.009*"boat" + 0.008*"coast" + 0.008*"water" + 0.008*"vessel" + 0.007*"crew" + 0.006*"navy"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"standard"
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #10 (0.050): 0.016*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"vote" + 0.008*"political" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.005*"seat"
topic #4 (0.050): 0.012*"war" + 0.011*"battle" + 0.010*"route" + 0.010*"road" + 0.010*"force" + 0.009*"army" + 0.007*"highway" + 0.007*"king" + 0.006*"north" + 0.006*"attack"
topic diff=0.040255, rho=0.130371
PROGRESS: pass 8, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.011*"site" + 0.010*"park" + 0.009*"street" + 0.008*"design" + 0.007*"church" + 0.006*"town" + 0.006*"stone" + 0.006*"construction"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.008*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"want" + 0.005*"death" + 0.004*"ask"
topic #17 (0.050): 0.006*"study" + 0.006*"increase" + 0.006*"process" + 0.006*"information" + 0.006*"human" + 0.006*"result" + 0.006*"research" + 0.005*"cell" + 0.005*"datum" + 0.005*"system"
topic #0 (0.050): 0.009*"displaystyle" + 0.009*"language" + 0.007*"example" + 0.006*"word" + 0.005*"term" + 0.005*"text" + 0.005*"mean" + 0.005*"refer" + 0.005*"theory" + 0.005*"point"
topic #10 (0.050): 0.016*"government" + 0.013*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"political" + 0.008*"vote" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.005*"candidate"
topic diff=0.045891, rho=0.130371
PROGRESS: pass 8, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.006*"word" + 0.005*"term" + 0.005*"mean" + 0.005*"text" + 0.005*"refer" + 0.005*"theory" + 0.005*"point"
topic #17 (0.050): 0.006*"study" + 0.006*"increase" + 0.006*"process" + 0.006*"human" + 0.006*"cell" + 0.006*"information" + 0.006*"result" + 0.005*"research" + 0.005*"system" + 0.005*"datum"
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.015*"final" + 0.013*"football" + 0.011*"event" + 0.010*"player" + 0.010*"compete"
topic #6 (0.050): 0.029*"ship" + 0.025*"island" + 0.010*"sea" + 0.009*"boat" + 0.009*"port" + 0.008*"water" + 0.008*"coast" + 0.007*"vessel" + 0.007*"crew" + 0.007*"navy"
topic #10 (0.050): 0.016*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"political" + 0.008*"vote" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.006*"seat"
topic diff=0.038416, rho=0.130371
PROGRESS: pass 8, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.017*"championship" + 0.017*"match" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #10 (0.050): 0.016*"government" + 0.014*"election" + 0.012*"party" + 0.009*"elect" + 0.008*"political" + 0.008*"vote" + 0.006*"law" + 0.006*"support" + 0.006*"general" + 0.006*"seat"
topic #9 (0.050): 0.021*"book" + 0.016*"publish" + 0.010*"series" + 0.010*"award" + 0.009*"novel" + 0.008*"television" + 0.008*"story" + 0.007*"writer" + 0.007*"magazine" + 0.007*"life"
topic #1 (0.050): 0.074*"film" + 0.018*"series" + 0.016*"star" + 0.014*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"production" + 0.007*"actor"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"type" + 0.005*"vehicle" + 0.005*"produce" + 0.004*"low"
topic diff=0.038562, rho=0.130371
PROGRESS: pass 8, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.050): 0.018*"army" + 0.017*"war" + 0.015*"military" + 0.014*"german" + 0.014*"unit" + 0.013*"aircraft" + 0.013*"force" + 0.012*"operation" + 0.011*"command" + 0.010*"air"
topic #8 (0.050): 0.033*"album" + 0.032*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.017*"championship" + 0.017*"match" + 0.015*"league" + 0.015*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #1 (0.050): 0.075*"film" + 0.018*"series" + 0.016*"star" + 0.014*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"production" + 0.007*"actor"
topic #13 (0.050): 0.020*"student" + 0.016*"university" + 0.016*"college" + 0.014*"study" + 0.011*"education" + 0.010*"award" + 0.010*"science" + 0.010*"research" + 0.009*"director" + 0.009*"graduate"
topic diff=0.040929, rho=0.130371
PROGRESS: pass 8, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.050): 0.048*"game" + 0.043*"season" + 0.020*"player" + 0.017*"race" + 0.011*"football" + 0.010*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"provide" + 0.006*"purchase" + 0.006*"found" + 0.006*"country" + 0.005*"development"
topic #1 (0.050): 0.074*"film" + 0.018*"series" + 0.016*"star" + 0.015*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"production" + 0.006*"actor"
topic #8 (0.050): 0.034*"album" + 0.032*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.005*"text" + 0.005*"term" + 0.005*"mean" + 0.005*"refer" + 0.005*"theory" + 0.005*"point"
topic diff=0.039524, rho=0.130371
PROGRESS: pass 8, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"type" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"low"
topic #2 (0.050): 0.031*"club" + 0.022*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.015*"final" + 0.014*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #9 (0.050): 0.021*"book" + 0.017*"publish" + 0.010*"series" + 0.010*"award" + 0.009*"novel" + 0.008*"story" + 0.008*"magazine" + 0.008*"television" + 0.007*"writer" + 0.007*"life"
topic #14 (0.050): 0.018*"population" + 0.016*"village" + 0.013*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"north" + 0.007*"water"
topic #1 (0.050): 0.074*"film" + 0.018*"series" + 0.018*"star" + 0.015*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"production" + 0.007*"actor"
topic diff=0.040781, rho=0.130371
PROGRESS: pass 8, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.029*"ship" + 0.025*"island" + 0.011*"sea" + 0.009*"coast" + 0.009*"port" + 0.008*"boat" + 0.008*"water" + 0.008*"crew" + 0.007*"vessel" + 0.007*"fleet"
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.013*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #1 (0.050): 0.075*"film" + 0.018*"series" + 0.017*"star" + 0.015*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"actor" + 0.007*"production"
topic #17 (0.050): 0.007*"study" + 0.006*"increase" + 0.006*"human" + 0.006*"process" + 0.006*"result" + 0.006*"research" + 0.006*"cell" + 0.005*"system" + 0.005*"datum" + 0.005*"effect"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.012*"site" + 0.010*"street" + 0.010*"park" + 0.009*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.006*"main"
topic diff=0.043121, rho=0.130371
PROGRESS: pass 8, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.050): 0.021*"book" + 0.017*"publish" + 0.010*"series" + 0.009*"award" + 0.008*"novel" + 0.008*"story" + 0.008*"magazine" + 0.007*"television" + 0.007*"writer" + 0.007*"life"
topic #8 (0.050): 0.035*"album" + 0.033*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #15 (0.050): 0.046*"game" + 0.044*"season" + 0.019*"player" + 0.017*"race" + 0.012*"football" + 0.011*"league" + 0.010*"finish" + 0.009*"point" + 0.009*"basketball" + 0.008*"coach"
topic #13 (0.050): 0.020*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.011*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #1 (0.050): 0.076*"film" + 0.017*"star" + 0.017*"series" + 0.014*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"actor" + 0.007*"production"
topic diff=0.037348, rho=0.130371
PROGRESS: pass 8, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.012*"site" + 0.010*"street" + 0.010*"park" + 0.009*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.005*"main"
topic #0 (0.050): 0.010*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.006*"mean" + 0.005*"refer" + 0.005*"text" + 0.005*"point" + 0.005*"theory"
topic #16 (0.050): 0.018*"army" + 0.017*"war" + 0.015*"german" + 0.015*"military" + 0.014*"unit" + 0.012*"command" + 0.012*"aircraft" + 0.012*"force" + 0.011*"operation" + 0.011*"air"
topic #4 (0.050): 0.011*"battle" + 0.011*"war" + 0.010*"force" + 0.010*"road" + 0.009*"route" + 0.009*"army" + 0.007*"king" + 0.007*"attack" + 0.007*"highway" + 0.006*"north"
topic #7 (0.050): 0.019*"church" + 0.018*"art" + 0.013*"son" + 0.011*"german" + 0.009*"museum" + 0.009*"father" + 0.009*"artist" + 0.008*"marry" + 0.008*"daughter" + 0.007*"death"
topic diff=0.037432, rho=0.130371
-8.177 per-word bound, 289.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.020*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.011*"education" + 0.011*"award" + 0.010*"science" + 0.010*"research" + 0.009*"graduate" + 0.009*"program"
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.012*"site" + 0.010*"street" + 0.010*"park" + 0.009*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.006*"side"
topic #6 (0.050): 0.028*"ship" + 0.025*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"coast" + 0.009*"water" + 0.008*"boat" + 0.008*"vessel" + 0.007*"crew" + 0.007*"fleet"
topic #12 (0.050): 0.042*"station" + 0.019*"line" + 0.016*"court" + 0.012*"railway" + 0.009*"operate" + 0.009*"train" + 0.009*"radio" + 0.009*"law" + 0.008*"case" + 0.007*"network"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"country" + 0.006*"found" + 0.005*"provide" + 0.005*"development" + 0.005*"product"
topic diff=0.038225, rho=0.130371
-8.183 per-word bound, 290.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.017*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #18 (0.050): 0.033*"company" + 0.012*"business" + 0.008*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"country" + 0.006*"found" + 0.005*"provide" + 0.005*"development" + 0.005*"public"
topic #17 (0.050): 0.007*"study" + 0.006*"cell" + 0.006*"increase" + 0.006*"human" + 0.006*"result" + 0.006*"process" + 0.006*"research" + 0.005*"system" + 0.005*"information" + 0.005*"level"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"want" + 0.004*"ask"
topic #15 (0.050): 0.047*"game" + 0.044*"season" + 0.020*"player" + 0.018*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic diff=0.038461, rho=0.130371
-8.186 per-word bound, 291.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #1 (0.050): 0.077*"film" + 0.017*"star" + 0.017*"series" + 0.013*"game" + 0.013*"character" + 0.012*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"actor" + 0.007*"production"
topic #9 (0.050): 0.022*"book" + 0.016*"publish" + 0.010*"series" + 0.009*"award" + 0.009*"novel" + 0.008*"story" + 0.008*"magazine" + 0.007*"writer" + 0.007*"television" + 0.006*"life"
topic #5 (0.050): 0.035*"woman" + 0.017*"hospital" + 0.017*"medical" + 0.016*"child" + 0.015*"health" + 0.010*"patient" + 0.009*"black" + 0.009*"treatment" + 0.007*"female" + 0.007*"medicine"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"power" + 0.006*"engine" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"standard" + 0.004*"allow"
topic #19 (0.050): 0.025*"building" + 0.014*"house" + 0.011*"site" + 0.009*"park" + 0.009*"street" + 0.009*"design" + 0.007*"stone" + 0.006*"church" + 0.006*"town" + 0.006*"construction"
topic diff=0.050975, rho=0.130371
-8.057 per-word bound, 266.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.019*"student" + 0.017*"university" + 0.016*"college" + 0.015*"study" + 0.011*"education" + 0.011*"award" + 0.010*"research" + 0.010*"science" + 0.009*"program" + 0.009*"graduate"
topic #17 (0.050): 0.006*"study" + 0.006*"cell" + 0.006*"increase" + 0.006*"human" + 0.006*"result" + 0.006*"process" + 0.005*"datum" + 0.005*"system" + 0.005*"information" + 0.005*"research"
topic #7 (0.050): 0.017*"art" + 0.017*"church" + 0.014*"son" + 0.010*"german" + 0.010*"museum" + 0.009*"father" + 0.008*"marry" + 0.008*"artist" + 0.008*"daughter" + 0.008*"death"
topic #6 (0.050): 0.029*"ship" + 0.023*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"vessel" + 0.008*"boat" + 0.008*"coast" + 0.008*"water" + 0.007*"crew" + 0.007*"navy"
topic #18 (0.050): 0.033*"company" + 0.012*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"provide" + 0.006*"country" + 0.006*"development" + 0.006*"found" + 0.006*"purchase"
topic diff=0.039469, rho=0.129277
PROGRESS: pass 9, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.004*"type" + 0.004*"vehicle" + 0.004*"standard"
topic #18 (0.050): 0.033*"company" + 0.012*"business" + 0.009*"market" + 0.008*"sell" + 0.007*"industry" + 0.006*"country" + 0.006*"provide" + 0.006*"found" + 0.006*"development" + 0.005*"purchase"
topic #12 (0.050): 0.044*"station" + 0.019*"line" + 0.016*"court" + 0.013*"railway" + 0.009*"train" + 0.009*"operate" + 0.008*"law" + 0.008*"radio" + 0.008*"case" + 0.007*"network"
topic #7 (0.050): 0.017*"art" + 0.017*"church" + 0.014*"son" + 0.010*"german" + 0.010*"museum" + 0.009*"father" + 0.008*"artist" + 0.008*"marry" + 0.008*"daughter" + 0.008*"death"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.005*"want" + 0.004*"ask"
topic diff=0.043527, rho=0.129277
PROGRESS: pass 9, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.050): 0.035*"woman" + 0.018*"hospital" + 0.018*"child" + 0.016*"medical" + 0.015*"health" + 0.011*"patient" + 0.009*"black" + 0.009*"treatment" + 0.008*"female" + 0.007*"medicine"
topic #18 (0.050): 0.034*"company" + 0.012*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"provide" + 0.006*"found" + 0.006*"purchase" + 0.006*"country" + 0.005*"product"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.005*"type" + 0.004*"vehicle" + 0.004*"standard"
topic #1 (0.050): 0.076*"film" + 0.017*"series" + 0.017*"star" + 0.014*"game" + 0.013*"character" + 0.012*"direct" + 0.009*"movie" + 0.009*"role" + 0.007*"production" + 0.007*"actor"
topic #16 (0.050): 0.018*"army" + 0.018*"war" + 0.015*"military" + 0.013*"unit" + 0.013*"aircraft" + 0.013*"force" + 0.012*"operation" + 0.012*"german" + 0.011*"command" + 0.010*"air"
topic diff=0.038188, rho=0.129277
PROGRESS: pass 9, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.029*"ship" + 0.026*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"boat" + 0.008*"water" + 0.008*"coast" + 0.008*"vessel" + 0.008*"crew" + 0.007*"navy"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"model" + 0.006*"power" + 0.006*"engine" + 0.005*"car" + 0.005*"type" + 0.004*"produce" + 0.004*"vehicle" + 0.004*"standard"
topic #16 (0.050): 0.018*"army" + 0.017*"war" + 0.015*"military" + 0.013*"unit" + 0.013*"force" + 0.013*"aircraft" + 0.012*"operation" + 0.012*"german" + 0.011*"command" + 0.010*"air"
topic #2 (0.050): 0.029*"club" + 0.021*"season" + 0.019*"match" + 0.017*"championship" + 0.015*"final" + 0.014*"league" + 0.012*"event" + 0.012*"football" + 0.010*"compete" + 0.010*"player"
topic #0 (0.050): 0.010*"displaystyle" + 0.010*"language" + 0.007*"example" + 0.007*"word" + 0.006*"text" + 0.005*"term" + 0.005*"refer" + 0.005*"mean" + 0.005*"point" + 0.005*"theory"
topic diff=0.040075, rho=0.129277
PROGRESS: pass 9, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.050): 0.011*"war" + 0.010*"battle" + 0.010*"route" + 0.009*"force" + 0.009*"road" + 0.009*"army" + 0.007*"king" + 0.007*"highway" + 0.006*"north" + 0.006*"attack"
topic #8 (0.050): 0.033*"album" + 0.033*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.008*"video" + 0.007*"chart"
topic #1 (0.050): 0.075*"film" + 0.017*"series" + 0.016*"star" + 0.015*"game" + 0.013*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"production" + 0.007*"actor"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"type" + 0.004*"standard"
topic #2 (0.050): 0.030*"club" + 0.021*"season" + 0.018*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.012*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic diff=0.037340, rho=0.129277
PROGRESS: pass 9, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.050): 0.021*"student" + 0.017*"college" + 0.016*"university" + 0.014*"study" + 0.011*"education" + 0.010*"research" + 0.010*"science" + 0.010*"award" + 0.009*"director" + 0.009*"program"
topic #0 (0.050): 0.009*"language" + 0.009*"displaystyle" + 0.007*"example" + 0.006*"word" + 0.005*"term" + 0.005*"text" + 0.005*"mean" + 0.005*"refer" + 0.005*"theory" + 0.005*"point"
topic #14 (0.050): 0.019*"population" + 0.017*"village" + 0.014*"specie" + 0.010*"river" + 0.010*"town" + 0.010*"county" + 0.009*"age" + 0.008*"region" + 0.008*"census" + 0.007*"km"
topic #16 (0.050): 0.018*"army" + 0.017*"war" + 0.015*"military" + 0.014*"unit" + 0.013*"force" + 0.013*"aircraft" + 0.012*"operation" + 0.012*"german" + 0.012*"command" + 0.010*"air"
topic #12 (0.050): 0.042*"station" + 0.019*"line" + 0.016*"court" + 0.013*"railway" + 0.009*"train" + 0.009*"operate" + 0.009*"radio" + 0.008*"law" + 0.008*"case" + 0.007*"network"
topic diff=0.043117, rho=0.129277
PROGRESS: pass 9, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.050): 0.018*"army" + 0.017*"war" + 0.015*"military" + 0.014*"unit" + 0.013*"force" + 0.013*"aircraft" + 0.012*"operation" + 0.012*"german" + 0.011*"command" + 0.010*"air"
topic #13 (0.050): 0.020*"student" + 0.017*"college" + 0.016*"university" + 0.014*"study" + 0.011*"education" + 0.010*"award" + 0.010*"research" + 0.010*"science" + 0.009*"director" + 0.009*"graduate"
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.014*"specie" + 0.011*"river" + 0.010*"town" + 0.010*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"municipality"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"vehicle" + 0.005*"type" + 0.005*"produce" + 0.004*"low"
topic #15 (0.050): 0.048*"game" + 0.045*"season" + 0.020*"player" + 0.017*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.010*"point" + 0.009*"basketball" + 0.008*"coach"
topic diff=0.035594, rho=0.129277
PROGRESS: pass 9, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.029*"ship" + 0.025*"island" + 0.011*"sea" + 0.009*"boat" + 0.009*"port" + 0.008*"water" + 0.008*"coast" + 0.008*"vessel" + 0.008*"crew" + 0.007*"navy"
topic #5 (0.050): 0.038*"woman" + 0.020*"hospital" + 0.018*"medical" + 0.016*"child" + 0.015*"health" + 0.011*"patient" + 0.009*"treatment" + 0.008*"black" + 0.008*"medicine" + 0.007*"age"
topic #15 (0.050): 0.047*"game" + 0.044*"season" + 0.020*"player" + 0.018*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.009*"point" + 0.009*"basketball" + 0.008*"coach"
topic #16 (0.050): 0.018*"army" + 0.018*"war" + 0.015*"military" + 0.014*"unit" + 0.013*"aircraft" + 0.013*"force" + 0.012*"operation" + 0.012*"german" + 0.011*"command" + 0.010*"air"
topic #1 (0.050): 0.074*"film" + 0.019*"series" + 0.017*"star" + 0.014*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.007*"production" + 0.007*"actor"
topic diff=0.035843, rho=0.129277
PROGRESS: pass 9, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.050): 0.029*"ship" + 0.026*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"boat" + 0.009*"water" + 0.008*"coast" + 0.007*"vessel" + 0.007*"crew" + 0.007*"navy"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"provide" + 0.006*"product" + 0.006*"found" + 0.006*"country" + 0.005*"purchase"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #12 (0.050): 0.042*"station" + 0.019*"line" + 0.015*"court" + 0.013*"railway" + 0.010*"train" + 0.010*"radio" + 0.009*"operate" + 0.009*"law" + 0.008*"case" + 0.007*"network"
topic #0 (0.050): 0.009*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"text" + 0.005*"term" + 0.005*"mean" + 0.005*"refer" + 0.005*"point" + 0.005*"theory"
topic diff=0.038313, rho=0.129277
PROGRESS: pass 9, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.014*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #5 (0.050): 0.036*"woman" + 0.019*"hospital" + 0.017*"medical" + 0.016*"child" + 0.014*"health" + 0.011*"patient" + 0.010*"treatment" + 0.008*"black" + 0.007*"care" + 0.007*"medicine"
topic #8 (0.050): 0.034*"album" + 0.033*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #18 (0.050): 0.033*"company" + 0.013*"business" + 0.008*"market" + 0.008*"sell" + 0.008*"industry" + 0.006*"provide" + 0.006*"purchase" + 0.006*"product" + 0.006*"country" + 0.006*"found"
topic #2 (0.050): 0.031*"club" + 0.021*"season" + 0.017*"match" + 0.017*"championship" + 0.015*"league" + 0.014*"final" + 0.013*"football" + 0.012*"event" + 0.010*"player" + 0.010*"compete"
topic diff=0.036786, rho=0.129277
PROGRESS: pass 9, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.050): 0.006*"increase" + 0.006*"study" + 0.006*"human" + 0.006*"process" + 0.006*"result" + 0.006*"cell" + 0.006*"research" + 0.005*"datum" + 0.005*"effect" + 0.005*"protein"
topic #13 (0.050): 0.020*"student" + 0.017*"university" + 0.016*"college" + 0.014*"study" + 0.011*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.010*"director"
topic #2 (0.050): 0.031*"club" + 0.021*"season" + 0.017*"match" + 0.017*"championship" + 0.016*"league" + 0.015*"final" + 0.013*"football" + 0.012*"event" + 0.010*"compete" + 0.010*"player"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"model" + 0.006*"engine" + 0.006*"power" + 0.005*"car" + 0.005*"type" + 0.005*"produce" + 0.004*"vehicle" + 0.004*"control"
topic #19 (0.050): 0.025*"building" + 0.015*"house" + 0.012*"site" + 0.010*"street" + 0.009*"park" + 0.009*"design" + 0.007*"church" + 0.006*"town" + 0.006*"construction" + 0.006*"main"
topic diff=0.041020, rho=0.129277
PROGRESS: pass 9, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.050): 0.021*"church" + 0.018*"art" + 0.013*"german" + 0.013*"son" + 0.009*"father" + 0.009*"museum" + 0.008*"artist" + 0.008*"marry" + 0.008*"daughter" + 0.008*"painting"
topic #5 (0.050): 0.036*"woman" + 0.017*"hospital" + 0.016*"medical" + 0.016*"child" + 0.014*"health" + 0.010*"black" + 0.010*"patient" + 0.009*"treatment" + 0.007*"medicine" + 0.007*"care"
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.014*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #16 (0.050): 0.018*"army" + 0.017*"war" + 0.015*"german" + 0.014*"military" + 0.013*"unit" + 0.013*"aircraft" + 0.013*"force" + 0.012*"command" + 0.011*"air" + 0.011*"operation"
topic #8 (0.050): 0.035*"album" + 0.033*"song" + 0.031*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic diff=0.040695, rho=0.129277
PROGRESS: pass 9, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.006*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"want" + 0.004*"ask" + 0.004*"death"
topic #0 (0.050): 0.010*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"term" + 0.006*"mean" + 0.005*"refer" + 0.005*"text" + 0.005*"theory" + 0.005*"point"
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.014*"specie" + 0.011*"river" + 0.010*"town" + 0.009*"county" + 0.008*"age" + 0.008*"region" + 0.007*"census" + 0.007*"north"
topic #1 (0.050): 0.076*"film" + 0.018*"series" + 0.018*"star" + 0.015*"game" + 0.014*"character" + 0.011*"direct" + 0.010*"role" + 0.009*"movie" + 0.008*"actor" + 0.007*"production"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"type" + 0.005*"produce" + 0.004*"version" + 0.004*"vehicle"
topic diff=0.036746, rho=0.129277
PROGRESS: pass 9, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.007*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"type" + 0.005*"produce" + 0.004*"light" + 0.004*"standard"
topic #8 (0.050): 0.035*"album" + 0.033*"song" + 0.030*"music" + 0.022*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #9 (0.050): 0.022*"book" + 0.017*"publish" + 0.009*"series" + 0.009*"award" + 0.009*"novel" + 0.008*"story" + 0.008*"magazine" + 0.008*"writer" + 0.007*"television" + 0.007*"life"
topic #19 (0.050): 0.024*"building" + 0.015*"house" + 0.012*"site" + 0.010*"street" + 0.010*"park" + 0.009*"design" + 0.008*"church" + 0.006*"town" + 0.006*"construction" + 0.005*"main"
topic #12 (0.050): 0.044*"station" + 0.020*"line" + 0.016*"court" + 0.013*"railway" + 0.010*"train" + 0.009*"operate" + 0.009*"radio" + 0.008*"case" + 0.008*"law" + 0.008*"network"
topic diff=0.034950, rho=0.129277
-8.165 per-word bound, 287.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #5 (0.050): 0.036*"woman" + 0.017*"hospital" + 0.017*"medical" + 0.015*"health" + 0.015*"child" + 0.011*"patient" + 0.009*"treatment" + 0.009*"black" + 0.007*"medicine" + 0.007*"disease"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"want" + 0.004*"ask"
topic #3 (0.050): 0.012*"system" + 0.010*"design" + 0.006*"power" + 0.006*"model" + 0.006*"engine" + 0.005*"car" + 0.005*"type" + 0.004*"produce" + 0.004*"allow" + 0.004*"version"
topic #0 (0.050): 0.010*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"mean" + 0.006*"term" + 0.005*"refer" + 0.005*"text" + 0.005*"point" + 0.005*"theory"
topic #17 (0.050): 0.007*"study" + 0.006*"cell" + 0.006*"increase" + 0.006*"human" + 0.006*"result" + 0.006*"process" + 0.005*"research" + 0.005*"information" + 0.005*"datum" + 0.005*"system"
topic diff=0.033594, rho=0.129277
-8.170 per-word bound, 288.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #0 (0.050): 0.009*"language" + 0.008*"displaystyle" + 0.007*"example" + 0.007*"word" + 0.006*"mean" + 0.006*"term" + 0.005*"refer" + 0.005*"text" + 0.005*"point" + 0.005*"theory"
topic #17 (0.050): 0.007*"study" + 0.007*"cell" + 0.006*"human" + 0.006*"increase" + 0.006*"result" + 0.006*"process" + 0.005*"system" + 0.005*"research" + 0.005*"datum" + 0.005*"level"
topic #11 (0.050): 0.010*"say" + 0.008*"get" + 0.007*"tell" + 0.007*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"help" + 0.005*"death" + 0.004*"want" + 0.004*"ask"
topic #6 (0.050): 0.029*"ship" + 0.024*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"coast" + 0.009*"water" + 0.009*"boat" + 0.008*"vessel" + 0.007*"crew" + 0.007*"fleet"
topic #14 (0.050): 0.018*"population" + 0.017*"village" + 0.014*"specie" + 0.010*"river" + 0.010*"town" + 0.009*"county" + 0.009*"age" + 0.008*"region" + 0.007*"census" + 0.007*"km"
topic diff=0.035200, rho=0.129277
-8.138 per-word bound, 281.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=26262, num_topics=20, decay=0.5, chunksize=1000) in 828.75s', 'datetime': '2022-02-06T23:30:08.270937', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows
1 batches submitted to accumulate stats from 64 documents (2187 virtual)
2 batches submitted to accumulate stats from 128 documents (6627 virtual)
3 batches submitted to accumulate stats from 192 documents (7982 virtual)
5 batches submitted to accumulate stats from 320 documents (8972 virtual)
6 batches submitted to accumulate stats from 384 documents (13088 virtual)
9 batches submitted to accumulate stats from 576 documents (14288 virtual)
10 batches submitted to accumulate stats from 640 documents (16741 virtual)
11 batches submitted to accumulate stats from 704 documents (17170 virtual)
12 batches submitted to accumulate stats from 768 documents (22749 virtual)
13 batches submitted to accumulate stats from 832 documents (24698 virtual)
14 batches submitted to accumulate stats from 896 documents (24856 virtual)
16 batches submitted to accumulate stats from 1024 documents (26302 virtual)
17 batches submitted to accumulate stats from 1088 documents (27406 virtual)
18 batches submitted to accumulate stats from 1152 documents (31446 virtual)
19 batches submitted to accumulate stats from 1216 documents (31735 virtual)
20 batches submitted to accumulate stats from 1280 documents (34984 virtual)
21 batches submitted to accumulate stats from 1344 documents (37962 virtual)
22 batches submitted to accumulate stats from 1408 documents (38126 virtual)
23 batches submitted to accumulate stats from 1472 documents (42074 virtual)
24 batches submitted to accumulate stats from 1536 documents (44337 virtual)
25 batches submitted to accumulate stats from 1600 documents (46548 virtual)
27 batches submitted to accumulate stats from 1728 documents (46838 virtual)
28 batches submitted to accumulate stats from 1792 documents (58590 virtual)
30 batches submitted to accumulate stats from 1920 documents (58368 virtual)
31 batches submitted to accumulate stats from 1984 documents (59904 virtual)
33 batches submitted to accumulate stats from 2112 documents (59954 virtual)
34 batches submitted to accumulate stats from 2176 documents (60644 virtual)
35 batches submitted to accumulate stats from 2240 documents (65178 virtual)
36 batches submitted to accumulate stats from 2304 documents (67820 virtual)
39 batches submitted to accumulate stats from 2496 documents (71983 virtual)
40 batches submitted to accumulate stats from 2560 documents (73508 virtual)
42 batches submitted to accumulate stats from 2688 documents (79375 virtual)
45 batches submitted to accumulate stats from 2880 documents (82012 virtual)
46 batches submitted to accumulate stats from 2944 documents (85876 virtual)
47 batches submitted to accumulate stats from 3008 documents (90295 virtual)
50 batches submitted to accumulate stats from 3200 documents (91396 virtual)
51 batches submitted to accumulate stats from 3264 documents (95179 virtual)
52 batches submitted to accumulate stats from 3328 documents (95682 virtual)
53 batches submitted to accumulate stats from 3392 documents (97380 virtual)
54 batches submitted to accumulate stats from 3456 documents (101889 virtual)
55 batches submitted to accumulate stats from 3520 documents (104670 virtual)
56 batches submitted to accumulate stats from 3584 documents (107983 virtual)
57 batches submitted to accumulate stats from 3648 documents (111368 virtual)
58 batches submitted to accumulate stats from 3712 documents (112325 virtual)
60 batches submitted to accumulate stats from 3840 documents (112885 virtual)
61 batches submitted to accumulate stats from 3904 documents (114240 virtual)
63 batches submitted to accumulate stats from 4032 documents (115168 virtual)
64 batches submitted to accumulate stats from 4096 documents (116789 virtual)
67 batches submitted to accumulate stats from 4288 documents (118225 virtual)
68 batches submitted to accumulate stats from 4352 documents (131006 virtual)
69 batches submitted to accumulate stats from 4416 documents (131813 virtual)
70 batches submitted to accumulate stats from 4480 documents (136435 virtual)
71 batches submitted to accumulate stats from 4544 documents (138508 virtual)
72 batches submitted to accumulate stats from 4608 documents (139643 virtual)
75 batches submitted to accumulate stats from 4800 documents (142809 virtual)
76 batches submitted to accumulate stats from 4864 documents (146339 virtual)
77 batches submitted to accumulate stats from 4928 documents (148272 virtual)
78 batches submitted to accumulate stats from 4992 documents (149685 virtual)
79 batches submitted to accumulate stats from 5056 documents (150380 virtual)
80 batches submitted to accumulate stats from 5120 documents (152127 virtual)
81 batches submitted to accumulate stats from 5184 documents (152139 virtual)
83 batches submitted to accumulate stats from 5312 documents (152075 virtual)
84 batches submitted to accumulate stats from 5376 documents (152646 virtual)
85 batches submitted to accumulate stats from 5440 documents (155137 virtual)
86 batches submitted to accumulate stats from 5504 documents (155462 virtual)
88 batches submitted to accumulate stats from 5632 documents (156950 virtual)
89 batches submitted to accumulate stats from 5696 documents (159812 virtual)
90 batches submitted to accumulate stats from 5760 documents (161342 virtual)
91 batches submitted to accumulate stats from 5824 documents (161640 virtual)
92 batches submitted to accumulate stats from 5888 documents (166102 virtual)
93 batches submitted to accumulate stats from 5952 documents (169977 virtual)
94 batches submitted to accumulate stats from 6016 documents (171299 virtual)
95 batches submitted to accumulate stats from 6080 documents (171404 virtual)
96 batches submitted to accumulate stats from 6144 documents (173402 virtual)
97 batches submitted to accumulate stats from 6208 documents (177633 virtual)
98 batches submitted to accumulate stats from 6272 documents (180194 virtual)
101 batches submitted to accumulate stats from 6464 documents (183875 virtual)
104 batches submitted to accumulate stats from 6656 documents (183666 virtual)
105 batches submitted to accumulate stats from 6720 documents (184997 virtual)
106 batches submitted to accumulate stats from 6784 documents (187051 virtual)
107 batches submitted to accumulate stats from 6848 documents (187680 virtual)
108 batches submitted to accumulate stats from 6912 documents (189683 virtual)
110 batches submitted to accumulate stats from 7040 documents (187640 virtual)
111 batches submitted to accumulate stats from 7104 documents (190704 virtual)
112 batches submitted to accumulate stats from 7168 documents (191560 virtual)
114 batches submitted to accumulate stats from 7296 documents (195498 virtual)
115 batches submitted to accumulate stats from 7360 documents (198615 virtual)
117 batches submitted to accumulate stats from 7488 documents (201060 virtual)
118 batches submitted to accumulate stats from 7552 documents (202777 virtual)
119 batches submitted to accumulate stats from 7616 documents (203793 virtual)
121 batches submitted to accumulate stats from 7744 documents (203611 virtual)
122 batches submitted to accumulate stats from 7808 documents (203709 virtual)
123 batches submitted to accumulate stats from 7872 documents (206077 virtual)
124 batches submitted to accumulate stats from 7936 documents (207170 virtual)
125 batches submitted to accumulate stats from 8000 documents (209365 virtual)
127 batches submitted to accumulate stats from 8128 documents (213343 virtual)
128 batches submitted to accumulate stats from 8192 documents (215777 virtual)
129 batches submitted to accumulate stats from 8256 documents (219605 virtual)
132 batches submitted to accumulate stats from 8448 documents (218848 virtual)
133 batches submitted to accumulate stats from 8512 documents (222674 virtual)
134 batches submitted to accumulate stats from 8576 documents (222902 virtual)
137 batches submitted to accumulate stats from 8768 documents (226874 virtual)
138 batches submitted to accumulate stats from 8832 documents (233672 virtual)
139 batches submitted to accumulate stats from 8896 documents (235076 virtual)
140 batches submitted to accumulate stats from 8960 documents (237440 virtual)
141 batches submitted to accumulate stats from 9024 documents (237939 virtual)
142 batches submitted to accumulate stats from 9088 documents (244459 virtual)
143 batches submitted to accumulate stats from 9152 documents (250059 virtual)
145 batches submitted to accumulate stats from 9280 documents (249815 virtual)
147 batches submitted to accumulate stats from 9408 documents (254154 virtual)
148 batches submitted to accumulate stats from 9472 documents (255361 virtual)
149 batches submitted to accumulate stats from 9536 documents (257710 virtual)
150 batches submitted to accumulate stats from 9600 documents (259312 virtual)
152 batches submitted to accumulate stats from 9728 documents (263378 virtual)
154 batches submitted to accumulate stats from 9856 documents (264142 virtual)
155 batches submitted to accumulate stats from 9920 documents (270079 virtual)
158 batches submitted to accumulate stats from 10112 documents (269744 virtual)
159 batches submitted to accumulate stats from 10176 documents (271391 virtual)
160 batches submitted to accumulate stats from 10240 documents (271435 virtual)
161 batches submitted to accumulate stats from 10304 documents (277806 virtual)
163 batches submitted to accumulate stats from 10432 documents (277805 virtual)
166 batches submitted to accumulate stats from 10624 documents (277711 virtual)
167 batches submitted to accumulate stats from 10688 documents (277859 virtual)
169 batches submitted to accumulate stats from 10816 documents (281485 virtual)
170 batches submitted to accumulate stats from 10880 documents (282907 virtual)
171 batches submitted to accumulate stats from 10944 documents (284584 virtual)
172 batches submitted to accumulate stats from 11008 documents (291090 virtual)
173 batches submitted to accumulate stats from 11072 documents (292892 virtual)
174 batches submitted to accumulate stats from 11136 documents (295802 virtual)
175 batches submitted to accumulate stats from 11200 documents (296317 virtual)
176 batches submitted to accumulate stats from 11264 documents (300837 virtual)
177 batches submitted to accumulate stats from 11328 documents (303005 virtual)
178 batches submitted to accumulate stats from 11392 documents (307751 virtual)
179 batches submitted to accumulate stats from 11456 documents (309012 virtual)
182 batches submitted to accumulate stats from 11648 documents (309155 virtual)
183 batches submitted to accumulate stats from 11712 documents (310479 virtual)
184 batches submitted to accumulate stats from 11776 documents (313625 virtual)
187 batches submitted to accumulate stats from 11968 documents (313655 virtual)
189 batches submitted to accumulate stats from 12096 documents (315084 virtual)
190 batches submitted to accumulate stats from 12160 documents (317090 virtual)
191 batches submitted to accumulate stats from 12224 documents (319933 virtual)
192 batches submitted to accumulate stats from 12288 documents (323128 virtual)
194 batches submitted to accumulate stats from 12416 documents (325439 virtual)
195 batches submitted to accumulate stats from 12480 documents (331095 virtual)
197 batches submitted to accumulate stats from 12608 documents (337111 virtual)
199 batches submitted to accumulate stats from 12736 documents (336114 virtual)
200 batches submitted to accumulate stats from 12800 documents (336390 virtual)
201 batches submitted to accumulate stats from 12864 documents (337872 virtual)
203 batches submitted to accumulate stats from 12992 documents (338578 virtual)
205 batches submitted to accumulate stats from 13120 documents (338232 virtual)
206 batches submitted to accumulate stats from 13184 documents (342016 virtual)
208 batches submitted to accumulate stats from 13312 documents (343988 virtual)
209 batches submitted to accumulate stats from 13376 documents (344066 virtual)
210 batches submitted to accumulate stats from 13440 documents (348777 virtual)
211 batches submitted to accumulate stats from 13504 documents (349811 virtual)
212 batches submitted to accumulate stats from 13568 documents (352985 virtual)
213 batches submitted to accumulate stats from 13632 documents (356410 virtual)
214 batches submitted to accumulate stats from 13696 documents (360238 virtual)
215 batches submitted to accumulate stats from 13760 documents (361149 virtual)
216 batches submitted to accumulate stats from 13824 documents (361969 virtual)
217 batches submitted to accumulate stats from 13888 documents (366646 virtual)
218 batches submitted to accumulate stats from 13952 documents (367476 virtual)
219 batches submitted to accumulate stats from 14016 documents (371295 virtual)
222 batches submitted to accumulate stats from 14208 documents (376064 virtual)
224 batches submitted to accumulate stats from 14336 documents (376643 virtual)
225 batches submitted to accumulate stats from 14400 documents (378998 virtual)
227 batches submitted to accumulate stats from 14528 documents (378823 virtual)
228 batches submitted to accumulate stats from 14592 documents (382427 virtual)
229 batches submitted to accumulate stats from 14656 documents (385388 virtual)
230 batches submitted to accumulate stats from 14720 documents (386244 virtual)
231 batches submitted to accumulate stats from 14784 documents (388929 virtual)
232 batches submitted to accumulate stats from 14848 documents (389068 virtual)
233 batches submitted to accumulate stats from 14912 documents (389321 virtual)
235 batches submitted to accumulate stats from 15040 documents (389446 virtual)
236 batches submitted to accumulate stats from 15104 documents (391924 virtual)
237 batches submitted to accumulate stats from 15168 documents (392854 virtual)
238 batches submitted to accumulate stats from 15232 documents (393928 virtual)
239 batches submitted to accumulate stats from 15296 documents (394073 virtual)
240 batches submitted to accumulate stats from 15360 documents (395275 virtual)
241 batches submitted to accumulate stats from 15424 documents (395512 virtual)
242 batches submitted to accumulate stats from 15488 documents (396097 virtual)
243 batches submitted to accumulate stats from 15552 documents (400569 virtual)
244 batches submitted to accumulate stats from 15616 documents (401433 virtual)
245 batches submitted to accumulate stats from 15680 documents (405791 virtual)
246 batches submitted to accumulate stats from 15744 documents (405800 virtual)
247 batches submitted to accumulate stats from 15808 documents (406163 virtual)
248 batches submitted to accumulate stats from 15872 documents (409336 virtual)
249 batches submitted to accumulate stats from 15936 documents (413796 virtual)
250 batches submitted to accumulate stats from 16000 documents (423157 virtual)
251 batches submitted to accumulate stats from 16064 documents (425579 virtual)
252 batches submitted to accumulate stats from 16128 documents (426791 virtual)
253 batches submitted to accumulate stats from 16192 documents (438549 virtual)
254 batches submitted to accumulate stats from 16256 documents (444172 virtual)
255 batches submitted to accumulate stats from 16320 documents (447699 virtual)
256 batches submitted to accumulate stats from 16384 documents (449207 virtual)
257 batches submitted to accumulate stats from 16448 documents (452731 virtual)
258 batches submitted to accumulate stats from 16512 documents (453067 virtual)
259 batches submitted to accumulate stats from 16576 documents (454268 virtual)
260 batches submitted to accumulate stats from 16640 documents (455076 virtual)
261 batches submitted to accumulate stats from 16704 documents (457810 virtual)
262 batches submitted to accumulate stats from 16768 documents (461550 virtual)
263 batches submitted to accumulate stats from 16832 documents (462367 virtual)
264 batches submitted to accumulate stats from 16896 documents (464157 virtual)
265 batches submitted to accumulate stats from 16960 documents (470178 virtual)
266 batches submitted to accumulate stats from 17024 documents (470964 virtual)
267 batches submitted to accumulate stats from 17088 documents (472456 virtual)
268 batches submitted to accumulate stats from 17152 documents (482001 virtual)
269 batches submitted to accumulate stats from 17216 documents (482857 virtual)
270 batches submitted to accumulate stats from 17280 documents (483977 virtual)
271 batches submitted to accumulate stats from 17344 documents (484304 virtual)
272 batches submitted to accumulate stats from 17408 documents (488605 virtual)
273 batches submitted to accumulate stats from 17472 documents (492561 virtual)
275 batches submitted to accumulate stats from 17600 documents (493318 virtual)
277 batches submitted to accumulate stats from 17728 documents (493359 virtual)
281 batches submitted to accumulate stats from 17984 documents (491751 virtual)
282 batches submitted to accumulate stats from 18048 documents (492835 virtual)
283 batches submitted to accumulate stats from 18112 documents (495416 virtual)
284 batches submitted to accumulate stats from 18176 documents (497367 virtual)
285 batches submitted to accumulate stats from 18240 documents (501423 virtual)
286 batches submitted to accumulate stats from 18304 documents (502989 virtual)
287 batches submitted to accumulate stats from 18368 documents (503521 virtual)
288 batches submitted to accumulate stats from 18432 documents (505754 virtual)
290 batches submitted to accumulate stats from 18560 documents (507061 virtual)
291 batches submitted to accumulate stats from 18624 documents (507738 virtual)
292 batches submitted to accumulate stats from 18688 documents (508679 virtual)
293 batches submitted to accumulate stats from 18752 documents (508879 virtual)
295 batches submitted to accumulate stats from 18880 documents (508666 virtual)
296 batches submitted to accumulate stats from 18944 documents (511061 virtual)
297 batches submitted to accumulate stats from 19008 documents (513248 virtual)
298 batches submitted to accumulate stats from 19072 documents (517190 virtual)
299 batches submitted to accumulate stats from 19136 documents (519626 virtual)
300 batches submitted to accumulate stats from 19200 documents (519728 virtual)
301 batches submitted to accumulate stats from 19264 documents (527131 virtual)
302 batches submitted to accumulate stats from 19328 documents (530753 virtual)
307 batches submitted to accumulate stats from 19648 documents (529213 virtual)
308 batches submitted to accumulate stats from 19712 documents (531909 virtual)
309 batches submitted to accumulate stats from 19776 documents (532471 virtual)
310 batches submitted to accumulate stats from 19840 documents (533970 virtual)
311 batches submitted to accumulate stats from 19904 documents (534997 virtual)
312 batches submitted to accumulate stats from 19968 documents (536596 virtual)
313 batches submitted to accumulate stats from 20032 documents (546834 virtual)
314 batches submitted to accumulate stats from 20096 documents (549500 virtual)
315 batches submitted to accumulate stats from 20160 documents (550319 virtual)
316 batches submitted to accumulate stats from 20224 documents (559806 virtual)
318 batches submitted to accumulate stats from 20352 documents (559870 virtual)
319 batches submitted to accumulate stats from 20416 documents (561406 virtual)
320 batches submitted to accumulate stats from 20480 documents (562267 virtual)
321 batches submitted to accumulate stats from 20544 documents (565342 virtual)
322 batches submitted to accumulate stats from 20608 documents (567785 virtual)
323 batches submitted to accumulate stats from 20672 documents (569626 virtual)
324 batches submitted to accumulate stats from 20736 documents (570872 virtual)
327 batches submitted to accumulate stats from 20928 documents (572105 virtual)
328 batches submitted to accumulate stats from 20992 documents (572305 virtual)
329 batches submitted to accumulate stats from 21056 documents (573199 virtual)
330 batches submitted to accumulate stats from 21120 documents (576617 virtual)
331 batches submitted to accumulate stats from 21184 documents (577959 virtual)
332 batches submitted to accumulate stats from 21248 documents (582622 virtual)
333 batches submitted to accumulate stats from 21312 documents (586340 virtual)
335 batches submitted to accumulate stats from 21440 documents (587727 virtual)
338 batches submitted to accumulate stats from 21632 documents (586554 virtual)
339 batches submitted to accumulate stats from 21696 documents (591113 virtual)
344 batches submitted to accumulate stats from 22016 documents (594467 virtual)
345 batches submitted to accumulate stats from 22080 documents (596206 virtual)
346 batches submitted to accumulate stats from 22144 documents (597483 virtual)
347 batches submitted to accumulate stats from 22208 documents (599536 virtual)
348 batches submitted to accumulate stats from 22272 documents (601567 virtual)
349 batches submitted to accumulate stats from 22336 documents (604743 virtual)
350 batches submitted to accumulate stats from 22400 documents (605750 virtual)
351 batches submitted to accumulate stats from 22464 documents (607083 virtual)
352 batches submitted to accumulate stats from 22528 documents (609351 virtual)
353 batches submitted to accumulate stats from 22592 documents (610319 virtual)
355 batches submitted to accumulate stats from 22720 documents (614153 virtual)
356 batches submitted to accumulate stats from 22784 documents (620851 virtual)
357 batches submitted to accumulate stats from 22848 documents (621306 virtual)
359 batches submitted to accumulate stats from 22976 documents (620311 virtual)
360 batches submitted to accumulate stats from 23040 documents (621651 virtual)
361 batches submitted to accumulate stats from 23104 documents (624347 virtual)
362 batches submitted to accumulate stats from 23168 documents (625109 virtual)
364 batches submitted to accumulate stats from 23296 documents (626603 virtual)
365 batches submitted to accumulate stats from 23360 documents (627996 virtual)
366 batches submitted to accumulate stats from 23424 documents (627997 virtual)
367 batches submitted to accumulate stats from 23488 documents (630182 virtual)
368 batches submitted to accumulate stats from 23552 documents (631684 virtual)
369 batches submitted to accumulate stats from 23616 documents (632317 virtual)
371 batches submitted to accumulate stats from 23744 documents (633014 virtual)
372 batches submitted to accumulate stats from 23808 documents (633118 virtual)
373 batches submitted to accumulate stats from 23872 documents (637807 virtual)
374 batches submitted to accumulate stats from 23936 documents (638034 virtual)
375 batches submitted to accumulate stats from 24000 documents (643387 virtual)
376 batches submitted to accumulate stats from 24064 documents (644266 virtual)
377 batches submitted to accumulate stats from 24128 documents (647499 virtual)
378 batches submitted to accumulate stats from 24192 documents (648584 virtual)
379 batches submitted to accumulate stats from 24256 documents (656185 virtual)
380 batches submitted to accumulate stats from 24320 documents (659325 virtual)
381 batches submitted to accumulate stats from 24384 documents (660237 virtual)
382 batches submitted to accumulate stats from 24448 documents (661006 virtual)
383 batches submitted to accumulate stats from 24512 documents (663396 virtual)
385 batches submitted to accumulate stats from 24640 documents (671097 virtual)
386 batches submitted to accumulate stats from 24704 documents (676448 virtual)
387 batches submitted to accumulate stats from 24768 documents (678716 virtual)
388 batches submitted to accumulate stats from 24832 documents (681708 virtual)
389 batches submitted to accumulate stats from 24896 documents (683208 virtual)
391 batches submitted to accumulate stats from 25024 documents (681674 virtual)
393 batches submitted to accumulate stats from 25152 documents (682422 virtual)
394 batches submitted to accumulate stats from 25216 documents (686259 virtual)
395 batches submitted to accumulate stats from 25280 documents (687568 virtual)
397 batches submitted to accumulate stats from 25408 documents (688997 virtual)
398 batches submitted to accumulate stats from 25472 documents (690776 virtual)
400 batches submitted to accumulate stats from 25600 documents (689937 virtual)
402 batches submitted to accumulate stats from 25728 documents (692673 virtual)
404 batches submitted to accumulate stats from 25856 documents (693509 virtual)
405 batches submitted to accumulate stats from 25920 documents (694000 virtual)
406 batches submitted to accumulate stats from 25984 documents (695183 virtual)
408 batches submitted to accumulate stats from 26112 documents (696786 virtual)
410 batches submitted to accumulate stats from 26240 documents (697461 virtual)
411 batches submitted to accumulate stats from 26304 documents (699696 virtual)
412 batches submitted to accumulate stats from 26368 documents (705613 virtual)
414 batches submitted to accumulate stats from 26496 documents (706754 virtual)
415 batches submitted to accumulate stats from 26560 documents (717518 virtual)
416 batches submitted to accumulate stats from 26624 documents (722102 virtual)
418 batches submitted to accumulate stats from 26752 documents (723176 virtual)
419 batches submitted to accumulate stats from 26816 documents (724346 virtual)
420 batches submitted to accumulate stats from 26880 documents (727108 virtual)
421 batches submitted to accumulate stats from 26944 documents (727997 virtual)
422 batches submitted to accumulate stats from 27008 documents (729595 virtual)
424 batches submitted to accumulate stats from 27136 documents (733386 virtual)
425 batches submitted to accumulate stats from 27200 documents (735322 virtual)
426 batches submitted to accumulate stats from 27264 documents (738813 virtual)
427 batches submitted to accumulate stats from 27328 documents (739541 virtual)
430 batches submitted to accumulate stats from 27520 documents (738293 virtual)
431 batches submitted to accumulate stats from 27584 documents (738639 virtual)
432 batches submitted to accumulate stats from 27648 documents (740950 virtual)
433 batches submitted to accumulate stats from 27712 documents (741571 virtual)
434 batches submitted to accumulate stats from 27776 documents (742215 virtual)
435 batches submitted to accumulate stats from 27840 documents (742244 virtual)
436 batches submitted to accumulate stats from 27904 documents (743453 virtual)
437 batches submitted to accumulate stats from 27968 documents (744892 virtual)
438 batches submitted to accumulate stats from 28032 documents (750357 virtual)
439 batches submitted to accumulate stats from 28096 documents (752925 virtual)
440 batches submitted to accumulate stats from 28160 documents (755231 virtual)
441 batches submitted to accumulate stats from 28224 documents (756790 virtual)
443 batches submitted to accumulate stats from 28352 documents (755965 virtual)
444 batches submitted to accumulate stats from 28416 documents (757428 virtual)
445 batches submitted to accumulate stats from 28480 documents (760393 virtual)
447 batches submitted to accumulate stats from 28608 documents (759950 virtual)
448 batches submitted to accumulate stats from 28672 documents (764524 virtual)
449 batches submitted to accumulate stats from 28736 documents (766155 virtual)
450 batches submitted to accumulate stats from 28800 documents (768301 virtual)
451 batches submitted to accumulate stats from 28864 documents (773525 virtual)
452 batches submitted to accumulate stats from 28928 documents (776328 virtual)
453 batches submitted to accumulate stats from 28992 documents (779630 virtual)
455 batches submitted to accumulate stats from 29120 documents (785170 virtual)
456 batches submitted to accumulate stats from 29184 documents (791569 virtual)
457 batches submitted to accumulate stats from 29248 documents (793442 virtual)
459 batches submitted to accumulate stats from 29376 documents (796176 virtual)
460 batches submitted to accumulate stats from 29440 documents (800803 virtual)
461 batches submitted to accumulate stats from 29504 documents (803566 virtual)
462 batches submitted to accumulate stats from 29568 documents (812280 virtual)
463 batches submitted to accumulate stats from 29632 documents (813667 virtual)
464 batches submitted to accumulate stats from 29696 documents (814851 virtual)
468 batches submitted to accumulate stats from 29952 documents (814288 virtual)
470 batches submitted to accumulate stats from 30080 documents (814078 virtual)
471 batches submitted to accumulate stats from 30144 documents (818616 virtual)
472 batches submitted to accumulate stats from 30208 documents (819091 virtual)
474 batches submitted to accumulate stats from 30336 documents (820474 virtual)
475 batches submitted to accumulate stats from 30400 documents (821094 virtual)
476 batches submitted to accumulate stats from 30464 documents (828361 virtual)
477 batches submitted to accumulate stats from 30528 documents (829930 virtual)
478 batches submitted to accumulate stats from 30592 documents (833335 virtual)
479 batches submitted to accumulate stats from 30656 documents (837188 virtual)
481 batches submitted to accumulate stats from 30784 documents (836635 virtual)
482 batches submitted to accumulate stats from 30848 documents (847904 virtual)
483 batches submitted to accumulate stats from 30912 documents (851697 virtual)
485 batches submitted to accumulate stats from 31040 documents (852287 virtual)
486 batches submitted to accumulate stats from 31104 documents (855211 virtual)
487 batches submitted to accumulate stats from 31168 documents (858956 virtual)
488 batches submitted to accumulate stats from 31232 documents (859365 virtual)
489 batches submitted to accumulate stats from 31296 documents (864685 virtual)
490 batches submitted to accumulate stats from 31360 documents (866630 virtual)
491 batches submitted to accumulate stats from 31424 documents (867015 virtual)
492 batches submitted to accumulate stats from 31488 documents (867602 virtual)
493 batches submitted to accumulate stats from 31552 documents (871439 virtual)
494 batches submitted to accumulate stats from 31616 documents (877468 virtual)
495 batches submitted to accumulate stats from 31680 documents (878062 virtual)
496 batches submitted to accumulate stats from 31744 documents (878269 virtual)
497 batches submitted to accumulate stats from 31808 documents (878977 virtual)
498 batches submitted to accumulate stats from 31872 documents (883952 virtual)
499 batches submitted to accumulate stats from 31936 documents (894669 virtual)
500 batches submitted to accumulate stats from 32000 documents (898721 virtual)
502 batches submitted to accumulate stats from 32128 documents (901773 virtual)
503 batches submitted to accumulate stats from 32192 documents (901816 virtual)
504 batches submitted to accumulate stats from 32256 documents (902614 virtual)
505 batches submitted to accumulate stats from 32320 documents (903284 virtual)
506 batches submitted to accumulate stats from 32384 documents (906089 virtual)
507 batches submitted to accumulate stats from 32448 documents (908585 virtual)
508 batches submitted to accumulate stats from 32512 documents (908810 virtual)
509 batches submitted to accumulate stats from 32576 documents (912186 virtual)
510 batches submitted to accumulate stats from 32640 documents (913824 virtual)
511 batches submitted to accumulate stats from 32704 documents (916133 virtual)
512 batches submitted to accumulate stats from 32768 documents (917837 virtual)
515 batches submitted to accumulate stats from 32960 documents (919872 virtual)
517 batches submitted to accumulate stats from 33088 documents (925970 virtual)
519 batches submitted to accumulate stats from 33216 documents (923827 virtual)
520 batches submitted to accumulate stats from 33280 documents (924193 virtual)
521 batches submitted to accumulate stats from 33344 documents (934735 virtual)
522 batches submitted to accumulate stats from 33408 documents (938801 virtual)
524 batches submitted to accumulate stats from 33536 documents (943624 virtual)
525 batches submitted to accumulate stats from 33600 documents (946330 virtual)
526 batches submitted to accumulate stats from 33664 documents (948325 virtual)
527 batches submitted to accumulate stats from 33728 documents (950406 virtual)
528 batches submitted to accumulate stats from 33792 documents (950847 virtual)
529 batches submitted to accumulate stats from 33856 documents (952346 virtual)
530 batches submitted to accumulate stats from 33920 documents (952678 virtual)
531 batches submitted to accumulate stats from 33984 documents (955499 virtual)
532 batches submitted to accumulate stats from 34048 documents (956959 virtual)
533 batches submitted to accumulate stats from 34112 documents (961950 virtual)
534 batches submitted to accumulate stats from 34176 documents (964653 virtual)
536 batches submitted to accumulate stats from 34304 documents (963350 virtual)
537 batches submitted to accumulate stats from 34368 documents (964615 virtual)
538 batches submitted to accumulate stats from 34432 documents (968123 virtual)
539 batches submitted to accumulate stats from 34496 documents (969966 virtual)
540 batches submitted to accumulate stats from 34560 documents (970121 virtual)
541 batches submitted to accumulate stats from 34624 documents (971374 virtual)
543 batches submitted to accumulate stats from 34752 documents (971594 virtual)
545 batches submitted to accumulate stats from 34880 documents (970643 virtual)
546 batches submitted to accumulate stats from 34944 documents (970656 virtual)
547 batches submitted to accumulate stats from 35008 documents (973153 virtual)
548 batches submitted to accumulate stats from 35072 documents (976555 virtual)
550 batches submitted to accumulate stats from 35200 documents (980913 virtual)
551 batches submitted to accumulate stats from 35264 documents (985976 virtual)
552 batches submitted to accumulate stats from 35328 documents (987871 virtual)
553 batches submitted to accumulate stats from 35392 documents (988746 virtual)
554 batches submitted to accumulate stats from 35456 documents (990694 virtual)
555 batches submitted to accumulate stats from 35520 documents (991855 virtual)
557 batches submitted to accumulate stats from 35648 documents (993570 virtual)
558 batches submitted to accumulate stats from 35712 documents (995186 virtual)
560 batches submitted to accumulate stats from 35840 documents (997090 virtual)
561 batches submitted to accumulate stats from 35904 documents (1000442 virtual)
564 batches submitted to accumulate stats from 36096 documents (1000542 virtual)
565 batches submitted to accumulate stats from 36160 documents (1004289 virtual)
567 batches submitted to accumulate stats from 36288 documents (1008408 virtual)
568 batches submitted to accumulate stats from 36352 documents (1011227 virtual)
570 batches submitted to accumulate stats from 36480 documents (1022285 virtual)
571 batches submitted to accumulate stats from 36544 documents (1022680 virtual)
572 batches submitted to accumulate stats from 36608 documents (1028540 virtual)
573 batches submitted to accumulate stats from 36672 documents (1034220 virtual)
574 batches submitted to accumulate stats from 36736 documents (1038896 virtual)
577 batches submitted to accumulate stats from 36928 documents (1038557 virtual)
579 batches submitted to accumulate stats from 37056 documents (1039562 virtual)
580 batches submitted to accumulate stats from 37120 documents (1040167 virtual)
581 batches submitted to accumulate stats from 37184 documents (1041107 virtual)
582 batches submitted to accumulate stats from 37248 documents (1044438 virtual)
583 batches submitted to accumulate stats from 37312 documents (1049350 virtual)
585 batches submitted to accumulate stats from 37440 documents (1048918 virtual)
586 batches submitted to accumulate stats from 37504 documents (1050386 virtual)
587 batches submitted to accumulate stats from 37568 documents (1051538 virtual)
588 batches submitted to accumulate stats from 37632 documents (1053788 virtual)
589 batches submitted to accumulate stats from 37696 documents (1059507 virtual)
591 batches submitted to accumulate stats from 37824 documents (1064378 virtual)
593 batches submitted to accumulate stats from 37952 documents (1065191 virtual)
595 batches submitted to accumulate stats from 38080 documents (1070301 virtual)
596 batches submitted to accumulate stats from 38144 documents (1072687 virtual)
598 batches submitted to accumulate stats from 38272 documents (1073472 virtual)
601 batches submitted to accumulate stats from 38464 documents (1072111 virtual)
602 batches submitted to accumulate stats from 38528 documents (1074776 virtual)
603 batches submitted to accumulate stats from 38592 documents (1076075 virtual)
604 batches submitted to accumulate stats from 38656 documents (1079803 virtual)
605 batches submitted to accumulate stats from 38720 documents (1084289 virtual)
606 batches submitted to accumulate stats from 38784 documents (1087639 virtual)
607 batches submitted to accumulate stats from 38848 documents (1090531 virtual)
608 batches submitted to accumulate stats from 38912 documents (1093205 virtual)
609 batches submitted to accumulate stats from 38976 documents (1095026 virtual)
611 batches submitted to accumulate stats from 39104 documents (1092943 virtual)
613 batches submitted to accumulate stats from 39232 documents (1095727 virtual)
614 batches submitted to accumulate stats from 39296 documents (1099294 virtual)
616 batches submitted to accumulate stats from 39424 documents (1103421 virtual)
617 batches submitted to accumulate stats from 39488 documents (1104312 virtual)
618 batches submitted to accumulate stats from 39552 documents (1106735 virtual)
619 batches submitted to accumulate stats from 39616 documents (1108472 virtual)
621 batches submitted to accumulate stats from 39744 documents (1108949 virtual)
622 batches submitted to accumulate stats from 39808 documents (1110004 virtual)
623 batches submitted to accumulate stats from 39872 documents (1110204 virtual)
625 batches submitted to accumulate stats from 40000 documents (1114496 virtual)
626 batches submitted to accumulate stats from 40064 documents (1115458 virtual)
629 batches submitted to accumulate stats from 40256 documents (1117631 virtual)
631 batches submitted to accumulate stats from 40384 documents (1118321 virtual)
632 batches submitted to accumulate stats from 40448 documents (1122589 virtual)
633 batches submitted to accumulate stats from 40512 documents (1126869 virtual)
634 batches submitted to accumulate stats from 40576 documents (1128898 virtual)
635 batches submitted to accumulate stats from 40640 documents (1140764 virtual)
637 batches submitted to accumulate stats from 40768 documents (1141912 virtual)
639 batches submitted to accumulate stats from 40896 documents (1143677 virtual)
640 batches submitted to accumulate stats from 40960 documents (1145380 virtual)
641 batches submitted to accumulate stats from 41024 documents (1148608 virtual)
642 batches submitted to accumulate stats from 41088 documents (1154038 virtual)
643 batches submitted to accumulate stats from 41152 documents (1158779 virtual)
644 batches submitted to accumulate stats from 41216 documents (1158786 virtual)
647 batches submitted to accumulate stats from 41408 documents (1162347 virtual)
648 batches submitted to accumulate stats from 41472 documents (1163393 virtual)
649 batches submitted to accumulate stats from 41536 documents (1163943 virtual)
650 batches submitted to accumulate stats from 41600 documents (1165080 virtual)
651 batches submitted to accumulate stats from 41664 documents (1170416 virtual)
653 batches submitted to accumulate stats from 41792 documents (1169954 virtual)
656 batches submitted to accumulate stats from 41984 documents (1171325 virtual)
657 batches submitted to accumulate stats from 42048 documents (1173914 virtual)
658 batches submitted to accumulate stats from 42112 documents (1178158 virtual)
659 batches submitted to accumulate stats from 42176 documents (1179753 virtual)
661 batches submitted to accumulate stats from 42304 documents (1182225 virtual)
663 batches submitted to accumulate stats from 42432 documents (1186913 virtual)
664 batches submitted to accumulate stats from 42496 documents (1189383 virtual)
665 batches submitted to accumulate stats from 42560 documents (1190487 virtual)
666 batches submitted to accumulate stats from 42624 documents (1193969 virtual)
668 batches submitted to accumulate stats from 42752 documents (1199146 virtual)
669 batches submitted to accumulate stats from 42816 documents (1199749 virtual)
670 batches submitted to accumulate stats from 42880 documents (1199882 virtual)
672 batches submitted to accumulate stats from 43008 documents (1200492 virtual)
674 batches submitted to accumulate stats from 43136 documents (1202445 virtual)
675 batches submitted to accumulate stats from 43200 documents (1203538 virtual)
676 batches submitted to accumulate stats from 43264 documents (1206951 virtual)
678 batches submitted to accumulate stats from 43392 documents (1208186 virtual)
681 batches submitted to accumulate stats from 43584 documents (1208027 virtual)
682 batches submitted to accumulate stats from 43648 documents (1209309 virtual)
683 batches submitted to accumulate stats from 43712 documents (1214645 virtual)
684 batches submitted to accumulate stats from 43776 documents (1215783 virtual)
685 batches submitted to accumulate stats from 43840 documents (1223761 virtual)
686 batches submitted to accumulate stats from 43904 documents (1226208 virtual)
687 batches submitted to accumulate stats from 43968 documents (1226788 virtual)
688 batches submitted to accumulate stats from 44032 documents (1227243 virtual)
689 batches submitted to accumulate stats from 44096 documents (1231165 virtual)
690 batches submitted to accumulate stats from 44160 documents (1236914 virtual)
692 batches submitted to accumulate stats from 44288 documents (1238456 virtual)
693 batches submitted to accumulate stats from 44352 documents (1241706 virtual)
696 batches submitted to accumulate stats from 44544 documents (1241436 virtual)
697 batches submitted to accumulate stats from 44608 documents (1246398 virtual)
698 batches submitted to accumulate stats from 44672 documents (1248999 virtual)
699 batches submitted to accumulate stats from 44736 documents (1249253 virtual)
700 batches submitted to accumulate stats from 44800 documents (1252718 virtual)
701 batches submitted to accumulate stats from 44864 documents (1254519 virtual)
702 batches submitted to accumulate stats from 44928 documents (1256835 virtual)
703 batches submitted to accumulate stats from 44992 documents (1270150 virtual)
704 batches submitted to accumulate stats from 45056 documents (1271332 virtual)
705 batches submitted to accumulate stats from 45120 documents (1272767 virtual)
706 batches submitted to accumulate stats from 45184 documents (1273348 virtual)
707 batches submitted to accumulate stats from 45248 documents (1278518 virtual)
708 batches submitted to accumulate stats from 45312 documents (1291901 virtual)
709 batches submitted to accumulate stats from 45376 documents (1294878 virtual)
710 batches submitted to accumulate stats from 45440 documents (1297182 virtual)
711 batches submitted to accumulate stats from 45504 documents (1297274 virtual)
712 batches submitted to accumulate stats from 45568 documents (1299082 virtual)
714 batches submitted to accumulate stats from 45696 documents (1301215 virtual)
715 batches submitted to accumulate stats from 45760 documents (1305054 virtual)
716 batches submitted to accumulate stats from 45824 documents (1306161 virtual)
717 batches submitted to accumulate stats from 45888 documents (1309099 virtual)
718 batches submitted to accumulate stats from 45952 documents (1311063 virtual)
719 batches submitted to accumulate stats from 46016 documents (1313987 virtual)
720 batches submitted to accumulate stats from 46080 documents (1317192 virtual)
721 batches submitted to accumulate stats from 46144 documents (1324935 virtual)
722 batches submitted to accumulate stats from 46208 documents (1328367 virtual)
723 batches submitted to accumulate stats from 46272 documents (1333443 virtual)
724 batches submitted to accumulate stats from 46336 documents (1337178 virtual)
725 batches submitted to accumulate stats from 46400 documents (1339992 virtual)
726 batches submitted to accumulate stats from 46464 documents (1341338 virtual)
727 batches submitted to accumulate stats from 46528 documents (1343513 virtual)
729 batches submitted to accumulate stats from 46656 documents (1343650 virtual)
731 batches submitted to accumulate stats from 46784 documents (1345814 virtual)
733 batches submitted to accumulate stats from 46912 documents (1345746 virtual)
734 batches submitted to accumulate stats from 46976 documents (1350618 virtual)
737 batches submitted to accumulate stats from 47168 documents (1352081 virtual)
738 batches submitted to accumulate stats from 47232 documents (1355393 virtual)
739 batches submitted to accumulate stats from 47296 documents (1360766 virtual)
740 batches submitted to accumulate stats from 47360 documents (1363813 virtual)
741 batches submitted to accumulate stats from 47424 documents (1364393 virtual)
742 batches submitted to accumulate stats from 47488 documents (1367573 virtual)
743 batches submitted to accumulate stats from 47552 documents (1368591 virtual)
744 batches submitted to accumulate stats from 47616 documents (1371434 virtual)
746 batches submitted to accumulate stats from 47744 documents (1372382 virtual)
747 batches submitted to accumulate stats from 47808 documents (1373699 virtual)
748 batches submitted to accumulate stats from 47872 documents (1374326 virtual)
749 batches submitted to accumulate stats from 47936 documents (1376781 virtual)
750 batches submitted to accumulate stats from 48000 documents (1380597 virtual)
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
3 accumulators retrieved from output queue
accumulated word occurrence stats for 3847666 virtual documents
K=20, Coherence Score: 0.625526571064658
Starting K=25
using symmetric alpha at 0.04
using symmetric eta at 0.04
using serial LDA version on this node
running online LDA training, 25 topics, 10 passes over the supplied corpus of 49835 documents, updating every 3000 documents, evaluating every ~49835 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 3 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.007*"film" + 0.006*"album" + 0.006*"season" + 0.003*"music" + 0.003*"game" + 0.003*"song" + 0.003*"art" + 0.002*"church" + 0.002*"sign" + 0.002*"war"
topic #22 (0.040): 0.004*"company" + 0.004*"building" + 0.003*"album" + 0.003*"band" + 0.003*"house" + 0.002*"track" + 0.002*"system" + 0.002*"station" + 0.002*"award" + 0.002*"north"
topic #23 (0.040): 0.003*"building" + 0.003*"town" + 0.003*"law" + 0.003*"award" + 0.003*"village" + 0.002*"public" + 0.002*"class" + 0.002*"house" + 0.002*"site" + 0.002*"local"
topic #0 (0.040): 0.006*"define" + 0.005*"nationality" + 0.005*"rules_player" + 0.005*"game" + 0.004*"non_fifa" + 0.004*"displaystyle" + 0.003*"building" + 0.003*"woman" + 0.003*"note_flag" + 0.002*"order"
topic #11 (0.040): 0.004*"season" + 0.004*"game" + 0.003*"series" + 0.003*"park" + 0.003*"film" + 0.003*"character" + 0.002*"version" + 0.002*"player" + 0.002*"get" + 0.002*"episode"
topic diff=11.057239, rho=1.000000
PROGRESS: pass 0, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.040): 0.004*"album" + 0.003*"band" + 0.003*"award" + 0.003*"film" + 0.003*"company" + 0.003*"samantha" + 0.003*"public" + 0.003*"study" + 0.002*"series" + 0.002*"design"
topic #20 (0.040): 0.003*"say" + 0.003*"building" + 0.003*"station" + 0.003*"book" + 0.003*"publish" + 0.003*"film" + 0.003*"company" + 0.002*"variety" + 0.002*"game" + 0.002*"season"
topic #11 (0.040): 0.004*"game" + 0.004*"season" + 0.003*"series" + 0.003*"park" + 0.002*"club" + 0.002*"episode" + 0.002*"version" + 0.002*"song" + 0.002*"player" + 0.002*"get"
topic #0 (0.040): 0.005*"game" + 0.004*"define" + 0.003*"nationality" + 0.003*"season" + 0.003*"event" + 0.003*"rules_player" + 0.003*"displaystyle" + 0.003*"non_fifa" + 0.003*"woman" + 0.002*"title"
topic #9 (0.040): 0.005*"song" + 0.005*"series" + 0.004*"band" + 0.004*"film" + 0.004*"episode" + 0.004*"album" + 0.003*"produce" + 0.003*"television" + 0.003*"award" + 0.002*"say"
topic diff=4.315764, rho=0.500000
PROGRESS: pass 0, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.040): 0.004*"government" + 0.003*"child" + 0.003*"student" + 0.003*"son" + 0.003*"community" + 0.003*"election" + 0.003*"village" + 0.002*"say" + 0.002*"album" + 0.002*"house"
topic #23 (0.040): 0.005*"law" + 0.003*"town" + 0.003*"election" + 0.003*"public" + 0.003*"local" + 0.003*"government" + 0.003*"church" + 0.002*"court" + 0.002*"house" + 0.002*"building"
topic #0 (0.040): 0.007*"displaystyle" + 0.004*"game" + 0.004*"define" + 0.003*"event" + 0.003*"nationality" + 0.003*"rules_player" + 0.003*"woman" + 0.003*"season" + 0.002*"non_fifa" + 0.002*"order"
topic #19 (0.040): 0.004*"river" + 0.004*"house" + 0.003*"design" + 0.003*"town" + 0.003*"building" + 0.003*"woman" + 0.002*"company" + 0.002*"theater" + 0.002*"main" + 0.002*"south"
topic #6 (0.040): 0.004*"specie" + 0.004*"season" + 0.003*"church" + 0.003*"island" + 0.003*"gun" + 0.003*"describe" + 0.003*"ship" + 0.003*"point" + 0.002*"village" + 0.002*"building"
topic diff=1.347907, rho=0.377964
PROGRESS: pass 0, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 10
PROGRESS: pass 0, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.040): 0.020*"film" + 0.004*"series" + 0.004*"final" + 0.003*"event" + 0.003*"story" + 0.003*"character" + 0.003*"award" + 0.003*"game" + 0.003*"title" + 0.003*"star"
topic #18 (0.040): 0.008*"club" + 0.007*"company" + 0.006*"population" + 0.005*"league" + 0.005*"government" + 0.005*"village" + 0.004*"game" + 0.004*"season" + 0.003*"municipality" + 0.003*"football"
topic #4 (0.040): 0.007*"game" + 0.006*"film" + 0.005*"line" + 0.004*"player" + 0.004*"route" + 0.003*"season" + 0.003*"series" + 0.003*"road" + 0.003*"army" + 0.003*"station"
topic #7 (0.040): 0.009*"album" + 0.008*"film" + 0.007*"music" + 0.005*"season" + 0.005*"art" + 0.004*"festival" + 0.004*"church" + 0.003*"song" + 0.003*"band" + 0.003*"single"
topic #14 (0.040): 0.006*"park" + 0.006*"age" + 0.005*"population" + 0.004*"race" + 0.003*"village" + 0.003*"island" + 0.003*"male" + 0.003*"child" + 0.003*"specie" + 0.003*"land"
topic diff=0.629496, rho=0.316228
PROGRESS: pass 0, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.004*"woman" + 0.003*"support" + 0.003*"say" + 0.003*"life" + 0.003*"game" + 0.003*"event" + 0.002*"death" + 0.002*"public" + 0.002*"hospital" + 0.002*"set"
topic #9 (0.040): 0.008*"series" + 0.007*"film" + 0.006*"episode" + 0.006*"television" + 0.005*"award" + 0.004*"song" + 0.004*"produce" + 0.003*"tv" + 0.003*"channel" + 0.003*"album"
topic #12 (0.040): 0.016*"station" + 0.007*"line" + 0.003*"season" + 0.003*"community" + 0.003*"railway" + 0.003*"tournament" + 0.003*"court" + 0.003*"county" + 0.003*"public" + 0.003*"train"
topic #1 (0.040): 0.025*"film" + 0.004*"series" + 0.004*"award" + 0.004*"story" + 0.004*"character" + 0.003*"star" + 0.003*"event" + 0.003*"final" + 0.003*"title" + 0.003*"game"
topic #6 (0.040): 0.007*"specie" + 0.005*"island" + 0.005*"ship" + 0.004*"genus" + 0.004*"church" + 0.003*"describe" + 0.003*"point" + 0.003*"gun" + 0.002*"site" + 0.002*"season"
topic diff=0.586378, rho=0.277350
PROGRESS: pass 0, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.008*"album" + 0.008*"music" + 0.008*"art" + 0.007*"film" + 0.006*"church" + 0.006*"festival" + 0.004*"season" + 0.004*"artist" + 0.003*"study" + 0.003*"award"
topic #11 (0.040): 0.010*"game" + 0.005*"version" + 0.005*"character" + 0.005*"series" + 0.004*"season" + 0.004*"get" + 0.003*"player" + 0.003*"system" + 0.003*"episode" + 0.002*"allow"
topic #5 (0.040): 0.004*"woman" + 0.004*"support" + 0.003*"say" + 0.003*"life" + 0.002*"event" + 0.002*"game" + 0.002*"death" + 0.002*"public" + 0.002*"hospital" + 0.002*"set"
topic #20 (0.040): 0.011*"book" + 0.008*"publish" + 0.004*"study" + 0.003*"theory" + 0.003*"student" + 0.003*"life" + 0.003*"say" + 0.003*"language" + 0.003*"author" + 0.003*"company"
topic #9 (0.040): 0.009*"series" + 0.007*"film" + 0.006*"episode" + 0.006*"television" + 0.005*"award" + 0.004*"produce" + 0.004*"appear" + 0.003*"tv" + 0.003*"channel" + 0.003*"novel"
topic diff=0.562581, rho=0.250000
PROGRESS: pass 0, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 10
PROGRESS: pass 0, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.040): 0.008*"park" + 0.008*"population" + 0.007*"age" + 0.005*"village" + 0.005*"specie" + 0.004*"male" + 0.004*"river" + 0.004*"island" + 0.004*"land" + 0.004*"water"
topic #10 (0.040): 0.006*"government" + 0.004*"student" + 0.003*"support" + 0.003*"french" + 0.003*"son" + 0.003*"political" + 0.003*"system" + 0.003*"child" + 0.002*"election" + 0.002*"king"
topic #19 (0.040): 0.010*"building" + 0.008*"river" + 0.008*"house" + 0.006*"design" + 0.004*"church" + 0.004*"town" + 0.004*"main" + 0.004*"lake" + 0.003*"south" + 0.003*"style"
topic #12 (0.040): 0.028*"station" + 0.009*"line" + 0.005*"railway" + 0.004*"operate" + 0.003*"train" + 0.003*"community" + 0.003*"public" + 0.003*"season" + 0.003*"bus" + 0.003*"county"
topic #3 (0.040): 0.008*"system" + 0.006*"engine" + 0.006*"design" + 0.006*"model" + 0.004*"vehicle" + 0.004*"aircraft" + 0.004*"car" + 0.003*"power" + 0.003*"control" + 0.003*"type"
topic diff=0.524606, rho=0.229416
PROGRESS: pass 0, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.040): 0.030*"season" + 0.019*"club" + 0.016*"game" + 0.014*"league" + 0.013*"football" + 0.010*"player" + 0.010*"match" + 0.010*"championship" + 0.009*"final" + 0.008*"score"
topic #20 (0.040): 0.015*"book" + 0.011*"publish" + 0.005*"study" + 0.004*"language" + 0.004*"author" + 0.003*"life" + 0.003*"student" + 0.003*"history" + 0.003*"editor" + 0.003*"theory"
topic #22 (0.040): 0.009*"company" + 0.007*"road" + 0.007*"line" + 0.006*"building" + 0.005*"railway" + 0.005*"street" + 0.005*"north" + 0.004*"station" + 0.004*"house" + 0.004*"route"
topic #10 (0.040): 0.006*"government" + 0.003*"student" + 0.003*"french" + 0.003*"support" + 0.003*"son" + 0.003*"political" + 0.003*"child" + 0.002*"king" + 0.002*"system" + 0.002*"cause"
topic #0 (0.040): 0.011*"displaystyle" + 0.006*"define" + 0.004*"example" + 0.004*"function" + 0.004*"point" + 0.003*"language" + 0.003*"term" + 0.003*"event" + 0.003*"value" + 0.003*"word"
topic diff=0.500059, rho=0.213201
PROGRESS: pass 0, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.040): 0.015*"book" + 0.011*"publish" + 0.005*"study" + 0.004*"language" + 0.004*"author" + 0.004*"life" + 0.003*"student" + 0.003*"editor" + 0.003*"history" + 0.003*"magazine"
topic #16 (0.040): 0.011*"race" + 0.009*"event" + 0.008*"championship" + 0.007*"compete" + 0.005*"squadron" + 0.005*"german" + 0.004*"cell" + 0.003*"protein" + 0.003*"result" + 0.003*"fly"
topic #7 (0.040): 0.012*"art" + 0.009*"church" + 0.009*"music" + 0.007*"festival" + 0.007*"artist" + 0.006*"film" + 0.006*"album" + 0.005*"painting" + 0.004*"bishop" + 0.003*"study"
topic #19 (0.040): 0.013*"building" + 0.010*"river" + 0.009*"house" + 0.006*"design" + 0.005*"church" + 0.004*"town" + 0.004*"main" + 0.004*"style" + 0.004*"lake" + 0.004*"street"
topic #14 (0.040): 0.008*"population" + 0.007*"park" + 0.007*"age" + 0.006*"village" + 0.005*"specie" + 0.005*"river" + 0.004*"water" + 0.004*"town" + 0.004*"land" + 0.004*"km"
topic diff=0.482227, rho=0.200000
PROGRESS: pass 0, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.040): 0.047*"film" + 0.006*"star" + 0.006*"story" + 0.005*"direct" + 0.005*"award" + 0.005*"character" + 0.005*"series" + 0.004*"director" + 0.003*"title" + 0.003*"event"
topic #9 (0.040): 0.012*"series" + 0.008*"episode" + 0.007*"television" + 0.007*"film" + 0.006*"award" + 0.004*"appear" + 0.004*"novel" + 0.004*"produce" + 0.004*"radio" + 0.004*"channel"
topic #6 (0.040): 0.015*"ship" + 0.012*"specie" + 0.008*"island" + 0.007*"genus" + 0.005*"describe" + 0.004*"sea" + 0.004*"vessel" + 0.004*"gun" + 0.003*"navy" + 0.003*"water"
topic #14 (0.040): 0.008*"population" + 0.007*"park" + 0.006*"age" + 0.006*"village" + 0.006*"specie" + 0.005*"river" + 0.005*"water" + 0.004*"town" + 0.004*"region" + 0.004*"county"
topic #8 (0.040): 0.023*"album" + 0.022*"song" + 0.017*"music" + 0.015*"band" + 0.010*"single" + 0.007*"track" + 0.007*"perform" + 0.005*"tour" + 0.005*"video" + 0.005*"chart"
topic diff=0.476149, rho=0.188982
PROGRESS: pass 0, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.009*"system" + 0.007*"design" + 0.006*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.004*"power" + 0.004*"control" + 0.004*"vehicle" + 0.003*"low" + 0.003*"type"
topic #18 (0.040): 0.018*"company" + 0.011*"village" + 0.010*"population" + 0.009*"municipality" + 0.007*"business" + 0.006*"club" + 0.005*"found" + 0.005*"government" + 0.005*"country" + 0.004*"establish"
topic #11 (0.040): 0.027*"game" + 0.008*"character" + 0.008*"player" + 0.006*"series" + 0.005*"version" + 0.004*"magic" + 0.004*"get" + 0.004*"system" + 0.003*"kill" + 0.003*"allow"
topic #14 (0.040): 0.009*"population" + 0.006*"park" + 0.006*"age" + 0.006*"village" + 0.006*"specie" + 0.005*"river" + 0.005*"town" + 0.005*"water" + 0.004*"county" + 0.004*"region"
topic #7 (0.040): 0.015*"art" + 0.011*"church" + 0.008*"music" + 0.008*"artist" + 0.008*"festival" + 0.007*"painting" + 0.005*"bishop" + 0.005*"film" + 0.004*"album" + 0.004*"study"
topic diff=0.466522, rho=0.179605
PROGRESS: pass 0, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 10
PROGRESS: pass 0, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.017*"building" + 0.011*"house" + 0.009*"river" + 0.008*"church" + 0.007*"design" + 0.005*"town" + 0.004*"main" + 0.004*"street" + 0.004*"style" + 0.004*"th_century"
topic #18 (0.040): 0.019*"company" + 0.012*"village" + 0.011*"population" + 0.010*"municipality" + 0.008*"business" + 0.006*"club" + 0.006*"found" + 0.006*"country" + 0.005*"government" + 0.004*"industry"
topic #12 (0.040): 0.054*"station" + 0.015*"line" + 0.006*"railway" + 0.006*"operate" + 0.006*"platform" + 0.006*"train" + 0.005*"radio" + 0.004*"license" + 0.004*"local" + 0.003*"program"
topic #20 (0.040): 0.017*"book" + 0.013*"publish" + 0.006*"study" + 0.005*"language" + 0.005*"author" + 0.004*"magazine" + 0.004*"editor" + 0.004*"life" + 0.004*"history" + 0.003*"theory"
topic #16 (0.040): 0.016*"race" + 0.012*"event" + 0.009*"championship" + 0.008*"compete" + 0.006*"squadron" + 0.006*"protein" + 0.004*"cell" + 0.004*"wing" + 0.004*"german" + 0.004*"woman"
topic diff=0.440450, rho=0.171499
PROGRESS: pass 0, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.017*"building" + 0.011*"house" + 0.009*"river" + 0.009*"church" + 0.007*"design" + 0.005*"town" + 0.004*"main" + 0.004*"th_century" + 0.004*"street" + 0.004*"style"
topic #15 (0.040): 0.033*"game" + 0.013*"player" + 0.013*"basketball" + 0.013*"season" + 0.008*"conference" + 0.007*"yard" + 0.005*"point" + 0.005*"finish" + 0.005*"raider" + 0.004*"field"
topic #18 (0.040): 0.021*"company" + 0.013*"village" + 0.011*"population" + 0.010*"municipality" + 0.008*"business" + 0.006*"found" + 0.006*"club" + 0.006*"country" + 0.005*"government" + 0.005*"industry"
topic #23 (0.040): 0.008*"law" + 0.007*"government" + 0.006*"party" + 0.005*"election" + 0.005*"elect" + 0.005*"court" + 0.005*"church" + 0.004*"political" + 0.004*"president" + 0.004*"public"
topic #7 (0.040): 0.018*"art" + 0.014*"church" + 0.010*"artist" + 0.008*"painting" + 0.007*"music" + 0.007*"festival" + 0.005*"bishop" + 0.004*"paint" + 0.004*"study" + 0.004*"exhibition"
topic diff=0.426751, rho=0.164399
PROGRESS: pass 0, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.008*"law" + 0.008*"government" + 0.006*"court" + 0.006*"party" + 0.005*"elect" + 0.005*"election" + 0.004*"church" + 0.004*"order" + 0.004*"president" + 0.004*"political"
topic #1 (0.040): 0.061*"film" + 0.009*"star" + 0.007*"direct" + 0.006*"story" + 0.006*"award" + 0.005*"character" + 0.005*"director" + 0.004*"series" + 0.004*"earth" + 0.004*"production"
topic #17 (0.040): 0.005*"information" + 0.004*"datum" + 0.004*"process" + 0.004*"provide" + 0.004*"study" + 0.004*"system" + 0.004*"increase" + 0.004*"product" + 0.004*"research" + 0.004*"need"
topic #21 (0.040): 0.011*"election" + 0.008*"party" + 0.007*"vote" + 0.005*"candidate" + 0.004*"tell" + 0.004*"house" + 0.004*"say" + 0.004*"get" + 0.003*"seat" + 0.003*"kill"
topic #20 (0.040): 0.018*"book" + 0.014*"publish" + 0.006*"study" + 0.005*"language" + 0.005*"author" + 0.004*"magazine" + 0.004*"history" + 0.004*"life" + 0.004*"editor" + 0.004*"theory"
topic diff=0.407766, rho=0.158114
-8.463 per-word bound, 352.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #20 (0.040): 0.018*"book" + 0.014*"publish" + 0.006*"study" + 0.006*"language" + 0.005*"author" + 0.004*"magazine" + 0.004*"history" + 0.004*"life" + 0.004*"editor" + 0.004*"publication"
topic #10 (0.040): 0.005*"government" + 0.005*"king" + 0.004*"cause" + 0.004*"son" + 0.003*"increase" + 0.003*"french" + 0.003*"support" + 0.003*"death" + 0.003*"child" + 0.003*"effect"
topic #9 (0.040): 0.014*"series" + 0.010*"episode" + 0.008*"television" + 0.006*"film" + 0.005*"tv" + 0.005*"award" + 0.005*"appear" + 0.005*"character" + 0.004*"novel" + 0.004*"story"
topic #5 (0.040): 0.018*"woman" + 0.004*"life" + 0.004*"hospital" + 0.004*"say" + 0.003*"wine" + 0.003*"support" + 0.003*"death" + 0.003*"event" + 0.002*"ukrainian" + 0.002*"care"
topic #12 (0.040): 0.063*"station" + 0.016*"line" + 0.008*"operate" + 0.007*"platform" + 0.007*"railway" + 0.007*"radio" + 0.006*"train" + 0.005*"local" + 0.004*"license" + 0.004*"program"
topic diff=0.395265, rho=0.152499
-8.449 per-word bound, 349.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #8 (0.040): 0.027*"album" + 0.026*"song" + 0.020*"music" + 0.017*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic #10 (0.040): 0.006*"king" + 0.005*"government" + 0.005*"son" + 0.004*"cause" + 0.003*"increase" + 0.003*"child" + 0.003*"death" + 0.003*"french" + 0.003*"support" + 0.003*"effect"
topic #5 (0.040): 0.019*"woman" + 0.004*"life" + 0.004*"hospital" + 0.004*"say" + 0.003*"support" + 0.003*"wine" + 0.003*"death" + 0.003*"event" + 0.002*"ukrainian" + 0.002*"flag"
topic #2 (0.040): 0.034*"season" + 0.020*"club" + 0.018*"game" + 0.016*"league" + 0.015*"football" + 0.012*"match" + 0.012*"championship" + 0.012*"player" + 0.011*"final" + 0.009*"score"
topic #20 (0.040): 0.019*"book" + 0.014*"publish" + 0.006*"study" + 0.006*"language" + 0.006*"author" + 0.004*"history" + 0.004*"magazine" + 0.004*"life" + 0.004*"editor" + 0.004*"writer"
topic diff=0.382771, rho=0.145865
-8.357 per-word bound, 327.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.040): 0.010*"population" + 0.008*"age" + 0.007*"village" + 0.006*"specie" + 0.006*"park" + 0.006*"town" + 0.005*"male" + 0.005*"river" + 0.005*"county" + 0.005*"water"
topic #19 (0.040): 0.023*"building" + 0.013*"house" + 0.009*"church" + 0.008*"design" + 0.007*"river" + 0.005*"wall" + 0.005*"site" + 0.005*"side" + 0.005*"th_century" + 0.004*"main"
topic #17 (0.040): 0.006*"information" + 0.005*"provide" + 0.005*"increase" + 0.005*"consumer" + 0.004*"datum" + 0.004*"process" + 0.004*"system" + 0.004*"study" + 0.004*"product" + 0.004*"need"
topic #10 (0.040): 0.006*"king" + 0.005*"son" + 0.005*"government" + 0.004*"cause" + 0.004*"child" + 0.003*"increase" + 0.003*"death" + 0.003*"french" + 0.003*"support" + 0.003*"effect"
topic #20 (0.040): 0.019*"book" + 0.015*"publish" + 0.006*"language" + 0.006*"study" + 0.005*"author" + 0.005*"history" + 0.004*"magazine" + 0.004*"life" + 0.004*"editor" + 0.004*"publication"
topic diff=0.367096, rho=0.138896
PROGRESS: pass 1, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.040): 0.015*"army" + 0.011*"battle" + 0.011*"war" + 0.010*"force" + 0.008*"attack" + 0.007*"command" + 0.006*"troop" + 0.006*"division" + 0.006*"regiment" + 0.006*"unit"
topic #16 (0.040): 0.025*"race" + 0.018*"event" + 0.011*"compete" + 0.009*"championship" + 0.007*"cell" + 0.005*"protein" + 0.005*"squadron" + 0.005*"athlete" + 0.004*"driver" + 0.004*"wing"
topic #1 (0.040): 0.070*"film" + 0.010*"star" + 0.008*"direct" + 0.007*"story" + 0.007*"award" + 0.005*"character" + 0.005*"director" + 0.004*"production" + 0.004*"earth" + 0.004*"movie"
topic #14 (0.040): 0.010*"population" + 0.008*"age" + 0.007*"village" + 0.007*"specie" + 0.006*"park" + 0.006*"town" + 0.005*"river" + 0.005*"male" + 0.005*"county" + 0.005*"water"
topic #18 (0.040): 0.026*"company" + 0.016*"village" + 0.014*"population" + 0.011*"municipality" + 0.009*"business" + 0.007*"found" + 0.006*"government" + 0.006*"country" + 0.006*"census" + 0.005*"industry"
topic diff=0.361951, rho=0.138896
PROGRESS: pass 1, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.040): 0.072*"station" + 0.017*"line" + 0.009*"radio" + 0.009*"operate" + 0.008*"platform" + 0.007*"railway" + 0.007*"train" + 0.006*"network" + 0.006*"broadcast" + 0.006*"local"
topic #6 (0.040): 0.022*"ship" + 0.015*"specie" + 0.014*"island" + 0.009*"genus" + 0.006*"describe" + 0.006*"sea" + 0.006*"vessel" + 0.006*"navy" + 0.005*"port" + 0.005*"gun"
topic #16 (0.040): 0.026*"race" + 0.019*"event" + 0.012*"compete" + 0.009*"championship" + 0.007*"cell" + 0.005*"squadron" + 0.005*"protein" + 0.005*"athlete" + 0.004*"wing" + 0.004*"driver"
topic #4 (0.040): 0.016*"army" + 0.012*"battle" + 0.011*"war" + 0.011*"force" + 0.009*"attack" + 0.007*"command" + 0.006*"troop" + 0.006*"regiment" + 0.006*"division" + 0.006*"unit"
topic #17 (0.040): 0.006*"information" + 0.005*"provide" + 0.004*"system" + 0.004*"increase" + 0.004*"datum" + 0.004*"process" + 0.004*"need" + 0.004*"consumer" + 0.004*"study" + 0.004*"product"
topic diff=0.351265, rho=0.138896
PROGRESS: pass 1, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.040): 0.026*"race" + 0.020*"event" + 0.012*"compete" + 0.010*"championship" + 0.007*"cell" + 0.005*"squadron" + 0.005*"athlete" + 0.005*"protein" + 0.004*"metre" + 0.004*"driver"
topic #19 (0.040): 0.024*"building" + 0.014*"house" + 0.009*"church" + 0.009*"design" + 0.006*"river" + 0.005*"site" + 0.005*"th_century" + 0.005*"wall" + 0.005*"town" + 0.005*"stone"
topic #20 (0.040): 0.020*"book" + 0.016*"publish" + 0.006*"language" + 0.006*"study" + 0.006*"author" + 0.005*"history" + 0.005*"life" + 0.004*"magazine" + 0.004*"editor" + 0.004*"writer"
topic #23 (0.040): 0.009*"government" + 0.009*"law" + 0.007*"court" + 0.007*"election" + 0.007*"party" + 0.006*"elect" + 0.004*"political" + 0.004*"order" + 0.004*"act" + 0.004*"general"
topic #10 (0.040): 0.007*"king" + 0.005*"son" + 0.005*"government" + 0.005*"cause" + 0.004*"french" + 0.004*"child" + 0.004*"death" + 0.003*"increase" + 0.003*"effect" + 0.003*"support"
topic diff=0.357241, rho=0.138896
PROGRESS: pass 1, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.010*"german" + 0.010*"government" + 0.010*"war" + 0.008*"military" + 0.006*"force" + 0.005*"department" + 0.005*"general" + 0.005*"training" + 0.005*"operation" + 0.004*"korean"
topic #2 (0.040): 0.034*"season" + 0.021*"club" + 0.017*"league" + 0.017*"game" + 0.015*"football" + 0.014*"match" + 0.013*"championship" + 0.012*"player" + 0.012*"final" + 0.009*"score"
topic #3 (0.040): 0.010*"system" + 0.008*"design" + 0.007*"engine" + 0.006*"model" + 0.006*"aircraft" + 0.006*"power" + 0.004*"vehicle" + 0.004*"car" + 0.004*"produce" + 0.004*"low"
topic #5 (0.040): 0.023*"woman" + 0.006*"life" + 0.005*"hospital" + 0.004*"death" + 0.004*"say" + 0.004*"ukrainian" + 0.003*"buddhist" + 0.003*"soul" + 0.003*"buddha" + 0.003*"wine"
topic #12 (0.040): 0.073*"station" + 0.019*"line" + 0.012*"radio" + 0.010*"operate" + 0.008*"train" + 0.008*"railway" + 0.007*"network" + 0.007*"platform" + 0.007*"broadcast" + 0.006*"air"
topic diff=0.348970, rho=0.138896
PROGRESS: pass 1, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.023*"woman" + 0.005*"life" + 0.005*"hospital" + 0.004*"death" + 0.004*"say" + 0.004*"buddhist" + 0.004*"ukrainian" + 0.003*"buddha" + 0.003*"practice" + 0.003*"wine"
topic #18 (0.040): 0.029*"company" + 0.017*"village" + 0.015*"population" + 0.012*"municipality" + 0.010*"business" + 0.007*"found" + 0.006*"government" + 0.006*"country" + 0.006*"industry" + 0.006*"census"
topic #22 (0.040): 0.018*"road" + 0.012*"route" + 0.011*"line" + 0.010*"street" + 0.009*"north" + 0.008*"railway" + 0.008*"bridge" + 0.008*"highway" + 0.007*"company" + 0.007*"park"
topic #21 (0.040): 0.008*"election" + 0.006*"tell" + 0.006*"vote" + 0.006*"say" + 0.006*"party" + 0.005*"kill" + 0.005*"candidate" + 0.005*"get" + 0.005*"house" + 0.004*"father"
topic #1 (0.040): 0.075*"film" + 0.012*"star" + 0.009*"direct" + 0.008*"story" + 0.008*"award" + 0.005*"director" + 0.005*"character" + 0.005*"production" + 0.005*"movie" + 0.004*"earth"
topic diff=0.349747, rho=0.138896
PROGRESS: pass 1, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.040): 0.028*"race" + 0.020*"event" + 0.013*"compete" + 0.009*"championship" + 0.007*"cell" + 0.005*"driver" + 0.005*"protein" + 0.005*"squadron" + 0.005*"athlete" + 0.005*"car"
topic #12 (0.040): 0.076*"station" + 0.018*"line" + 0.013*"radio" + 0.010*"operate" + 0.008*"network" + 0.008*"train" + 0.008*"railway" + 0.007*"platform" + 0.007*"air" + 0.007*"broadcast"
topic #0 (0.040): 0.013*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.007*"define" + 0.005*"case" + 0.005*"term" + 0.005*"point" + 0.005*"language" + 0.005*"word" + 0.004*"value"
topic #11 (0.040): 0.037*"game" + 0.013*"player" + 0.010*"character" + 0.008*"version" + 0.005*"series" + 0.005*"video" + 0.004*"power" + 0.004*"allow" + 0.004*"card" + 0.004*"magic"
topic #4 (0.040): 0.017*"army" + 0.013*"force" + 0.013*"war" + 0.012*"battle" + 0.009*"attack" + 0.008*"command" + 0.007*"troop" + 0.006*"military" + 0.006*"division" + 0.006*"regiment"
topic diff=0.343956, rho=0.138896
PROGRESS: pass 1, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.021*"art" + 0.016*"church" + 0.012*"artist" + 0.009*"painting" + 0.007*"festival" + 0.007*"museum" + 0.006*"music" + 0.006*"exhibition" + 0.006*"bishop" + 0.005*"paint"
topic #19 (0.040): 0.026*"building" + 0.015*"house" + 0.011*"church" + 0.009*"design" + 0.006*"site" + 0.006*"th_century" + 0.005*"river" + 0.005*"wall" + 0.005*"main" + 0.005*"town"
topic #16 (0.040): 0.030*"race" + 0.023*"event" + 0.013*"compete" + 0.010*"championship" + 0.008*"cell" + 0.005*"driver" + 0.005*"car" + 0.005*"athlete" + 0.005*"protein" + 0.005*"lap"
topic #9 (0.040): 0.016*"series" + 0.011*"episode" + 0.010*"television" + 0.006*"appear" + 0.005*"character" + 0.005*"tv" + 0.005*"award" + 0.005*"film" + 0.005*"novel" + 0.004*"role"
topic #24 (0.040): 0.014*"college" + 0.014*"university" + 0.014*"student" + 0.012*"award" + 0.010*"study" + 0.010*"research" + 0.009*"science" + 0.008*"education" + 0.007*"program" + 0.007*"graduate"
topic diff=0.332629, rho=0.138896
PROGRESS: pass 1, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.010*"system" + 0.009*"design" + 0.007*"engine" + 0.006*"model" + 0.006*"aircraft" + 0.005*"power" + 0.005*"vehicle" + 0.004*"produce" + 0.004*"type" + 0.004*"low"
topic #12 (0.040): 0.076*"station" + 0.019*"line" + 0.017*"radio" + 0.011*"operate" + 0.009*"train" + 0.009*"network" + 0.009*"platform" + 0.009*"broadcast" + 0.008*"railway" + 0.008*"air"
topic #24 (0.040): 0.015*"student" + 0.014*"college" + 0.014*"university" + 0.013*"award" + 0.010*"study" + 0.010*"research" + 0.009*"science" + 0.008*"education" + 0.007*"program" + 0.007*"graduate"
topic #21 (0.040): 0.007*"election" + 0.006*"tell" + 0.006*"say" + 0.006*"vote" + 0.006*"kill" + 0.005*"get" + 0.005*"father" + 0.005*"party" + 0.005*"death" + 0.005*"house"
topic #18 (0.040): 0.031*"company" + 0.018*"village" + 0.015*"population" + 0.013*"municipality" + 0.010*"business" + 0.008*"found" + 0.007*"industry" + 0.006*"government" + 0.006*"country" + 0.006*"census"
topic diff=0.325405, rho=0.138896
PROGRESS: pass 1, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.040): 0.034*"season" + 0.023*"club" + 0.018*"league" + 0.017*"game" + 0.016*"football" + 0.013*"championship" + 0.013*"match" + 0.012*"final" + 0.012*"player" + 0.009*"score"
topic #9 (0.040): 0.017*"series" + 0.012*"episode" + 0.010*"television" + 0.006*"appear" + 0.005*"character" + 0.005*"tv" + 0.005*"award" + 0.005*"story" + 0.004*"say" + 0.004*"film"
topic #23 (0.040): 0.011*"government" + 0.010*"law" + 0.009*"party" + 0.008*"election" + 0.007*"elect" + 0.007*"court" + 0.005*"political" + 0.005*"act" + 0.004*"president" + 0.004*"vote"
topic #24 (0.040): 0.015*"student" + 0.015*"university" + 0.014*"college" + 0.013*"award" + 0.011*"research" + 0.010*"study" + 0.009*"science" + 0.008*"education" + 0.008*"program" + 0.007*"graduate"
topic #14 (0.040): 0.010*"population" + 0.009*"river" + 0.008*"age" + 0.008*"specie" + 0.007*"village" + 0.007*"town" + 0.006*"water" + 0.006*"park" + 0.006*"region" + 0.005*"county"
topic diff=0.325003, rho=0.138896
PROGRESS: pass 1, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.026*"building" + 0.016*"house" + 0.011*"church" + 0.010*"design" + 0.007*"site" + 0.006*"th_century" + 0.005*"wall" + 0.005*"stone" + 0.005*"main" + 0.005*"room"
topic #20 (0.040): 0.021*"book" + 0.017*"publish" + 0.006*"author" + 0.006*"language" + 0.006*"study" + 0.005*"magazine" + 0.005*"life" + 0.005*"history" + 0.005*"editor" + 0.005*"writer"
topic #16 (0.040): 0.031*"race" + 0.023*"event" + 0.014*"compete" + 0.010*"championship" + 0.008*"cell" + 0.006*"protein" + 0.006*"metre" + 0.005*"woman" + 0.005*"finish" + 0.005*"squadron"
topic #11 (0.040): 0.040*"game" + 0.014*"player" + 0.010*"character" + 0.008*"version" + 0.006*"series" + 0.005*"video" + 0.005*"magic" + 0.004*"computer" + 0.004*"allow" + 0.004*"power"
topic #22 (0.040): 0.019*"road" + 0.013*"route" + 0.012*"line" + 0.011*"street" + 0.010*"north" + 0.009*"railway" + 0.009*"bridge" + 0.008*"highway" + 0.007*"county" + 0.007*"park"
topic diff=0.323668, rho=0.138896
PROGRESS: pass 1, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.007*"tell" + 0.006*"say" + 0.006*"election" + 0.005*"kill" + 0.005*"get" + 0.005*"vote" + 0.005*"father" + 0.005*"death" + 0.004*"house" + 0.004*"try"
topic #16 (0.040): 0.031*"race" + 0.024*"event" + 0.015*"compete" + 0.011*"championship" + 0.007*"cell" + 0.006*"protein" + 0.005*"woman" + 0.005*"metre" + 0.005*"finish" + 0.005*"squadron"
topic #13 (0.040): 0.020*"german" + 0.011*"war" + 0.011*"government" + 0.009*"military" + 0.006*"force" + 0.005*"department" + 0.005*"general" + 0.005*"korean" + 0.005*"chief" + 0.005*"soviet"
topic #0 (0.040): 0.013*"displaystyle" + 0.008*"example" + 0.006*"language" + 0.006*"define" + 0.006*"function" + 0.006*"word" + 0.006*"case" + 0.005*"point" + 0.005*"term" + 0.004*"set"
topic #7 (0.040): 0.024*"art" + 0.020*"church" + 0.013*"artist" + 0.010*"painting" + 0.008*"museum" + 0.007*"festival" + 0.007*"bishop" + 0.006*"exhibition" + 0.005*"music" + 0.005*"paint"
topic diff=0.311380, rho=0.138896
PROGRESS: pass 1, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.040): 0.032*"album" + 0.029*"song" + 0.023*"music" + 0.020*"band" + 0.013*"single" + 0.009*"perform" + 0.009*"track" + 0.007*"tour" + 0.006*"chart" + 0.006*"video"
topic #15 (0.040): 0.041*"game" + 0.025*"season" + 0.017*"basketball" + 0.013*"conference" + 0.012*"player" + 0.010*"yard" + 0.008*"point" + 0.007*"finish" + 0.007*"college" + 0.007*"field"
topic #12 (0.040): 0.081*"station" + 0.020*"line" + 0.018*"radio" + 0.011*"operate" + 0.011*"broadcast" + 0.010*"network" + 0.009*"air" + 0.009*"channel" + 0.008*"platform" + 0.008*"train"
topic #2 (0.040): 0.035*"season" + 0.023*"club" + 0.018*"league" + 0.016*"football" + 0.016*"game" + 0.014*"match" + 0.014*"championship" + 0.013*"final" + 0.012*"player" + 0.010*"score"
topic #5 (0.040): 0.031*"woman" + 0.006*"wine" + 0.006*"life" + 0.004*"temple" + 0.004*"practice" + 0.004*"say" + 0.004*"death" + 0.004*"hospital" + 0.003*"white" + 0.003*"buddhist"
topic diff=0.300548, rho=0.138896
PROGRESS: pass 1, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.041*"game" + 0.026*"season" + 0.018*"basketball" + 0.013*"conference" + 0.013*"player" + 0.009*"yard" + 0.008*"college" + 0.008*"point" + 0.007*"finish" + 0.006*"field"
topic #14 (0.040): 0.010*"population" + 0.008*"age" + 0.008*"specie" + 0.008*"river" + 0.007*"village" + 0.006*"town" + 0.006*"water" + 0.006*"park" + 0.005*"male" + 0.005*"county"
topic #12 (0.040): 0.080*"station" + 0.019*"line" + 0.017*"radio" + 0.012*"operate" + 0.011*"network" + 0.011*"broadcast" + 0.010*"channel" + 0.010*"air" + 0.008*"platform" + 0.008*"train"
topic #3 (0.040): 0.011*"system" + 0.009*"design" + 0.006*"power" + 0.006*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.004*"type" + 0.004*"vehicle" + 0.004*"low"
topic #19 (0.040): 0.026*"building" + 0.016*"house" + 0.012*"church" + 0.010*"design" + 0.007*"site" + 0.006*"th_century" + 0.006*"stone" + 0.006*"wall" + 0.005*"tower" + 0.005*"side"
topic diff=0.289295, rho=0.138896
-8.328 per-word bound, 321.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.011*"government" + 0.010*"law" + 0.010*"election" + 0.010*"party" + 0.008*"elect" + 0.008*"court" + 0.005*"political" + 0.005*"vote" + 0.005*"act" + 0.005*"president"
topic #20 (0.040): 0.022*"book" + 0.018*"publish" + 0.007*"author" + 0.007*"language" + 0.006*"study" + 0.005*"magazine" + 0.005*"history" + 0.005*"writer" + 0.005*"life" + 0.005*"editor"
topic #13 (0.040): 0.018*"german" + 0.011*"war" + 0.010*"government" + 0.010*"military" + 0.006*"department" + 0.006*"force" + 0.006*"chief" + 0.005*"general" + 0.005*"operation" + 0.005*"soviet"
topic #1 (0.040): 0.087*"film" + 0.015*"star" + 0.011*"direct" + 0.009*"award" + 0.008*"story" + 0.007*"movie" + 0.007*"director" + 0.006*"production" + 0.006*"role" + 0.006*"actor"
topic #12 (0.040): 0.079*"station" + 0.019*"line" + 0.018*"radio" + 0.012*"operate" + 0.011*"network" + 0.011*"broadcast" + 0.010*"channel" + 0.009*"air" + 0.009*"platform" + 0.008*"train"
topic diff=0.286181, rho=0.138896
-8.328 per-word bound, 321.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #11 (0.040): 0.038*"game" + 0.014*"player" + 0.009*"character" + 0.008*"version" + 0.005*"video" + 0.005*"series" + 0.004*"allow" + 0.004*"code" + 0.004*"file" + 0.004*"power"
topic #0 (0.040): 0.011*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.007*"define" + 0.006*"term" + 0.006*"case" + 0.006*"point" + 0.006*"language" + 0.005*"word" + 0.005*"different"
topic #13 (0.040): 0.017*"german" + 0.012*"war" + 0.011*"government" + 0.010*"military" + 0.006*"force" + 0.006*"department" + 0.006*"chief" + 0.006*"soviet" + 0.005*"operation" + 0.005*"general"
topic #17 (0.040): 0.006*"provide" + 0.006*"information" + 0.006*"increase" + 0.005*"system" + 0.005*"datum" + 0.005*"report" + 0.004*"social" + 0.004*"process" + 0.004*"individual" + 0.004*"need"
topic #22 (0.040): 0.020*"road" + 0.013*"route" + 0.012*"line" + 0.011*"north" + 0.010*"street" + 0.010*"railway" + 0.009*"county" + 0.008*"park" + 0.008*"bridge" + 0.008*"highway"
topic diff=0.279635, rho=0.138896
-8.323 per-word bound, 320.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #19 (0.040): 0.027*"building" + 0.015*"house" + 0.010*"design" + 0.009*"church" + 0.008*"site" + 0.008*"stone" + 0.007*"wall" + 0.006*"room" + 0.006*"th_century" + 0.005*"monument"
topic #10 (0.040): 0.012*"king" + 0.008*"son" + 0.006*"cause" + 0.005*"death" + 0.004*"prince" + 0.004*"kingdom" + 0.004*"disease" + 0.004*"child" + 0.004*"increase" + 0.004*"emperor"
topic #0 (0.040): 0.014*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.007*"define" + 0.006*"case" + 0.005*"language" + 0.005*"term" + 0.005*"point" + 0.005*"word" + 0.005*"system"
topic #1 (0.040): 0.091*"film" + 0.017*"star" + 0.012*"direct" + 0.009*"award" + 0.008*"story" + 0.008*"movie" + 0.007*"director" + 0.006*"production" + 0.006*"role" + 0.006*"actor"
topic #5 (0.040): 0.029*"woman" + 0.011*"flag" + 0.006*"life" + 0.004*"wine" + 0.004*"temple" + 0.004*"white" + 0.004*"ritual" + 0.004*"practice" + 0.004*"say" + 0.003*"tradition"
topic diff=0.284839, rho=0.138896
-8.132 per-word bound, 280.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.040): 0.020*"road" + 0.012*"route" + 0.012*"line" + 0.011*"north" + 0.011*"street" + 0.010*"park" + 0.009*"railway" + 0.009*"highway" + 0.009*"bridge" + 0.008*"county"
topic #15 (0.040): 0.045*"game" + 0.030*"season" + 0.016*"basketball" + 0.014*"player" + 0.012*"yard" + 0.011*"conference" + 0.007*"point" + 0.007*"field" + 0.007*"finish" + 0.007*"college"
topic #23 (0.040): 0.012*"government" + 0.011*"election" + 0.010*"law" + 0.010*"party" + 0.008*"elect" + 0.008*"court" + 0.006*"vote" + 0.005*"political" + 0.005*"president" + 0.005*"act"
topic #1 (0.040): 0.091*"film" + 0.017*"star" + 0.012*"direct" + 0.009*"award" + 0.009*"movie" + 0.008*"story" + 0.007*"director" + 0.006*"production" + 0.006*"role" + 0.006*"festival"
topic #12 (0.040): 0.079*"station" + 0.020*"line" + 0.017*"radio" + 0.013*"network" + 0.012*"operate" + 0.011*"channel" + 0.011*"broadcast" + 0.010*"air" + 0.010*"platform" + 0.009*"airport"
topic diff=0.262247, rho=0.137575
PROGRESS: pass 2, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.015*"german" + 0.013*"war" + 0.011*"government" + 0.011*"military" + 0.007*"department" + 0.007*"operation" + 0.006*"polish" + 0.006*"force" + 0.006*"soviet" + 0.006*"chief"
topic #21 (0.040): 0.007*"tell" + 0.007*"say" + 0.006*"kill" + 0.006*"get" + 0.006*"death" + 0.005*"father" + 0.005*"back" + 0.005*"try" + 0.005*"wife" + 0.005*"son"
topic #19 (0.040): 0.029*"building" + 0.016*"house" + 0.010*"design" + 0.009*"church" + 0.008*"site" + 0.007*"stone" + 0.007*"wall" + 0.006*"room" + 0.006*"th_century" + 0.005*"floor"
topic #12 (0.040): 0.080*"station" + 0.019*"line" + 0.018*"radio" + 0.013*"network" + 0.012*"channel" + 0.011*"operate" + 0.011*"broadcast" + 0.010*"air" + 0.010*"platform" + 0.009*"train"
topic #2 (0.040): 0.033*"season" + 0.023*"club" + 0.018*"league" + 0.016*"game" + 0.015*"match" + 0.015*"football" + 0.013*"final" + 0.013*"championship" + 0.012*"player" + 0.010*"score"
topic diff=0.252491, rho=0.137575
PROGRESS: pass 2, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.007*"say" + 0.007*"tell" + 0.006*"kill" + 0.006*"death" + 0.006*"get" + 0.006*"father" + 0.005*"back" + 0.005*"try" + 0.005*"son" + 0.005*"wife"
topic #1 (0.040): 0.090*"film" + 0.016*"star" + 0.012*"direct" + 0.009*"award" + 0.008*"story" + 0.008*"movie" + 0.007*"production" + 0.007*"director" + 0.006*"festival" + 0.006*"role"
topic #19 (0.040): 0.029*"building" + 0.016*"house" + 0.010*"design" + 0.010*"church" + 0.008*"site" + 0.007*"stone" + 0.007*"wall" + 0.006*"room" + 0.006*"th_century" + 0.005*"floor"
topic #12 (0.040): 0.080*"station" + 0.019*"line" + 0.018*"radio" + 0.013*"network" + 0.012*"channel" + 0.012*"operate" + 0.011*"broadcast" + 0.011*"air" + 0.010*"platform" + 0.009*"airport"
topic #24 (0.040): 0.017*"student" + 0.017*"college" + 0.016*"university" + 0.013*"award" + 0.011*"study" + 0.011*"research" + 0.010*"education" + 0.010*"science" + 0.009*"program" + 0.007*"graduate"
topic diff=0.238032, rho=0.137575
PROGRESS: pass 2, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.040): 0.091*"film" + 0.017*"star" + 0.012*"direct" + 0.009*"award" + 0.009*"story" + 0.008*"movie" + 0.007*"production" + 0.007*"director" + 0.006*"role" + 0.006*"actor"
topic #2 (0.040): 0.033*"season" + 0.024*"club" + 0.018*"league" + 0.016*"match" + 0.015*"game" + 0.014*"football" + 0.014*"championship" + 0.014*"final" + 0.012*"player" + 0.010*"round"
topic #18 (0.040): 0.037*"company" + 0.020*"village" + 0.017*"population" + 0.012*"municipality" + 0.011*"business" + 0.008*"found" + 0.007*"industry" + 0.007*"census" + 0.007*"country" + 0.006*"government"
topic #6 (0.040): 0.027*"ship" + 0.020*"island" + 0.014*"specie" + 0.010*"genus" + 0.009*"sea" + 0.008*"vessel" + 0.007*"navy" + 0.007*"crew" + 0.007*"boat" + 0.007*"port"
topic #16 (0.040): 0.035*"race" + 0.029*"event" + 0.016*"compete" + 0.012*"championship" + 0.009*"cell" + 0.007*"athlete" + 0.007*"metre" + 0.006*"finish" + 0.006*"olympic" + 0.006*"car"
topic diff=0.238773, rho=0.137575
PROGRESS: pass 2, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.014*"displaystyle" + 0.008*"example" + 0.007*"define" + 0.007*"function" + 0.006*"case" + 0.006*"term" + 0.005*"point" + 0.005*"language" + 0.005*"word" + 0.005*"system"
topic #5 (0.040): 0.029*"woman" + 0.007*"flag" + 0.006*"life" + 0.005*"temple" + 0.004*"practice" + 0.004*"buddhist" + 0.004*"tradition" + 0.004*"white" + 0.004*"wine" + 0.004*"death"
topic #17 (0.040): 0.007*"information" + 0.007*"provide" + 0.006*"system" + 0.006*"increase" + 0.005*"datum" + 0.005*"social" + 0.004*"individual" + 0.004*"report" + 0.004*"need" + 0.004*"process"
topic #7 (0.040): 0.025*"art" + 0.017*"church" + 0.013*"artist" + 0.011*"museum" + 0.010*"painting" + 0.007*"exhibition" + 0.006*"bishop" + 0.006*"festival" + 0.006*"paint" + 0.005*"study"
topic #16 (0.040): 0.035*"race" + 0.030*"event" + 0.017*"compete" + 0.012*"championship" + 0.009*"cell" + 0.007*"athlete" + 0.007*"finish" + 0.006*"olympic" + 0.006*"metre" + 0.006*"driver"
topic diff=0.226671, rho=0.137575
PROGRESS: pass 2, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.019*"series" + 0.013*"episode" + 0.010*"television" + 0.007*"character" + 0.007*"appear" + 0.005*"tv" + 0.005*"say" + 0.005*"role" + 0.005*"story" + 0.005*"novel"
topic #12 (0.040): 0.079*"station" + 0.021*"radio" + 0.019*"line" + 0.013*"network" + 0.012*"channel" + 0.012*"broadcast" + 0.012*"operate" + 0.010*"air" + 0.009*"platform" + 0.009*"train"
topic #7 (0.040): 0.025*"art" + 0.018*"church" + 0.013*"artist" + 0.011*"museum" + 0.010*"painting" + 0.007*"exhibition" + 0.006*"bishop" + 0.006*"festival" + 0.006*"paint" + 0.005*"study"
topic #18 (0.040): 0.038*"company" + 0.021*"village" + 0.018*"population" + 0.013*"municipality" + 0.011*"business" + 0.008*"found" + 0.008*"industry" + 0.007*"census" + 0.007*"country" + 0.006*"government"
topic #22 (0.040): 0.022*"road" + 0.015*"route" + 0.013*"line" + 0.011*"north" + 0.011*"park" + 0.011*"street" + 0.010*"highway" + 0.010*"railway" + 0.010*"county" + 0.009*"bridge"
topic diff=0.227746, rho=0.137575
PROGRESS: pass 2, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.010*"system" + 0.009*"design" + 0.007*"model" + 0.007*"engine" + 0.006*"power" + 0.006*"aircraft" + 0.006*"vehicle" + 0.005*"produce" + 0.004*"type" + 0.004*"car"
topic #6 (0.040): 0.028*"ship" + 0.021*"island" + 0.013*"specie" + 0.009*"genus" + 0.009*"sea" + 0.008*"navy" + 0.008*"boat" + 0.007*"vessel" + 0.007*"crew" + 0.007*"port"
topic #14 (0.040): 0.010*"population" + 0.010*"specie" + 0.010*"river" + 0.009*"age" + 0.007*"town" + 0.007*"water" + 0.006*"village" + 0.006*"male" + 0.006*"park" + 0.006*"region"
topic #0 (0.040): 0.013*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.006*"define" + 0.006*"case" + 0.006*"term" + 0.005*"point" + 0.005*"language" + 0.005*"system" + 0.004*"type"
topic #18 (0.040): 0.038*"company" + 0.022*"village" + 0.018*"population" + 0.013*"municipality" + 0.012*"business" + 0.008*"found" + 0.008*"industry" + 0.007*"census" + 0.007*"town" + 0.006*"country"
topic diff=0.213092, rho=0.137575
PROGRESS: pass 2, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.044*"game" + 0.033*"season" + 0.017*"basketball" + 0.013*"player" + 0.012*"conference" + 0.011*"football" + 0.010*"yard" + 0.009*"college" + 0.009*"point" + 0.009*"finish"
topic #2 (0.040): 0.032*"season" + 0.025*"club" + 0.019*"league" + 0.015*"game" + 0.015*"match" + 0.014*"football" + 0.014*"championship" + 0.014*"final" + 0.013*"player" + 0.010*"score"
topic #8 (0.040): 0.032*"album" + 0.031*"song" + 0.026*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #10 (0.040): 0.013*"king" + 0.009*"son" + 0.007*"cause" + 0.006*"death" + 0.005*"kingdom" + 0.005*"prince" + 0.004*"disease" + 0.004*"emperor" + 0.004*"child" + 0.004*"french"
topic #5 (0.040): 0.030*"woman" + 0.006*"life" + 0.006*"flag" + 0.005*"temple" + 0.004*"practice" + 0.004*"tradition" + 0.004*"buddhist" + 0.004*"wine" + 0.004*"accord" + 0.004*"wear"
topic diff=0.205071, rho=0.137575
PROGRESS: pass 2, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.013*"election" + 0.012*"government" + 0.011*"law" + 0.011*"party" + 0.009*"elect" + 0.008*"court" + 0.007*"vote" + 0.006*"political" + 0.005*"president" + 0.005*"act"
topic #17 (0.040): 0.007*"provide" + 0.006*"information" + 0.006*"system" + 0.005*"increase" + 0.005*"datum" + 0.004*"social" + 0.004*"process" + 0.004*"report" + 0.004*"individual" + 0.004*"need"
topic #4 (0.040): 0.019*"army" + 0.016*"war" + 0.015*"force" + 0.013*"battle" + 0.010*"attack" + 0.008*"command" + 0.008*"military" + 0.008*"troop" + 0.007*"order" + 0.007*"unit"
topic #12 (0.040): 0.079*"station" + 0.024*"radio" + 0.018*"line" + 0.014*"network" + 0.013*"broadcast" + 0.013*"channel" + 0.012*"operate" + 0.011*"air" + 0.010*"platform" + 0.010*"train"
topic #18 (0.040): 0.038*"company" + 0.022*"village" + 0.018*"population" + 0.013*"municipality" + 0.012*"business" + 0.008*"found" + 0.008*"industry" + 0.007*"census" + 0.007*"town" + 0.006*"region"
topic diff=0.198045, rho=0.137575
PROGRESS: pass 2, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.013*"election" + 0.012*"government" + 0.012*"party" + 0.011*"law" + 0.009*"elect" + 0.007*"court" + 0.007*"vote" + 0.006*"political" + 0.005*"act" + 0.005*"president"
topic #18 (0.040): 0.038*"company" + 0.022*"village" + 0.018*"population" + 0.013*"municipality" + 0.012*"business" + 0.008*"found" + 0.007*"industry" + 0.007*"census" + 0.007*"town" + 0.007*"region"
topic #20 (0.040): 0.023*"book" + 0.020*"publish" + 0.007*"author" + 0.007*"language" + 0.006*"magazine" + 0.005*"history" + 0.005*"writer" + 0.005*"life" + 0.005*"study" + 0.005*"editor"
topic #12 (0.040): 0.082*"station" + 0.023*"radio" + 0.019*"line" + 0.014*"network" + 0.013*"channel" + 0.013*"broadcast" + 0.012*"operate" + 0.012*"air" + 0.010*"train" + 0.010*"platform"
topic #14 (0.040): 0.011*"river" + 0.010*"specie" + 0.010*"population" + 0.009*"age" + 0.007*"water" + 0.006*"town" + 0.006*"region" + 0.006*"village" + 0.006*"mountain" + 0.006*"plant"
topic diff=0.195503, rho=0.137575
PROGRESS: pass 2, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.031*"woman" + 0.006*"life" + 0.006*"temple" + 0.005*"wine" + 0.005*"flag" + 0.005*"practice" + 0.005*"wear" + 0.004*"accord" + 0.004*"tradition" + 0.004*"religious"
topic #18 (0.040): 0.038*"company" + 0.022*"village" + 0.018*"population" + 0.013*"municipality" + 0.012*"business" + 0.008*"found" + 0.007*"industry" + 0.007*"census" + 0.007*"town" + 0.007*"region"
topic #20 (0.040): 0.024*"book" + 0.019*"publish" + 0.007*"author" + 0.007*"language" + 0.006*"magazine" + 0.005*"history" + 0.005*"life" + 0.005*"writer" + 0.005*"study" + 0.005*"editor"
topic #2 (0.040): 0.032*"season" + 0.026*"club" + 0.020*"league" + 0.015*"match" + 0.015*"game" + 0.014*"football" + 0.014*"final" + 0.014*"championship" + 0.012*"player" + 0.010*"score"
topic #4 (0.040): 0.019*"army" + 0.016*"war" + 0.015*"force" + 0.013*"battle" + 0.011*"attack" + 0.009*"command" + 0.008*"military" + 0.008*"troop" + 0.007*"order" + 0.006*"unit"
topic diff=0.194907, rho=0.137575
PROGRESS: pass 2, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 10
PROGRESS: pass 2, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.040): 0.013*"king" + 0.009*"son" + 0.007*"cause" + 0.006*"death" + 0.006*"disease" + 0.005*"kingdom" + 0.004*"prince" + 0.004*"increase" + 0.004*"emperor" + 0.004*"reign"
topic #15 (0.040): 0.044*"game" + 0.036*"season" + 0.015*"basketball" + 0.013*"football" + 0.013*"conference" + 0.013*"player" + 0.010*"yard" + 0.010*"college" + 0.009*"coach" + 0.008*"finish"
topic #4 (0.040): 0.019*"army" + 0.016*"war" + 0.015*"force" + 0.013*"battle" + 0.011*"attack" + 0.009*"command" + 0.008*"troop" + 0.008*"military" + 0.007*"order" + 0.007*"unit"
topic #23 (0.040): 0.013*"election" + 0.012*"government" + 0.011*"party" + 0.011*"law" + 0.009*"elect" + 0.007*"vote" + 0.007*"court" + 0.006*"political" + 0.005*"act" + 0.005*"president"
topic #16 (0.040): 0.038*"race" + 0.032*"event" + 0.019*"compete" + 0.014*"championship" + 0.008*"cell" + 0.008*"finish" + 0.007*"woman" + 0.007*"metre" + 0.007*"olympic" + 0.007*"athlete"
topic diff=0.185743, rho=0.137575
PROGRESS: pass 2, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"get" + 0.006*"father" + 0.006*"death" + 0.005*"back" + 0.005*"try" + 0.005*"son" + 0.005*"wife"
topic #1 (0.040): 0.099*"film" + 0.020*"star" + 0.014*"direct" + 0.011*"award" + 0.009*"movie" + 0.009*"story" + 0.008*"director" + 0.008*"production" + 0.008*"actor" + 0.007*"role"
topic #4 (0.040): 0.019*"army" + 0.017*"war" + 0.015*"force" + 0.014*"battle" + 0.011*"attack" + 0.009*"command" + 0.008*"military" + 0.008*"troop" + 0.007*"order" + 0.006*"fight"
topic #11 (0.040): 0.039*"game" + 0.014*"player" + 0.010*"character" + 0.009*"version" + 0.006*"video" + 0.005*"series" + 0.005*"power" + 0.005*"computer" + 0.005*"code" + 0.004*"allow"
topic #5 (0.040): 0.032*"woman" + 0.006*"temple" + 0.006*"wine" + 0.006*"life" + 0.005*"practice" + 0.004*"wear" + 0.004*"tradition" + 0.004*"accord" + 0.004*"flag" + 0.004*"white"
topic diff=0.176011, rho=0.137575
PROGRESS: pass 2, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.040): 0.018*"student" + 0.017*"university" + 0.017*"college" + 0.015*"award" + 0.012*"study" + 0.011*"research" + 0.010*"science" + 0.010*"education" + 0.009*"program" + 0.009*"graduate"
topic #23 (0.040): 0.013*"election" + 0.012*"government" + 0.012*"party" + 0.011*"law" + 0.009*"elect" + 0.008*"court" + 0.007*"vote" + 0.006*"political" + 0.005*"president" + 0.005*"act"
topic #21 (0.040): 0.008*"say" + 0.007*"kill" + 0.007*"tell" + 0.006*"get" + 0.006*"death" + 0.006*"father" + 0.005*"back" + 0.005*"try" + 0.005*"wife" + 0.005*"son"
topic #9 (0.040): 0.021*"series" + 0.015*"episode" + 0.010*"television" + 0.008*"character" + 0.007*"appear" + 0.005*"say" + 0.005*"role" + 0.005*"story" + 0.005*"season" + 0.005*"tv"
topic #16 (0.040): 0.039*"race" + 0.032*"event" + 0.019*"compete" + 0.014*"championship" + 0.009*"cell" + 0.009*"finish" + 0.008*"athlete" + 0.007*"sport" + 0.007*"woman" + 0.007*"driver"
topic diff=0.167198, rho=0.137575
-8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.011*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.006*"define" + 0.006*"term" + 0.006*"case" + 0.006*"point" + 0.005*"language" + 0.005*"system" + 0.005*"different"
topic #2 (0.040): 0.032*"season" + 0.027*"club" + 0.020*"league" + 0.016*"match" + 0.015*"game" + 0.014*"football" + 0.014*"final" + 0.014*"championship" + 0.013*"player" + 0.010*"score"
topic #4 (0.040): 0.020*"army" + 0.016*"war" + 0.015*"force" + 0.013*"battle" + 0.012*"attack" + 0.009*"command" + 0.008*"division" + 0.008*"military" + 0.008*"troop" + 0.007*"order"
topic #15 (0.040): 0.043*"game" + 0.039*"season" + 0.017*"basketball" + 0.015*"football" + 0.013*"player" + 0.013*"conference" + 0.011*"coach" + 0.010*"college" + 0.009*"finish" + 0.008*"yard"
topic #10 (0.040): 0.013*"king" + 0.009*"son" + 0.008*"cause" + 0.006*"disease" + 0.006*"death" + 0.005*"kingdom" + 0.004*"emperor" + 0.004*"treatment" + 0.004*"prince" + 0.004*"patient"
topic diff=0.165429, rho=0.137575
-8.233 per-word bound, 300.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #6 (0.040): 0.029*"ship" + 0.022*"island" + 0.012*"specie" + 0.010*"sea" + 0.009*"genus" + 0.008*"navy" + 0.008*"vessel" + 0.008*"port" + 0.008*"boat" + 0.008*"crew"
topic #11 (0.040): 0.040*"game" + 0.015*"player" + 0.009*"version" + 0.009*"character" + 0.006*"video" + 0.006*"code" + 0.005*"series" + 0.005*"computer" + 0.005*"file" + 0.005*"power"
topic #23 (0.040): 0.014*"election" + 0.013*"government" + 0.012*"party" + 0.011*"law" + 0.009*"elect" + 0.008*"court" + 0.008*"vote" + 0.006*"political" + 0.005*"president" + 0.005*"act"
topic #12 (0.040): 0.078*"station" + 0.021*"radio" + 0.018*"line" + 0.016*"network" + 0.014*"channel" + 0.013*"broadcast" + 0.012*"operate" + 0.012*"air" + 0.010*"airport" + 0.009*"platform"
topic #22 (0.040): 0.022*"road" + 0.014*"route" + 0.013*"line" + 0.013*"north" + 0.012*"park" + 0.011*"county" + 0.011*"street" + 0.010*"railway" + 0.010*"south" + 0.009*"highway"
topic diff=0.159110, rho=0.137575
-8.196 per-word bound, 293.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.040): 0.102*"film" + 0.019*"star" + 0.014*"direct" + 0.012*"award" + 0.010*"movie" + 0.009*"story" + 0.008*"production" + 0.008*"role" + 0.008*"director" + 0.007*"actor"
topic #6 (0.040): 0.028*"ship" + 0.023*"island" + 0.012*"specie" + 0.010*"sea" + 0.009*"genus" + 0.008*"navy" + 0.008*"port" + 0.008*"vessel" + 0.008*"boat" + 0.007*"crew"
topic #3 (0.040): 0.011*"system" + 0.010*"design" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"aircraft" + 0.005*"produce" + 0.005*"vehicle" + 0.004*"type" + 0.004*"car"
topic #5 (0.040): 0.030*"woman" + 0.006*"temple" + 0.006*"life" + 0.005*"wine" + 0.005*"flag" + 0.005*"wear" + 0.005*"tradition" + 0.004*"practice" + 0.004*"accord" + 0.004*"religious"
topic #22 (0.040): 0.022*"road" + 0.013*"route" + 0.013*"park" + 0.013*"north" + 0.013*"line" + 0.011*"county" + 0.011*"street" + 0.010*"railway" + 0.010*"bridge" + 0.010*"south"
topic diff=0.154593, rho=0.136291
PROGRESS: pass 3, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.011*"system" + 0.010*"design" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.006*"aircraft" + 0.005*"produce" + 0.005*"vehicle" + 0.004*"type" + 0.004*"control"
topic #23 (0.040): 0.014*"election" + 0.013*"government" + 0.012*"party" + 0.011*"law" + 0.010*"elect" + 0.008*"court" + 0.008*"vote" + 0.006*"political" + 0.005*"president" + 0.005*"act"
topic #20 (0.040): 0.023*"book" + 0.020*"publish" + 0.008*"language" + 0.007*"author" + 0.006*"history" + 0.006*"writer" + 0.005*"magazine" + 0.005*"life" + 0.005*"study" + 0.005*"novel"
topic #0 (0.040): 0.013*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.007*"define" + 0.006*"case" + 0.006*"point" + 0.006*"term" + 0.005*"language" + 0.005*"system" + 0.005*"word"
topic #1 (0.040): 0.101*"film" + 0.019*"star" + 0.014*"direct" + 0.012*"award" + 0.010*"movie" + 0.009*"story" + 0.009*"production" + 0.008*"role" + 0.008*"director" + 0.007*"actor"
topic diff=0.148930, rho=0.136291
PROGRESS: pass 3, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.040): 0.079*"station" + 0.021*"radio" + 0.016*"network" + 0.016*"line" + 0.015*"channel" + 0.013*"broadcast" + 0.013*"air" + 0.012*"operate" + 0.010*"airport" + 0.009*"platform"
topic #8 (0.040): 0.033*"album" + 0.032*"song" + 0.028*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #13 (0.040): 0.018*"german" + 0.014*"war" + 0.013*"government" + 0.012*"military" + 0.008*"russian" + 0.007*"country" + 0.007*"operation" + 0.007*"polish" + 0.007*"force" + 0.007*"soviet"
topic #9 (0.040): 0.022*"series" + 0.016*"episode" + 0.011*"television" + 0.008*"character" + 0.007*"appear" + 0.005*"say" + 0.005*"season" + 0.005*"tv" + 0.005*"story" + 0.005*"role"
topic #3 (0.040): 0.010*"system" + 0.010*"design" + 0.007*"engine" + 0.006*"model" + 0.006*"power" + 0.006*"aircraft" + 0.005*"produce" + 0.005*"vehicle" + 0.005*"type" + 0.004*"car"
topic diff=0.139195, rho=0.136291
PROGRESS: pass 3, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.040): 0.079*"station" + 0.023*"radio" + 0.016*"network" + 0.016*"line" + 0.015*"channel" + 0.013*"broadcast" + 0.013*"air" + 0.012*"operate" + 0.010*"airport" + 0.009*"platform"
topic #17 (0.040): 0.007*"provide" + 0.006*"information" + 0.006*"system" + 0.006*"increase" + 0.005*"report" + 0.005*"social" + 0.005*"datum" + 0.004*"need" + 0.004*"individual" + 0.004*"process"
topic #22 (0.040): 0.022*"road" + 0.014*"route" + 0.013*"park" + 0.013*"line" + 0.013*"north" + 0.013*"county" + 0.011*"street" + 0.011*"railway" + 0.010*"town" + 0.010*"south"
topic #2 (0.040): 0.031*"season" + 0.026*"club" + 0.020*"league" + 0.017*"match" + 0.014*"game" + 0.014*"final" + 0.014*"championship" + 0.013*"player" + 0.013*"football" + 0.011*"round"
topic #8 (0.040): 0.033*"album" + 0.032*"song" + 0.028*"music" + 0.023*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic diff=0.142538, rho=0.136291
PROGRESS: pass 3, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.044*"game" + 0.041*"season" + 0.017*"basketball" + 0.015*"football" + 0.012*"player" + 0.011*"conference" + 0.011*"coach" + 0.010*"college" + 0.009*"yard" + 0.009*"finish"
topic #16 (0.040): 0.039*"race" + 0.035*"event" + 0.021*"compete" + 0.016*"championship" + 0.010*"finish" + 0.009*"athlete" + 0.008*"olympic" + 0.008*"woman" + 0.008*"sport" + 0.007*"cell"
topic #20 (0.040): 0.023*"book" + 0.020*"publish" + 0.007*"language" + 0.007*"author" + 0.006*"writer" + 0.006*"history" + 0.006*"life" + 0.006*"magazine" + 0.005*"novel" + 0.005*"study"
topic #9 (0.040): 0.022*"series" + 0.015*"episode" + 0.011*"television" + 0.008*"character" + 0.008*"appear" + 0.006*"season" + 0.005*"say" + 0.005*"story" + 0.005*"role" + 0.005*"tv"
topic #5 (0.040): 0.029*"woman" + 0.006*"life" + 0.006*"temple" + 0.005*"tradition" + 0.005*"practice" + 0.005*"accord" + 0.004*"flag" + 0.004*"buddhist" + 0.004*"religious" + 0.004*"white"
topic diff=0.134316, rho=0.136291
PROGRESS: pass 3, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.040): 0.020*"army" + 0.017*"war" + 0.016*"force" + 0.013*"battle" + 0.011*"attack" + 0.009*"command" + 0.009*"military" + 0.008*"troop" + 0.007*"order" + 0.007*"division"
topic #8 (0.040): 0.033*"album" + 0.033*"song" + 0.029*"music" + 0.021*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #21 (0.040): 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"father" + 0.006*"get" + 0.006*"death" + 0.005*"back" + 0.005*"son" + 0.005*"wife" + 0.005*"try"
topic #5 (0.040): 0.027*"woman" + 0.006*"life" + 0.006*"temple" + 0.005*"tradition" + 0.005*"accord" + 0.005*"practice" + 0.004*"buddhist" + 0.004*"religious" + 0.004*"wine" + 0.004*"white"
topic #0 (0.040): 0.013*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"define" + 0.006*"case" + 0.006*"term" + 0.005*"point" + 0.005*"system" + 0.005*"different" + 0.005*"language"
topic diff=0.136403, rho=0.136291
PROGRESS: pass 3, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"define" + 0.006*"case" + 0.006*"term" + 0.005*"point" + 0.005*"system" + 0.005*"type" + 0.005*"structure"
topic #14 (0.040): 0.012*"specie" + 0.010*"river" + 0.010*"age" + 0.009*"population" + 0.007*"water" + 0.006*"male" + 0.006*"plant" + 0.006*"town" + 0.006*"mountain" + 0.005*"region"
topic #5 (0.040): 0.027*"woman" + 0.006*"temple" + 0.006*"life" + 0.005*"tradition" + 0.005*"accord" + 0.005*"practice" + 0.004*"flag" + 0.004*"buddhist" + 0.004*"white" + 0.004*"religious"
topic #23 (0.040): 0.014*"election" + 0.013*"government" + 0.012*"party" + 0.011*"law" + 0.009*"elect" + 0.008*"court" + 0.008*"vote" + 0.007*"political" + 0.006*"president" + 0.006*"seat"
topic #4 (0.040): 0.020*"army" + 0.018*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.009*"command" + 0.009*"military" + 0.008*"troop" + 0.007*"order" + 0.007*"division"
topic diff=0.126347, rho=0.136291
PROGRESS: pass 3, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.040): 0.015*"king" + 0.010*"son" + 0.008*"cause" + 0.006*"death" + 0.006*"disease" + 0.006*"kingdom" + 0.005*"prince" + 0.005*"emperor" + 0.004*"treatment" + 0.004*"patient"
topic #21 (0.040): 0.008*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"father" + 0.006*"get" + 0.006*"death" + 0.005*"back" + 0.005*"son" + 0.005*"try" + 0.005*"wife"
topic #19 (0.040): 0.032*"building" + 0.018*"house" + 0.011*"church" + 0.011*"design" + 0.010*"site" + 0.007*"th_century" + 0.006*"wall" + 0.006*"stone" + 0.006*"room" + 0.006*"tower"
topic #15 (0.040): 0.045*"game" + 0.041*"season" + 0.016*"basketball" + 0.016*"football" + 0.013*"player" + 0.012*"conference" + 0.011*"coach" + 0.010*"college" + 0.009*"finish" + 0.009*"point"
topic #23 (0.040): 0.014*"election" + 0.013*"government" + 0.012*"party" + 0.011*"law" + 0.010*"elect" + 0.008*"court" + 0.008*"vote" + 0.007*"political" + 0.006*"act" + 0.006*"seat"
topic diff=0.121419, rho=0.136291
PROGRESS: pass 3, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.039*"company" + 0.025*"village" + 0.021*"population" + 0.014*"municipality" + 0.013*"business" + 0.009*"found" + 0.008*"town" + 0.008*"census" + 0.008*"industry" + 0.008*"region"
topic #12 (0.040): 0.078*"station" + 0.026*"radio" + 0.016*"network" + 0.015*"channel" + 0.015*"line" + 0.014*"broadcast" + 0.013*"air" + 0.012*"operate" + 0.010*"platform" + 0.010*"airport"
topic #10 (0.040): 0.014*"king" + 0.010*"son" + 0.008*"cause" + 0.006*"death" + 0.006*"disease" + 0.005*"kingdom" + 0.005*"prince" + 0.005*"emperor" + 0.004*"treatment" + 0.004*"patient"
topic #14 (0.040): 0.012*"river" + 0.012*"specie" + 0.010*"age" + 0.009*"population" + 0.008*"water" + 0.006*"plant" + 0.006*"mountain" + 0.006*"male" + 0.006*"region" + 0.005*"forest"
topic #24 (0.040): 0.019*"student" + 0.017*"college" + 0.016*"university" + 0.015*"award" + 0.013*"study" + 0.011*"research" + 0.010*"science" + 0.010*"education" + 0.009*"program" + 0.009*"graduate"
topic diff=0.119385, rho=0.136291
PROGRESS: pass 3, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.019*"german" + 0.013*"government" + 0.013*"war" + 0.012*"military" + 0.008*"russian" + 0.008*"country" + 0.008*"korean" + 0.007*"force" + 0.007*"operation" + 0.007*"soviet"
topic #1 (0.040): 0.102*"film" + 0.020*"star" + 0.015*"award" + 0.014*"direct" + 0.010*"movie" + 0.009*"production" + 0.009*"role" + 0.009*"story" + 0.008*"theatre" + 0.008*"director"
topic #23 (0.040): 0.015*"election" + 0.013*"government" + 0.013*"party" + 0.012*"law" + 0.010*"elect" + 0.008*"vote" + 0.008*"court" + 0.007*"political" + 0.006*"act" + 0.006*"president"
topic #19 (0.040): 0.032*"building" + 0.019*"house" + 0.011*"site" + 0.011*"design" + 0.011*"church" + 0.007*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.006*"construction"
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.007*"engine" + 0.007*"model" + 0.006*"power" + 0.006*"aircraft" + 0.005*"vehicle" + 0.005*"produce" + 0.005*"type" + 0.004*"low"
topic diff=0.117324, rho=0.136291
PROGRESS: pass 3, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.040): 0.082*"station" + 0.025*"radio" + 0.016*"network" + 0.016*"line" + 0.015*"channel" + 0.014*"broadcast" + 0.013*"air" + 0.012*"operate" + 0.010*"airport" + 0.010*"platform"
topic #2 (0.040): 0.030*"season" + 0.028*"club" + 0.022*"league" + 0.016*"match" + 0.015*"final" + 0.014*"game" + 0.014*"football" + 0.013*"championship" + 0.013*"player" + 0.010*"score"
topic #20 (0.040): 0.025*"book" + 0.020*"publish" + 0.008*"author" + 0.007*"language" + 0.006*"magazine" + 0.006*"history" + 0.006*"writer" + 0.006*"life" + 0.005*"novel" + 0.005*"editor"
topic #24 (0.040): 0.019*"student" + 0.017*"university" + 0.017*"college" + 0.015*"award" + 0.013*"study" + 0.012*"research" + 0.010*"education" + 0.010*"science" + 0.009*"program" + 0.009*"graduate"
topic #17 (0.040): 0.007*"provide" + 0.006*"increase" + 0.006*"system" + 0.006*"information" + 0.005*"social" + 0.005*"report" + 0.005*"individual" + 0.004*"process" + 0.004*"need" + 0.004*"datum"
topic diff=0.118830, rho=0.136291
PROGRESS: pass 3, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.044*"game" + 0.042*"season" + 0.017*"football" + 0.014*"basketball" + 0.012*"conference" + 0.012*"player" + 0.011*"coach" + 0.010*"college" + 0.009*"yard" + 0.009*"finish"
topic #23 (0.040): 0.015*"election" + 0.013*"government" + 0.013*"party" + 0.012*"law" + 0.009*"elect" + 0.009*"vote" + 0.008*"court" + 0.007*"political" + 0.006*"act" + 0.006*"candidate"
topic #11 (0.040): 0.042*"game" + 0.015*"player" + 0.010*"character" + 0.010*"version" + 0.006*"video" + 0.006*"computer" + 0.005*"series" + 0.005*"code" + 0.005*"card" + 0.005*"power"
topic #1 (0.040): 0.102*"film" + 0.022*"star" + 0.015*"direct" + 0.014*"award" + 0.011*"movie" + 0.009*"role" + 0.009*"production" + 0.009*"story" + 0.009*"actor" + 0.009*"theatre"
topic #5 (0.040): 0.029*"woman" + 0.006*"temple" + 0.006*"life" + 0.005*"practice" + 0.005*"wine" + 0.005*"wear" + 0.005*"accord" + 0.005*"tradition" + 0.004*"religious" + 0.004*"traditional"
topic diff=0.115139, rho=0.136291
PROGRESS: pass 3, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.040): 0.039*"game" + 0.014*"player" + 0.010*"character" + 0.009*"version" + 0.006*"video" + 0.006*"computer" + 0.005*"power" + 0.005*"series" + 0.005*"code" + 0.005*"card"
topic #22 (0.040): 0.022*"road" + 0.014*"route" + 0.014*"line" + 0.014*"park" + 0.014*"north" + 0.013*"county" + 0.011*"street" + 0.011*"railway" + 0.011*"town" + 0.010*"bridge"
topic #17 (0.040): 0.007*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.005*"report" + 0.005*"social" + 0.005*"datum" + 0.004*"individual" + 0.004*"need" + 0.004*"process"
topic #15 (0.040): 0.044*"game" + 0.043*"season" + 0.017*"football" + 0.015*"basketball" + 0.012*"conference" + 0.012*"player" + 0.012*"coach" + 0.010*"college" + 0.009*"finish" + 0.009*"point"
topic #14 (0.040): 0.013*"specie" + 0.010*"river" + 0.010*"age" + 0.009*"population" + 0.007*"water" + 0.006*"mountain" + 0.006*"plant" + 0.006*"male" + 0.006*"white" + 0.005*"region"
topic diff=0.108607, rho=0.136291
PROGRESS: pass 3, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.005*"try" + 0.005*"son" + 0.005*"child"
topic #17 (0.040): 0.007*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.005*"report" + 0.005*"social" + 0.005*"datum" + 0.004*"need" + 0.004*"individual" + 0.004*"require"
topic #3 (0.040): 0.010*"system" + 0.010*"design" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"type" + 0.005*"car" + 0.005*"light"
topic #16 (0.040): 0.042*"race" + 0.037*"event" + 0.023*"compete" + 0.018*"championship" + 0.012*"finish" + 0.010*"woman" + 0.010*"sport" + 0.009*"athlete" + 0.008*"olympic" + 0.008*"competition"
topic #11 (0.040): 0.041*"game" + 0.014*"player" + 0.010*"character" + 0.009*"version" + 0.006*"video" + 0.006*"computer" + 0.005*"series" + 0.005*"power" + 0.005*"code" + 0.005*"card"
topic diff=0.103217, rho=0.136291
-8.245 per-word bound, 303.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #23 (0.040): 0.014*"election" + 0.013*"government" + 0.013*"party" + 0.012*"law" + 0.010*"elect" + 0.009*"court" + 0.008*"vote" + 0.007*"political" + 0.006*"president" + 0.006*"act"
topic #6 (0.040): 0.030*"ship" + 0.025*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"specie" + 0.009*"navy" + 0.008*"crew" + 0.008*"boat" + 0.008*"coast" + 0.008*"vessel"
topic #24 (0.040): 0.020*"student" + 0.017*"college" + 0.017*"university" + 0.015*"award" + 0.013*"study" + 0.012*"research" + 0.010*"science" + 0.010*"education" + 0.009*"graduate" + 0.009*"program"
topic #12 (0.040): 0.077*"station" + 0.023*"radio" + 0.017*"network" + 0.016*"channel" + 0.014*"broadcast" + 0.014*"line" + 0.013*"air" + 0.012*"operate" + 0.011*"airport" + 0.011*"news"
topic #15 (0.040): 0.045*"game" + 0.043*"season" + 0.017*"football" + 0.016*"basketball" + 0.013*"player" + 0.013*"coach" + 0.012*"conference" + 0.010*"college" + 0.009*"finish" + 0.009*"point"
topic diff=0.100248, rho=0.136291
-8.248 per-word bound, 304.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #16 (0.040): 0.043*"race" + 0.038*"event" + 0.023*"compete" + 0.018*"championship" + 0.012*"finish" + 0.011*"woman" + 0.011*"sport" + 0.009*"athlete" + 0.008*"olympic" + 0.008*"competition"
topic #9 (0.040): 0.024*"series" + 0.017*"episode" + 0.010*"television" + 0.009*"character" + 0.008*"appear" + 0.006*"season" + 0.005*"say" + 0.005*"story" + 0.005*"role" + 0.005*"love"
topic #2 (0.040): 0.030*"season" + 0.029*"club" + 0.021*"league" + 0.017*"match" + 0.014*"final" + 0.014*"game" + 0.014*"player" + 0.013*"football" + 0.013*"championship" + 0.011*"score"
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.007*"define" + 0.006*"term" + 0.006*"case" + 0.006*"system" + 0.006*"point" + 0.005*"structure" + 0.005*"different"
topic #3 (0.040): 0.010*"system" + 0.010*"design" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"car" + 0.004*"type"
topic diff=0.101154, rho=0.136291
-8.196 per-word bound, 293.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.024*"series" + 0.017*"episode" + 0.010*"television" + 0.009*"character" + 0.008*"appear" + 0.006*"season" + 0.005*"say" + 0.005*"story" + 0.005*"role" + 0.005*"air"
topic #12 (0.040): 0.076*"station" + 0.022*"radio" + 0.018*"network" + 0.017*"channel" + 0.014*"broadcast" + 0.014*"line" + 0.013*"air" + 0.012*"operate" + 0.010*"airport" + 0.010*"news"
topic #4 (0.040): 0.020*"army" + 0.018*"war" + 0.015*"force" + 0.014*"battle" + 0.011*"attack" + 0.009*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic #22 (0.040): 0.023*"road" + 0.014*"park" + 0.014*"line" + 0.014*"north" + 0.014*"county" + 0.014*"route" + 0.011*"railway" + 0.011*"town" + 0.011*"street" + 0.010*"south"
topic #0 (0.040): 0.012*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.007*"define" + 0.006*"system" + 0.006*"term" + 0.006*"case" + 0.006*"point" + 0.005*"language" + 0.005*"structure"
topic diff=0.097162, rho=0.135043
PROGRESS: pass 4, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.027*"woman" + 0.006*"temple" + 0.005*"practice" + 0.005*"life" + 0.005*"accord" + 0.005*"tradition" + 0.005*"wine" + 0.005*"wear" + 0.004*"traditional" + 0.004*"religious"
topic #9 (0.040): 0.024*"series" + 0.017*"episode" + 0.011*"television" + 0.009*"character" + 0.008*"appear" + 0.006*"season" + 0.005*"say" + 0.005*"story" + 0.005*"role" + 0.005*"love"
topic #10 (0.040): 0.016*"king" + 0.010*"son" + 0.008*"cause" + 0.006*"death" + 0.006*"disease" + 0.006*"kingdom" + 0.005*"treatment" + 0.005*"patient" + 0.005*"prince" + 0.005*"emperor"
topic #19 (0.040): 0.033*"building" + 0.019*"house" + 0.011*"site" + 0.011*"design" + 0.010*"church" + 0.007*"stone" + 0.006*"wall" + 0.006*"th_century" + 0.006*"construction" + 0.006*"room"
topic #6 (0.040): 0.030*"ship" + 0.026*"island" + 0.011*"sea" + 0.009*"navy" + 0.008*"port" + 0.008*"crew" + 0.008*"specie" + 0.008*"boat" + 0.008*"coast" + 0.008*"vessel"
topic diff=0.095702, rho=0.135043
PROGRESS: pass 4, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.040): 0.077*"station" + 0.023*"radio" + 0.018*"network" + 0.016*"channel" + 0.014*"broadcast" + 0.014*"air" + 0.013*"line" + 0.012*"operate" + 0.011*"tv" + 0.010*"television"
topic #24 (0.040): 0.019*"student" + 0.018*"college" + 0.017*"university" + 0.014*"award" + 0.013*"study" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"program" + 0.009*"graduate"
topic #22 (0.040): 0.023*"road" + 0.015*"line" + 0.015*"park" + 0.015*"route" + 0.014*"county" + 0.013*"north" + 0.011*"railway" + 0.011*"town" + 0.011*"street" + 0.010*"south"
topic #8 (0.040): 0.034*"album" + 0.033*"song" + 0.029*"music" + 0.023*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #13 (0.040): 0.018*"german" + 0.014*"war" + 0.014*"government" + 0.012*"military" + 0.010*"russian" + 0.009*"country" + 0.007*"polish" + 0.007*"operation" + 0.007*"force" + 0.007*"soviet"
topic diff=0.089618, rho=0.135043
PROGRESS: pass 4, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.040): 0.029*"season" + 0.028*"club" + 0.021*"league" + 0.018*"match" + 0.014*"final" + 0.013*"game" + 0.013*"player" + 0.013*"championship" + 0.013*"football" + 0.011*"round"
topic #23 (0.040): 0.015*"election" + 0.013*"government" + 0.013*"party" + 0.012*"law" + 0.010*"elect" + 0.009*"court" + 0.009*"vote" + 0.006*"political" + 0.006*"act" + 0.006*"general"
topic #5 (0.040): 0.027*"woman" + 0.006*"temple" + 0.006*"life" + 0.005*"tradition" + 0.005*"accord" + 0.005*"practice" + 0.004*"wine" + 0.004*"traditional" + 0.004*"religious" + 0.004*"wear"
topic #0 (0.040): 0.013*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.007*"define" + 0.006*"term" + 0.006*"case" + 0.005*"point" + 0.005*"system" + 0.005*"structure" + 0.005*"different"
topic #17 (0.040): 0.007*"provide" + 0.006*"information" + 0.006*"system" + 0.006*"increase" + 0.005*"report" + 0.005*"social" + 0.005*"need" + 0.004*"datum" + 0.004*"individual" + 0.004*"require"
topic diff=0.093961, rho=0.135043
PROGRESS: pass 4, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.017*"german" + 0.014*"government" + 0.013*"war" + 0.012*"military" + 0.009*"russian" + 0.009*"korean" + 0.009*"country" + 0.007*"polish" + 0.007*"force" + 0.007*"operation"
topic #14 (0.040): 0.013*"specie" + 0.011*"age" + 0.010*"river" + 0.009*"population" + 0.008*"water" + 0.007*"male" + 0.006*"plant" + 0.006*"forest" + 0.006*"mountain" + 0.006*"white"
topic #6 (0.040): 0.029*"ship" + 0.028*"island" + 0.010*"sea" + 0.009*"boat" + 0.009*"crew" + 0.009*"port" + 0.008*"navy" + 0.008*"specie" + 0.008*"vessel" + 0.007*"coast"
topic #10 (0.040): 0.016*"king" + 0.010*"son" + 0.008*"cause" + 0.006*"death" + 0.006*"disease" + 0.006*"kingdom" + 0.006*"patient" + 0.005*"treatment" + 0.005*"cell" + 0.005*"emperor"
topic #18 (0.040): 0.040*"company" + 0.026*"village" + 0.022*"population" + 0.013*"municipality" + 0.012*"business" + 0.009*"town" + 0.008*"census" + 0.008*"region" + 0.008*"found" + 0.008*"industry"
topic diff=0.088120, rho=0.135043
PROGRESS: pass 4, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 10
PROGRESS: pass 4, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.040): 0.024*"road" + 0.016*"line" + 0.015*"route" + 0.015*"park" + 0.015*"county" + 0.013*"north" + 0.011*"railway" + 0.011*"town" + 0.011*"highway" + 0.011*"street"
topic #23 (0.040): 0.015*"election" + 0.013*"government" + 0.013*"party" + 0.011*"law" + 0.010*"elect" + 0.009*"court" + 0.009*"vote" + 0.007*"political" + 0.006*"act" + 0.006*"president"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"father" + 0.006*"get" + 0.006*"death" + 0.006*"back" + 0.005*"son" + 0.005*"child" + 0.005*"try"
topic #1 (0.040): 0.106*"film" + 0.021*"star" + 0.016*"award" + 0.015*"direct" + 0.011*"movie" + 0.010*"production" + 0.010*"role" + 0.009*"theatre" + 0.009*"story" + 0.009*"actor"
topic #18 (0.040): 0.040*"company" + 0.026*"village" + 0.022*"population" + 0.013*"municipality" + 0.012*"business" + 0.009*"town" + 0.009*"census" + 0.008*"region" + 0.008*"industry" + 0.008*"sell"
topic diff=0.090039, rho=0.135043
PROGRESS: pass 4, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.025*"woman" + 0.006*"temple" + 0.006*"life" + 0.005*"accord" + 0.005*"tradition" + 0.005*"practice" + 0.005*"religious" + 0.004*"wear" + 0.004*"traditional" + 0.004*"white"
topic #9 (0.040): 0.025*"series" + 0.017*"episode" + 0.011*"television" + 0.009*"character" + 0.008*"appear" + 0.008*"season" + 0.005*"say" + 0.005*"story" + 0.005*"role" + 0.005*"love"
topic #21 (0.040): 0.009*"say" + 0.007*"tell" + 0.007*"kill" + 0.007*"father" + 0.006*"get" + 0.006*"death" + 0.006*"back" + 0.005*"son" + 0.005*"child" + 0.005*"wife"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.008*"author" + 0.008*"language" + 0.006*"novel" + 0.006*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"study"
topic #14 (0.040): 0.014*"specie" + 0.011*"river" + 0.010*"age" + 0.009*"population" + 0.008*"water" + 0.006*"plant" + 0.006*"male" + 0.006*"mountain" + 0.006*"white" + 0.006*"forest"
topic diff=0.083210, rho=0.135043
PROGRESS: pass 4, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.040*"company" + 0.027*"village" + 0.022*"population" + 0.014*"municipality" + 0.013*"business" + 0.009*"town" + 0.009*"region" + 0.008*"found" + 0.008*"census" + 0.008*"industry"
topic #13 (0.040): 0.017*"german" + 0.014*"government" + 0.013*"war" + 0.012*"military" + 0.009*"russian" + 0.009*"korean" + 0.009*"country" + 0.007*"operation" + 0.007*"polish" + 0.007*"force"
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"define" + 0.006*"case" + 0.006*"term" + 0.005*"system" + 0.005*"point" + 0.005*"type" + 0.005*"structure"
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"award" + 0.013*"study" + 0.011*"research" + 0.010*"science" + 0.010*"education" + 0.009*"graduate" + 0.009*"program"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.009*"military" + 0.009*"command" + 0.008*"troop" + 0.007*"order" + 0.007*"soldier"
topic diff=0.080634, rho=0.135043
PROGRESS: pass 4, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.040): 0.023*"road" + 0.015*"line" + 0.015*"route" + 0.015*"county" + 0.014*"park" + 0.014*"north" + 0.012*"town" + 0.012*"railway" + 0.011*"highway" + 0.011*"south"
topic #14 (0.040): 0.014*"specie" + 0.012*"river" + 0.010*"age" + 0.009*"population" + 0.008*"water" + 0.006*"plant" + 0.006*"mountain" + 0.006*"male" + 0.006*"forest" + 0.005*"white"
topic #2 (0.040): 0.030*"club" + 0.029*"season" + 0.022*"league" + 0.017*"match" + 0.014*"final" + 0.014*"player" + 0.014*"football" + 0.013*"championship" + 0.013*"game" + 0.011*"score"
topic #7 (0.040): 0.027*"art" + 0.024*"church" + 0.014*"artist" + 0.013*"museum" + 0.011*"painting" + 0.007*"exhibition" + 0.006*"bishop" + 0.006*"french" + 0.006*"collection" + 0.006*"paint"
topic #12 (0.040): 0.077*"station" + 0.027*"radio" + 0.017*"network" + 0.016*"channel" + 0.015*"broadcast" + 0.014*"air" + 0.012*"line" + 0.012*"operate" + 0.010*"platform" + 0.010*"television"
topic diff=0.080790, rho=0.135043
PROGRESS: pass 4, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.006*"get" + 0.006*"father" + 0.006*"death" + 0.005*"child" + 0.005*"back" + 0.005*"try" + 0.005*"son"
topic #6 (0.040): 0.031*"ship" + 0.027*"island" + 0.011*"sea" + 0.009*"port" + 0.009*"navy" + 0.008*"boat" + 0.008*"crew" + 0.008*"coast" + 0.007*"vessel" + 0.007*"water"
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.014*"government" + 0.012*"law" + 0.010*"elect" + 0.009*"vote" + 0.008*"court" + 0.007*"political" + 0.006*"act" + 0.006*"president"
topic #15 (0.040): 0.046*"game" + 0.043*"season" + 0.016*"football" + 0.014*"basketball" + 0.013*"coach" + 0.012*"player" + 0.012*"conference" + 0.010*"college" + 0.009*"point" + 0.009*"yard"
topic #2 (0.040): 0.031*"club" + 0.030*"season" + 0.022*"league" + 0.017*"match" + 0.014*"final" + 0.014*"football" + 0.014*"player" + 0.013*"game" + 0.013*"championship" + 0.011*"score"
topic diff=0.078757, rho=0.135043
PROGRESS: pass 4, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.040): 0.036*"album" + 0.034*"song" + 0.030*"music" + 0.023*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #18 (0.040): 0.040*"company" + 0.027*"village" + 0.022*"population" + 0.014*"municipality" + 0.013*"business" + 0.010*"town" + 0.009*"region" + 0.008*"found" + 0.008*"sell" + 0.008*"census"
topic #22 (0.040): 0.023*"road" + 0.016*"line" + 0.015*"route" + 0.015*"county" + 0.014*"north" + 0.014*"park" + 0.012*"town" + 0.011*"railway" + 0.011*"street" + 0.011*"south"
topic #16 (0.040): 0.043*"race" + 0.041*"event" + 0.026*"compete" + 0.022*"championship" + 0.014*"finish" + 0.013*"woman" + 0.010*"sport" + 0.009*"competition" + 0.009*"metre" + 0.009*"olympic"
topic #9 (0.040): 0.026*"series" + 0.018*"episode" + 0.011*"television" + 0.009*"character" + 0.008*"appear" + 0.007*"season" + 0.005*"story" + 0.005*"say" + 0.005*"air" + 0.005*"role"
topic diff=0.080965, rho=0.135043
PROGRESS: pass 4, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 10
PROGRESS: pass 4, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.011*"displaystyle" + 0.008*"example" + 0.006*"function" + 0.006*"define" + 0.006*"system" + 0.006*"protein" + 0.006*"term" + 0.006*"point" + 0.006*"case" + 0.005*"structure"
topic #4 (0.040): 0.020*"army" + 0.019*"war" + 0.016*"force" + 0.014*"battle" + 0.012*"attack" + 0.009*"command" + 0.009*"military" + 0.008*"troop" + 0.007*"order" + 0.007*"soldier"
topic #12 (0.040): 0.078*"station" + 0.026*"radio" + 0.018*"network" + 0.016*"channel" + 0.015*"broadcast" + 0.014*"air" + 0.012*"line" + 0.012*"operate" + 0.010*"news" + 0.010*"television"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"father" + 0.006*"death" + 0.006*"back" + 0.006*"child" + 0.006*"try" + 0.005*"son"
topic #24 (0.040): 0.020*"student" + 0.017*"university" + 0.017*"college" + 0.014*"award" + 0.013*"study" + 0.012*"research" + 0.010*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic diff=0.079440, rho=0.135043
PROGRESS: pass 4, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.015*"election" + 0.013*"party" + 0.013*"government" + 0.012*"law" + 0.010*"elect" + 0.009*"vote" + 0.009*"court" + 0.007*"political" + 0.006*"president" + 0.006*"act"
topic #11 (0.040): 0.039*"game" + 0.014*"player" + 0.010*"version" + 0.010*"character" + 0.007*"video" + 0.006*"computer" + 0.005*"power" + 0.005*"series" + 0.005*"code" + 0.005*"system"
topic #1 (0.040): 0.109*"film" + 0.023*"star" + 0.017*"award" + 0.016*"direct" + 0.012*"movie" + 0.010*"production" + 0.010*"role" + 0.010*"actor" + 0.010*"theatre" + 0.009*"director"
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"car" + 0.005*"type"
topic #19 (0.040): 0.032*"building" + 0.019*"house" + 0.012*"site" + 0.011*"design" + 0.010*"church" + 0.007*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"construction" + 0.006*"room"
topic diff=0.074379, rho=0.135043
PROGRESS: pass 4, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.040): 0.023*"road" + 0.016*"line" + 0.015*"county" + 0.014*"park" + 0.014*"route" + 0.014*"north" + 0.012*"town" + 0.011*"railway" + 0.011*"south" + 0.011*"street"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.006*"history" + 0.006*"writer" + 0.006*"novel" + 0.006*"magazine" + 0.005*"life" + 0.005*"editor"
topic #15 (0.040): 0.046*"game" + 0.045*"season" + 0.018*"football" + 0.015*"basketball" + 0.013*"coach" + 0.013*"player" + 0.012*"conference" + 0.010*"college" + 0.009*"finish" + 0.009*"point"
topic #9 (0.040): 0.025*"series" + 0.018*"episode" + 0.010*"television" + 0.009*"character" + 0.008*"appear" + 0.007*"season" + 0.005*"story" + 0.005*"say" + 0.005*"role" + 0.005*"air"
topic #14 (0.040): 0.014*"specie" + 0.010*"river" + 0.010*"age" + 0.009*"population" + 0.008*"water" + 0.006*"plant" + 0.006*"male" + 0.006*"mountain" + 0.006*"white" + 0.005*"region"
topic diff=0.072461, rho=0.135043
-8.243 per-word bound, 302.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #4 (0.040): 0.021*"army" + 0.018*"war" + 0.015*"force" + 0.014*"battle" + 0.012*"attack" + 0.009*"command" + 0.009*"military" + 0.008*"troop" + 0.008*"order" + 0.008*"division"
topic #22 (0.040): 0.023*"road" + 0.016*"line" + 0.015*"county" + 0.014*"park" + 0.014*"route" + 0.014*"north" + 0.012*"town" + 0.011*"south" + 0.011*"railway" + 0.010*"street"
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.007*"engine" + 0.007*"power" + 0.006*"model" + 0.005*"aircraft" + 0.005*"produce" + 0.005*"vehicle" + 0.005*"car" + 0.005*"light"
topic #2 (0.040): 0.031*"club" + 0.030*"season" + 0.022*"league" + 0.018*"match" + 0.014*"final" + 0.014*"player" + 0.014*"football" + 0.013*"game" + 0.012*"championship" + 0.011*"score"
topic #12 (0.040): 0.075*"station" + 0.024*"radio" + 0.018*"network" + 0.017*"channel" + 0.015*"broadcast" + 0.014*"air" + 0.012*"operate" + 0.011*"news" + 0.011*"line" + 0.011*"airport"
topic diff=0.069650, rho=0.135043
-8.245 per-word bound, 303.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #15 (0.040): 0.047*"game" + 0.045*"season" + 0.017*"football" + 0.015*"basketball" + 0.013*"player" + 0.013*"coach" + 0.011*"conference" + 0.010*"college" + 0.009*"point" + 0.009*"finish"
topic #22 (0.040): 0.023*"road" + 0.016*"line" + 0.016*"county" + 0.014*"park" + 0.014*"north" + 0.014*"route" + 0.012*"town" + 0.012*"railway" + 0.011*"south" + 0.010*"street"
topic #10 (0.040): 0.015*"king" + 0.010*"son" + 0.008*"cause" + 0.008*"cell" + 0.007*"disease" + 0.006*"death" + 0.006*"treatment" + 0.005*"patient" + 0.005*"kingdom" + 0.004*"increase"
topic #7 (0.040): 0.028*"art" + 0.026*"church" + 0.014*"museum" + 0.014*"artist" + 0.010*"painting" + 0.007*"bishop" + 0.007*"exhibition" + 0.006*"french" + 0.006*"collection" + 0.006*"paint"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.006*"history" + 0.006*"writer" + 0.006*"novel" + 0.006*"magazine" + 0.005*"life" + 0.005*"publication"
topic diff=0.071292, rho=0.135043
-8.194 per-word bound, 292.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.013*"government" + 0.012*"law" + 0.010*"elect" + 0.009*"vote" + 0.009*"court" + 0.007*"political" + 0.006*"president" + 0.006*"act"
topic #15 (0.040): 0.048*"game" + 0.045*"season" + 0.016*"football" + 0.014*"basketball" + 0.013*"player" + 0.012*"coach" + 0.010*"conference" + 0.009*"college" + 0.009*"finish" + 0.009*"point"
topic #24 (0.040): 0.020*"student" + 0.017*"university" + 0.017*"college" + 0.014*"award" + 0.014*"study" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #8 (0.040): 0.035*"album" + 0.034*"song" + 0.030*"music" + 0.023*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #14 (0.040): 0.015*"specie" + 0.011*"age" + 0.010*"river" + 0.009*"population" + 0.008*"water" + 0.007*"male" + 0.006*"plant" + 0.006*"mountain" + 0.006*"white" + 0.006*"forest"
topic diff=0.069341, rho=0.133828
PROGRESS: pass 5, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.033*"building" + 0.019*"house" + 0.012*"site" + 0.011*"design" + 0.010*"church" + 0.007*"stone" + 0.006*"wall" + 0.006*"th_century" + 0.006*"construction" + 0.006*"room"
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"award" + 0.014*"study" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #6 (0.040): 0.031*"ship" + 0.027*"island" + 0.011*"sea" + 0.009*"navy" + 0.009*"port" + 0.009*"crew" + 0.008*"boat" + 0.008*"coast" + 0.008*"vessel" + 0.007*"fleet"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.015*"force" + 0.014*"battle" + 0.011*"attack" + 0.009*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic #1 (0.040): 0.111*"film" + 0.022*"star" + 0.017*"award" + 0.016*"direct" + 0.012*"movie" + 0.011*"production" + 0.010*"role" + 0.010*"theatre" + 0.009*"actor" + 0.009*"festival"
topic diff=0.069907, rho=0.133828
PROGRESS: pass 5, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.009*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.011*"sea" + 0.009*"navy" + 0.009*"crew" + 0.009*"port" + 0.008*"boat" + 0.008*"coast" + 0.008*"vessel" + 0.008*"fleet"
topic #5 (0.040): 0.024*"woman" + 0.006*"temple" + 0.006*"accord" + 0.005*"tradition" + 0.005*"practice" + 0.005*"traditional" + 0.005*"life" + 0.005*"religious" + 0.005*"wine" + 0.005*"wear"
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"produce" + 0.006*"aircraft" + 0.005*"vehicle" + 0.005*"car" + 0.005*"type"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"need" + 0.005*"social" + 0.004*"require" + 0.004*"individual" + 0.004*"process"
topic diff=0.065660, rho=0.133828
PROGRESS: pass 5, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.040): 0.015*"specie" + 0.011*"age" + 0.010*"river" + 0.009*"population" + 0.008*"water" + 0.007*"male" + 0.006*"plant" + 0.006*"mountain" + 0.006*"white" + 0.006*"forest"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.006*"death" + 0.006*"get" + 0.006*"father" + 0.006*"child" + 0.006*"back" + 0.005*"son" + 0.005*"wife"
topic #5 (0.040): 0.025*"woman" + 0.006*"temple" + 0.005*"tradition" + 0.005*"accord" + 0.005*"life" + 0.005*"traditional" + 0.005*"practice" + 0.004*"religious" + 0.004*"wear" + 0.004*"wine"
topic #22 (0.040): 0.023*"road" + 0.016*"county" + 0.016*"line" + 0.015*"park" + 0.014*"route" + 0.014*"north" + 0.012*"town" + 0.012*"railway" + 0.011*"south" + 0.011*"street"
topic #9 (0.040): 0.026*"series" + 0.017*"episode" + 0.010*"television" + 0.009*"character" + 0.008*"appear" + 0.007*"season" + 0.005*"say" + 0.005*"story" + 0.005*"air" + 0.005*"love"
topic diff=0.070035, rho=0.133828
PROGRESS: pass 5, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.016*"election" + 0.014*"government" + 0.014*"party" + 0.012*"law" + 0.010*"elect" + 0.010*"court" + 0.009*"vote" + 0.007*"political" + 0.006*"candidate" + 0.006*"seat"
topic #8 (0.040): 0.034*"album" + 0.034*"song" + 0.031*"music" + 0.023*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #19 (0.040): 0.032*"building" + 0.019*"house" + 0.012*"site" + 0.011*"design" + 0.009*"church" + 0.007*"stone" + 0.006*"wall" + 0.006*"th_century" + 0.006*"room" + 0.006*"construction"
topic #12 (0.040): 0.075*"station" + 0.025*"radio" + 0.018*"network" + 0.017*"channel" + 0.015*"broadcast" + 0.014*"air" + 0.012*"operate" + 0.011*"news" + 0.011*"television" + 0.010*"airport"
topic #18 (0.040): 0.040*"company" + 0.028*"village" + 0.023*"population" + 0.013*"municipality" + 0.012*"business" + 0.010*"town" + 0.009*"census" + 0.009*"region" + 0.008*"sell" + 0.008*"industry"
topic diff=0.065347, rho=0.133828
PROGRESS: pass 5, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.007*"model" + 0.007*"engine" + 0.007*"power" + 0.006*"vehicle" + 0.006*"produce" + 0.005*"aircraft" + 0.005*"car" + 0.005*"low"
topic #24 (0.040): 0.021*"student" + 0.018*"college" + 0.017*"university" + 0.013*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.011*"sea" + 0.009*"boat" + 0.009*"navy" + 0.009*"crew" + 0.009*"port" + 0.008*"coast" + 0.008*"vessel" + 0.007*"sail"
topic #23 (0.040): 0.016*"election" + 0.014*"government" + 0.013*"party" + 0.012*"law" + 0.010*"elect" + 0.010*"court" + 0.009*"vote" + 0.007*"political" + 0.006*"act" + 0.006*"president"
topic #14 (0.040): 0.015*"specie" + 0.011*"age" + 0.010*"river" + 0.009*"population" + 0.008*"water" + 0.007*"male" + 0.007*"plant" + 0.006*"mountain" + 0.006*"forest" + 0.006*"white"
topic diff=0.067673, rho=0.133828
PROGRESS: pass 5, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.040): 0.035*"song" + 0.034*"album" + 0.030*"music" + 0.023*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"social" + 0.004*"individual" + 0.004*"need" + 0.004*"require" + 0.004*"process"
topic #14 (0.040): 0.016*"specie" + 0.011*"age" + 0.011*"river" + 0.009*"population" + 0.008*"water" + 0.007*"plant" + 0.007*"male" + 0.006*"mountain" + 0.006*"white" + 0.006*"forest"
topic #12 (0.040): 0.077*"station" + 0.025*"radio" + 0.018*"network" + 0.017*"channel" + 0.015*"air" + 0.014*"broadcast" + 0.012*"operate" + 0.011*"television" + 0.011*"news" + 0.010*"tv"
topic #2 (0.040): 0.031*"club" + 0.029*"season" + 0.022*"league" + 0.018*"match" + 0.014*"final" + 0.014*"player" + 0.014*"football" + 0.013*"game" + 0.012*"championship" + 0.011*"score"
topic diff=0.061592, rho=0.133828
PROGRESS: pass 5, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"case" + 0.006*"define" + 0.006*"term" + 0.006*"system" + 0.005*"point" + 0.005*"structure" + 0.005*"type"
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"award" + 0.013*"study" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.007*"order" + 0.007*"soldier"
topic #6 (0.040): 0.032*"ship" + 0.028*"island" + 0.011*"sea" + 0.010*"boat" + 0.009*"crew" + 0.009*"port" + 0.009*"navy" + 0.008*"vessel" + 0.008*"coast" + 0.007*"sail"
topic #12 (0.040): 0.075*"station" + 0.027*"radio" + 0.017*"network" + 0.017*"channel" + 0.015*"broadcast" + 0.014*"air" + 0.012*"operate" + 0.011*"television" + 0.010*"news" + 0.010*"airport"
topic diff=0.060438, rho=0.133828
PROGRESS: pass 5, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.040): 0.040*"game" + 0.015*"player" + 0.011*"character" + 0.010*"version" + 0.006*"video" + 0.006*"card" + 0.006*"computer" + 0.006*"code" + 0.006*"magic" + 0.006*"series"
topic #8 (0.040): 0.035*"album" + 0.034*"song" + 0.030*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"need" + 0.004*"require" + 0.004*"social" + 0.004*"individual" + 0.004*"process"
topic #20 (0.040): 0.025*"book" + 0.022*"publish" + 0.008*"author" + 0.008*"language" + 0.007*"novel" + 0.006*"writer" + 0.006*"history" + 0.006*"life" + 0.006*"magazine" + 0.005*"publication"
topic #9 (0.040): 0.027*"series" + 0.018*"episode" + 0.010*"television" + 0.009*"character" + 0.008*"appear" + 0.008*"season" + 0.005*"say" + 0.005*"air" + 0.005*"story" + 0.004*"get"
topic diff=0.061518, rho=0.133828
PROGRESS: pass 5, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.005*"social" + 0.005*"report" + 0.005*"need" + 0.005*"require" + 0.004*"individual" + 0.004*"process"
topic #5 (0.040): 0.023*"woman" + 0.006*"temple" + 0.006*"accord" + 0.005*"tradition" + 0.005*"wear" + 0.005*"traditional" + 0.005*"wine" + 0.005*"life" + 0.005*"practice" + 0.005*"religious"
topic #18 (0.040): 0.040*"company" + 0.029*"village" + 0.023*"population" + 0.014*"municipality" + 0.013*"business" + 0.011*"town" + 0.010*"region" + 0.009*"census" + 0.008*"sell" + 0.008*"found"
topic #10 (0.040): 0.014*"king" + 0.009*"son" + 0.009*"cause" + 0.008*"cell" + 0.007*"treatment" + 0.006*"disease" + 0.006*"death" + 0.006*"patient" + 0.005*"kingdom" + 0.005*"increase"
topic #15 (0.040): 0.047*"game" + 0.045*"season" + 0.016*"football" + 0.014*"basketball" + 0.013*"coach" + 0.013*"player" + 0.011*"conference" + 0.010*"college" + 0.010*"point" + 0.009*"yard"
topic diff=0.059349, rho=0.133828
PROGRESS: pass 5, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.040): 0.032*"club" + 0.029*"season" + 0.023*"league" + 0.018*"match" + 0.015*"final" + 0.014*"football" + 0.014*"player" + 0.012*"game" + 0.012*"championship" + 0.011*"score"
topic #16 (0.040): 0.043*"event" + 0.043*"race" + 0.027*"compete" + 0.026*"championship" + 0.016*"finish" + 0.016*"woman" + 0.012*"sport" + 0.011*"competition" + 0.009*"olympic" + 0.009*"metre"
topic #1 (0.040): 0.109*"film" + 0.024*"star" + 0.019*"award" + 0.016*"direct" + 0.012*"movie" + 0.011*"production" + 0.011*"role" + 0.011*"theatre" + 0.010*"actor" + 0.009*"festival"
topic #8 (0.040): 0.037*"album" + 0.035*"song" + 0.030*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #18 (0.040): 0.040*"company" + 0.029*"village" + 0.023*"population" + 0.014*"municipality" + 0.013*"business" + 0.011*"town" + 0.010*"region" + 0.009*"census" + 0.008*"sell" + 0.008*"found"
topic diff=0.061842, rho=0.133828
PROGRESS: pass 5, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.008*"engine" + 0.007*"power" + 0.007*"model" + 0.005*"produce" + 0.005*"aircraft" + 0.005*"car" + 0.005*"vehicle" + 0.005*"type"
topic #7 (0.040): 0.029*"church" + 0.028*"art" + 0.014*"artist" + 0.013*"museum" + 0.011*"painting" + 0.007*"bishop" + 0.007*"exhibition" + 0.006*"french" + 0.006*"paint" + 0.006*"collection"
topic #19 (0.040): 0.032*"building" + 0.020*"house" + 0.013*"site" + 0.011*"design" + 0.010*"church" + 0.007*"th_century" + 0.007*"stone" + 0.006*"construction" + 0.006*"wall" + 0.006*"room"
topic #14 (0.040): 0.016*"specie" + 0.011*"river" + 0.010*"age" + 0.008*"water" + 0.008*"population" + 0.007*"mountain" + 0.007*"plant" + 0.006*"male" + 0.006*"white" + 0.006*"forest"
topic #2 (0.040): 0.034*"club" + 0.030*"season" + 0.023*"league" + 0.019*"match" + 0.015*"final" + 0.014*"football" + 0.014*"player" + 0.012*"game" + 0.012*"championship" + 0.011*"score"
topic diff=0.061983, rho=0.133828
PROGRESS: pass 5, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.007*"novel" + 0.006*"history" + 0.006*"writer" + 0.006*"magazine" + 0.006*"life" + 0.005*"editor"
topic #24 (0.040): 0.020*"student" + 0.018*"university" + 0.017*"college" + 0.014*"award" + 0.014*"study" + 0.012*"research" + 0.010*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #6 (0.040): 0.032*"ship" + 0.027*"island" + 0.011*"sea" + 0.010*"port" + 0.010*"navy" + 0.009*"crew" + 0.009*"coast" + 0.009*"boat" + 0.008*"fleet" + 0.008*"vessel"
topic #22 (0.040): 0.023*"road" + 0.017*"line" + 0.016*"county" + 0.015*"park" + 0.015*"route" + 0.014*"north" + 0.012*"town" + 0.011*"railway" + 0.011*"south" + 0.010*"street"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.016*"force" + 0.015*"battle" + 0.012*"attack" + 0.010*"command" + 0.009*"military" + 0.008*"troop" + 0.008*"order" + 0.007*"fight"
topic diff=0.057483, rho=0.133828
PROGRESS: pass 5, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.026*"series" + 0.018*"episode" + 0.010*"television" + 0.010*"character" + 0.008*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"say" + 0.005*"air" + 0.005*"get"
topic #0 (0.040): 0.010*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.006*"term" + 0.006*"protein" + 0.006*"define" + 0.006*"point" + 0.006*"system" + 0.006*"case" + 0.005*"structure"
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.013*"government" + 0.012*"law" + 0.011*"elect" + 0.010*"court" + 0.009*"vote" + 0.007*"political" + 0.006*"president" + 0.006*"act"
topic #15 (0.040): 0.047*"game" + 0.047*"season" + 0.017*"football" + 0.014*"basketball" + 0.013*"player" + 0.013*"coach" + 0.011*"conference" + 0.010*"college" + 0.009*"finish" + 0.009*"point"
topic #1 (0.040): 0.112*"film" + 0.024*"star" + 0.019*"award" + 0.016*"direct" + 0.012*"movie" + 0.012*"role" + 0.012*"production" + 0.011*"theatre" + 0.011*"actor" + 0.010*"director"
topic diff=0.056356, rho=0.133828
-8.239 per-word bound, 302.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.041*"company" + 0.029*"village" + 0.024*"population" + 0.013*"municipality" + 0.013*"business" + 0.011*"town" + 0.009*"region" + 0.009*"sell" + 0.009*"census" + 0.008*"found"
topic #5 (0.040): 0.023*"woman" + 0.006*"temple" + 0.006*"accord" + 0.006*"traditional" + 0.005*"tradition" + 0.005*"religious" + 0.005*"black" + 0.005*"practice" + 0.005*"life" + 0.005*"wear"
topic #11 (0.040): 0.039*"game" + 0.014*"player" + 0.010*"version" + 0.009*"character" + 0.007*"computer" + 0.006*"video" + 0.006*"code" + 0.006*"user" + 0.005*"software" + 0.005*"system"
topic #13 (0.040): 0.022*"german" + 0.014*"government" + 0.013*"war" + 0.012*"russian" + 0.011*"military" + 0.010*"country" + 0.007*"force" + 0.007*"polish" + 0.007*"soviet" + 0.006*"korean"
topic #6 (0.040): 0.031*"ship" + 0.027*"island" + 0.011*"sea" + 0.009*"navy" + 0.009*"port" + 0.009*"crew" + 0.009*"coast" + 0.008*"boat" + 0.008*"vessel" + 0.008*"fleet"
topic diff=0.054613, rho=0.133828
-8.241 per-word bound, 302.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #19 (0.040): 0.032*"building" + 0.019*"house" + 0.013*"site" + 0.011*"design" + 0.010*"church" + 0.007*"stone" + 0.007*"th_century" + 0.007*"wall" + 0.006*"construction" + 0.006*"room"
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.014*"government" + 0.012*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.006*"president" + 0.006*"act"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.007*"novel" + 0.006*"history" + 0.006*"writer" + 0.006*"magazine" + 0.006*"life" + 0.005*"publication"
topic #12 (0.040): 0.073*"station" + 0.024*"radio" + 0.018*"network" + 0.017*"channel" + 0.015*"broadcast" + 0.014*"air" + 0.012*"operate" + 0.012*"news" + 0.011*"television" + 0.011*"airport"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.015*"force" + 0.014*"battle" + 0.012*"attack" + 0.009*"military" + 0.009*"command" + 0.008*"division" + 0.008*"troop" + 0.008*"order"
topic diff=0.053183, rho=0.133828
-8.201 per-word bound, 294.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.033*"building" + 0.019*"house" + 0.013*"site" + 0.011*"design" + 0.009*"church" + 0.007*"wall" + 0.007*"stone" + 0.006*"th_century" + 0.006*"construction" + 0.006*"room"
topic #24 (0.040): 0.020*"student" + 0.017*"university" + 0.017*"college" + 0.014*"award" + 0.014*"study" + 0.012*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #3 (0.040): 0.011*"design" + 0.010*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"produce" + 0.005*"vehicle" + 0.005*"aircraft" + 0.005*"car" + 0.005*"light"
topic #0 (0.040): 0.012*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.007*"define" + 0.006*"system" + 0.006*"term" + 0.006*"case" + 0.005*"point" + 0.005*"structure" + 0.005*"protein"
topic #5 (0.040): 0.022*"woman" + 0.006*"accord" + 0.006*"temple" + 0.005*"tradition" + 0.005*"traditional" + 0.005*"religious" + 0.005*"wear" + 0.005*"wine" + 0.005*"practice" + 0.005*"life"
topic diff=0.055266, rho=0.132645
PROGRESS: pass 6, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 11
PROGRESS: pass 6, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 12
PROGRESS: pass 6, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 13
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.040*"company" + 0.030*"village" + 0.024*"population" + 0.013*"municipality" + 0.012*"business" + 0.011*"town" + 0.010*"region" + 0.009*"census" + 0.008*"sell" + 0.008*"found"
topic #20 (0.040): 0.024*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"magazine" + 0.006*"life" + 0.005*"publication"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.011*"sea" + 0.009*"navy" + 0.009*"port" + 0.009*"crew" + 0.009*"boat" + 0.008*"coast" + 0.008*"vessel" + 0.008*"fleet"
topic #5 (0.040): 0.022*"woman" + 0.006*"temple" + 0.006*"accord" + 0.005*"tradition" + 0.005*"practice" + 0.005*"traditional" + 0.005*"religious" + 0.005*"wine" + 0.005*"wear" + 0.005*"life"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"get" + 0.007*"tell" + 0.007*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic diff=0.056567, rho=0.132645
PROGRESS: pass 6, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 13
merging changes from 4000 documents into a model of 49835 documents
topic #7 (0.040): 0.027*"art" + 0.025*"church" + 0.014*"museum" + 0.014*"artist" + 0.011*"painting" + 0.007*"exhibition" + 0.007*"french" + 0.007*"bishop" + 0.006*"collection" + 0.006*"paint"
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"produce" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"car" + 0.005*"type"
topic #19 (0.040): 0.033*"building" + 0.020*"house" + 0.013*"site" + 0.011*"design" + 0.009*"church" + 0.007*"stone" + 0.006*"wall" + 0.006*"th_century" + 0.006*"construction" + 0.006*"room"
topic #8 (0.040): 0.036*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"publication"
topic diff=0.051776, rho=0.132645
PROGRESS: pass 6, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"define" + 0.006*"system" + 0.006*"term" + 0.006*"case" + 0.005*"point" + 0.005*"structure" + 0.005*"different"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"need" + 0.005*"social" + 0.004*"require" + 0.004*"individual" + 0.004*"public"
topic #9 (0.040): 0.027*"series" + 0.018*"episode" + 0.010*"television" + 0.010*"character" + 0.009*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"say" + 0.005*"air" + 0.004*"love"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic diff=0.052628, rho=0.132645
PROGRESS: pass 6, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 11
PROGRESS: pass 6, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 12
PROGRESS: pass 6, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 13
merging changes from 4000 documents into a model of 49835 documents
topic #10 (0.040): 0.015*"king" + 0.009*"cause" + 0.009*"cell" + 0.009*"son" + 0.007*"patient" + 0.007*"disease" + 0.006*"treatment" + 0.006*"death" + 0.005*"kingdom" + 0.005*"increase"
topic #8 (0.040): 0.035*"album" + 0.035*"song" + 0.031*"music" + 0.023*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #19 (0.040): 0.032*"building" + 0.019*"house" + 0.012*"site" + 0.011*"design" + 0.009*"church" + 0.007*"th_century" + 0.007*"wall" + 0.007*"stone" + 0.006*"room" + 0.006*"construction"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic #5 (0.040): 0.020*"woman" + 0.006*"tradition" + 0.006*"temple" + 0.006*"accord" + 0.005*"traditional" + 0.005*"life" + 0.005*"religious" + 0.005*"practice" + 0.004*"mean" + 0.004*"ancient"
topic diff=0.053151, rho=0.132645
PROGRESS: pass 6, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.027*"series" + 0.018*"episode" + 0.010*"television" + 0.010*"character" + 0.009*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"say" + 0.005*"air" + 0.004*"get"
topic #3 (0.040): 0.010*"design" + 0.010*"system" + 0.008*"engine" + 0.007*"model" + 0.007*"power" + 0.006*"vehicle" + 0.006*"produce" + 0.005*"car" + 0.005*"aircraft" + 0.005*"type"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"father" + 0.006*"death" + 0.006*"get" + 0.006*"back" + 0.006*"child" + 0.006*"son" + 0.005*"try"
topic #2 (0.040): 0.033*"club" + 0.029*"season" + 0.022*"league" + 0.019*"match" + 0.014*"football" + 0.014*"final" + 0.014*"player" + 0.012*"game" + 0.012*"score" + 0.011*"championship"
topic #7 (0.040): 0.027*"art" + 0.025*"church" + 0.014*"museum" + 0.014*"artist" + 0.011*"painting" + 0.007*"exhibition" + 0.007*"french" + 0.007*"bishop" + 0.006*"collection" + 0.006*"paint"
topic diff=0.049894, rho=0.132645
PROGRESS: pass 6, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.011*"science" + 0.009*"graduate" + 0.009*"program"
topic #16 (0.040): 0.042*"race" + 0.041*"event" + 0.026*"compete" + 0.026*"championship" + 0.017*"finish" + 0.016*"woman" + 0.013*"sport" + 0.012*"competition" + 0.010*"olympic" + 0.009*"athlete"
topic #22 (0.040): 0.023*"road" + 0.018*"county" + 0.017*"line" + 0.016*"park" + 0.015*"route" + 0.014*"north" + 0.013*"town" + 0.012*"railway" + 0.011*"south" + 0.011*"highway"
topic #14 (0.040): 0.017*"specie" + 0.011*"age" + 0.010*"river" + 0.008*"water" + 0.008*"population" + 0.007*"plant" + 0.007*"male" + 0.006*"forest" + 0.006*"mountain" + 0.006*"white"
topic #2 (0.040): 0.033*"club" + 0.029*"season" + 0.022*"league" + 0.019*"match" + 0.014*"football" + 0.014*"player" + 0.014*"final" + 0.012*"game" + 0.011*"score" + 0.011*"championship"
topic diff=0.049723, rho=0.132645
PROGRESS: pass 6, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.039*"company" + 0.030*"village" + 0.025*"population" + 0.014*"municipality" + 0.013*"business" + 0.012*"town" + 0.010*"region" + 0.010*"census" + 0.008*"sell" + 0.008*"found"
topic #15 (0.040): 0.048*"season" + 0.047*"game" + 0.017*"football" + 0.014*"basketball" + 0.014*"player" + 0.013*"coach" + 0.011*"conference" + 0.010*"college" + 0.009*"point" + 0.009*"finish"
topic #2 (0.040): 0.033*"club" + 0.029*"season" + 0.022*"league" + 0.019*"match" + 0.014*"football" + 0.014*"player" + 0.014*"final" + 0.012*"game" + 0.011*"score" + 0.011*"championship"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"need" + 0.005*"require" + 0.004*"social" + 0.004*"public" + 0.004*"individual"
topic #22 (0.040): 0.023*"road" + 0.017*"county" + 0.017*"line" + 0.015*"route" + 0.015*"park" + 0.014*"north" + 0.013*"town" + 0.012*"railway" + 0.011*"south" + 0.010*"highway"
topic diff=0.051143, rho=0.132645
PROGRESS: pass 6, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.040): 0.022*"road" + 0.018*"line" + 0.017*"county" + 0.015*"park" + 0.015*"route" + 0.015*"north" + 0.013*"town" + 0.012*"railway" + 0.011*"south" + 0.010*"highway"
topic #2 (0.040): 0.033*"club" + 0.029*"season" + 0.023*"league" + 0.019*"match" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.012*"game" + 0.011*"score" + 0.011*"championship"
topic #20 (0.040): 0.026*"book" + 0.022*"publish" + 0.008*"author" + 0.008*"language" + 0.007*"novel" + 0.006*"writer" + 0.006*"history" + 0.006*"life" + 0.006*"magazine" + 0.005*"editor"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"father" + 0.006*"child" + 0.006*"back" + 0.005*"son" + 0.005*"try"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.005*"report" + 0.005*"require" + 0.005*"need" + 0.005*"social" + 0.004*"individual" + 0.004*"public"
topic diff=0.049998, rho=0.132645
PROGRESS: pass 6, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.040*"company" + 0.030*"village" + 0.024*"population" + 0.014*"municipality" + 0.013*"business" + 0.012*"town" + 0.010*"region" + 0.009*"census" + 0.008*"sell" + 0.008*"found"
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"system" + 0.006*"protein" + 0.006*"case" + 0.006*"term" + 0.005*"define" + 0.005*"structure" + 0.005*"different"
topic #6 (0.040): 0.032*"ship" + 0.029*"island" + 0.012*"sea" + 0.009*"crew" + 0.009*"port" + 0.009*"boat" + 0.009*"coast" + 0.009*"navy" + 0.008*"vessel" + 0.007*"naval"
topic #10 (0.040): 0.012*"king" + 0.010*"cell" + 0.009*"cause" + 0.008*"son" + 0.007*"treatment" + 0.007*"disease" + 0.006*"patient" + 0.006*"increase" + 0.005*"death" + 0.005*"effect"
topic #3 (0.040): 0.010*"design" + 0.009*"system" + 0.008*"engine" + 0.007*"model" + 0.007*"power" + 0.006*"aircraft" + 0.006*"produce" + 0.005*"vehicle" + 0.005*"car" + 0.005*"light"
topic diff=0.052610, rho=0.132645
PROGRESS: pass 6, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.033*"building" + 0.020*"house" + 0.013*"site" + 0.011*"design" + 0.010*"church" + 0.007*"stone" + 0.007*"th_century" + 0.006*"construction" + 0.006*"room" + 0.006*"wall"
topic #16 (0.040): 0.043*"event" + 0.041*"race" + 0.028*"compete" + 0.028*"championship" + 0.018*"woman" + 0.017*"finish" + 0.013*"competition" + 0.012*"sport" + 0.009*"final" + 0.009*"olympic"
topic #17 (0.040): 0.008*"provide" + 0.006*"increase" + 0.006*"system" + 0.005*"report" + 0.005*"information" + 0.005*"social" + 0.005*"need" + 0.005*"require" + 0.004*"individual" + 0.004*"process"
topic #18 (0.040): 0.040*"company" + 0.030*"village" + 0.024*"population" + 0.014*"municipality" + 0.013*"business" + 0.012*"town" + 0.010*"region" + 0.009*"census" + 0.009*"sell" + 0.008*"found"
topic #9 (0.040): 0.028*"series" + 0.019*"episode" + 0.010*"television" + 0.010*"character" + 0.008*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"air" + 0.005*"say" + 0.005*"get"
topic diff=0.055542, rho=0.132645
PROGRESS: pass 6, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.011*"design" + 0.010*"system" + 0.008*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"produce" + 0.005*"aircraft" + 0.005*"car" + 0.005*"vehicle" + 0.005*"type"
topic #14 (0.040): 0.017*"specie" + 0.011*"age" + 0.010*"river" + 0.008*"water" + 0.008*"population" + 0.007*"plant" + 0.007*"mountain" + 0.007*"male" + 0.006*"white" + 0.006*"forest"
topic #17 (0.040): 0.008*"provide" + 0.007*"system" + 0.006*"increase" + 0.005*"report" + 0.005*"information" + 0.005*"social" + 0.005*"need" + 0.004*"individual" + 0.004*"require" + 0.004*"process"
topic #6 (0.040): 0.032*"ship" + 0.028*"island" + 0.011*"sea" + 0.010*"port" + 0.010*"crew" + 0.009*"navy" + 0.009*"coast" + 0.009*"boat" + 0.008*"naval" + 0.008*"vessel"
topic #2 (0.040): 0.034*"club" + 0.030*"season" + 0.023*"league" + 0.020*"match" + 0.015*"football" + 0.014*"final" + 0.014*"player" + 0.012*"score" + 0.012*"game" + 0.011*"goal"
topic diff=0.047565, rho=0.132645
PROGRESS: pass 6, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.048*"season" + 0.047*"game" + 0.017*"football" + 0.014*"basketball" + 0.013*"player" + 0.013*"coach" + 0.011*"conference" + 0.010*"college" + 0.009*"finish" + 0.008*"point"
topic #14 (0.040): 0.017*"specie" + 0.011*"age" + 0.009*"river" + 0.008*"water" + 0.008*"population" + 0.007*"plant" + 0.007*"male" + 0.006*"mountain" + 0.006*"white" + 0.005*"forest"
topic #11 (0.040): 0.040*"game" + 0.014*"player" + 0.010*"version" + 0.010*"character" + 0.007*"computer" + 0.007*"video" + 0.005*"system" + 0.005*"user" + 0.005*"software" + 0.005*"card"
topic #6 (0.040): 0.032*"ship" + 0.028*"island" + 0.012*"sea" + 0.010*"crew" + 0.010*"navy" + 0.010*"port" + 0.009*"coast" + 0.009*"boat" + 0.008*"naval" + 0.008*"fleet"
topic #7 (0.040): 0.029*"church" + 0.027*"art" + 0.014*"artist" + 0.014*"museum" + 0.010*"painting" + 0.007*"french" + 0.007*"bishop" + 0.006*"exhibition" + 0.006*"son" + 0.006*"collection"
topic diff=0.047830, rho=0.132645
PROGRESS: pass 6, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.008*"function" + 0.006*"system" + 0.006*"term" + 0.006*"point" + 0.006*"case" + 0.006*"define" + 0.005*"structure" + 0.005*"protein"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"report" + 0.006*"information" + 0.005*"need" + 0.005*"require" + 0.004*"individual" + 0.004*"social" + 0.004*"process"
topic #5 (0.040): 0.020*"woman" + 0.006*"traditional" + 0.006*"accord" + 0.006*"temple" + 0.005*"tradition" + 0.005*"black" + 0.005*"religious" + 0.005*"often" + 0.005*"wear" + 0.005*"mean"
topic #2 (0.040): 0.034*"club" + 0.030*"season" + 0.022*"league" + 0.020*"match" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.012*"game" + 0.012*"score" + 0.011*"goal"
topic diff=0.043842, rho=0.132645
-8.241 per-word bound, 302.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #18 (0.040): 0.040*"company" + 0.030*"village" + 0.025*"population" + 0.013*"municipality" + 0.013*"business" + 0.011*"town" + 0.010*"region" + 0.009*"census" + 0.009*"sell" + 0.008*"found"
topic #2 (0.040): 0.034*"club" + 0.029*"season" + 0.022*"league" + 0.020*"match" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.012*"score" + 0.012*"game" + 0.011*"goal"
topic #3 (0.040): 0.011*"design" + 0.010*"system" + 0.007*"power" + 0.007*"engine" + 0.007*"model" + 0.006*"produce" + 0.005*"aircraft" + 0.005*"car" + 0.005*"vehicle" + 0.005*"light"
topic #23 (0.040): 0.016*"election" + 0.015*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.006*"general" + 0.006*"president"
topic #16 (0.040): 0.043*"race" + 0.042*"event" + 0.026*"championship" + 0.026*"compete" + 0.018*"woman" + 0.017*"finish" + 0.013*"sport" + 0.013*"competition" + 0.009*"olympic" + 0.009*"final"
topic diff=0.050098, rho=0.132645
-8.243 per-word bound, 303.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #5 (0.040): 0.019*"woman" + 0.006*"accord" + 0.006*"temple" + 0.006*"traditional" + 0.005*"tradition" + 0.005*"flag" + 0.005*"ancient" + 0.005*"religious" + 0.005*"mean" + 0.005*"black"
topic #11 (0.040): 0.039*"game" + 0.014*"player" + 0.010*"version" + 0.009*"character" + 0.007*"computer" + 0.006*"code" + 0.006*"video" + 0.006*"system" + 0.006*"user" + 0.005*"file"
topic #18 (0.040): 0.041*"company" + 0.030*"village" + 0.025*"population" + 0.013*"municipality" + 0.013*"business" + 0.012*"town" + 0.010*"region" + 0.009*"census" + 0.009*"sell" + 0.008*"found"
topic #20 (0.040): 0.026*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"magazine" + 0.006*"life" + 0.005*"publication"
topic #2 (0.040): 0.034*"club" + 0.029*"season" + 0.022*"league" + 0.020*"match" + 0.015*"football" + 0.015*"player" + 0.014*"final" + 0.012*"game" + 0.012*"score" + 0.011*"goal"
topic diff=0.049664, rho=0.132645
-8.154 per-word bound, 284.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.027*"art" + 0.026*"church" + 0.015*"museum" + 0.013*"artist" + 0.011*"painting" + 0.007*"french" + 0.007*"exhibition" + 0.007*"bishop" + 0.007*"son" + 0.006*"collection"
topic #21 (0.040): 0.009*"say" + 0.007*"kill" + 0.007*"get" + 0.007*"tell" + 0.007*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.006*"try" + 0.005*"son"
topic #0 (0.040): 0.012*"displaystyle" + 0.008*"example" + 0.007*"define" + 0.007*"function" + 0.006*"system" + 0.006*"case" + 0.006*"term" + 0.005*"point" + 0.005*"structure" + 0.005*"different"
topic #22 (0.040): 0.022*"road" + 0.018*"line" + 0.018*"county" + 0.016*"park" + 0.015*"north" + 0.013*"route" + 0.013*"town" + 0.013*"railway" + 0.012*"south" + 0.010*"bridge"
topic #5 (0.040): 0.018*"woman" + 0.006*"accord" + 0.006*"tradition" + 0.006*"temple" + 0.005*"traditional" + 0.005*"ancient" + 0.005*"flag" + 0.005*"religious" + 0.005*"wear" + 0.005*"mean"
topic diff=0.047376, rho=0.131494
PROGRESS: pass 7, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.006*"report" + 0.005*"need" + 0.005*"social" + 0.005*"require" + 0.004*"public" + 0.004*"individual"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.015*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.008*"division"
topic #24 (0.040): 0.020*"student" + 0.018*"university" + 0.017*"college" + 0.014*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic #10 (0.040): 0.014*"king" + 0.009*"cell" + 0.008*"cause" + 0.008*"son" + 0.007*"patient" + 0.007*"disease" + 0.007*"treatment" + 0.006*"death" + 0.005*"increase" + 0.005*"prince"
topic #6 (0.040): 0.032*"ship" + 0.028*"island" + 0.012*"sea" + 0.009*"navy" + 0.009*"port" + 0.009*"crew" + 0.009*"boat" + 0.009*"coast" + 0.008*"vessel" + 0.008*"fleet"
topic diff=0.049061, rho=0.131494
PROGRESS: pass 7, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.040): 0.071*"station" + 0.024*"radio" + 0.019*"network" + 0.018*"channel" + 0.015*"broadcast" + 0.015*"air" + 0.012*"television" + 0.012*"news" + 0.011*"tv" + 0.011*"operate"
topic #9 (0.040): 0.028*"series" + 0.018*"episode" + 0.010*"character" + 0.010*"television" + 0.008*"appear" + 0.007*"season" + 0.005*"say" + 0.005*"air" + 0.005*"story" + 0.004*"get"
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.006*"general" + 0.006*"president"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"need" + 0.005*"social" + 0.005*"require" + 0.004*"public" + 0.004*"individual"
topic #18 (0.040): 0.040*"company" + 0.030*"village" + 0.025*"population" + 0.013*"municipality" + 0.012*"business" + 0.012*"town" + 0.010*"region" + 0.010*"census" + 0.009*"sell" + 0.008*"found"
topic diff=0.045603, rho=0.131494
PROGRESS: pass 7, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.040): 0.018*"specie" + 0.012*"age" + 0.009*"river" + 0.009*"water" + 0.008*"population" + 0.007*"male" + 0.007*"plant" + 0.006*"mountain" + 0.006*"white" + 0.006*"forest"
topic #4 (0.040): 0.022*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic #5 (0.040): 0.018*"woman" + 0.006*"tradition" + 0.006*"accord" + 0.005*"temple" + 0.005*"traditional" + 0.005*"ancient" + 0.005*"flag" + 0.004*"practice" + 0.004*"king" + 0.004*"mean"
topic #2 (0.040): 0.033*"club" + 0.029*"season" + 0.021*"league" + 0.021*"match" + 0.014*"player" + 0.014*"football" + 0.014*"final" + 0.012*"game" + 0.012*"score" + 0.011*"round"
topic #13 (0.040): 0.018*"german" + 0.015*"government" + 0.013*"war" + 0.011*"military" + 0.011*"russian" + 0.011*"country" + 0.009*"korean" + 0.007*"polish" + 0.007*"force" + 0.007*"soviet"
topic diff=0.049894, rho=0.131494
PROGRESS: pass 7, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.040): 0.039*"company" + 0.030*"village" + 0.025*"population" + 0.013*"municipality" + 0.012*"business" + 0.012*"town" + 0.010*"region" + 0.010*"census" + 0.009*"sell" + 0.008*"industry"
topic #13 (0.040): 0.018*"german" + 0.015*"government" + 0.013*"war" + 0.011*"military" + 0.011*"russian" + 0.011*"country" + 0.008*"korean" + 0.008*"polish" + 0.007*"force" + 0.007*"operation"
topic #23 (0.040): 0.016*"election" + 0.014*"government" + 0.014*"party" + 0.013*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.006*"act" + 0.006*"president"
topic #21 (0.040): 0.010*"say" + 0.007*"tell" + 0.007*"kill" + 0.007*"death" + 0.006*"get" + 0.006*"father" + 0.006*"child" + 0.006*"back" + 0.006*"son" + 0.005*"try"
topic #2 (0.040): 0.034*"club" + 0.028*"season" + 0.022*"league" + 0.021*"match" + 0.015*"football" + 0.014*"player" + 0.013*"final" + 0.012*"game" + 0.012*"score" + 0.011*"round"
topic diff=0.045963, rho=0.131494
PROGRESS: pass 7, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.027*"art" + 0.025*"church" + 0.014*"museum" + 0.014*"artist" + 0.011*"painting" + 0.008*"french" + 0.008*"exhibition" + 0.007*"bishop" + 0.007*"son" + 0.006*"father"
topic #15 (0.040): 0.049*"season" + 0.048*"game" + 0.016*"football" + 0.014*"basketball" + 0.014*"player" + 0.012*"coach" + 0.010*"conference" + 0.009*"college" + 0.009*"point" + 0.009*"baseball"
topic #5 (0.040): 0.017*"woman" + 0.006*"accord" + 0.006*"tradition" + 0.006*"temple" + 0.005*"traditional" + 0.005*"ancient" + 0.005*"mean" + 0.005*"religious" + 0.005*"often" + 0.004*"black"
topic #9 (0.040): 0.027*"series" + 0.018*"episode" + 0.010*"character" + 0.010*"television" + 0.009*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"say" + 0.005*"air" + 0.005*"get"
topic #14 (0.040): 0.018*"specie" + 0.012*"age" + 0.009*"river" + 0.009*"water" + 0.008*"population" + 0.007*"male" + 0.007*"plant" + 0.006*"forest" + 0.006*"mountain" + 0.006*"white"
topic diff=0.047860, rho=0.131494
PROGRESS: pass 7, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.040): 0.113*"film" + 0.024*"star" + 0.021*"award" + 0.016*"direct" + 0.013*"movie" + 0.013*"role" + 0.012*"production" + 0.012*"theatre" + 0.010*"actor" + 0.010*"festival"
topic #8 (0.040): 0.035*"song" + 0.035*"album" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #5 (0.040): 0.017*"woman" + 0.006*"temple" + 0.006*"tradition" + 0.006*"accord" + 0.005*"ancient" + 0.005*"traditional" + 0.005*"mean" + 0.005*"religious" + 0.005*"often" + 0.004*"king"
topic #11 (0.040): 0.041*"game" + 0.014*"player" + 0.011*"version" + 0.010*"character" + 0.007*"computer" + 0.007*"card" + 0.006*"video" + 0.006*"software" + 0.006*"system" + 0.006*"code"
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.009*"graduate" + 0.009*"program"
topic diff=0.042498, rho=0.131494
PROGRESS: pass 7, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.018*"woman" + 0.006*"accord" + 0.006*"tradition" + 0.005*"temple" + 0.005*"traditional" + 0.005*"ancient" + 0.005*"mean" + 0.005*"religious" + 0.005*"often" + 0.004*"wear"
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.007*"president" + 0.007*"seat"
topic #19 (0.040): 0.033*"building" + 0.019*"house" + 0.013*"site" + 0.011*"design" + 0.009*"church" + 0.007*"wall" + 0.007*"stone" + 0.006*"th_century" + 0.006*"room" + 0.006*"construction"
topic #21 (0.040): 0.009*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"son" + 0.005*"try"
topic #15 (0.040): 0.048*"season" + 0.047*"game" + 0.016*"football" + 0.014*"player" + 0.014*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.010*"college" + 0.009*"point" + 0.009*"finish"
topic diff=0.042841, rho=0.131494
PROGRESS: pass 7, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.048*"season" + 0.047*"game" + 0.016*"football" + 0.014*"player" + 0.014*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.009*"college" + 0.009*"point" + 0.009*"finish"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.007*"order" + 0.007*"general"
topic #6 (0.040): 0.033*"ship" + 0.029*"island" + 0.012*"sea" + 0.009*"boat" + 0.009*"navy" + 0.009*"port" + 0.009*"crew" + 0.008*"coast" + 0.008*"vessel" + 0.007*"naval"
topic #11 (0.040): 0.039*"game" + 0.014*"player" + 0.011*"version" + 0.011*"character" + 0.007*"computer" + 0.006*"video" + 0.006*"card" + 0.006*"code" + 0.006*"system" + 0.006*"software"
topic #22 (0.040): 0.022*"road" + 0.018*"county" + 0.018*"line" + 0.015*"park" + 0.015*"route" + 0.015*"north" + 0.014*"town" + 0.013*"railway" + 0.012*"south" + 0.010*"highway"
topic diff=0.044427, rho=0.131494
PROGRESS: pass 7, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.040): 0.021*"student" + 0.018*"university" + 0.017*"college" + 0.014*"study" + 0.013*"award" + 0.012*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #2 (0.040): 0.035*"club" + 0.029*"season" + 0.022*"league" + 0.020*"match" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.012*"game" + 0.012*"score" + 0.011*"championship"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.005*"report" + 0.005*"need" + 0.005*"social" + 0.005*"require" + 0.004*"public" + 0.004*"individual"
topic #18 (0.040): 0.040*"company" + 0.031*"village" + 0.025*"population" + 0.014*"municipality" + 0.013*"business" + 0.012*"town" + 0.011*"region" + 0.009*"census" + 0.009*"sell" + 0.008*"found"
topic #20 (0.040): 0.026*"book" + 0.022*"publish" + 0.008*"author" + 0.008*"language" + 0.007*"novel" + 0.006*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"publication"
topic diff=0.042413, rho=0.131494
PROGRESS: pass 7, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.040): 0.026*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.007*"novel" + 0.006*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"editor"
topic #24 (0.040): 0.020*"student" + 0.018*"university" + 0.017*"college" + 0.014*"study" + 0.013*"award" + 0.012*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #19 (0.040): 0.033*"building" + 0.020*"house" + 0.014*"site" + 0.011*"design" + 0.008*"church" + 0.007*"stone" + 0.006*"th_century" + 0.006*"wall" + 0.006*"construction" + 0.006*"street"
topic #7 (0.040): 0.028*"art" + 0.027*"church" + 0.014*"artist" + 0.013*"museum" + 0.011*"painting" + 0.007*"french" + 0.007*"bishop" + 0.007*"son" + 0.007*"exhibition" + 0.006*"collection"
topic #13 (0.040): 0.019*"german" + 0.015*"government" + 0.012*"war" + 0.011*"russian" + 0.011*"country" + 0.011*"military" + 0.008*"korean" + 0.007*"soviet" + 0.007*"force" + 0.007*"polish"
topic diff=0.045150, rho=0.131494
PROGRESS: pass 7, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.040): 0.042*"event" + 0.039*"race" + 0.029*"championship" + 0.028*"compete" + 0.020*"woman" + 0.017*"finish" + 0.014*"competition" + 0.013*"sport" + 0.010*"final" + 0.009*"winner"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.005*"report" + 0.005*"information" + 0.005*"social" + 0.005*"need" + 0.005*"require" + 0.004*"individual" + 0.004*"public"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.010*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"general"
topic #2 (0.040): 0.036*"club" + 0.029*"season" + 0.023*"league" + 0.021*"match" + 0.015*"football" + 0.014*"final" + 0.014*"player" + 0.012*"score" + 0.012*"game" + 0.011*"goal"
topic #8 (0.040): 0.037*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic diff=0.047041, rho=0.131494
PROGRESS: pass 7, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.040): 0.019*"specie" + 0.011*"age" + 0.009*"river" + 0.008*"water" + 0.008*"population" + 0.007*"plant" + 0.007*"male" + 0.007*"mountain" + 0.006*"white" + 0.006*"forest"
topic #23 (0.040): 0.016*"election" + 0.014*"party" + 0.013*"government" + 0.013*"law" + 0.011*"elect" + 0.010*"vote" + 0.010*"court" + 0.007*"political" + 0.007*"president" + 0.006*"candidate"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.005*"report" + 0.005*"information" + 0.005*"social" + 0.005*"need" + 0.004*"require" + 0.004*"individual" + 0.004*"public"
topic #16 (0.040): 0.041*"event" + 0.040*"race" + 0.028*"championship" + 0.027*"compete" + 0.019*"woman" + 0.017*"finish" + 0.014*"competition" + 0.013*"sport" + 0.010*"final" + 0.009*"winner"
topic #10 (0.040): 0.012*"king" + 0.010*"cell" + 0.009*"cause" + 0.008*"disease" + 0.007*"treatment" + 0.006*"patient" + 0.006*"son" + 0.005*"increase" + 0.005*"human" + 0.005*"death"
topic diff=0.044253, rho=0.131494
PROGRESS: pass 7, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.028*"series" + 0.019*"episode" + 0.010*"character" + 0.009*"television" + 0.008*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"say" + 0.005*"air" + 0.005*"get"
topic #18 (0.040): 0.040*"company" + 0.031*"village" + 0.025*"population" + 0.014*"municipality" + 0.013*"business" + 0.012*"town" + 0.010*"region" + 0.009*"census" + 0.009*"sell" + 0.007*"industry"
topic #0 (0.040): 0.011*"displaystyle" + 0.008*"example" + 0.007*"function" + 0.006*"system" + 0.006*"define" + 0.006*"point" + 0.006*"term" + 0.006*"case" + 0.006*"structure" + 0.005*"different"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #5 (0.040): 0.017*"woman" + 0.006*"traditional" + 0.006*"temple" + 0.006*"accord" + 0.005*"tradition" + 0.005*"black" + 0.005*"mean" + 0.005*"religious" + 0.005*"often" + 0.005*"ancient"
topic diff=0.041648, rho=0.131494
-8.219 per-word bound, 297.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.032*"building" + 0.020*"house" + 0.013*"site" + 0.011*"design" + 0.009*"church" + 0.007*"stone" + 0.007*"th_century" + 0.007*"wall" + 0.006*"construction" + 0.006*"tower"
topic #7 (0.040): 0.028*"church" + 0.027*"art" + 0.014*"museum" + 0.014*"artist" + 0.011*"painting" + 0.008*"french" + 0.008*"son" + 0.007*"bishop" + 0.007*"exhibition" + 0.006*"father"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"death" + 0.006*"get" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #22 (0.040): 0.022*"road" + 0.019*"line" + 0.018*"county" + 0.015*"park" + 0.015*"north" + 0.014*"route" + 0.013*"town" + 0.012*"south" + 0.012*"railway" + 0.010*"river"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.011*"sea" + 0.010*"port" + 0.009*"navy" + 0.009*"coast" + 0.009*"crew" + 0.009*"boat" + 0.008*"vessel" + 0.008*"fleet"
topic diff=0.039581, rho=0.131494
-8.222 per-word bound, 298.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"magazine" + 0.006*"life" + 0.005*"publication"
topic #1 (0.040): 0.117*"film" + 0.024*"star" + 0.022*"award" + 0.017*"direct" + 0.013*"role" + 0.013*"movie" + 0.013*"production" + 0.011*"actor" + 0.011*"theatre" + 0.010*"director"
topic #14 (0.040): 0.018*"specie" + 0.011*"age" + 0.009*"river" + 0.008*"water" + 0.008*"population" + 0.007*"male" + 0.007*"plant" + 0.006*"mountain" + 0.006*"forest" + 0.006*"white"
topic #24 (0.040): 0.021*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.012*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #2 (0.040): 0.036*"club" + 0.029*"season" + 0.022*"league" + 0.020*"match" + 0.015*"football" + 0.015*"player" + 0.014*"final" + 0.013*"score" + 0.012*"game" + 0.012*"goal"
topic diff=0.047579, rho=0.131494
-8.227 per-word bound, 299.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.007*"engine" + 0.007*"model" + 0.007*"power" + 0.006*"vehicle" + 0.006*"produce" + 0.005*"car" + 0.005*"aircraft" + 0.005*"type"
topic #7 (0.040): 0.027*"church" + 0.027*"art" + 0.014*"museum" + 0.013*"artist" + 0.010*"painting" + 0.008*"son" + 0.008*"french" + 0.007*"exhibition" + 0.007*"bishop" + 0.007*"marry"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.015*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"division" + 0.008*"order"
topic #5 (0.040): 0.016*"woman" + 0.006*"accord" + 0.006*"temple" + 0.006*"traditional" + 0.006*"flag" + 0.005*"tradition" + 0.005*"ancient" + 0.005*"mean" + 0.005*"king" + 0.005*"black"
topic #16 (0.040): 0.042*"event" + 0.039*"race" + 0.027*"championship" + 0.026*"compete" + 0.019*"woman" + 0.016*"finish" + 0.015*"competition" + 0.013*"sport" + 0.010*"final" + 0.009*"winner"
topic diff=0.043433, rho=0.131494
-8.145 per-word bound, 283.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.040): 0.040*"game" + 0.016*"player" + 0.010*"version" + 0.009*"character" + 0.007*"computer" + 0.007*"video" + 0.006*"system" + 0.006*"code" + 0.006*"user" + 0.005*"file"
topic #12 (0.040): 0.068*"station" + 0.024*"radio" + 0.021*"network" + 0.019*"channel" + 0.016*"broadcast" + 0.015*"air" + 0.012*"news" + 0.012*"television" + 0.011*"program" + 0.011*"tv"
topic #13 (0.040): 0.020*"german" + 0.015*"government" + 0.013*"war" + 0.012*"russian" + 0.011*"country" + 0.011*"military" + 0.008*"polish" + 0.007*"soviet" + 0.006*"force" + 0.006*"operation"
topic #10 (0.040): 0.013*"king" + 0.011*"cell" + 0.009*"cause" + 0.007*"patient" + 0.007*"disease" + 0.007*"treatment" + 0.007*"son" + 0.005*"increase" + 0.005*"human" + 0.005*"death"
topic #1 (0.040): 0.118*"film" + 0.024*"star" + 0.022*"award" + 0.017*"direct" + 0.014*"movie" + 0.013*"role" + 0.012*"production" + 0.011*"theatre" + 0.011*"actor" + 0.010*"festival"
topic diff=0.041764, rho=0.130371
PROGRESS: pass 8, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.013*"displaystyle" + 0.009*"example" + 0.007*"define" + 0.007*"function" + 0.006*"system" + 0.006*"point" + 0.006*"case" + 0.006*"structure" + 0.005*"term" + 0.005*"method"
topic #1 (0.040): 0.117*"film" + 0.024*"star" + 0.022*"award" + 0.017*"direct" + 0.014*"movie" + 0.013*"role" + 0.012*"production" + 0.011*"theatre" + 0.010*"actor" + 0.010*"festival"
topic #7 (0.040): 0.026*"church" + 0.026*"art" + 0.014*"museum" + 0.013*"artist" + 0.010*"painting" + 0.008*"son" + 0.008*"french" + 0.007*"exhibition" + 0.007*"bishop" + 0.006*"marry"
topic #8 (0.040): 0.036*"album" + 0.035*"song" + 0.031*"music" + 0.025*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"publication"
topic diff=0.043864, rho=0.130371
PROGRESS: pass 8, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.018*"german" + 0.015*"government" + 0.013*"war" + 0.012*"russian" + 0.011*"country" + 0.011*"military" + 0.007*"polish" + 0.007*"soviet" + 0.007*"force" + 0.007*"operation"
topic #2 (0.040): 0.033*"club" + 0.029*"season" + 0.021*"match" + 0.021*"league" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.013*"score" + 0.012*"game" + 0.012*"goal"
topic #11 (0.040): 0.041*"game" + 0.015*"player" + 0.010*"version" + 0.009*"character" + 0.008*"computer" + 0.006*"video" + 0.006*"system" + 0.006*"user" + 0.006*"code" + 0.006*"software"
topic #22 (0.040): 0.022*"road" + 0.018*"county" + 0.018*"line" + 0.016*"park" + 0.015*"north" + 0.014*"route" + 0.014*"town" + 0.013*"railway" + 0.012*"south" + 0.010*"river"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic diff=0.040648, rho=0.130371
PROGRESS: pass 8, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.040): 0.042*"event" + 0.037*"race" + 0.029*"championship" + 0.026*"compete" + 0.019*"woman" + 0.017*"finish" + 0.014*"competition" + 0.012*"sport" + 0.010*"final" + 0.010*"winner"
topic #3 (0.040): 0.010*"design" + 0.009*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"vehicle" + 0.006*"aircraft" + 0.006*"produce" + 0.005*"car" + 0.005*"type"
topic #2 (0.040): 0.034*"club" + 0.029*"season" + 0.022*"match" + 0.021*"league" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.012*"score" + 0.012*"game" + 0.011*"round"
topic #22 (0.040): 0.022*"road" + 0.019*"county" + 0.018*"line" + 0.016*"park" + 0.015*"north" + 0.014*"route" + 0.014*"town" + 0.013*"railway" + 0.012*"south" + 0.010*"river"
topic #4 (0.040): 0.022*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.011*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic diff=0.044435, rho=0.130371
PROGRESS: pass 8, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.040): 0.012*"king" + 0.011*"cell" + 0.009*"cause" + 0.007*"patient" + 0.007*"disease" + 0.007*"treatment" + 0.006*"son" + 0.006*"human" + 0.006*"increase" + 0.005*"death"
topic #23 (0.040): 0.017*"election" + 0.014*"government" + 0.014*"party" + 0.013*"law" + 0.011*"elect" + 0.011*"court" + 0.010*"vote" + 0.007*"political" + 0.007*"general" + 0.007*"seat"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.011*"military" + 0.009*"command" + 0.008*"troop" + 0.007*"order" + 0.007*"unit"
topic #11 (0.040): 0.042*"game" + 0.015*"player" + 0.011*"version" + 0.010*"character" + 0.008*"computer" + 0.007*"video" + 0.007*"card" + 0.006*"system" + 0.006*"code" + 0.006*"user"
topic #12 (0.040): 0.069*"station" + 0.026*"radio" + 0.019*"network" + 0.018*"channel" + 0.016*"broadcast" + 0.015*"air" + 0.012*"television" + 0.012*"news" + 0.011*"operate" + 0.011*"tv"
topic diff=0.040519, rho=0.130371
PROGRESS: pass 8, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.040): 0.019*"specie" + 0.012*"age" + 0.009*"water" + 0.008*"river" + 0.008*"population" + 0.007*"male" + 0.007*"plant" + 0.006*"forest" + 0.006*"white" + 0.006*"mountain"
topic #23 (0.040): 0.017*"election" + 0.014*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.007*"act" + 0.007*"president"
topic #5 (0.040): 0.014*"woman" + 0.006*"accord" + 0.006*"tradition" + 0.005*"temple" + 0.005*"king" + 0.005*"traditional" + 0.005*"ancient" + 0.005*"mean" + 0.005*"refer" + 0.005*"often"
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"define" + 0.006*"system" + 0.006*"case" + 0.006*"structure" + 0.006*"term" + 0.005*"point" + 0.005*"different"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"son" + 0.005*"try"
topic diff=0.045770, rho=0.130371
PROGRESS: pass 8, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 10
PROGRESS: pass 8, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.040): 0.026*"book" + 0.022*"publish" + 0.008*"language" + 0.008*"author" + 0.008*"novel" + 0.007*"history" + 0.006*"life" + 0.006*"writer" + 0.006*"magazine" + 0.005*"publication"
topic #23 (0.040): 0.017*"election" + 0.014*"party" + 0.014*"government" + 0.012*"law" + 0.011*"elect" + 0.010*"vote" + 0.010*"court" + 0.007*"political" + 0.007*"president" + 0.007*"seat"
topic #19 (0.040): 0.033*"building" + 0.019*"house" + 0.013*"site" + 0.011*"design" + 0.008*"church" + 0.007*"wall" + 0.007*"stone" + 0.006*"room" + 0.006*"construction" + 0.006*"th_century"
topic #12 (0.040): 0.070*"station" + 0.026*"radio" + 0.019*"network" + 0.018*"channel" + 0.016*"air" + 0.015*"broadcast" + 0.012*"news" + 0.012*"television" + 0.011*"operate" + 0.011*"tv"
topic #14 (0.040): 0.019*"specie" + 0.011*"age" + 0.009*"water" + 0.009*"river" + 0.008*"population" + 0.007*"male" + 0.007*"plant" + 0.006*"white" + 0.006*"forest" + 0.006*"mountain"
topic diff=0.037800, rho=0.130371
PROGRESS: pass 8, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.040): 0.048*"season" + 0.047*"game" + 0.016*"football" + 0.015*"player" + 0.014*"basketball" + 0.012*"coach" + 0.010*"conference" + 0.009*"college" + 0.009*"point" + 0.009*"baseball"
topic #24 (0.040): 0.020*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #2 (0.040): 0.035*"club" + 0.029*"season" + 0.022*"league" + 0.020*"match" + 0.015*"football" + 0.014*"player" + 0.014*"final" + 0.012*"score" + 0.012*"game" + 0.011*"goal"
topic #8 (0.040): 0.036*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.016*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.008*"engine" + 0.007*"model" + 0.007*"power" + 0.006*"vehicle" + 0.006*"produce" + 0.006*"car" + 0.005*"aircraft" + 0.005*"type"
topic diff=0.038150, rho=0.130371
PROGRESS: pass 8, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.019*"german" + 0.015*"government" + 0.013*"war" + 0.011*"country" + 0.011*"military" + 0.011*"russian" + 0.008*"korean" + 0.007*"polish" + 0.007*"force" + 0.007*"soviet"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"information" + 0.005*"report" + 0.005*"public" + 0.005*"need" + 0.005*"require" + 0.004*"social" + 0.004*"individual"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"government" + 0.014*"law" + 0.011*"elect" + 0.010*"vote" + 0.010*"court" + 0.007*"political" + 0.007*"president" + 0.007*"candidate"
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.008*"engine" + 0.007*"model" + 0.007*"power" + 0.006*"vehicle" + 0.006*"produce" + 0.006*"aircraft" + 0.005*"car" + 0.005*"type"
topic #6 (0.040): 0.033*"ship" + 0.029*"island" + 0.012*"sea" + 0.010*"boat" + 0.009*"port" + 0.009*"navy" + 0.009*"crew" + 0.008*"coast" + 0.008*"vessel" + 0.007*"naval"
topic diff=0.039856, rho=0.130371
PROGRESS: pass 8, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.030*"series" + 0.018*"episode" + 0.010*"character" + 0.009*"television" + 0.008*"season" + 0.008*"appear" + 0.005*"air" + 0.005*"story" + 0.005*"say" + 0.005*"get"
topic #22 (0.040): 0.022*"road" + 0.019*"line" + 0.018*"county" + 0.015*"north" + 0.015*"park" + 0.015*"route" + 0.014*"town" + 0.012*"railway" + 0.012*"south" + 0.012*"river"
topic #8 (0.040): 0.037*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.011*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"system" + 0.006*"define" + 0.006*"case" + 0.006*"term" + 0.005*"structure" + 0.005*"point" + 0.005*"different"
topic #14 (0.040): 0.019*"specie" + 0.011*"age" + 0.009*"river" + 0.009*"water" + 0.007*"population" + 0.007*"plant" + 0.007*"male" + 0.007*"mountain" + 0.006*"forest" + 0.006*"white"
topic diff=0.037765, rho=0.130371
PROGRESS: pass 8, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 10
PROGRESS: pass 8, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.040): 0.022*"road" + 0.019*"line" + 0.018*"county" + 0.015*"north" + 0.015*"route" + 0.014*"park" + 0.014*"town" + 0.012*"railway" + 0.012*"south" + 0.012*"river"
topic #12 (0.040): 0.071*"station" + 0.027*"radio" + 0.019*"network" + 0.017*"channel" + 0.016*"broadcast" + 0.015*"air" + 0.012*"television" + 0.012*"news" + 0.011*"launch" + 0.011*"program"
topic #9 (0.040): 0.030*"series" + 0.019*"episode" + 0.010*"character" + 0.009*"television" + 0.009*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"air" + 0.005*"say" + 0.005*"get"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.012*"sea" + 0.010*"crew" + 0.009*"coast" + 0.009*"navy" + 0.009*"port" + 0.009*"boat" + 0.008*"vessel" + 0.008*"naval"
topic #8 (0.040): 0.038*"album" + 0.036*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic diff=0.040388, rho=0.130371
PROGRESS: pass 8, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.030*"church" + 0.026*"art" + 0.013*"artist" + 0.012*"museum" + 0.011*"painting" + 0.008*"son" + 0.008*"french" + 0.007*"bishop" + 0.007*"marry" + 0.006*"father"
topic #2 (0.040): 0.037*"club" + 0.030*"season" + 0.023*"league" + 0.021*"match" + 0.016*"football" + 0.014*"final" + 0.014*"player" + 0.012*"score" + 0.012*"game" + 0.011*"goal"
topic #20 (0.040): 0.026*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.007*"novel" + 0.006*"writer" + 0.006*"history" + 0.006*"life" + 0.006*"magazine" + 0.005*"editor"
topic #22 (0.040): 0.023*"road" + 0.019*"line" + 0.018*"county" + 0.015*"north" + 0.015*"park" + 0.015*"route" + 0.014*"town" + 0.012*"railway" + 0.012*"south" + 0.012*"river"
topic #12 (0.040): 0.070*"station" + 0.028*"radio" + 0.018*"network" + 0.017*"channel" + 0.017*"broadcast" + 0.016*"air" + 0.012*"television" + 0.012*"news" + 0.011*"program" + 0.011*"operate"
topic diff=0.042610, rho=0.130371
PROGRESS: pass 8, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.040): 0.028*"series" + 0.019*"episode" + 0.011*"character" + 0.009*"television" + 0.008*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"air" + 0.005*"say" + 0.005*"get"
topic #8 (0.040): 0.038*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #16 (0.040): 0.040*"event" + 0.039*"race" + 0.029*"championship" + 0.026*"compete" + 0.021*"woman" + 0.017*"finish" + 0.015*"competition" + 0.013*"sport" + 0.011*"final" + 0.009*"winner"
topic #0 (0.040): 0.010*"displaystyle" + 0.009*"example" + 0.007*"system" + 0.007*"function" + 0.006*"define" + 0.006*"point" + 0.006*"case" + 0.006*"structure" + 0.006*"term" + 0.005*"different"
topic #15 (0.040): 0.048*"season" + 0.046*"game" + 0.016*"football" + 0.014*"player" + 0.013*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.009*"college" + 0.009*"point" + 0.009*"finish"
topic diff=0.039813, rho=0.130371
PROGRESS: pass 8, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.007*"system" + 0.006*"define" + 0.006*"point" + 0.006*"term" + 0.006*"case" + 0.006*"structure" + 0.005*"different"
topic #2 (0.040): 0.036*"club" + 0.030*"season" + 0.022*"league" + 0.021*"match" + 0.016*"football" + 0.014*"player" + 0.014*"final" + 0.013*"score" + 0.012*"game" + 0.011*"goal"
topic #22 (0.040): 0.022*"road" + 0.019*"line" + 0.019*"county" + 0.015*"north" + 0.015*"park" + 0.014*"route" + 0.013*"town" + 0.012*"railway" + 0.012*"south" + 0.011*"river"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.015*"force" + 0.015*"battle" + 0.011*"attack" + 0.010*"military" + 0.010*"command" + 0.008*"order" + 0.008*"troop" + 0.007*"general"
topic #1 (0.040): 0.116*"film" + 0.025*"star" + 0.023*"award" + 0.017*"direct" + 0.013*"role" + 0.013*"movie" + 0.013*"production" + 0.012*"theatre" + 0.012*"actor" + 0.010*"director"
topic diff=0.037460, rho=0.130371
-8.210 per-word bound, 296.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.040): 0.028*"church" + 0.026*"art" + 0.013*"museum" + 0.013*"artist" + 0.011*"painting" + 0.010*"son" + 0.008*"french" + 0.007*"bishop" + 0.007*"marry" + 0.007*"father"
topic #24 (0.040): 0.021*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.012*"research" + 0.011*"education" + 0.011*"science" + 0.010*"graduate" + 0.009*"program"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.011*"sea" + 0.010*"port" + 0.009*"navy" + 0.009*"coast" + 0.009*"crew" + 0.009*"boat" + 0.008*"vessel" + 0.008*"fleet"
topic #3 (0.040): 0.010*"design" + 0.009*"system" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.006*"produce" + 0.005*"car" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"light"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.006*"report" + 0.005*"information" + 0.005*"need" + 0.005*"require" + 0.004*"social" + 0.004*"public" + 0.004*"individual"
topic diff=0.035469, rho=0.130371
-8.215 per-word bound, 297.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #24 (0.040): 0.021*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.012*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #14 (0.040): 0.019*"specie" + 0.011*"age" + 0.009*"water" + 0.008*"population" + 0.008*"river" + 0.008*"male" + 0.007*"plant" + 0.006*"forest" + 0.006*"white" + 0.006*"mountain"
topic #13 (0.040): 0.021*"german" + 0.015*"government" + 0.013*"war" + 0.012*"russian" + 0.011*"country" + 0.010*"military" + 0.007*"polish" + 0.007*"soviet" + 0.007*"force" + 0.006*"police"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"death" + 0.006*"get" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #6 (0.040): 0.032*"ship" + 0.027*"island" + 0.012*"sea" + 0.010*"port" + 0.010*"navy" + 0.009*"coast" + 0.009*"crew" + 0.009*"boat" + 0.009*"vessel" + 0.008*"fleet"
topic diff=0.034429, rho=0.130371
-8.182 per-word bound, 290.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.040): 0.037*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.007*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"produce" + 0.006*"vehicle" + 0.005*"aircraft" + 0.005*"car" + 0.005*"light"
topic #5 (0.040): 0.013*"woman" + 0.006*"accord" + 0.006*"king" + 0.006*"traditional" + 0.005*"tradition" + 0.005*"ancient" + 0.005*"temple" + 0.005*"mean" + 0.005*"refer" + 0.005*"black"
topic #7 (0.040): 0.027*"church" + 0.025*"art" + 0.014*"museum" + 0.013*"artist" + 0.010*"son" + 0.010*"painting" + 0.008*"french" + 0.007*"marry" + 0.007*"father" + 0.007*"daughter"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"government" + 0.013*"law" + 0.012*"elect" + 0.010*"vote" + 0.010*"court" + 0.007*"political" + 0.007*"president" + 0.007*"general"
topic diff=0.037791, rho=0.129277
PROGRESS: pass 9, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"get" + 0.007*"tell" + 0.007*"death" + 0.006*"back" + 0.006*"father" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #20 (0.040): 0.025*"book" + 0.021*"publish" + 0.009*"language" + 0.008*"author" + 0.008*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"publication"
topic #9 (0.040): 0.029*"series" + 0.019*"episode" + 0.010*"character" + 0.009*"television" + 0.009*"appear" + 0.008*"season" + 0.005*"story" + 0.005*"air" + 0.005*"say" + 0.005*"get"
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.007*"define" + 0.007*"system" + 0.006*"point" + 0.006*"case" + 0.006*"structure" + 0.006*"term" + 0.005*"method"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"government" + 0.013*"law" + 0.012*"elect" + 0.010*"vote" + 0.010*"court" + 0.007*"political" + 0.007*"general" + 0.007*"president"
topic diff=0.039802, rho=0.129277
PROGRESS: pass 9, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.040): 0.116*"film" + 0.024*"star" + 0.023*"award" + 0.017*"direct" + 0.013*"movie" + 0.013*"production" + 0.013*"role" + 0.012*"theatre" + 0.011*"actor" + 0.011*"festival"
topic #7 (0.040): 0.026*"church" + 0.026*"art" + 0.014*"museum" + 0.013*"artist" + 0.010*"son" + 0.010*"painting" + 0.008*"french" + 0.008*"marry" + 0.007*"daughter" + 0.007*"bishop"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"child" + 0.006*"father" + 0.006*"back" + 0.005*"try" + 0.005*"son"
topic #9 (0.040): 0.029*"series" + 0.019*"episode" + 0.010*"character" + 0.009*"television" + 0.009*"appear" + 0.008*"season" + 0.005*"air" + 0.005*"story" + 0.005*"say" + 0.005*"get"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.007*"division"
topic diff=0.037002, rho=0.129277
PROGRESS: pass 9, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.040): 0.018*"german" + 0.015*"government" + 0.013*"war" + 0.011*"country" + 0.011*"russian" + 0.011*"military" + 0.010*"korean" + 0.007*"polish" + 0.007*"force" + 0.007*"soviet"
topic #6 (0.040): 0.031*"ship" + 0.030*"island" + 0.011*"sea" + 0.010*"crew" + 0.010*"port" + 0.009*"boat" + 0.009*"navy" + 0.009*"coast" + 0.008*"vessel" + 0.008*"fleet"
topic #16 (0.040): 0.041*"event" + 0.037*"race" + 0.029*"championship" + 0.026*"compete" + 0.021*"woman" + 0.017*"finish" + 0.014*"competition" + 0.012*"sport" + 0.011*"final" + 0.010*"winner"
topic #0 (0.040): 0.013*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"define" + 0.006*"system" + 0.006*"point" + 0.006*"term" + 0.006*"structure" + 0.006*"case" + 0.005*"method"
topic #15 (0.040): 0.049*"season" + 0.047*"game" + 0.015*"football" + 0.015*"player" + 0.014*"basketball" + 0.012*"coach" + 0.009*"league" + 0.009*"conference" + 0.009*"college" + 0.009*"baseball"
topic diff=0.040508, rho=0.129277
PROGRESS: pass 9, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
PROGRESS: pass 9, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.011*"court" + 0.010*"vote" + 0.007*"political" + 0.007*"president" + 0.007*"general"
topic #11 (0.040): 0.043*"game" + 0.016*"player" + 0.011*"version" + 0.009*"character" + 0.008*"computer" + 0.007*"card" + 0.007*"video" + 0.006*"system" + 0.006*"user" + 0.006*"software"
topic #15 (0.040): 0.049*"season" + 0.047*"game" + 0.015*"football" + 0.014*"player" + 0.014*"basketball" + 0.012*"coach" + 0.009*"league" + 0.009*"baseball" + 0.009*"conference" + 0.009*"college"
topic #12 (0.040): 0.066*"station" + 0.025*"radio" + 0.019*"network" + 0.018*"channel" + 0.015*"broadcast" + 0.015*"air" + 0.013*"news" + 0.013*"television" + 0.011*"tv" + 0.011*"operate"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"information" + 0.006*"increase" + 0.005*"report" + 0.005*"public" + 0.005*"need" + 0.004*"social" + 0.004*"require" + 0.004*"individual"
topic diff=0.037447, rho=0.129277
PROGRESS: pass 9, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.008*"engine" + 0.007*"power" + 0.007*"model" + 0.006*"car" + 0.006*"vehicle" + 0.006*"produce" + 0.005*"aircraft" + 0.005*"type"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.010*"court" + 0.010*"vote" + 0.007*"political" + 0.007*"candidate" + 0.007*"president"
topic #7 (0.040): 0.026*"art" + 0.025*"church" + 0.013*"museum" + 0.013*"artist" + 0.011*"son" + 0.010*"painting" + 0.009*"french" + 0.008*"father" + 0.008*"marry" + 0.007*"daughter"
topic #14 (0.040): 0.019*"specie" + 0.012*"age" + 0.009*"water" + 0.008*"river" + 0.008*"population" + 0.007*"plant" + 0.007*"male" + 0.006*"forest" + 0.006*"white" + 0.006*"mountain"
topic #12 (0.040): 0.069*"station" + 0.026*"radio" + 0.019*"network" + 0.017*"channel" + 0.015*"broadcast" + 0.015*"air" + 0.012*"news" + 0.012*"television" + 0.011*"host" + 0.011*"tv"
topic diff=0.038417, rho=0.129277
PROGRESS: pass 9, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"system" + 0.006*"case" + 0.006*"define" + 0.006*"structure" + 0.006*"term" + 0.006*"point" + 0.005*"method"
topic #6 (0.040): 0.032*"ship" + 0.029*"island" + 0.011*"sea" + 0.010*"boat" + 0.010*"port" + 0.009*"crew" + 0.009*"navy" + 0.008*"coast" + 0.008*"vessel" + 0.007*"fleet"
topic #12 (0.040): 0.068*"station" + 0.026*"radio" + 0.019*"network" + 0.017*"channel" + 0.016*"broadcast" + 0.015*"air" + 0.013*"news" + 0.012*"television" + 0.011*"host" + 0.011*"operate"
topic #24 (0.040): 0.021*"student" + 0.018*"college" + 0.017*"university" + 0.014*"study" + 0.013*"award" + 0.011*"research" + 0.011*"education" + 0.010*"science" + 0.010*"graduate" + 0.009*"program"
topic #16 (0.040): 0.039*"event" + 0.038*"race" + 0.027*"championship" + 0.026*"compete" + 0.021*"woman" + 0.017*"finish" + 0.014*"competition" + 0.013*"sport" + 0.011*"final" + 0.010*"winner"
topic diff=0.035048, rho=0.129277
PROGRESS: pass 9, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.012*"woman" + 0.007*"king" + 0.006*"accord" + 0.005*"tradition" + 0.005*"ancient" + 0.005*"traditional" + 0.005*"temple" + 0.005*"mean" + 0.005*"refer" + 0.005*"often"
topic #10 (0.040): 0.012*"cell" + 0.009*"cause" + 0.008*"patient" + 0.008*"disease" + 0.007*"treatment" + 0.007*"king" + 0.007*"human" + 0.006*"increase" + 0.005*"drug" + 0.005*"effect"
topic #1 (0.040): 0.114*"film" + 0.025*"award" + 0.024*"star" + 0.017*"direct" + 0.014*"role" + 0.013*"movie" + 0.013*"production" + 0.012*"theatre" + 0.011*"actor" + 0.010*"festival"
topic #0 (0.040): 0.011*"displaystyle" + 0.009*"example" + 0.007*"function" + 0.006*"system" + 0.006*"case" + 0.006*"define" + 0.006*"point" + 0.006*"term" + 0.006*"structure" + 0.005*"value"
topic #21 (0.040): 0.010*"say" + 0.007*"tell" + 0.007*"kill" + 0.006*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic diff=0.038504, rho=0.129277
PROGRESS: pass 9, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 10
PROGRESS: pass 9, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.033*"building" + 0.019*"house" + 0.014*"site" + 0.011*"design" + 0.008*"church" + 0.006*"stone" + 0.006*"wall" + 0.006*"construction" + 0.006*"room" + 0.006*"street"
topic #12 (0.040): 0.067*"station" + 0.028*"radio" + 0.019*"network" + 0.017*"channel" + 0.016*"broadcast" + 0.015*"air" + 0.012*"television" + 0.012*"news" + 0.011*"operate" + 0.011*"host"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"father" + 0.006*"back" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.008*"engine" + 0.007*"model" + 0.007*"power" + 0.006*"vehicle" + 0.006*"produce" + 0.006*"aircraft" + 0.005*"car" + 0.005*"type"
topic #14 (0.040): 0.019*"specie" + 0.011*"age" + 0.009*"water" + 0.009*"river" + 0.007*"population" + 0.007*"plant" + 0.007*"male" + 0.006*"mountain" + 0.006*"forest" + 0.006*"white"
topic diff=0.036691, rho=0.129277
PROGRESS: pass 9, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.040): 0.037*"album" + 0.035*"song" + 0.032*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.011*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #10 (0.040): 0.012*"cell" + 0.010*"cause" + 0.008*"treatment" + 0.008*"patient" + 0.007*"disease" + 0.007*"human" + 0.006*"king" + 0.006*"increase" + 0.005*"effect" + 0.005*"protein"
topic #7 (0.040): 0.026*"church" + 0.026*"art" + 0.013*"artist" + 0.012*"museum" + 0.011*"painting" + 0.010*"son" + 0.008*"french" + 0.007*"father" + 0.007*"marry" + 0.007*"daughter"
topic #4 (0.040): 0.021*"army" + 0.020*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"troop" + 0.008*"order" + 0.008*"general"
topic #18 (0.040): 0.039*"company" + 0.031*"village" + 0.026*"population" + 0.014*"municipality" + 0.014*"town" + 0.012*"business" + 0.011*"region" + 0.010*"census" + 0.009*"sell" + 0.008*"found"
topic diff=0.034523, rho=0.129277
PROGRESS: pass 9, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.040): 0.012*"displaystyle" + 0.009*"example" + 0.007*"system" + 0.007*"function" + 0.006*"case" + 0.006*"term" + 0.006*"define" + 0.005*"structure" + 0.005*"point" + 0.005*"different"
topic #22 (0.040): 0.022*"road" + 0.019*"line" + 0.019*"county" + 0.015*"north" + 0.015*"route" + 0.014*"park" + 0.013*"town" + 0.013*"river" + 0.012*"railway" + 0.012*"south"
topic #12 (0.040): 0.069*"station" + 0.027*"radio" + 0.019*"network" + 0.017*"channel" + 0.016*"broadcast" + 0.016*"air" + 0.012*"television" + 0.012*"news" + 0.011*"launch" + 0.011*"program"
topic #19 (0.040): 0.033*"building" + 0.020*"house" + 0.014*"site" + 0.012*"design" + 0.008*"church" + 0.006*"stone" + 0.006*"street" + 0.006*"construction" + 0.006*"wall" + 0.006*"th_century"
topic #13 (0.040): 0.019*"german" + 0.016*"government" + 0.012*"war" + 0.012*"country" + 0.012*"russian" + 0.010*"military" + 0.008*"korean" + 0.007*"soviet" + 0.007*"polish" + 0.007*"force"
topic diff=0.037347, rho=0.129277
PROGRESS: pass 9, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.013*"woman" + 0.007*"king" + 0.006*"accord" + 0.006*"ancient" + 0.005*"traditional" + 0.005*"black" + 0.005*"mean" + 0.005*"refer" + 0.005*"tradition" + 0.005*"temple"
topic #1 (0.040): 0.115*"film" + 0.026*"star" + 0.024*"award" + 0.017*"direct" + 0.014*"role" + 0.013*"movie" + 0.013*"production" + 0.013*"theatre" + 0.012*"actor" + 0.010*"festival"
topic #18 (0.040): 0.039*"company" + 0.031*"village" + 0.027*"population" + 0.014*"municipality" + 0.014*"town" + 0.013*"business" + 0.011*"region" + 0.010*"census" + 0.009*"sell" + 0.008*"found"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.013*"law" + 0.013*"government" + 0.011*"elect" + 0.011*"vote" + 0.009*"court" + 0.007*"political" + 0.007*"president" + 0.007*"candidate"
topic #7 (0.040): 0.030*"church" + 0.026*"art" + 0.013*"artist" + 0.012*"museum" + 0.011*"painting" + 0.010*"son" + 0.008*"french" + 0.007*"marry" + 0.007*"father" + 0.007*"bishop"
topic diff=0.039732, rho=0.129277
PROGRESS: pass 9, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 10
PROGRESS: pass 9, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.040): 0.037*"club" + 0.030*"season" + 0.022*"league" + 0.021*"match" + 0.016*"football" + 0.014*"final" + 0.014*"player" + 0.013*"score" + 0.012*"goal" + 0.011*"game"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"law" + 0.013*"government" + 0.011*"elect" + 0.010*"vote" + 0.010*"court" + 0.007*"political" + 0.007*"president" + 0.007*"candidate"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.007*"get" + 0.006*"death" + 0.006*"back" + 0.006*"father" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic #14 (0.040): 0.020*"specie" + 0.011*"age" + 0.009*"water" + 0.008*"river" + 0.007*"population" + 0.007*"plant" + 0.007*"male" + 0.007*"white" + 0.006*"mountain" + 0.006*"forest"
topic #7 (0.040): 0.029*"church" + 0.026*"art" + 0.013*"artist" + 0.013*"museum" + 0.011*"son" + 0.010*"painting" + 0.008*"french" + 0.007*"father" + 0.007*"marry" + 0.007*"daughter"
topic diff=0.036618, rho=0.129277
PROGRESS: pass 9, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.040): 0.032*"building" + 0.020*"house" + 0.014*"site" + 0.011*"design" + 0.009*"church" + 0.007*"wall" + 0.007*"construction" + 0.007*"stone" + 0.006*"th_century" + 0.006*"street"
topic #6 (0.040): 0.031*"ship" + 0.028*"island" + 0.012*"sea" + 0.010*"port" + 0.010*"crew" + 0.010*"coast" + 0.009*"navy" + 0.009*"boat" + 0.008*"vessel" + 0.008*"naval"
topic #20 (0.040): 0.026*"book" + 0.022*"publish" + 0.008*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.007*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"story"
topic #17 (0.040): 0.008*"provide" + 0.006*"system" + 0.006*"increase" + 0.005*"report" + 0.005*"information" + 0.005*"need" + 0.005*"social" + 0.005*"require" + 0.004*"public" + 0.004*"individual"
topic #7 (0.040): 0.029*"church" + 0.025*"art" + 0.013*"museum" + 0.013*"artist" + 0.011*"son" + 0.010*"painting" + 0.008*"french" + 0.008*"marry" + 0.007*"daughter" + 0.007*"father"
topic diff=0.034694, rho=0.129277
-8.223 per-word bound, 298.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.040): 0.012*"woman" + 0.008*"king" + 0.006*"traditional" + 0.006*"accord" + 0.005*"ancient" + 0.005*"tradition" + 0.005*"temple" + 0.005*"mean" + 0.005*"refer" + 0.005*"black"
topic #15 (0.040): 0.049*"season" + 0.046*"game" + 0.016*"football" + 0.015*"player" + 0.014*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.009*"league" + 0.009*"college" + 0.009*"point"
topic #18 (0.040): 0.040*"company" + 0.032*"village" + 0.027*"population" + 0.014*"municipality" + 0.014*"town" + 0.013*"business" + 0.011*"region" + 0.010*"census" + 0.009*"sell" + 0.008*"industry"
topic #3 (0.040): 0.011*"design" + 0.009*"system" + 0.007*"power" + 0.007*"engine" + 0.006*"model" + 0.006*"produce" + 0.006*"car" + 0.005*"aircraft" + 0.005*"vehicle" + 0.005*"light"
topic #21 (0.040): 0.010*"say" + 0.007*"kill" + 0.007*"tell" + 0.006*"get" + 0.006*"death" + 0.006*"back" + 0.006*"father" + 0.006*"child" + 0.005*"try" + 0.005*"son"
topic diff=0.032682, rho=0.129277
-8.227 per-word bound, 299.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.040): 0.026*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.007*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"publication"
topic #12 (0.040): 0.065*"station" + 0.025*"radio" + 0.019*"network" + 0.017*"channel" + 0.016*"broadcast" + 0.015*"air" + 0.013*"news" + 0.012*"television" + 0.011*"tv" + 0.011*"operate"
topic #22 (0.040): 0.023*"road" + 0.020*"county" + 0.019*"line" + 0.015*"north" + 0.014*"park" + 0.014*"route" + 0.013*"town" + 0.013*"south" + 0.012*"river" + 0.011*"railway"
topic #10 (0.040): 0.013*"cell" + 0.010*"cause" + 0.008*"disease" + 0.008*"patient" + 0.007*"treatment" + 0.007*"human" + 0.006*"protein" + 0.006*"increase" + 0.006*"gene" + 0.006*"drug"
topic #4 (0.040): 0.021*"army" + 0.019*"war" + 0.016*"force" + 0.014*"battle" + 0.011*"attack" + 0.010*"military" + 0.009*"command" + 0.008*"division" + 0.008*"order" + 0.008*"troop"
topic diff=0.040854, rho=0.129277
-8.231 per-word bound, 300.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #20 (0.040): 0.026*"book" + 0.021*"publish" + 0.008*"language" + 0.008*"author" + 0.007*"novel" + 0.007*"history" + 0.006*"writer" + 0.006*"life" + 0.006*"magazine" + 0.005*"story"
topic #18 (0.040): 0.040*"company" + 0.032*"village" + 0.027*"population" + 0.014*"town" + 0.013*"municipality" + 0.012*"business" + 0.011*"region" + 0.010*"census" + 0.009*"sell" + 0.008*"total"
topic #13 (0.040): 0.020*"german" + 0.015*"government" + 0.013*"war" + 0.012*"russian" + 0.012*"country" + 0.010*"military" + 0.007*"polish" + 0.007*"soviet" + 0.006*"force" + 0.006*"police"
topic #23 (0.040): 0.017*"election" + 0.015*"party" + 0.014*"government" + 0.013*"law" + 0.011*"elect" + 0.011*"court" + 0.011*"vote" + 0.007*"political" + 0.007*"general" + 0.007*"president"
topic #5 (0.040): 0.011*"woman" + 0.008*"king" + 0.006*"accord" + 0.006*"ancient" + 0.006*"traditional" + 0.005*"tradition" + 0.005*"temple" + 0.005*"mean" + 0.005*"refer" + 0.005*"flag"
topic diff=0.037175, rho=0.129277
-8.146 per-word bound, 283.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=26262, num_topics=25, decay=0.5, chunksize=1000) in 1263.69s', 'datetime': '2022-02-06T23:53:48.232538', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows
1 batches submitted to accumulate stats from 64 documents (2187 virtual)
2 batches submitted to accumulate stats from 128 documents (6627 virtual)
3 batches submitted to accumulate stats from 192 documents (7985 virtual)
4 batches submitted to accumulate stats from 256 documents (8076 virtual)
5 batches submitted to accumulate stats from 320 documents (9017 virtual)
6 batches submitted to accumulate stats from 384 documents (13106 virtual)
9 batches submitted to accumulate stats from 576 documents (14221 virtual)
10 batches submitted to accumulate stats from 640 documents (16306 virtual)
11 batches submitted to accumulate stats from 704 documents (17037 virtual)
12 batches submitted to accumulate stats from 768 documents (21470 virtual)
13 batches submitted to accumulate stats from 832 documents (24265 virtual)
14 batches submitted to accumulate stats from 896 documents (24296 virtual)
16 batches submitted to accumulate stats from 1024 documents (25785 virtual)
17 batches submitted to accumulate stats from 1088 documents (27173 virtual)
18 batches submitted to accumulate stats from 1152 documents (31133 virtual)
20 batches submitted to accumulate stats from 1280 documents (34520 virtual)
21 batches submitted to accumulate stats from 1344 documents (36066 virtual)
22 batches submitted to accumulate stats from 1408 documents (36851 virtual)
23 batches submitted to accumulate stats from 1472 documents (41824 virtual)
24 batches submitted to accumulate stats from 1536 documents (43552 virtual)
25 batches submitted to accumulate stats from 1600 documents (45359 virtual)
27 batches submitted to accumulate stats from 1728 documents (46108 virtual)
28 batches submitted to accumulate stats from 1792 documents (53552 virtual)
29 batches submitted to accumulate stats from 1856 documents (57074 virtual)
30 batches submitted to accumulate stats from 1920 documents (57951 virtual)
32 batches submitted to accumulate stats from 2048 documents (57638 virtual)
33 batches submitted to accumulate stats from 2112 documents (58719 virtual)
35 batches submitted to accumulate stats from 2240 documents (62908 virtual)
36 batches submitted to accumulate stats from 2304 documents (64456 virtual)
37 batches submitted to accumulate stats from 2368 documents (65597 virtual)
39 batches submitted to accumulate stats from 2496 documents (69766 virtual)
40 batches submitted to accumulate stats from 2560 documents (70921 virtual)
41 batches submitted to accumulate stats from 2624 documents (71109 virtual)
42 batches submitted to accumulate stats from 2688 documents (71178 virtual)
43 batches submitted to accumulate stats from 2752 documents (76082 virtual)
45 batches submitted to accumulate stats from 2880 documents (79210 virtual)
46 batches submitted to accumulate stats from 2944 documents (82440 virtual)
47 batches submitted to accumulate stats from 3008 documents (87942 virtual)
48 batches submitted to accumulate stats from 3072 documents (88860 virtual)
50 batches submitted to accumulate stats from 3200 documents (89887 virtual)
52 batches submitted to accumulate stats from 3328 documents (93174 virtual)
53 batches submitted to accumulate stats from 3392 documents (94412 virtual)
54 batches submitted to accumulate stats from 3456 documents (99286 virtual)
56 batches submitted to accumulate stats from 3584 documents (101028 virtual)
57 batches submitted to accumulate stats from 3648 documents (105944 virtual)
58 batches submitted to accumulate stats from 3712 documents (107454 virtual)
59 batches submitted to accumulate stats from 3776 documents (108263 virtual)
60 batches submitted to accumulate stats from 3840 documents (108886 virtual)
62 batches submitted to accumulate stats from 3968 documents (110170 virtual)
64 batches submitted to accumulate stats from 4096 documents (111855 virtual)
65 batches submitted to accumulate stats from 4160 documents (112840 virtual)
68 batches submitted to accumulate stats from 4352 documents (119084 virtual)
69 batches submitted to accumulate stats from 4416 documents (127953 virtual)
70 batches submitted to accumulate stats from 4480 documents (128270 virtual)
71 batches submitted to accumulate stats from 4544 documents (132541 virtual)
72 batches submitted to accumulate stats from 4608 documents (135743 virtual)
75 batches submitted to accumulate stats from 4800 documents (134523 virtual)
76 batches submitted to accumulate stats from 4864 documents (138997 virtual)
77 batches submitted to accumulate stats from 4928 documents (141922 virtual)
78 batches submitted to accumulate stats from 4992 documents (143640 virtual)
79 batches submitted to accumulate stats from 5056 documents (146163 virtual)
81 batches submitted to accumulate stats from 5184 documents (146732 virtual)
84 batches submitted to accumulate stats from 5376 documents (147243 virtual)
85 batches submitted to accumulate stats from 5440 documents (148422 virtual)
86 batches submitted to accumulate stats from 5504 documents (150110 virtual)
89 batches submitted to accumulate stats from 5696 documents (151436 virtual)
90 batches submitted to accumulate stats from 5760 documents (154672 virtual)
91 batches submitted to accumulate stats from 5824 documents (155971 virtual)
92 batches submitted to accumulate stats from 5888 documents (156491 virtual)
93 batches submitted to accumulate stats from 5952 documents (162297 virtual)
94 batches submitted to accumulate stats from 6016 documents (165092 virtual)
95 batches submitted to accumulate stats from 6080 documents (166015 virtual)
96 batches submitted to accumulate stats from 6144 documents (168909 virtual)
98 batches submitted to accumulate stats from 6272 documents (172265 virtual)
99 batches submitted to accumulate stats from 6336 documents (174304 virtual)
102 batches submitted to accumulate stats from 6528 documents (177888 virtual)
105 batches submitted to accumulate stats from 6720 documents (179168 virtual)
106 batches submitted to accumulate stats from 6784 documents (179615 virtual)
107 batches submitted to accumulate stats from 6848 documents (181710 virtual)
108 batches submitted to accumulate stats from 6912 documents (181786 virtual)
109 batches submitted to accumulate stats from 6976 documents (183292 virtual)
111 batches submitted to accumulate stats from 7104 documents (181433 virtual)
112 batches submitted to accumulate stats from 7168 documents (184315 virtual)
113 batches submitted to accumulate stats from 7232 documents (185111 virtual)
115 batches submitted to accumulate stats from 7360 documents (190181 virtual)
116 batches submitted to accumulate stats from 7424 documents (192131 virtual)
118 batches submitted to accumulate stats from 7552 documents (194591 virtual)
119 batches submitted to accumulate stats from 7616 documents (196760 virtual)
120 batches submitted to accumulate stats from 7680 documents (197270 virtual)
122 batches submitted to accumulate stats from 7808 documents (197083 virtual)
123 batches submitted to accumulate stats from 7872 documents (197181 virtual)
124 batches submitted to accumulate stats from 7936 documents (199549 virtual)
125 batches submitted to accumulate stats from 8000 documents (200647 virtual)
126 batches submitted to accumulate stats from 8064 documents (202808 virtual)
128 batches submitted to accumulate stats from 8192 documents (206701 virtual)
129 batches submitted to accumulate stats from 8256 documents (207922 virtual)
130 batches submitted to accumulate stats from 8320 documents (212220 virtual)
133 batches submitted to accumulate stats from 8512 documents (211453 virtual)
134 batches submitted to accumulate stats from 8576 documents (215935 virtual)
135 batches submitted to accumulate stats from 8640 documents (215992 virtual)
138 batches submitted to accumulate stats from 8832 documents (219990 virtual)
139 batches submitted to accumulate stats from 8896 documents (226322 virtual)
140 batches submitted to accumulate stats from 8960 documents (228173 virtual)
141 batches submitted to accumulate stats from 9024 documents (230107 virtual)
142 batches submitted to accumulate stats from 9088 documents (231260 virtual)
143 batches submitted to accumulate stats from 9152 documents (237236 virtual)
144 batches submitted to accumulate stats from 9216 documents (241832 virtual)
145 batches submitted to accumulate stats from 9280 documents (242342 virtual)
148 batches submitted to accumulate stats from 9472 documents (247024 virtual)
149 batches submitted to accumulate stats from 9536 documents (248318 virtual)
150 batches submitted to accumulate stats from 9600 documents (249660 virtual)
151 batches submitted to accumulate stats from 9664 documents (252300 virtual)
153 batches submitted to accumulate stats from 9792 documents (255839 virtual)
155 batches submitted to accumulate stats from 9920 documents (257224 virtual)
156 batches submitted to accumulate stats from 9984 documents (262716 virtual)
159 batches submitted to accumulate stats from 10176 documents (261990 virtual)
160 batches submitted to accumulate stats from 10240 documents (262728 virtual)
161 batches submitted to accumulate stats from 10304 documents (263833 virtual)
162 batches submitted to accumulate stats from 10368 documents (270005 virtual)
164 batches submitted to accumulate stats from 10496 documents (268664 virtual)
167 batches submitted to accumulate stats from 10688 documents (270173 virtual)
170 batches submitted to accumulate stats from 10880 documents (272998 virtual)
171 batches submitted to accumulate stats from 10944 documents (273706 virtual)
172 batches submitted to accumulate stats from 11008 documents (277562 virtual)
173 batches submitted to accumulate stats from 11072 documents (282943 virtual)
174 batches submitted to accumulate stats from 11136 documents (285137 virtual)
175 batches submitted to accumulate stats from 11200 documents (285230 virtual)
176 batches submitted to accumulate stats from 11264 documents (286675 virtual)
177 batches submitted to accumulate stats from 11328 documents (289316 virtual)
178 batches submitted to accumulate stats from 11392 documents (294950 virtual)
179 batches submitted to accumulate stats from 11456 documents (299009 virtual)
181 batches submitted to accumulate stats from 11584 documents (299733 virtual)
183 batches submitted to accumulate stats from 11712 documents (301551 virtual)
184 batches submitted to accumulate stats from 11776 documents (302034 virtual)
185 batches submitted to accumulate stats from 11840 documents (305720 virtual)
188 batches submitted to accumulate stats from 12032 documents (304717 virtual)
190 batches submitted to accumulate stats from 12160 documents (304727 virtual)
191 batches submitted to accumulate stats from 12224 documents (307234 virtual)
192 batches submitted to accumulate stats from 12288 documents (308881 virtual)
193 batches submitted to accumulate stats from 12352 documents (310391 virtual)
194 batches submitted to accumulate stats from 12416 documents (313829 virtual)
195 batches submitted to accumulate stats from 12480 documents (315983 virtual)
196 batches submitted to accumulate stats from 12544 documents (319125 virtual)
197 batches submitted to accumulate stats from 12608 documents (321049 virtual)
198 batches submitted to accumulate stats from 12672 documents (326596 virtual)
199 batches submitted to accumulate stats from 12736 documents (327145 virtual)
202 batches submitted to accumulate stats from 12928 documents (326823 virtual)
205 batches submitted to accumulate stats from 13120 documents (327858 virtual)
206 batches submitted to accumulate stats from 13184 documents (327965 virtual)
207 batches submitted to accumulate stats from 13248 documents (329823 virtual)
208 batches submitted to accumulate stats from 13312 documents (330512 virtual)
209 batches submitted to accumulate stats from 13376 documents (332419 virtual)
210 batches submitted to accumulate stats from 13440 documents (333654 virtual)
212 batches submitted to accumulate stats from 13568 documents (337773 virtual)
213 batches submitted to accumulate stats from 13632 documents (338661 virtual)
214 batches submitted to accumulate stats from 13696 documents (342349 virtual)
215 batches submitted to accumulate stats from 13760 documents (345259 virtual)
216 batches submitted to accumulate stats from 13824 documents (348262 virtual)
217 batches submitted to accumulate stats from 13888 documents (349574 virtual)
218 batches submitted to accumulate stats from 13952 documents (350690 virtual)
219 batches submitted to accumulate stats from 14016 documents (354429 virtual)
220 batches submitted to accumulate stats from 14080 documents (355800 virtual)
221 batches submitted to accumulate stats from 14144 documents (358949 virtual)
224 batches submitted to accumulate stats from 14336 documents (363609 virtual)
226 batches submitted to accumulate stats from 14464 documents (366818 virtual)
227 batches submitted to accumulate stats from 14528 documents (367041 virtual)
229 batches submitted to accumulate stats from 14656 documents (368407 virtual)
230 batches submitted to accumulate stats from 14720 documents (370596 virtual)
231 batches submitted to accumulate stats from 14784 documents (372365 virtual)
232 batches submitted to accumulate stats from 14848 documents (373314 virtual)
233 batches submitted to accumulate stats from 14912 documents (375895 virtual)
234 batches submitted to accumulate stats from 14976 documents (375977 virtual)
235 batches submitted to accumulate stats from 15040 documents (376226 virtual)
237 batches submitted to accumulate stats from 15168 documents (376189 virtual)
238 batches submitted to accumulate stats from 15232 documents (378705 virtual)
239 batches submitted to accumulate stats from 15296 documents (378882 virtual)
240 batches submitted to accumulate stats from 15360 documents (380137 virtual)
242 batches submitted to accumulate stats from 15488 documents (380272 virtual)
243 batches submitted to accumulate stats from 15552 documents (381859 virtual)
244 batches submitted to accumulate stats from 15616 documents (382184 virtual)
245 batches submitted to accumulate stats from 15680 documents (386941 virtual)
247 batches submitted to accumulate stats from 15808 documents (391586 virtual)
248 batches submitted to accumulate stats from 15872 documents (391742 virtual)
250 batches submitted to accumulate stats from 16000 documents (395777 virtual)
251 batches submitted to accumulate stats from 16064 documents (400019 virtual)
252 batches submitted to accumulate stats from 16128 documents (409065 virtual)
254 batches submitted to accumulate stats from 16256 documents (412643 virtual)
255 batches submitted to accumulate stats from 16320 documents (423545 virtual)
256 batches submitted to accumulate stats from 16384 documents (427833 virtual)
257 batches submitted to accumulate stats from 16448 documents (432431 virtual)
258 batches submitted to accumulate stats from 16512 documents (433454 virtual)
259 batches submitted to accumulate stats from 16576 documents (436379 virtual)
260 batches submitted to accumulate stats from 16640 documents (437808 virtual)
261 batches submitted to accumulate stats from 16704 documents (439369 virtual)
262 batches submitted to accumulate stats from 16768 documents (440452 virtual)
263 batches submitted to accumulate stats from 16832 documents (441481 virtual)
264 batches submitted to accumulate stats from 16896 documents (445627 virtual)
265 batches submitted to accumulate stats from 16960 documents (447758 virtual)
266 batches submitted to accumulate stats from 17024 documents (450345 virtual)
267 batches submitted to accumulate stats from 17088 documents (452379 virtual)
268 batches submitted to accumulate stats from 17152 documents (454525 virtual)
269 batches submitted to accumulate stats from 17216 documents (457690 virtual)
270 batches submitted to accumulate stats from 17280 documents (460899 virtual)
271 batches submitted to accumulate stats from 17344 documents (466350 virtual)
272 batches submitted to accumulate stats from 17408 documents (469216 virtual)
274 batches submitted to accumulate stats from 17536 documents (470457 virtual)
275 batches submitted to accumulate stats from 17600 documents (473948 virtual)
276 batches submitted to accumulate stats from 17664 documents (475179 virtual)
277 batches submitted to accumulate stats from 17728 documents (476477 virtual)
279 batches submitted to accumulate stats from 17856 documents (478489 virtual)
283 batches submitted to accumulate stats from 18112 documents (475828 virtual)
285 batches submitted to accumulate stats from 18240 documents (478779 virtual)
286 batches submitted to accumulate stats from 18304 documents (479112 virtual)
287 batches submitted to accumulate stats from 18368 documents (484452 virtual)
288 batches submitted to accumulate stats from 18432 documents (485143 virtual)
289 batches submitted to accumulate stats from 18496 documents (486580 virtual)
290 batches submitted to accumulate stats from 18560 documents (486811 virtual)
291 batches submitted to accumulate stats from 18624 documents (488610 virtual)
292 batches submitted to accumulate stats from 18688 documents (489224 virtual)
293 batches submitted to accumulate stats from 18752 documents (490227 virtual)
294 batches submitted to accumulate stats from 18816 documents (491885 virtual)
295 batches submitted to accumulate stats from 18880 documents (493349 virtual)
297 batches submitted to accumulate stats from 19008 documents (491772 virtual)
298 batches submitted to accumulate stats from 19072 documents (493089 virtual)
299 batches submitted to accumulate stats from 19136 documents (494383 virtual)
300 batches submitted to accumulate stats from 19200 documents (499674 virtual)
301 batches submitted to accumulate stats from 19264 documents (501151 virtual)
302 batches submitted to accumulate stats from 19328 documents (502419 virtual)
303 batches submitted to accumulate stats from 19392 documents (503675 virtual)
304 batches submitted to accumulate stats from 19456 documents (512032 virtual)
305 batches submitted to accumulate stats from 19520 documents (512743 virtual)
306 batches submitted to accumulate stats from 19584 documents (513431 virtual)
310 batches submitted to accumulate stats from 19840 documents (512507 virtual)
311 batches submitted to accumulate stats from 19904 documents (513341 virtual)
312 batches submitted to accumulate stats from 19968 documents (514565 virtual)
313 batches submitted to accumulate stats from 20032 documents (515461 virtual)
314 batches submitted to accumulate stats from 20096 documents (516798 virtual)
315 batches submitted to accumulate stats from 20160 documents (526179 virtual)
316 batches submitted to accumulate stats from 20224 documents (527690 virtual)
317 batches submitted to accumulate stats from 20288 documents (531506 virtual)
318 batches submitted to accumulate stats from 20352 documents (531692 virtual)
319 batches submitted to accumulate stats from 20416 documents (541331 virtual)
321 batches submitted to accumulate stats from 20544 documents (542530 virtual)
322 batches submitted to accumulate stats from 20608 documents (543266 virtual)
323 batches submitted to accumulate stats from 20672 documents (543735 virtual)
324 batches submitted to accumulate stats from 20736 documents (546203 virtual)
325 batches submitted to accumulate stats from 20800 documents (548546 virtual)
326 batches submitted to accumulate stats from 20864 documents (550245 virtual)
327 batches submitted to accumulate stats from 20928 documents (551354 virtual)
330 batches submitted to accumulate stats from 21120 documents (552523 virtual)
331 batches submitted to accumulate stats from 21184 documents (552677 virtual)
332 batches submitted to accumulate stats from 21248 documents (553629 virtual)
333 batches submitted to accumulate stats from 21312 documents (557843 virtual)
334 batches submitted to accumulate stats from 21376 documents (558382 virtual)
335 batches submitted to accumulate stats from 21440 documents (562933 virtual)
336 batches submitted to accumulate stats from 21504 documents (567057 virtual)
338 batches submitted to accumulate stats from 21632 documents (568000 virtual)
341 batches submitted to accumulate stats from 21824 documents (566775 virtual)
342 batches submitted to accumulate stats from 21888 documents (570640 virtual)
346 batches submitted to accumulate stats from 22144 documents (569606 virtual)
347 batches submitted to accumulate stats from 22208 documents (572456 virtual)
348 batches submitted to accumulate stats from 22272 documents (575746 virtual)
349 batches submitted to accumulate stats from 22336 documents (577128 virtual)
350 batches submitted to accumulate stats from 22400 documents (578194 virtual)
351 batches submitted to accumulate stats from 22464 documents (580183 virtual)
352 batches submitted to accumulate stats from 22528 documents (583731 virtual)
353 batches submitted to accumulate stats from 22592 documents (584694 virtual)
354 batches submitted to accumulate stats from 22656 documents (586051 virtual)
355 batches submitted to accumulate stats from 22720 documents (588344 virtual)
356 batches submitted to accumulate stats from 22784 documents (589559 virtual)
358 batches submitted to accumulate stats from 22912 documents (592202 virtual)
359 batches submitted to accumulate stats from 22976 documents (596989 virtual)
360 batches submitted to accumulate stats from 23040 documents (599818 virtual)
362 batches submitted to accumulate stats from 23168 documents (599465 virtual)
363 batches submitted to accumulate stats from 23232 documents (600651 virtual)
364 batches submitted to accumulate stats from 23296 documents (602650 virtual)
365 batches submitted to accumulate stats from 23360 documents (603486 virtual)
367 batches submitted to accumulate stats from 23488 documents (603904 virtual)
368 batches submitted to accumulate stats from 23552 documents (604059 virtual)
369 batches submitted to accumulate stats from 23616 documents (606758 virtual)
370 batches submitted to accumulate stats from 23680 documents (608925 virtual)
372 batches submitted to accumulate stats from 23808 documents (609399 virtual)
373 batches submitted to accumulate stats from 23872 documents (610264 virtual)
374 batches submitted to accumulate stats from 23936 documents (611220 virtual)
377 batches submitted to accumulate stats from 24128 documents (616465 virtual)
378 batches submitted to accumulate stats from 24192 documents (616627 virtual)
379 batches submitted to accumulate stats from 24256 documents (619592 virtual)
380 batches submitted to accumulate stats from 24320 documents (621756 virtual)
381 batches submitted to accumulate stats from 24384 documents (624630 virtual)
382 batches submitted to accumulate stats from 24448 documents (625576 virtual)
383 batches submitted to accumulate stats from 24512 documents (633275 virtual)
384 batches submitted to accumulate stats from 24576 documents (636976 virtual)
385 batches submitted to accumulate stats from 24640 documents (637182 virtual)
386 batches submitted to accumulate stats from 24704 documents (637632 virtual)
387 batches submitted to accumulate stats from 24768 documents (640880 virtual)
388 batches submitted to accumulate stats from 24832 documents (642558 virtual)
389 batches submitted to accumulate stats from 24896 documents (650526 virtual)
390 batches submitted to accumulate stats from 24960 documents (652726 virtual)
391 batches submitted to accumulate stats from 25024 documents (655013 virtual)
392 batches submitted to accumulate stats from 25088 documents (659286 virtual)
396 batches submitted to accumulate stats from 25344 documents (658482 virtual)
398 batches submitted to accumulate stats from 25472 documents (662621 virtual)
400 batches submitted to accumulate stats from 25600 documents (662913 virtual)
401 batches submitted to accumulate stats from 25664 documents (666150 virtual)
402 batches submitted to accumulate stats from 25728 documents (666360 virtual)
405 batches submitted to accumulate stats from 25920 documents (665959 virtual)
406 batches submitted to accumulate stats from 25984 documents (668269 virtual)
407 batches submitted to accumulate stats from 26048 documents (668313 virtual)
409 batches submitted to accumulate stats from 26176 documents (668387 virtual)
410 batches submitted to accumulate stats from 26240 documents (669981 virtual)
412 batches submitted to accumulate stats from 26368 documents (671124 virtual)
414 batches submitted to accumulate stats from 26496 documents (671653 virtual)
415 batches submitted to accumulate stats from 26560 documents (675115 virtual)
416 batches submitted to accumulate stats from 26624 documents (680247 virtual)
418 batches submitted to accumulate stats from 26752 documents (683280 virtual)
419 batches submitted to accumulate stats from 26816 documents (692215 virtual)
420 batches submitted to accumulate stats from 26880 documents (696563 virtual)
422 batches submitted to accumulate stats from 27008 documents (697114 virtual)
423 batches submitted to accumulate stats from 27072 documents (698318 virtual)
424 batches submitted to accumulate stats from 27136 documents (700842 virtual)
425 batches submitted to accumulate stats from 27200 documents (701624 virtual)
426 batches submitted to accumulate stats from 27264 documents (703222 virtual)
428 batches submitted to accumulate stats from 27392 documents (707394 virtual)
429 batches submitted to accumulate stats from 27456 documents (708963 virtual)
430 batches submitted to accumulate stats from 27520 documents (712361 virtual)
431 batches submitted to accumulate stats from 27584 documents (712887 virtual)
434 batches submitted to accumulate stats from 27776 documents (711775 virtual)
436 batches submitted to accumulate stats from 27904 documents (714154 virtual)
437 batches submitted to accumulate stats from 27968 documents (714869 virtual)
438 batches submitted to accumulate stats from 28032 documents (715380 virtual)
439 batches submitted to accumulate stats from 28096 documents (715427 virtual)
440 batches submitted to accumulate stats from 28160 documents (715658 virtual)
441 batches submitted to accumulate stats from 28224 documents (716294 virtual)
442 batches submitted to accumulate stats from 28288 documents (723635 virtual)
443 batches submitted to accumulate stats from 28352 documents (725984 virtual)
444 batches submitted to accumulate stats from 28416 documents (727658 virtual)
445 batches submitted to accumulate stats from 28480 documents (729537 virtual)
448 batches submitted to accumulate stats from 28672 documents (730525 virtual)
449 batches submitted to accumulate stats from 28736 documents (733512 virtual)
451 batches submitted to accumulate stats from 28864 documents (731960 virtual)
452 batches submitted to accumulate stats from 28928 documents (736572 virtual)
453 batches submitted to accumulate stats from 28992 documents (738734 virtual)
454 batches submitted to accumulate stats from 29056 documents (740977 virtual)
455 batches submitted to accumulate stats from 29120 documents (746008 virtual)
456 batches submitted to accumulate stats from 29184 documents (748370 virtual)
457 batches submitted to accumulate stats from 29248 documents (750920 virtual)
458 batches submitted to accumulate stats from 29312 documents (752073 virtual)
459 batches submitted to accumulate stats from 29376 documents (757301 virtual)
460 batches submitted to accumulate stats from 29440 documents (758943 virtual)
461 batches submitted to accumulate stats from 29504 documents (765819 virtual)
463 batches submitted to accumulate stats from 29632 documents (767501 virtual)
464 batches submitted to accumulate stats from 29696 documents (772939 virtual)
465 batches submitted to accumulate stats from 29760 documents (774439 virtual)
466 batches submitted to accumulate stats from 29824 documents (775580 virtual)
467 batches submitted to accumulate stats from 29888 documents (784149 virtual)
468 batches submitted to accumulate stats from 29952 documents (785535 virtual)
469 batches submitted to accumulate stats from 30016 documents (786310 virtual)
473 batches submitted to accumulate stats from 30272 documents (784880 virtual)
474 batches submitted to accumulate stats from 30336 documents (785312 virtual)
475 batches submitted to accumulate stats from 30400 documents (790216 virtual)
477 batches submitted to accumulate stats from 30528 documents (790285 virtual)
479 batches submitted to accumulate stats from 30656 documents (790090 virtual)
480 batches submitted to accumulate stats from 30720 documents (792660 virtual)
481 batches submitted to accumulate stats from 30784 documents (798809 virtual)
482 batches submitted to accumulate stats from 30848 documents (800261 virtual)
483 batches submitted to accumulate stats from 30912 documents (803789 virtual)
484 batches submitted to accumulate stats from 30976 documents (807357 virtual)
486 batches submitted to accumulate stats from 31104 documents (814965 virtual)
487 batches submitted to accumulate stats from 31168 documents (817845 virtual)
488 batches submitted to accumulate stats from 31232 documents (821422 virtual)
489 batches submitted to accumulate stats from 31296 documents (821648 virtual)
490 batches submitted to accumulate stats from 31360 documents (823954 virtual)
491 batches submitted to accumulate stats from 31424 documents (826655 virtual)
492 batches submitted to accumulate stats from 31488 documents (829187 virtual)
493 batches submitted to accumulate stats from 31552 documents (834306 virtual)
494 batches submitted to accumulate stats from 31616 documents (836093 virtual)
495 batches submitted to accumulate stats from 31680 documents (836669 virtual)
497 batches submitted to accumulate stats from 31808 documents (839694 virtual)
498 batches submitted to accumulate stats from 31872 documents (842406 virtual)
499 batches submitted to accumulate stats from 31936 documents (846415 virtual)
500 batches submitted to accumulate stats from 32000 documents (847008 virtual)
501 batches submitted to accumulate stats from 32064 documents (847848 virtual)
502 batches submitted to accumulate stats from 32128 documents (852903 virtual)
503 batches submitted to accumulate stats from 32192 documents (860396 virtual)
504 batches submitted to accumulate stats from 32256 documents (863425 virtual)
505 batches submitted to accumulate stats from 32320 documents (868444 virtual)
506 batches submitted to accumulate stats from 32384 documents (870289 virtual)
507 batches submitted to accumulate stats from 32448 documents (872094 virtual)
509 batches submitted to accumulate stats from 32576 documents (872434 virtual)
510 batches submitted to accumulate stats from 32640 documents (873215 virtual)
511 batches submitted to accumulate stats from 32704 documents (875531 virtual)
512 batches submitted to accumulate stats from 32768 documents (877516 virtual)
513 batches submitted to accumulate stats from 32832 documents (878877 virtual)
514 batches submitted to accumulate stats from 32896 documents (881486 virtual)
515 batches submitted to accumulate stats from 32960 documents (882504 virtual)
516 batches submitted to accumulate stats from 33024 documents (884973 virtual)
517 batches submitted to accumulate stats from 33088 documents (886091 virtual)
519 batches submitted to accumulate stats from 33216 documents (885467 virtual)
520 batches submitted to accumulate stats from 33280 documents (887677 virtual)
521 batches submitted to accumulate stats from 33344 documents (888225 virtual)
522 batches submitted to accumulate stats from 33408 documents (893854 virtual)
526 batches submitted to accumulate stats from 33664 documents (904900 virtual)
527 batches submitted to accumulate stats from 33728 documents (905963 virtual)
529 batches submitted to accumulate stats from 33856 documents (911722 virtual)
530 batches submitted to accumulate stats from 33920 documents (914381 virtual)
531 batches submitted to accumulate stats from 33984 documents (916666 virtual)
532 batches submitted to accumulate stats from 34048 documents (917609 virtual)
533 batches submitted to accumulate stats from 34112 documents (918721 virtual)
534 batches submitted to accumulate stats from 34176 documents (919717 virtual)
535 batches submitted to accumulate stats from 34240 documents (919773 virtual)
536 batches submitted to accumulate stats from 34304 documents (922489 virtual)
537 batches submitted to accumulate stats from 34368 documents (923949 virtual)
538 batches submitted to accumulate stats from 34432 documents (926760 virtual)
539 batches submitted to accumulate stats from 34496 documents (931538 virtual)
541 batches submitted to accumulate stats from 34624 documents (930311 virtual)
542 batches submitted to accumulate stats from 34688 documents (930727 virtual)
543 batches submitted to accumulate stats from 34752 documents (931202 virtual)
544 batches submitted to accumulate stats from 34816 documents (936372 virtual)
546 batches submitted to accumulate stats from 34944 documents (937210 virtual)
547 batches submitted to accumulate stats from 35008 documents (937804 virtual)
553 batches submitted to accumulate stats from 35392 documents (941623 virtual)
554 batches submitted to accumulate stats from 35456 documents (942468 virtual)
555 batches submitted to accumulate stats from 35520 documents (944975 virtual)
556 batches submitted to accumulate stats from 35584 documents (950946 virtual)
557 batches submitted to accumulate stats from 35648 documents (952778 virtual)
558 batches submitted to accumulate stats from 35712 documents (953945 virtual)
559 batches submitted to accumulate stats from 35776 documents (954792 virtual)
560 batches submitted to accumulate stats from 35840 documents (958188 virtual)
562 batches submitted to accumulate stats from 35968 documents (959266 virtual)
563 batches submitted to accumulate stats from 36032 documents (961082 virtual)
565 batches submitted to accumulate stats from 36160 documents (960471 virtual)
566 batches submitted to accumulate stats from 36224 documents (964448 virtual)
568 batches submitted to accumulate stats from 36352 documents (965257 virtual)
569 batches submitted to accumulate stats from 36416 documents (965557 virtual)
570 batches submitted to accumulate stats from 36480 documents (969912 virtual)
572 batches submitted to accumulate stats from 36608 documents (967794 virtual)
573 batches submitted to accumulate stats from 36672 documents (975951 virtual)
575 batches submitted to accumulate stats from 36800 documents (976848 virtual)
576 batches submitted to accumulate stats from 36864 documents (986954 virtual)
577 batches submitted to accumulate stats from 36928 documents (990379 virtual)
578 batches submitted to accumulate stats from 36992 documents (993462 virtual)
579 batches submitted to accumulate stats from 37056 documents (1000515 virtual)
580 batches submitted to accumulate stats from 37120 documents (1001547 virtual)
582 batches submitted to accumulate stats from 37248 documents (1001807 virtual)
585 batches submitted to accumulate stats from 37440 documents (1004611 virtual)
586 batches submitted to accumulate stats from 37504 documents (1004976 virtual)
587 batches submitted to accumulate stats from 37568 documents (1005467 virtual)
588 batches submitted to accumulate stats from 37632 documents (1008356 virtual)
589 batches submitted to accumulate stats from 37696 documents (1011949 virtual)
591 batches submitted to accumulate stats from 37824 documents (1011359 virtual)
592 batches submitted to accumulate stats from 37888 documents (1013213 virtual)
593 batches submitted to accumulate stats from 37952 documents (1014319 virtual)
594 batches submitted to accumulate stats from 38016 documents (1017891 virtual)
595 batches submitted to accumulate stats from 38080 documents (1021265 virtual)
596 batches submitted to accumulate stats from 38144 documents (1022495 virtual)
597 batches submitted to accumulate stats from 38208 documents (1026706 virtual)
598 batches submitted to accumulate stats from 38272 documents (1027498 virtual)
600 batches submitted to accumulate stats from 38400 documents (1027744 virtual)
601 batches submitted to accumulate stats from 38464 documents (1033930 virtual)
602 batches submitted to accumulate stats from 38528 documents (1034119 virtual)
604 batches submitted to accumulate stats from 38656 documents (1034432 virtual)
605 batches submitted to accumulate stats from 38720 documents (1034536 virtual)
607 batches submitted to accumulate stats from 38848 documents (1034070 virtual)
608 batches submitted to accumulate stats from 38912 documents (1036840 virtual)
609 batches submitted to accumulate stats from 38976 documents (1039510 virtual)
610 batches submitted to accumulate stats from 39040 documents (1041741 virtual)
611 batches submitted to accumulate stats from 39104 documents (1046262 virtual)
612 batches submitted to accumulate stats from 39168 documents (1049735 virtual)
613 batches submitted to accumulate stats from 39232 documents (1053314 virtual)
614 batches submitted to accumulate stats from 39296 documents (1054425 virtual)
615 batches submitted to accumulate stats from 39360 documents (1055658 virtual)
617 batches submitted to accumulate stats from 39488 documents (1055040 virtual)
619 batches submitted to accumulate stats from 39616 documents (1056178 virtual)
620 batches submitted to accumulate stats from 39680 documents (1059725 virtual)
621 batches submitted to accumulate stats from 39744 documents (1059835 virtual)
622 batches submitted to accumulate stats from 39808 documents (1063845 virtual)
623 batches submitted to accumulate stats from 39872 documents (1064870 virtual)
624 batches submitted to accumulate stats from 39936 documents (1067392 virtual)
625 batches submitted to accumulate stats from 40000 documents (1068930 virtual)
627 batches submitted to accumulate stats from 40128 documents (1068941 virtual)
628 batches submitted to accumulate stats from 40192 documents (1070081 virtual)
629 batches submitted to accumulate stats from 40256 documents (1070247 virtual)
631 batches submitted to accumulate stats from 40384 documents (1073126 virtual)
632 batches submitted to accumulate stats from 40448 documents (1074490 virtual)
633 batches submitted to accumulate stats from 40512 documents (1075102 virtual)
635 batches submitted to accumulate stats from 40640 documents (1075227 virtual)
636 batches submitted to accumulate stats from 40704 documents (1076972 virtual)
638 batches submitted to accumulate stats from 40832 documents (1077100 virtual)
639 batches submitted to accumulate stats from 40896 documents (1085351 virtual)
640 batches submitted to accumulate stats from 40960 documents (1088319 virtual)
641 batches submitted to accumulate stats from 41024 documents (1099968 virtual)
643 batches submitted to accumulate stats from 41152 documents (1100648 virtual)
645 batches submitted to accumulate stats from 41280 documents (1100743 virtual)
646 batches submitted to accumulate stats from 41344 documents (1104748 virtual)
647 batches submitted to accumulate stats from 41408 documents (1107892 virtual)
648 batches submitted to accumulate stats from 41472 documents (1111514 virtual)
649 batches submitted to accumulate stats from 41536 documents (1117955 virtual)
651 batches submitted to accumulate stats from 41664 documents (1118308 virtual)
653 batches submitted to accumulate stats from 41792 documents (1118611 virtual)
654 batches submitted to accumulate stats from 41856 documents (1123237 virtual)
656 batches submitted to accumulate stats from 41984 documents (1124338 virtual)
657 batches submitted to accumulate stats from 42048 documents (1128817 virtual)
659 batches submitted to accumulate stats from 42176 documents (1127564 virtual)
662 batches submitted to accumulate stats from 42368 documents (1128003 virtual)
663 batches submitted to accumulate stats from 42432 documents (1130813 virtual)
664 batches submitted to accumulate stats from 42496 documents (1132478 virtual)
665 batches submitted to accumulate stats from 42560 documents (1137530 virtual)
667 batches submitted to accumulate stats from 42688 documents (1137232 virtual)
668 batches submitted to accumulate stats from 42752 documents (1139143 virtual)
669 batches submitted to accumulate stats from 42816 documents (1139805 virtual)
670 batches submitted to accumulate stats from 42880 documents (1144675 virtual)
671 batches submitted to accumulate stats from 42944 documents (1147873 virtual)
672 batches submitted to accumulate stats from 43008 documents (1148976 virtual)
673 batches submitted to accumulate stats from 43072 documents (1152054 virtual)
675 batches submitted to accumulate stats from 43200 documents (1156698 virtual)
677 batches submitted to accumulate stats from 43328 documents (1157109 virtual)
680 batches submitted to accumulate stats from 43520 documents (1156863 virtual)
681 batches submitted to accumulate stats from 43584 documents (1158636 virtual)
682 batches submitted to accumulate stats from 43648 documents (1159247 virtual)
683 batches submitted to accumulate stats from 43712 documents (1163249 virtual)
685 batches submitted to accumulate stats from 43840 documents (1163821 virtual)
688 batches submitted to accumulate stats from 44032 documents (1164816 virtual)
689 batches submitted to accumulate stats from 44096 documents (1165792 virtual)
690 batches submitted to accumulate stats from 44160 documents (1170279 virtual)
691 batches submitted to accumulate stats from 44224 documents (1170818 virtual)
692 batches submitted to accumulate stats from 44288 documents (1179596 virtual)
693 batches submitted to accumulate stats from 44352 documents (1181094 virtual)
694 batches submitted to accumulate stats from 44416 documents (1182491 virtual)
695 batches submitted to accumulate stats from 44480 documents (1187034 virtual)
697 batches submitted to accumulate stats from 44608 documents (1191499 virtual)
699 batches submitted to accumulate stats from 44736 documents (1196554 virtual)
700 batches submitted to accumulate stats from 44800 documents (1197167 virtual)
703 batches submitted to accumulate stats from 44992 documents (1196055 virtual)
704 batches submitted to accumulate stats from 45056 documents (1202990 virtual)
705 batches submitted to accumulate stats from 45120 documents (1204057 virtual)
706 batches submitted to accumulate stats from 45184 documents (1204800 virtual)
707 batches submitted to accumulate stats from 45248 documents (1208007 virtual)
708 batches submitted to accumulate stats from 45312 documents (1209326 virtual)
709 batches submitted to accumulate stats from 45376 documents (1211192 virtual)
710 batches submitted to accumulate stats from 45440 documents (1225461 virtual)
711 batches submitted to accumulate stats from 45504 documents (1225881 virtual)
712 batches submitted to accumulate stats from 45568 documents (1227537 virtual)
714 batches submitted to accumulate stats from 45696 documents (1232892 virtual)
715 batches submitted to accumulate stats from 45760 documents (1246676 virtual)
716 batches submitted to accumulate stats from 45824 documents (1248853 virtual)
717 batches submitted to accumulate stats from 45888 documents (1251055 virtual)
718 batches submitted to accumulate stats from 45952 documents (1251604 virtual)
719 batches submitted to accumulate stats from 46016 documents (1253073 virtual)
721 batches submitted to accumulate stats from 46144 documents (1255317 virtual)
722 batches submitted to accumulate stats from 46208 documents (1258932 virtual)
723 batches submitted to accumulate stats from 46272 documents (1260014 virtual)
724 batches submitted to accumulate stats from 46336 documents (1262948 virtual)
725 batches submitted to accumulate stats from 46400 documents (1264823 virtual)
726 batches submitted to accumulate stats from 46464 documents (1267793 virtual)
727 batches submitted to accumulate stats from 46528 documents (1270621 virtual)
728 batches submitted to accumulate stats from 46592 documents (1277623 virtual)
729 batches submitted to accumulate stats from 46656 documents (1281689 virtual)
730 batches submitted to accumulate stats from 46720 documents (1286677 virtual)
731 batches submitted to accumulate stats from 46784 documents (1289917 virtual)
732 batches submitted to accumulate stats from 46848 documents (1292987 virtual)
733 batches submitted to accumulate stats from 46912 documents (1294045 virtual)
734 batches submitted to accumulate stats from 46976 documents (1295452 virtual)
735 batches submitted to accumulate stats from 47040 documents (1295884 virtual)
736 batches submitted to accumulate stats from 47104 documents (1296394 virtual)
738 batches submitted to accumulate stats from 47232 documents (1295942 virtual)
739 batches submitted to accumulate stats from 47296 documents (1297268 virtual)
740 batches submitted to accumulate stats from 47360 documents (1297989 virtual)
741 batches submitted to accumulate stats from 47424 documents (1302242 virtual)
744 batches submitted to accumulate stats from 47616 documents (1304858 virtual)
745 batches submitted to accumulate stats from 47680 documents (1306372 virtual)
746 batches submitted to accumulate stats from 47744 documents (1313032 virtual)
747 batches submitted to accumulate stats from 47808 documents (1315781 virtual)
748 batches submitted to accumulate stats from 47872 documents (1317894 virtual)
749 batches submitted to accumulate stats from 47936 documents (1321137 virtual)
751 batches submitted to accumulate stats from 48064 documents (1324337 virtual)
753 batches submitted to accumulate stats from 48192 documents (1324316 virtual)
754 batches submitted to accumulate stats from 48256 documents (1325623 virtual)
755 batches submitted to accumulate stats from 48320 documents (1325663 virtual)
756 batches submitted to accumulate stats from 48384 documents (1325867 virtual)
757 batches submitted to accumulate stats from 48448 documents (1329740 virtual)
758 batches submitted to accumulate stats from 48512 documents (1332204 virtual)
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
3 accumulators retrieved from output queue
accumulated word occurrence stats for 3848136 virtual documents
K=25, Coherence Score: 0.6334036273086382
Starting K=30
using symmetric alpha at 0.03333333333333333
using symmetric eta at 0.03333333333333333
using serial LDA version on this node
running online LDA training, 30 topics, 10 passes over the supplied corpus of 49835 documents, updating every 3000 documents, evaluating every ~49835 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 3 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 10
PROGRESS: pass 0, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.033): 0.008*"film" + 0.004*"game" + 0.003*"war" + 0.003*"age" + 0.003*"government" + 0.003*"season" + 0.003*"program" + 0.002*"population" + 0.002*"role" + 0.002*"town"
topic #14 (0.033): 0.005*"park" + 0.003*"village" + 0.003*"race" + 0.003*"population" + 0.003*"building" + 0.003*"age" + 0.003*"house" + 0.003*"book" + 0.003*"river" + 0.002*"island"
topic #20 (0.033): 0.004*"building" + 0.003*"set" + 0.003*"game" + 0.003*"engineering" + 0.003*"film" + 0.003*"company" + 0.002*"season" + 0.002*"say" + 0.002*"station" + 0.002*"book"
topic #24 (0.033): 0.006*"samantha" + 0.004*"award" + 0.003*"album" + 0.003*"public" + 0.003*"film" + 0.003*"band" + 0.002*"study" + 0.002*"company" + 0.002*"design" + 0.002*"get"
topic #28 (0.033): 0.007*"age" + 0.004*"road" + 0.004*"household" + 0.003*"population" + 0.003*"station" + 0.003*"male" + 0.003*"park" + 0.003*"film" + 0.002*"building" + 0.002*"north"
topic diff=13.488325, rho=1.000000
PROGRESS: pass 0, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.033): 0.004*"game" + 0.004*"define" + 0.004*"displaystyle" + 0.004*"nationality" + 0.003*"rules_player" + 0.003*"non_fifa" + 0.003*"season" + 0.003*"event" + 0.003*"building" + 0.002*"order"
topic #4 (0.033): 0.008*"film" + 0.005*"game" + 0.004*"station" + 0.004*"line" + 0.004*"player" + 0.004*"season" + 0.004*"series" + 0.003*"football" + 0.003*"route" + 0.003*"league"
topic #14 (0.033): 0.005*"park" + 0.004*"race" + 0.003*"population" + 0.003*"village" + 0.003*"age" + 0.003*"building" + 0.003*"house" + 0.003*"island" + 0.002*"river" + 0.002*"book"
topic #24 (0.033): 0.004*"album" + 0.003*"band" + 0.003*"award" + 0.003*"samantha" + 0.003*"public" + 0.002*"song" + 0.002*"art" + 0.002*"company" + 0.002*"design" + 0.002*"film"
topic #15 (0.033): 0.008*"game" + 0.006*"season" + 0.005*"player" + 0.003*"building" + 0.003*"finish" + 0.002*"company" + 0.002*"club" + 0.002*"championship" + 0.002*"series" + 0.002*"final"
topic diff=5.115579, rho=0.500000
PROGRESS: pass 0, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.013*"game" + 0.009*"season" + 0.008*"player" + 0.004*"finish" + 0.003*"lose" + 0.003*"championship" + 0.003*"final" + 0.003*"building" + 0.002*"patiala" + 0.002*"round"
topic #0 (0.033): 0.011*"displaystyle" + 0.005*"define" + 0.004*"game" + 0.003*"nationality" + 0.003*"rules_player" + 0.003*"non_fifa" + 0.003*"order" + 0.003*"building" + 0.003*"set" + 0.002*"municipality"
topic #7 (0.033): 0.008*"season" + 0.007*"film" + 0.007*"album" + 0.005*"art" + 0.005*"music" + 0.004*"game" + 0.004*"league" + 0.004*"festival" + 0.004*"sign" + 0.003*"song"
topic #9 (0.033): 0.007*"song" + 0.006*"series" + 0.006*"album" + 0.005*"film" + 0.004*"produce" + 0.004*"television" + 0.004*"band" + 0.004*"episode" + 0.003*"award" + 0.003*"tv"
topic #4 (0.033): 0.007*"film" + 0.006*"game" + 0.005*"line" + 0.004*"player" + 0.004*"station" + 0.004*"season" + 0.004*"series" + 0.004*"football" + 0.004*"route" + 0.003*"league"
topic diff=1.579416, rho=0.377964
PROGRESS: pass 0, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.033): 0.007*"hotel" + 0.004*"specie" + 0.003*"government" + 0.003*"series" + 0.003*"election" + 0.003*"round" + 0.002*"south" + 0.002*"tower" + 0.002*"war" + 0.002*"building"
topic #3 (0.033): 0.006*"system" + 0.005*"design" + 0.004*"engine" + 0.004*"model" + 0.004*"aircraft" + 0.004*"building" + 0.003*"control" + 0.003*"power" + 0.002*"car" + 0.002*"air"
topic #9 (0.033): 0.007*"series" + 0.006*"song" + 0.005*"film" + 0.005*"album" + 0.004*"korean" + 0.004*"television" + 0.004*"episode" + 0.004*"produce" + 0.004*"award" + 0.004*"band"
topic #4 (0.033): 0.007*"game" + 0.007*"film" + 0.006*"line" + 0.005*"route" + 0.004*"station" + 0.004*"player" + 0.004*"football" + 0.003*"season" + 0.003*"series" + 0.003*"road"
topic #14 (0.033): 0.007*"park" + 0.004*"population" + 0.004*"river" + 0.004*"village" + 0.004*"island" + 0.003*"race" + 0.003*"specie" + 0.003*"house" + 0.003*"land" + 0.003*"building"
topic diff=0.706497, rho=0.316228
PROGRESS: pass 0, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.033): 0.008*"park" + 0.005*"specie" + 0.005*"island" + 0.004*"population" + 0.004*"river" + 0.004*"village" + 0.003*"plant" + 0.003*"building" + 0.003*"water" + 0.003*"house"
topic #20 (0.033): 0.008*"book" + 0.007*"publish" + 0.004*"card" + 0.004*"study" + 0.003*"say" + 0.003*"college" + 0.003*"award" + 0.003*"life" + 0.003*"building" + 0.002*"story"
topic #19 (0.033): 0.006*"building" + 0.005*"design" + 0.004*"house" + 0.004*"style" + 0.003*"church" + 0.003*"river" + 0.003*"book" + 0.003*"town" + 0.003*"main" + 0.002*"important"
topic #12 (0.033): 0.016*"station" + 0.006*"line" + 0.005*"student" + 0.004*"public" + 0.003*"railway" + 0.003*"community" + 0.003*"season" + 0.003*"tournament" + 0.003*"court" + 0.003*"building"
topic #2 (0.033): 0.024*"season" + 0.020*"club" + 0.013*"league" + 0.012*"football" + 0.011*"match" + 0.010*"game" + 0.009*"goal" + 0.009*"score" + 0.008*"player" + 0.006*"sign"
topic diff=0.661916, rho=0.277350
PROGRESS: pass 0, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.033): 0.008*"woman" + 0.008*"election" + 0.006*"elect" + 0.004*"law" + 0.004*"child" + 0.004*"party" + 0.004*"appoint" + 0.004*"company" + 0.004*"house" + 0.004*"president"
topic #23 (0.033): 0.006*"law" + 0.005*"government" + 0.004*"court" + 0.003*"local" + 0.003*"church" + 0.003*"public" + 0.003*"say" + 0.003*"order" + 0.003*"claim" + 0.003*"party"
topic #20 (0.033): 0.011*"book" + 0.009*"publish" + 0.004*"study" + 0.003*"say" + 0.003*"card" + 0.003*"life" + 0.003*"award" + 0.003*"college" + 0.003*"author" + 0.003*"student"
topic #11 (0.033): 0.007*"game" + 0.005*"series" + 0.005*"season" + 0.004*"character" + 0.004*"episode" + 0.004*"get" + 0.003*"version" + 0.003*"car" + 0.003*"try" + 0.003*"kill"
topic #18 (0.033): 0.010*"club" + 0.008*"company" + 0.006*"league" + 0.006*"population" + 0.005*"government" + 0.004*"game" + 0.003*"business" + 0.003*"season" + 0.003*"found" + 0.003*"public"
topic diff=0.640577, rho=0.250000
PROGRESS: pass 0, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #27 (0.033): 0.006*"language" + 0.006*"company" + 0.004*"system" + 0.004*"research" + 0.003*"datum" + 0.003*"software" + 0.003*"gene" + 0.003*"development" + 0.003*"character" + 0.003*"study"
topic #12 (0.033): 0.026*"station" + 0.007*"line" + 0.006*"student" + 0.004*"railway" + 0.004*"public" + 0.004*"operate" + 0.003*"bus" + 0.003*"act" + 0.003*"community" + 0.003*"building"
topic #2 (0.033): 0.030*"season" + 0.024*"club" + 0.016*"league" + 0.015*"football" + 0.012*"game" + 0.012*"match" + 0.011*"goal" + 0.010*"score" + 0.009*"player" + 0.009*"final"
topic #6 (0.033): 0.011*"ship" + 0.006*"specie" + 0.005*"race" + 0.004*"island" + 0.004*"gun" + 0.004*"genus" + 0.003*"describe" + 0.003*"church" + 0.003*"point" + 0.003*"vessel"
topic #15 (0.033): 0.036*"game" + 0.018*"season" + 0.015*"player" + 0.007*"finish" + 0.006*"championship" + 0.006*"tournament" + 0.005*"point" + 0.005*"basketball" + 0.005*"conference" + 0.004*"final"
topic diff=0.593623, rho=0.229416
PROGRESS: pass 0, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.005*"hospital" + 0.004*"life" + 0.004*"woman" + 0.003*"event" + 0.003*"say" + 0.003*"support" + 0.003*"death" + 0.002*"order" + 0.002*"love" + 0.002*"medical"
topic #17 (0.033): 0.005*"consumer" + 0.004*"company" + 0.004*"brand" + 0.004*"society" + 0.004*"product" + 0.003*"study" + 0.003*"purchase" + 0.003*"bank" + 0.003*"information" + 0.003*"process"
topic #11 (0.033): 0.008*"game" + 0.005*"series" + 0.005*"magic" + 0.005*"get" + 0.005*"season" + 0.004*"character" + 0.004*"episode" + 0.004*"kill" + 0.004*"try" + 0.003*"help"
topic #7 (0.033): 0.012*"art" + 0.007*"film" + 0.007*"season" + 0.007*"festival" + 0.006*"music" + 0.005*"album" + 0.005*"sign" + 0.005*"artist" + 0.004*"church" + 0.004*"freiburg"
topic #28 (0.033): 0.014*"population" + 0.014*"county" + 0.013*"town" + 0.012*"road" + 0.012*"age" + 0.009*"village" + 0.009*"highway" + 0.008*"north" + 0.008*"km" + 0.007*"household"
topic diff=0.584209, rho=0.213201
PROGRESS: pass 0, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.033): 0.013*"building" + 0.009*"church" + 0.008*"house" + 0.007*"design" + 0.005*"style" + 0.004*"th_century" + 0.004*"street" + 0.003*"town" + 0.003*"main" + 0.003*"architect"
topic #11 (0.033): 0.008*"game" + 0.005*"series" + 0.005*"get" + 0.004*"episode" + 0.004*"kill" + 0.004*"season" + 0.004*"character" + 0.004*"try" + 0.004*"magic" + 0.003*"tell"
topic #3 (0.033): 0.008*"system" + 0.008*"design" + 0.007*"engine" + 0.006*"model" + 0.005*"aircraft" + 0.005*"car" + 0.004*"vehicle" + 0.004*"power" + 0.004*"control" + 0.003*"low"
topic #5 (0.033): 0.005*"hospital" + 0.004*"life" + 0.004*"woman" + 0.003*"say" + 0.003*"verse" + 0.003*"event" + 0.003*"death" + 0.003*"support" + 0.003*"wine" + 0.003*"care"
topic #6 (0.033): 0.016*"ship" + 0.007*"specie" + 0.006*"island" + 0.005*"race" + 0.005*"genus" + 0.004*"gun" + 0.004*"vessel" + 0.004*"describe" + 0.004*"point" + 0.004*"water"
topic diff=0.558629, rho=0.200000
PROGRESS: pass 0, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.033): 0.008*"specie" + 0.007*"river" + 0.007*"park" + 0.005*"island" + 0.005*"water" + 0.004*"plant" + 0.004*"site" + 0.004*"village" + 0.003*"region" + 0.003*"population"
topic #10 (0.033): 0.005*"government" + 0.004*"son" + 0.004*"student" + 0.003*"child" + 0.003*"say" + 0.003*"french" + 0.003*"king" + 0.003*"support" + 0.003*"cause" + 0.002*"act"
topic #4 (0.033): 0.012*"line" + 0.007*"battle" + 0.006*"route" + 0.006*"army" + 0.005*"station" + 0.004*"force" + 0.004*"train" + 0.004*"war" + 0.004*"attack" + 0.004*"railway"
topic #0 (0.033): 0.016*"displaystyle" + 0.007*"define" + 0.005*"example" + 0.005*"function" + 0.005*"set" + 0.004*"case" + 0.004*"cell" + 0.004*"point" + 0.004*"term" + 0.004*"theory"
topic #2 (0.033): 0.032*"season" + 0.026*"club" + 0.018*"league" + 0.018*"football" + 0.013*"match" + 0.012*"game" + 0.011*"player" + 0.011*"final" + 0.010*"score" + 0.010*"championship"
topic diff=0.550096, rho=0.188982
PROGRESS: pass 0, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.033): 0.019*"ship" + 0.007*"island" + 0.007*"specie" + 0.005*"vessel" + 0.005*"genus" + 0.005*"race" + 0.005*"crew" + 0.005*"gun" + 0.004*"describe" + 0.004*"sea"
topic #23 (0.033): 0.007*"government" + 0.007*"law" + 0.005*"court" + 0.004*"church" + 0.004*"say" + 0.004*"claim" + 0.004*"party" + 0.004*"act" + 0.004*"order" + 0.003*"right"
topic #21 (0.033): 0.006*"party" + 0.006*"vote" + 0.005*"election" + 0.004*"court" + 0.004*"say" + 0.003*"candidate" + 0.003*"house" + 0.003*"film" + 0.003*"tell" + 0.003*"get"
topic #1 (0.033): 0.047*"film" + 0.010*"star" + 0.008*"character" + 0.008*"event" + 0.006*"series" + 0.006*"direct" + 0.005*"story" + 0.004*"award" + 0.004*"game" + 0.004*"earth"
topic #8 (0.033): 0.027*"album" + 0.025*"song" + 0.020*"music" + 0.016*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.006*"video" + 0.005*"chart"
topic diff=0.538460, rho=0.179605
PROGRESS: pass 0, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.033): 0.019*"company" + 0.010*"club" + 0.007*"business" + 0.007*"found" + 0.005*"population" + 0.005*"country" + 0.004*"government" + 0.004*"worker" + 0.004*"firm" + 0.004*"union"
topic #23 (0.033): 0.007*"law" + 0.007*"government" + 0.006*"church" + 0.005*"court" + 0.004*"claim" + 0.004*"act" + 0.004*"order" + 0.004*"say" + 0.003*"case" + 0.003*"right"
topic #28 (0.033): 0.017*"town" + 0.017*"population" + 0.015*"county" + 0.015*"road" + 0.013*"village" + 0.011*"age" + 0.010*"north" + 0.009*"km" + 0.009*"south" + 0.008*"route"
topic #13 (0.033): 0.011*"german" + 0.009*"government" + 0.008*"war" + 0.007*"university" + 0.005*"college" + 0.005*"military" + 0.005*"student" + 0.004*"force" + 0.004*"program" + 0.004*"department"
topic #14 (0.033): 0.009*"specie" + 0.007*"park" + 0.007*"river" + 0.005*"water" + 0.005*"island" + 0.005*"plant" + 0.004*"site" + 0.004*"population" + 0.004*"region" + 0.003*"forest"
topic diff=0.513678, rho=0.171499
PROGRESS: pass 0, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #27 (0.033): 0.008*"system" + 0.007*"language" + 0.005*"datum" + 0.004*"research" + 0.004*"technology" + 0.004*"process" + 0.004*"human" + 0.004*"development" + 0.004*"develop" + 0.003*"study"
topic #19 (0.033): 0.016*"building" + 0.015*"church" + 0.010*"house" + 0.008*"design" + 0.006*"th_century" + 0.005*"style" + 0.004*"street" + 0.004*"architect" + 0.004*"main" + 0.003*"art"
topic #11 (0.033): 0.008*"get" + 0.006*"kill" + 0.006*"game" + 0.006*"try" + 0.006*"episode" + 0.005*"tell" + 0.005*"character" + 0.005*"back" + 0.005*"help" + 0.004*"series"
topic #10 (0.033): 0.005*"government" + 0.004*"child" + 0.004*"son" + 0.004*"increase" + 0.003*"student" + 0.003*"king" + 0.003*"say" + 0.003*"cause" + 0.003*"french" + 0.003*"economic"
topic #6 (0.033): 0.022*"ship" + 0.008*"island" + 0.007*"specie" + 0.006*"vessel" + 0.006*"genus" + 0.006*"port" + 0.006*"gun" + 0.005*"race" + 0.005*"crew" + 0.005*"sea"
topic diff=0.500297, rho=0.164399
PROGRESS: pass 0, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #28 (0.033): 0.017*"population" + 0.017*"town" + 0.017*"county" + 0.015*"road" + 0.014*"village" + 0.011*"age" + 0.011*"north" + 0.009*"km" + 0.009*"south" + 0.008*"route"
topic #7 (0.033): 0.026*"art" + 0.012*"artist" + 0.010*"festival" + 0.008*"painting" + 0.006*"museum" + 0.006*"bishop" + 0.006*"exhibition" + 0.005*"film" + 0.005*"paint" + 0.005*"music"
topic #4 (0.033): 0.012*"line" + 0.011*"battle" + 0.009*"army" + 0.007*"regiment" + 0.007*"attack" + 0.006*"force" + 0.006*"war" + 0.005*"route" + 0.005*"fight" + 0.005*"troop"
topic #20 (0.033): 0.022*"book" + 0.018*"publish" + 0.009*"study" + 0.006*"author" + 0.005*"history" + 0.005*"magazine" + 0.005*"editor" + 0.005*"writer" + 0.004*"publication" + 0.004*"life"
topic #17 (0.033): 0.006*"company" + 0.006*"bank" + 0.005*"market" + 0.005*"product" + 0.005*"consumer" + 0.004*"business" + 0.004*"brand" + 0.004*"price" + 0.004*"purchase" + 0.004*"provide"
topic diff=0.476522, rho=0.158114
-8.505 per-word bound, 363.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.033): 0.007*"government" + 0.007*"law" + 0.007*"court" + 0.004*"order" + 0.004*"claim" + 0.004*"church" + 0.004*"say" + 0.004*"act" + 0.004*"case" + 0.004*"police"
topic #9 (0.033): 0.014*"series" + 0.009*"television" + 0.008*"episode" + 0.006*"award" + 0.006*"film" + 0.006*"tv" + 0.005*"appear" + 0.005*"story" + 0.005*"novel" + 0.004*"produce"
topic #22 (0.033): 0.018*"company" + 0.008*"line" + 0.007*"bridge" + 0.007*"railway" + 0.006*"sell" + 0.006*"store" + 0.005*"street" + 0.005*"building" + 0.004*"house" + 0.004*"close"
topic #18 (0.033): 0.025*"company" + 0.010*"business" + 0.008*"club" + 0.008*"found" + 0.005*"worker" + 0.005*"union" + 0.005*"industry" + 0.004*"firm" + 0.004*"population" + 0.004*"government"
topic #1 (0.033): 0.060*"film" + 0.011*"star" + 0.008*"character" + 0.008*"event" + 0.007*"direct" + 0.006*"series" + 0.005*"story" + 0.005*"award" + 0.004*"role" + 0.004*"director"
topic diff=0.463351, rho=0.152499
-8.493 per-word bound, 360.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #1 (0.033): 0.063*"film" + 0.011*"star" + 0.008*"character" + 0.008*"direct" + 0.007*"event" + 0.006*"story" + 0.005*"series" + 0.005*"award" + 0.004*"role" + 0.004*"director"
topic #0 (0.033): 0.013*"displaystyle" + 0.007*"define" + 0.007*"function" + 0.006*"example" + 0.006*"cell" + 0.005*"point" + 0.005*"case" + 0.005*"term" + 0.004*"protein" + 0.004*"set"
topic #5 (0.033): 0.007*"woman" + 0.006*"life" + 0.006*"hospital" + 0.004*"say" + 0.004*"wine" + 0.004*"death" + 0.004*"patient" + 0.003*"medical" + 0.003*"person" + 0.003*"care"
topic #22 (0.033): 0.019*"company" + 0.008*"line" + 0.008*"railway" + 0.008*"bridge" + 0.007*"sell" + 0.006*"store" + 0.005*"street" + 0.005*"building" + 0.004*"house" + 0.004*"close"
topic #27 (0.033): 0.008*"system" + 0.007*"language" + 0.005*"datum" + 0.004*"research" + 0.004*"process" + 0.004*"develop" + 0.004*"development" + 0.004*"technology" + 0.004*"code" + 0.004*"user"
topic diff=0.451194, rho=0.147442
-8.409 per-word bound, 340.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.010*"system" + 0.009*"design" + 0.007*"engine" + 0.007*"model" + 0.006*"car" + 0.006*"power" + 0.005*"vehicle" + 0.004*"aircraft" + 0.004*"control" + 0.004*"produce"
topic #16 (0.033): 0.014*"army" + 0.013*"war" + 0.012*"unit" + 0.011*"command" + 0.010*"force" + 0.009*"military" + 0.009*"operation" + 0.008*"air" + 0.007*"squadron" + 0.007*"air_force"
topic #8 (0.033): 0.029*"album" + 0.027*"song" + 0.023*"music" + 0.018*"band" + 0.012*"single" + 0.009*"perform" + 0.008*"track" + 0.006*"video" + 0.006*"tour" + 0.006*"chart"
topic #6 (0.033): 0.025*"ship" + 0.011*"island" + 0.007*"vessel" + 0.007*"specie" + 0.006*"genus" + 0.006*"sea" + 0.006*"port" + 0.006*"crew" + 0.006*"gun" + 0.005*"boat"
topic #5 (0.033): 0.008*"hospital" + 0.007*"life" + 0.006*"woman" + 0.005*"patient" + 0.004*"wine" + 0.004*"say" + 0.004*"death" + 0.003*"medical" + 0.003*"care" + 0.003*"person"
topic diff=0.429398, rho=0.138896
PROGRESS: pass 1, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.033): 0.012*"battle" + 0.011*"line" + 0.011*"army" + 0.008*"force" + 0.008*"attack" + 0.007*"war" + 0.007*"regiment" + 0.006*"troop" + 0.005*"fight" + 0.005*"kill"
topic #29 (0.033): 0.030*"village" + 0.016*"russian" + 0.012*"championship" + 0.012*"population" + 0.010*"county" + 0.009*"census" + 0.009*"wicket" + 0.009*"cricket" + 0.009*"rural" + 0.008*"central"
topic #5 (0.033): 0.007*"woman" + 0.007*"hospital" + 0.007*"life" + 0.005*"patient" + 0.004*"wine" + 0.004*"say" + 0.004*"death" + 0.004*"practice" + 0.003*"medical" + 0.003*"care"
topic #10 (0.033): 0.006*"child" + 0.005*"government" + 0.004*"increase" + 0.004*"son" + 0.004*"french" + 0.003*"king" + 0.003*"cause" + 0.003*"drug" + 0.003*"say" + 0.003*"economic"
topic #28 (0.033): 0.019*"population" + 0.018*"town" + 0.017*"village" + 0.017*"county" + 0.016*"road" + 0.012*"age" + 0.011*"north" + 0.010*"south" + 0.010*"km" + 0.009*"river"
topic diff=0.422364, rho=0.138896
PROGRESS: pass 1, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.033): 0.017*"specie" + 0.016*"hotel" + 0.013*"black" + 0.012*"jewish" + 0.009*"white" + 0.008*"hungarian" + 0.006*"describe" + 0.006*"african" + 0.005*"genus" + 0.005*"forewing"
topic #4 (0.033): 0.013*"battle" + 0.011*"army" + 0.011*"line" + 0.008*"force" + 0.008*"attack" + 0.008*"war" + 0.007*"regiment" + 0.006*"troop" + 0.006*"fight" + 0.005*"kill"
topic #17 (0.033): 0.006*"product" + 0.006*"market" + 0.006*"consumer" + 0.006*"company" + 0.005*"brand" + 0.005*"bank" + 0.005*"information" + 0.005*"purchase" + 0.005*"provide" + 0.004*"business"
topic #2 (0.033): 0.032*"season" + 0.027*"club" + 0.017*"league" + 0.017*"football" + 0.016*"match" + 0.013*"final" + 0.013*"championship" + 0.012*"game" + 0.011*"goal" + 0.011*"score"
topic #24 (0.033): 0.015*"award" + 0.012*"research" + 0.011*"science" + 0.009*"university" + 0.009*"college" + 0.008*"museum" + 0.007*"study" + 0.007*"art" + 0.006*"program" + 0.005*"medical"
topic diff=0.413119, rho=0.138896
PROGRESS: pass 1, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.033): 0.016*"displaystyle" + 0.007*"define" + 0.007*"example" + 0.007*"function" + 0.006*"cell" + 0.005*"point" + 0.005*"case" + 0.005*"term" + 0.004*"protein" + 0.004*"theory"
topic #14 (0.033): 0.011*"specie" + 0.007*"park" + 0.006*"water" + 0.006*"plant" + 0.006*"river" + 0.004*"forest" + 0.004*"island" + 0.004*"site" + 0.004*"tree" + 0.004*"region"
topic #21 (0.033): 0.006*"say" + 0.005*"murder" + 0.004*"police" + 0.004*"tell" + 0.003*"party" + 0.003*"vote" + 0.003*"court" + 0.003*"kill" + 0.003*"death" + 0.003*"sentence"
topic #17 (0.033): 0.007*"product" + 0.007*"market" + 0.006*"company" + 0.006*"consumer" + 0.006*"brand" + 0.005*"bank" + 0.005*"information" + 0.005*"provide" + 0.004*"purchase" + 0.004*"global"
topic #8 (0.033): 0.029*"album" + 0.028*"song" + 0.024*"music" + 0.019*"band" + 0.012*"single" + 0.008*"perform" + 0.008*"track" + 0.007*"tour" + 0.007*"video" + 0.006*"chart"
topic diff=0.416907, rho=0.138896
PROGRESS: pass 1, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.033): 0.007*"child" + 0.005*"government" + 0.004*"increase" + 0.004*"french" + 0.004*"son" + 0.003*"cause" + 0.003*"drug" + 0.003*"king" + 0.003*"economic" + 0.003*"policy"
topic #16 (0.033): 0.015*"army" + 0.013*"war" + 0.013*"unit" + 0.012*"force" + 0.012*"command" + 0.010*"military" + 0.010*"operation" + 0.010*"air" + 0.008*"aircraft" + 0.008*"air_force"
topic #0 (0.033): 0.015*"displaystyle" + 0.007*"define" + 0.007*"example" + 0.007*"function" + 0.006*"cell" + 0.006*"case" + 0.005*"point" + 0.005*"term" + 0.004*"theory" + 0.004*"result"
topic #20 (0.033): 0.024*"book" + 0.021*"publish" + 0.009*"study" + 0.007*"author" + 0.006*"history" + 0.006*"writer" + 0.006*"magazine" + 0.005*"publication" + 0.005*"editor" + 0.005*"life"
topic #25 (0.033): 0.018*"election" + 0.013*"elect" + 0.012*"party" + 0.009*"woman" + 0.009*"vote" + 0.008*"president" + 0.007*"appoint" + 0.007*"candidate" + 0.007*"seat" + 0.006*"general"
topic diff=0.408201, rho=0.138896
PROGRESS: pass 1, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.033): 0.035*"art" + 0.017*"artist" + 0.013*"painting" + 0.011*"festival" + 0.010*"museum" + 0.010*"exhibition" + 0.007*"bishop" + 0.007*"paint" + 0.006*"painter" + 0.006*"study"
topic #5 (0.033): 0.008*"life" + 0.007*"woman" + 0.006*"hospital" + 0.005*"patient" + 0.004*"say" + 0.004*"death" + 0.004*"practice" + 0.003*"person" + 0.003*"believe" + 0.003*"accord"
topic #25 (0.033): 0.019*"election" + 0.013*"elect" + 0.012*"party" + 0.009*"vote" + 0.009*"woman" + 0.008*"president" + 0.007*"appoint" + 0.007*"candidate" + 0.007*"seat" + 0.006*"general"
topic #23 (0.033): 0.009*"government" + 0.008*"court" + 0.008*"law" + 0.005*"act" + 0.005*"order" + 0.005*"rule" + 0.004*"say" + 0.004*"case" + 0.004*"claim" + 0.004*"right"
topic #17 (0.033): 0.008*"consumer" + 0.008*"product" + 0.007*"market" + 0.006*"information" + 0.006*"company" + 0.006*"brand" + 0.005*"purchase" + 0.005*"provide" + 0.005*"bank" + 0.004*"customer"
topic diff=0.411343, rho=0.138896
PROGRESS: pass 1, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.033): 0.019*"election" + 0.013*"elect" + 0.012*"party" + 0.009*"vote" + 0.009*"woman" + 0.008*"president" + 0.007*"candidate" + 0.007*"seat" + 0.007*"appoint" + 0.007*"law"
topic #21 (0.033): 0.006*"say" + 0.005*"murder" + 0.005*"police" + 0.004*"tell" + 0.004*"kill" + 0.003*"death" + 0.003*"father" + 0.003*"house" + 0.003*"sentence" + 0.003*"claim"
topic #1 (0.033): 0.072*"film" + 0.012*"star" + 0.009*"character" + 0.009*"direct" + 0.007*"event" + 0.006*"story" + 0.005*"series" + 0.005*"award" + 0.005*"movie" + 0.005*"role"
topic #6 (0.033): 0.030*"ship" + 0.016*"island" + 0.008*"boat" + 0.008*"vessel" + 0.007*"port" + 0.007*"crew" + 0.007*"sea" + 0.007*"sail" + 0.006*"gun" + 0.005*"specie"
topic #3 (0.033): 0.010*"design" + 0.010*"system" + 0.008*"engine" + 0.008*"car" + 0.007*"model" + 0.007*"power" + 0.007*"vehicle" + 0.004*"low" + 0.004*"produce" + 0.004*"control"
topic diff=0.391525, rho=0.138896
PROGRESS: pass 1, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.033): 0.056*"station" + 0.015*"student" + 0.014*"line" + 0.010*"radio" + 0.009*"operate" + 0.008*"program" + 0.007*"network" + 0.007*"train" + 0.007*"railway" + 0.006*"local"
topic #11 (0.033): 0.009*"get" + 0.007*"kill" + 0.007*"tell" + 0.006*"try" + 0.006*"back" + 0.006*"help" + 0.005*"episode" + 0.004*"character" + 0.004*"want" + 0.004*"reveal"
topic #27 (0.033): 0.009*"system" + 0.007*"datum" + 0.006*"language" + 0.004*"develop" + 0.004*"code" + 0.004*"process" + 0.004*"computer" + 0.004*"software" + 0.004*"human" + 0.004*"user"
topic #16 (0.033): 0.015*"army" + 0.014*"war" + 0.014*"unit" + 0.012*"force" + 0.011*"command" + 0.011*"operation" + 0.011*"military" + 0.010*"air" + 0.009*"aircraft" + 0.009*"squadron"
topic #26 (0.033): 0.024*"specie" + 0.017*"hotel" + 0.017*"black" + 0.013*"white" + 0.012*"hungarian" + 0.012*"jewish" + 0.010*"african" + 0.010*"describe" + 0.009*"genus" + 0.006*"chile"
topic diff=0.388511, rho=0.138896
PROGRESS: pass 1, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.033): 0.015*"battle" + 0.013*"army" + 0.010*"force" + 0.009*"war" + 0.009*"attack" + 0.009*"line" + 0.008*"troop" + 0.007*"fight" + 0.006*"kill" + 0.006*"regiment"
topic #16 (0.033): 0.015*"army" + 0.014*"war" + 0.014*"unit" + 0.012*"force" + 0.012*"command" + 0.011*"military" + 0.011*"operation" + 0.010*"air" + 0.009*"aircraft" + 0.009*"squadron"
topic #17 (0.033): 0.008*"product" + 0.007*"consumer" + 0.007*"market" + 0.006*"company" + 0.006*"information" + 0.005*"provide" + 0.005*"brand" + 0.005*"bank" + 0.005*"purchase" + 0.004*"business"
topic #25 (0.033): 0.020*"election" + 0.014*"elect" + 0.013*"party" + 0.010*"vote" + 0.009*"woman" + 0.008*"president" + 0.008*"candidate" + 0.007*"seat" + 0.007*"appoint" + 0.007*"general"
topic #14 (0.033): 0.011*"specie" + 0.007*"park" + 0.007*"water" + 0.006*"plant" + 0.006*"river" + 0.005*"forest" + 0.004*"tree" + 0.004*"site" + 0.004*"region" + 0.004*"island"
topic diff=0.380580, rho=0.138896
PROGRESS: pass 1, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.027*"book" + 0.023*"publish" + 0.009*"study" + 0.008*"author" + 0.006*"history" + 0.006*"magazine" + 0.006*"writer" + 0.006*"editor" + 0.005*"publication" + 0.005*"novel"
topic #1 (0.033): 0.075*"film" + 0.013*"star" + 0.010*"direct" + 0.009*"character" + 0.007*"story" + 0.006*"event" + 0.006*"series" + 0.006*"award" + 0.005*"movie" + 0.005*"role"
topic #9 (0.033): 0.019*"series" + 0.012*"television" + 0.010*"episode" + 0.007*"award" + 0.006*"appear" + 0.006*"tv" + 0.005*"story" + 0.005*"role" + 0.005*"film" + 0.005*"novel"
topic #10 (0.033): 0.007*"child" + 0.005*"increase" + 0.005*"government" + 0.004*"cause" + 0.004*"son" + 0.003*"drug" + 0.003*"french" + 0.003*"individual" + 0.003*"social" + 0.003*"effect"
topic #24 (0.033): 0.018*"award" + 0.014*"research" + 0.014*"university" + 0.012*"science" + 0.012*"college" + 0.009*"study" + 0.008*"student" + 0.008*"program" + 0.007*"medical" + 0.007*"director"
topic diff=0.376747, rho=0.138896
PROGRESS: pass 1, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.033): 0.009*"government" + 0.009*"law" + 0.008*"court" + 0.005*"act" + 0.005*"claim" + 0.005*"rule" + 0.005*"order" + 0.005*"say" + 0.005*"case" + 0.004*"right"
topic #13 (0.033): 0.014*"government" + 0.013*"german" + 0.010*"war" + 0.007*"country" + 0.006*"military" + 0.005*"organization" + 0.005*"department" + 0.005*"korean" + 0.005*"university" + 0.005*"international"
topic #6 (0.033): 0.032*"ship" + 0.019*"island" + 0.009*"sea" + 0.008*"vessel" + 0.008*"crew" + 0.008*"port" + 0.007*"boat" + 0.007*"sail" + 0.006*"coast" + 0.006*"gun"
topic #21 (0.033): 0.007*"say" + 0.005*"police" + 0.005*"murder" + 0.004*"tell" + 0.004*"death" + 0.004*"wife" + 0.004*"father" + 0.004*"prison" + 0.004*"kill" + 0.003*"ask"
topic #17 (0.033): 0.007*"product" + 0.007*"market" + 0.007*"consumer" + 0.006*"company" + 0.006*"provide" + 0.005*"bank" + 0.005*"brand" + 0.005*"information" + 0.005*"purchase" + 0.005*"increase"
topic diff=0.374790, rho=0.138896
PROGRESS: pass 1, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.011*"design" + 0.010*"system" + 0.008*"engine" + 0.007*"car" + 0.007*"power" + 0.007*"model" + 0.006*"vehicle" + 0.004*"produce" + 0.004*"low" + 0.004*"control"
topic #18 (0.033): 0.044*"company" + 0.015*"business" + 0.011*"found" + 0.007*"industry" + 0.007*"union" + 0.006*"club" + 0.006*"worker" + 0.006*"development" + 0.006*"firm" + 0.005*"acquire"
topic #7 (0.033): 0.039*"art" + 0.019*"artist" + 0.015*"painting" + 0.011*"museum" + 0.011*"festival" + 0.009*"exhibition" + 0.008*"bishop" + 0.007*"paint" + 0.006*"study" + 0.006*"painter"
topic #2 (0.033): 0.030*"club" + 0.030*"season" + 0.018*"league" + 0.017*"football" + 0.017*"match" + 0.015*"championship" + 0.015*"final" + 0.011*"player" + 0.011*"game" + 0.011*"score"
topic #26 (0.033): 0.031*"black" + 0.028*"specie" + 0.020*"white" + 0.015*"jewish" + 0.014*"describe" + 0.013*"hotel" + 0.013*"hungarian" + 0.013*"genus" + 0.011*"african" + 0.006*"brown"
topic diff=0.358470, rho=0.138896
PROGRESS: pass 1, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.033): 0.007*"market" + 0.007*"product" + 0.006*"provide" + 0.006*"consumer" + 0.005*"company" + 0.005*"information" + 0.005*"bank" + 0.005*"brand" + 0.005*"increase" + 0.005*"price"
topic #24 (0.033): 0.018*"award" + 0.016*"university" + 0.015*"research" + 0.013*"college" + 0.013*"science" + 0.010*"study" + 0.009*"student" + 0.009*"program" + 0.007*"medical" + 0.007*"education"
topic #4 (0.033): 0.017*"battle" + 0.014*"army" + 0.011*"force" + 0.011*"war" + 0.010*"attack" + 0.008*"line" + 0.008*"fight" + 0.008*"troop" + 0.006*"french" + 0.006*"regiment"
topic #0 (0.033): 0.012*"displaystyle" + 0.007*"example" + 0.006*"cell" + 0.006*"function" + 0.006*"define" + 0.006*"point" + 0.006*"case" + 0.006*"protein" + 0.005*"term" + 0.004*"type"
topic #26 (0.033): 0.031*"specie" + 0.030*"black" + 0.020*"white" + 0.017*"jewish" + 0.015*"describe" + 0.014*"genus" + 0.012*"hungarian" + 0.012*"hotel" + 0.011*"african" + 0.007*"brown"
topic diff=0.347254, rho=0.138896
PROGRESS: pass 1, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.033): 0.017*"german" + 0.014*"government" + 0.010*"war" + 0.007*"country" + 0.006*"military" + 0.005*"department" + 0.005*"organization" + 0.005*"international" + 0.005*"korean" + 0.004*"political"
topic #8 (0.033): 0.033*"album" + 0.030*"song" + 0.026*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.006*"video"
topic #14 (0.033): 0.012*"specie" + 0.007*"water" + 0.006*"plant" + 0.006*"park" + 0.005*"tree" + 0.005*"river" + 0.005*"forest" + 0.004*"region" + 0.004*"site" + 0.004*"animal"
topic #4 (0.033): 0.017*"battle" + 0.014*"army" + 0.011*"force" + 0.010*"war" + 0.010*"attack" + 0.008*"line" + 0.008*"troop" + 0.008*"fight" + 0.007*"regiment" + 0.006*"french"
topic #18 (0.033): 0.047*"company" + 0.015*"business" + 0.011*"found" + 0.008*"industry" + 0.007*"union" + 0.006*"firm" + 0.006*"development" + 0.006*"worker" + 0.006*"club" + 0.005*"acquire"
topic diff=0.335617, rho=0.138896
-8.367 per-word bound, 330.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #16 (0.033): 0.016*"army" + 0.015*"unit" + 0.014*"war" + 0.013*"command" + 0.013*"military" + 0.013*"force" + 0.012*"operation" + 0.011*"air" + 0.010*"aircraft" + 0.008*"air_force"
topic #2 (0.033): 0.030*"club" + 0.029*"season" + 0.018*"league" + 0.017*"match" + 0.016*"football" + 0.015*"championship" + 0.014*"final" + 0.011*"player" + 0.011*"game" + 0.011*"goal"
topic #3 (0.033): 0.011*"design" + 0.010*"system" + 0.008*"engine" + 0.008*"car" + 0.008*"power" + 0.006*"model" + 0.006*"vehicle" + 0.005*"produce" + 0.004*"light" + 0.004*"low"
topic #5 (0.033): 0.010*"woman" + 0.008*"life" + 0.005*"hospital" + 0.005*"say" + 0.004*"accord" + 0.004*"death" + 0.004*"wine" + 0.004*"practice" + 0.004*"patient" + 0.004*"believe"
topic #15 (0.033): 0.058*"game" + 0.035*"season" + 0.022*"player" + 0.015*"race" + 0.010*"finish" + 0.009*"basketball" + 0.009*"point" + 0.009*"league" + 0.008*"football" + 0.008*"championship"
topic diff=0.327517, rho=0.138896
-8.362 per-word bound, 329.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #7 (0.033): 0.041*"art" + 0.020*"artist" + 0.014*"painting" + 0.014*"museum" + 0.011*"festival" + 0.010*"exhibition" + 0.008*"paint" + 0.007*"bishop" + 0.007*"study" + 0.006*"painter"
topic #24 (0.033): 0.019*"award" + 0.017*"university" + 0.015*"college" + 0.015*"research" + 0.013*"science" + 0.011*"student" + 0.011*"study" + 0.009*"program" + 0.008*"education" + 0.008*"medical"
topic #5 (0.033): 0.010*"woman" + 0.008*"life" + 0.005*"accord" + 0.005*"say" + 0.005*"hospital" + 0.004*"practice" + 0.004*"death" + 0.004*"wine" + 0.004*"person" + 0.003*"believe"
topic #22 (0.033): 0.014*"company" + 0.012*"railway" + 0.012*"line" + 0.011*"bridge" + 0.008*"store" + 0.008*"sell" + 0.007*"street" + 0.006*"close" + 0.006*"construction" + 0.005*"train"
topic #21 (0.033): 0.007*"say" + 0.007*"police" + 0.006*"murder" + 0.005*"death" + 0.005*"prison" + 0.004*"wife" + 0.004*"tell" + 0.004*"arrest" + 0.004*"father" + 0.004*"kill"
topic diff=0.322376, rho=0.138896
-8.299 per-word bound, 315.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.033): 0.058*"station" + 0.015*"student" + 0.015*"line" + 0.011*"radio" + 0.010*"operate" + 0.009*"program" + 0.009*"network" + 0.008*"grade" + 0.007*"local" + 0.007*"public"
topic #15 (0.033): 0.061*"game" + 0.035*"season" + 0.023*"player" + 0.014*"race" + 0.010*"finish" + 0.009*"point" + 0.009*"league" + 0.008*"basketball" + 0.008*"football" + 0.008*"championship"
topic #22 (0.033): 0.013*"company" + 0.012*"bridge" + 0.012*"railway" + 0.011*"line" + 0.008*"store" + 0.008*"street" + 0.007*"sell" + 0.007*"close" + 0.006*"construction" + 0.005*"train"
topic #3 (0.033): 0.011*"design" + 0.010*"system" + 0.008*"engine" + 0.008*"power" + 0.008*"car" + 0.007*"model" + 0.006*"vehicle" + 0.005*"produce" + 0.004*"light" + 0.004*"control"
topic #23 (0.033): 0.009*"law" + 0.009*"government" + 0.009*"court" + 0.005*"act" + 0.005*"order" + 0.005*"rule" + 0.005*"case" + 0.005*"claim" + 0.005*"say" + 0.004*"right"
topic diff=0.310302, rho=0.137575
PROGRESS: pass 2, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.087*"film" + 0.015*"star" + 0.011*"direct" + 0.009*"character" + 0.007*"movie" + 0.007*"story" + 0.006*"award" + 0.006*"director" + 0.006*"role" + 0.005*"series"
topic #0 (0.033): 0.015*"displaystyle" + 0.007*"define" + 0.007*"example" + 0.006*"function" + 0.006*"point" + 0.006*"cell" + 0.006*"case" + 0.005*"term" + 0.004*"protein" + 0.004*"result"
topic #9 (0.033): 0.020*"series" + 0.013*"television" + 0.011*"episode" + 0.007*"tv" + 0.007*"appear" + 0.006*"award" + 0.006*"role" + 0.005*"story" + 0.005*"character" + 0.005*"film"
topic #2 (0.033): 0.029*"club" + 0.028*"season" + 0.017*"match" + 0.016*"league" + 0.015*"championship" + 0.015*"football" + 0.015*"final" + 0.011*"game" + 0.011*"goal" + 0.011*"player"
topic #12 (0.033): 0.059*"station" + 0.015*"student" + 0.015*"line" + 0.011*"radio" + 0.010*"operate" + 0.009*"program" + 0.009*"network" + 0.008*"local" + 0.007*"public" + 0.007*"grade"
topic diff=0.297532, rho=0.137575
PROGRESS: pass 2, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.010*"woman" + 0.009*"life" + 0.005*"say" + 0.005*"practice" + 0.005*"accord" + 0.004*"hospital" + 0.004*"death" + 0.004*"patient" + 0.004*"wine" + 0.004*"tradition"
topic #20 (0.033): 0.029*"book" + 0.024*"publish" + 0.009*"study" + 0.009*"author" + 0.007*"language" + 0.007*"history" + 0.006*"writer" + 0.006*"magazine" + 0.006*"novel" + 0.006*"editor"
topic #23 (0.033): 0.010*"court" + 0.010*"law" + 0.009*"government" + 0.006*"order" + 0.005*"act" + 0.005*"rule" + 0.005*"case" + 0.005*"claim" + 0.005*"say" + 0.005*"right"
topic #27 (0.033): 0.011*"system" + 0.007*"datum" + 0.005*"computer" + 0.005*"process" + 0.005*"user" + 0.005*"develop" + 0.004*"software" + 0.004*"code" + 0.004*"human" + 0.004*"information"
topic #10 (0.033): 0.011*"child" + 0.007*"increase" + 0.004*"cause" + 0.004*"government" + 0.004*"drug" + 0.004*"social" + 0.004*"individual" + 0.003*"effect" + 0.003*"result" + 0.003*"report"
topic diff=0.283096, rho=0.137575
PROGRESS: pass 2, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.033): 0.040*"specie" + 0.033*"black" + 0.026*"white" + 0.021*"genus" + 0.020*"describe" + 0.015*"hotel" + 0.014*"jewish" + 0.014*"hungarian" + 0.012*"african" + 0.011*"brown"
topic #22 (0.033): 0.013*"company" + 0.012*"railway" + 0.012*"bridge" + 0.012*"line" + 0.009*"store" + 0.008*"street" + 0.007*"sell" + 0.007*"close" + 0.006*"operate" + 0.005*"train"
topic #21 (0.033): 0.009*"say" + 0.008*"murder" + 0.008*"police" + 0.006*"death" + 0.005*"prison" + 0.005*"wife" + 0.005*"arrest" + 0.005*"tell" + 0.004*"father" + 0.004*"kill"
topic #7 (0.033): 0.040*"art" + 0.021*"artist" + 0.016*"museum" + 0.016*"painting" + 0.011*"exhibition" + 0.011*"festival" + 0.008*"bishop" + 0.008*"paint" + 0.007*"study" + 0.007*"painter"
topic #6 (0.033): 0.034*"ship" + 0.025*"island" + 0.010*"sea" + 0.010*"crew" + 0.009*"port" + 0.009*"boat" + 0.009*"vessel" + 0.008*"sail" + 0.007*"coast" + 0.006*"fleet"
topic diff=0.281105, rho=0.137575
PROGRESS: pass 2, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 10
PROGRESS: pass 2, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.033): 0.030*"club" + 0.027*"season" + 0.018*"match" + 0.017*"league" + 0.016*"championship" + 0.015*"football" + 0.014*"final" + 0.011*"player" + 0.011*"game" + 0.010*"event"
topic #20 (0.033): 0.028*"book" + 0.024*"publish" + 0.009*"author" + 0.009*"study" + 0.007*"language" + 0.007*"writer" + 0.007*"history" + 0.007*"magazine" + 0.006*"novel" + 0.006*"editor"
topic #10 (0.033): 0.010*"child" + 0.007*"increase" + 0.005*"cause" + 0.004*"government" + 0.004*"social" + 0.004*"drug" + 0.004*"individual" + 0.004*"french" + 0.004*"effect" + 0.003*"result"
topic #29 (0.033): 0.037*"village" + 0.030*"russian" + 0.015*"cricket" + 0.014*"population" + 0.014*"county" + 0.011*"rural" + 0.011*"central" + 0.010*"polish" + 0.010*"german" + 0.010*"census"
topic #11 (0.033): 0.010*"get" + 0.008*"tell" + 0.007*"kill" + 0.007*"back" + 0.007*"try" + 0.006*"help" + 0.005*"episode" + 0.005*"want" + 0.005*"reveal" + 0.004*"character"
topic diff=0.267830, rho=0.137575
PROGRESS: pass 2, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.089*"film" + 0.016*"star" + 0.012*"direct" + 0.009*"character" + 0.008*"movie" + 0.008*"story" + 0.007*"award" + 0.006*"director" + 0.006*"role" + 0.005*"production"
topic #2 (0.033): 0.030*"club" + 0.027*"season" + 0.017*"match" + 0.017*"league" + 0.016*"championship" + 0.015*"final" + 0.015*"football" + 0.011*"player" + 0.011*"game" + 0.011*"goal"
topic #28 (0.033): 0.022*"population" + 0.022*"town" + 0.019*"county" + 0.019*"village" + 0.017*"road" + 0.013*"river" + 0.013*"north" + 0.013*"age" + 0.011*"route" + 0.011*"south"
topic #9 (0.033): 0.020*"series" + 0.013*"television" + 0.011*"episode" + 0.007*"appear" + 0.007*"tv" + 0.007*"award" + 0.006*"role" + 0.005*"story" + 0.005*"character" + 0.005*"actor"
topic #16 (0.033): 0.016*"unit" + 0.016*"army" + 0.014*"war" + 0.014*"military" + 0.013*"operation" + 0.013*"force" + 0.013*"command" + 0.012*"air" + 0.012*"aircraft" + 0.009*"training"
topic diff=0.267548, rho=0.137575
PROGRESS: pass 2, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.033): 0.035*"ship" + 0.026*"island" + 0.010*"sea" + 0.010*"boat" + 0.010*"port" + 0.010*"crew" + 0.009*"vessel" + 0.008*"sail" + 0.007*"coast" + 0.007*"navy"
topic #23 (0.033): 0.010*"court" + 0.010*"government" + 0.009*"law" + 0.006*"rule" + 0.006*"act" + 0.005*"order" + 0.005*"case" + 0.005*"claim" + 0.005*"right" + 0.005*"say"
topic #10 (0.033): 0.010*"child" + 0.007*"increase" + 0.005*"cause" + 0.004*"government" + 0.004*"individual" + 0.004*"drug" + 0.004*"social" + 0.004*"effect" + 0.004*"result" + 0.003*"treatment"
topic #18 (0.033): 0.054*"company" + 0.017*"business" + 0.012*"found" + 0.009*"industry" + 0.007*"acquire" + 0.007*"firm" + 0.007*"development" + 0.007*"union" + 0.006*"worker" + 0.006*"sell"
topic #4 (0.033): 0.017*"battle" + 0.016*"army" + 0.013*"force" + 0.013*"war" + 0.011*"attack" + 0.009*"troop" + 0.008*"fight" + 0.007*"french" + 0.007*"soldier" + 0.007*"capture"
topic diff=0.249072, rho=0.137575
PROGRESS: pass 2, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.033): 0.009*"say" + 0.008*"police" + 0.007*"murder" + 0.006*"death" + 0.005*"wife" + 0.005*"prison" + 0.005*"father" + 0.005*"arrest" + 0.005*"tell" + 0.004*"kill"
topic #10 (0.033): 0.010*"child" + 0.007*"increase" + 0.005*"cause" + 0.004*"government" + 0.004*"drug" + 0.004*"individual" + 0.004*"social" + 0.004*"effect" + 0.004*"result" + 0.004*"treatment"
topic #7 (0.033): 0.040*"art" + 0.021*"artist" + 0.017*"museum" + 0.014*"painting" + 0.010*"exhibition" + 0.010*"festival" + 0.008*"paint" + 0.008*"study" + 0.007*"bishop" + 0.006*"collection"
topic #9 (0.033): 0.022*"series" + 0.013*"television" + 0.012*"episode" + 0.007*"appear" + 0.007*"award" + 0.007*"tv" + 0.007*"role" + 0.005*"character" + 0.005*"story" + 0.005*"air"
topic #16 (0.033): 0.017*"unit" + 0.016*"army" + 0.014*"war" + 0.013*"military" + 0.013*"operation" + 0.013*"aircraft" + 0.013*"force" + 0.012*"command" + 0.012*"air" + 0.009*"fly"
topic diff=0.238229, rho=0.137575
PROGRESS: pass 2, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.010*"woman" + 0.009*"life" + 0.005*"accord" + 0.005*"say" + 0.004*"practice" + 0.004*"death" + 0.004*"wine" + 0.004*"religious" + 0.004*"tradition" + 0.004*"believe"
topic #29 (0.033): 0.040*"village" + 0.031*"russian" + 0.015*"cricket" + 0.015*"county" + 0.014*"population" + 0.011*"rural" + 0.011*"central" + 0.011*"german" + 0.010*"irish" + 0.010*"polish"
topic #28 (0.033): 0.022*"population" + 0.022*"town" + 0.019*"county" + 0.019*"village" + 0.016*"road" + 0.015*"river" + 0.014*"north" + 0.012*"age" + 0.011*"south" + 0.011*"route"
topic #14 (0.033): 0.012*"specie" + 0.008*"water" + 0.007*"plant" + 0.006*"park" + 0.005*"forest" + 0.005*"tree" + 0.005*"bird" + 0.005*"animal" + 0.004*"river" + 0.004*"region"
topic #2 (0.033): 0.030*"club" + 0.027*"season" + 0.017*"championship" + 0.017*"league" + 0.016*"match" + 0.015*"football" + 0.015*"final" + 0.011*"player" + 0.011*"event" + 0.011*"compete"
topic diff=0.233079, rho=0.137575
PROGRESS: pass 2, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.033): 0.017*"government" + 0.016*"german" + 0.011*"war" + 0.011*"country" + 0.007*"military" + 0.006*"korean" + 0.006*"organization" + 0.006*"international" + 0.006*"political" + 0.005*"department"
topic #2 (0.033): 0.031*"club" + 0.027*"season" + 0.018*"league" + 0.017*"championship" + 0.017*"match" + 0.015*"football" + 0.015*"final" + 0.011*"player" + 0.011*"event" + 0.010*"compete"
topic #23 (0.033): 0.011*"law" + 0.010*"government" + 0.010*"court" + 0.006*"act" + 0.006*"rule" + 0.005*"case" + 0.005*"order" + 0.005*"right" + 0.005*"say" + 0.005*"claim"
topic #6 (0.033): 0.036*"ship" + 0.027*"island" + 0.011*"sea" + 0.010*"port" + 0.010*"boat" + 0.009*"crew" + 0.009*"vessel" + 0.008*"coast" + 0.007*"sail" + 0.007*"water"
topic #7 (0.033): 0.042*"art" + 0.021*"artist" + 0.017*"painting" + 0.016*"museum" + 0.010*"festival" + 0.010*"exhibition" + 0.008*"paint" + 0.007*"study" + 0.007*"bishop" + 0.006*"painter"
topic diff=0.227570, rho=0.137575
PROGRESS: pass 2, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.033): 0.031*"club" + 0.027*"season" + 0.018*"league" + 0.017*"championship" + 0.017*"match" + 0.016*"final" + 0.015*"football" + 0.011*"event" + 0.011*"player" + 0.011*"compete"
topic #12 (0.033): 0.066*"station" + 0.017*"line" + 0.015*"radio" + 0.012*"student" + 0.011*"operate" + 0.009*"network" + 0.008*"program" + 0.008*"train" + 0.008*"broadcast" + 0.008*"local"
topic #28 (0.033): 0.022*"town" + 0.022*"population" + 0.020*"village" + 0.019*"county" + 0.017*"road" + 0.016*"river" + 0.014*"north" + 0.012*"route" + 0.011*"south" + 0.011*"age"
topic #6 (0.033): 0.036*"ship" + 0.027*"island" + 0.011*"sea" + 0.010*"crew" + 0.010*"boat" + 0.010*"port" + 0.010*"vessel" + 0.008*"coast" + 0.007*"sail" + 0.007*"fleet"
topic #0 (0.033): 0.014*"displaystyle" + 0.007*"example" + 0.007*"cell" + 0.006*"protein" + 0.006*"function" + 0.006*"case" + 0.006*"define" + 0.005*"term" + 0.005*"point" + 0.005*"type"
topic diff=0.225597, rho=0.137575
PROGRESS: pass 2, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.033): 0.010*"child" + 0.008*"increase" + 0.006*"cause" + 0.005*"social" + 0.005*"individual" + 0.004*"treatment" + 0.004*"drug" + 0.004*"effect" + 0.004*"result" + 0.004*"government"
topic #23 (0.033): 0.011*"law" + 0.010*"court" + 0.009*"government" + 0.006*"act" + 0.006*"rule" + 0.005*"order" + 0.005*"right" + 0.005*"case" + 0.005*"claim" + 0.005*"say"
topic #2 (0.033): 0.032*"club" + 0.027*"season" + 0.018*"league" + 0.018*"match" + 0.017*"championship" + 0.015*"final" + 0.015*"football" + 0.011*"player" + 0.011*"event" + 0.011*"compete"
topic #21 (0.033): 0.009*"say" + 0.008*"police" + 0.007*"murder" + 0.006*"death" + 0.006*"wife" + 0.005*"prison" + 0.005*"father" + 0.005*"arrest" + 0.005*"tell" + 0.004*"marry"
topic #8 (0.033): 0.035*"album" + 0.033*"song" + 0.028*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic diff=0.214824, rho=0.137575
PROGRESS: pass 2, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.033): 0.040*"village" + 0.038*"russian" + 0.016*"county" + 0.014*"population" + 0.014*"cricket" + 0.014*"irish" + 0.012*"central" + 0.012*"polish" + 0.011*"german" + 0.011*"rural"
topic #27 (0.033): 0.013*"system" + 0.007*"datum" + 0.005*"process" + 0.005*"computer" + 0.005*"develop" + 0.005*"user" + 0.005*"software" + 0.005*"code" + 0.004*"application" + 0.004*"human"
topic #15 (0.033): 0.060*"game" + 0.039*"season" + 0.022*"player" + 0.015*"race" + 0.011*"football" + 0.010*"league" + 0.010*"finish" + 0.009*"point" + 0.009*"basketball" + 0.007*"championship"
topic #11 (0.033): 0.010*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"kill" + 0.007*"try" + 0.006*"help" + 0.005*"reveal" + 0.005*"want" + 0.005*"turn" + 0.004*"decide"
topic #21 (0.033): 0.009*"say" + 0.009*"police" + 0.007*"murder" + 0.006*"death" + 0.006*"wife" + 0.005*"prison" + 0.005*"father" + 0.005*"arrest" + 0.005*"tell" + 0.004*"marry"
topic diff=0.202928, rho=0.137575
PROGRESS: pass 2, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.033): 0.040*"village" + 0.037*"russian" + 0.016*"county" + 0.014*"population" + 0.014*"irish" + 0.013*"cricket" + 0.012*"central" + 0.011*"polish" + 0.011*"german" + 0.011*"rural"
topic #8 (0.033): 0.035*"album" + 0.032*"song" + 0.028*"music" + 0.021*"band" + 0.014*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #9 (0.033): 0.023*"series" + 0.014*"television" + 0.013*"episode" + 0.008*"appear" + 0.007*"tv" + 0.007*"role" + 0.007*"award" + 0.006*"character" + 0.006*"actor" + 0.005*"story"
topic #18 (0.033): 0.058*"company" + 0.019*"business" + 0.012*"found" + 0.009*"industry" + 0.007*"sell" + 0.007*"union" + 0.007*"acquire" + 0.007*"firm" + 0.006*"development" + 0.006*"announce"
topic #27 (0.033): 0.013*"system" + 0.008*"datum" + 0.006*"process" + 0.005*"user" + 0.005*"develop" + 0.005*"computer" + 0.005*"code" + 0.005*"software" + 0.004*"human" + 0.004*"application"
topic diff=0.194291, rho=0.137575
-8.318 per-word bound, 319.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 5000 documents into a model of 49835 documents
topic #8 (0.033): 0.034*"album" + 0.033*"song" + 0.029*"music" + 0.022*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.007*"video"
topic #5 (0.033): 0.011*"woman" + 0.009*"life" + 0.006*"accord" + 0.005*"say" + 0.005*"practice" + 0.004*"religious" + 0.004*"tradition" + 0.004*"death" + 0.004*"believe" + 0.004*"wine"
topic #29 (0.033): 0.041*"village" + 0.038*"russian" + 0.016*"county" + 0.015*"population" + 0.014*"irish" + 0.013*"cricket" + 0.012*"central" + 0.012*"polish" + 0.011*"rural" + 0.011*"german"
topic #14 (0.033): 0.011*"specie" + 0.008*"water" + 0.007*"plant" + 0.006*"tree" + 0.005*"forest" + 0.005*"park" + 0.005*"animal" + 0.004*"region" + 0.004*"grow" + 0.004*"bird"
topic #3 (0.033): 0.012*"design" + 0.009*"system" + 0.009*"car" + 0.009*"power" + 0.009*"engine" + 0.007*"model" + 0.006*"vehicle" + 0.005*"produce" + 0.005*"light" + 0.004*"speed"
topic diff=0.188475, rho=0.137575
-8.317 per-word bound, 319.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 2835 documents into a model of 49835 documents
topic #3 (0.033): 0.012*"design" + 0.009*"system" + 0.009*"power" + 0.009*"car" + 0.008*"engine" + 0.007*"model" + 0.007*"vehicle" + 0.005*"produce" + 0.005*"light" + 0.004*"type"
topic #28 (0.033): 0.023*"population" + 0.023*"town" + 0.020*"village" + 0.020*"county" + 0.016*"road" + 0.014*"river" + 0.014*"north" + 0.012*"age" + 0.012*"south" + 0.011*"km"
topic #0 (0.033): 0.012*"displaystyle" + 0.008*"cell" + 0.007*"example" + 0.007*"define" + 0.007*"function" + 0.006*"case" + 0.006*"point" + 0.006*"protein" + 0.005*"term" + 0.005*"result"
topic #6 (0.033): 0.038*"ship" + 0.027*"island" + 0.011*"sea" + 0.011*"vessel" + 0.011*"port" + 0.010*"boat" + 0.010*"crew" + 0.009*"coast" + 0.008*"fleet" + 0.008*"sail"
topic #16 (0.033): 0.017*"unit" + 0.016*"army" + 0.014*"military" + 0.014*"war" + 0.014*"operation" + 0.013*"command" + 0.012*"force" + 0.012*"air" + 0.012*"aircraft" + 0.009*"training"
topic diff=0.188040, rho=0.137575
-8.249 per-word bound, 304.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.033): 0.061*"company" + 0.019*"business" + 0.013*"found" + 0.010*"industry" + 0.007*"sell" + 0.007*"union" + 0.007*"firm" + 0.007*"development" + 0.007*"acquire" + 0.006*"worker"
topic #26 (0.033): 0.049*"specie" + 0.045*"black" + 0.032*"white" + 0.027*"genus" + 0.024*"describe" + 0.013*"jewish" + 0.012*"african" + 0.012*"brown" + 0.011*"hungarian" + 0.010*"chile"
topic #27 (0.033): 0.013*"system" + 0.007*"datum" + 0.006*"computer" + 0.006*"user" + 0.005*"develop" + 0.005*"process" + 0.005*"code" + 0.005*"information" + 0.005*"application" + 0.004*"software"
topic #4 (0.033): 0.017*"battle" + 0.016*"army" + 0.014*"war" + 0.013*"force" + 0.012*"attack" + 0.009*"troop" + 0.008*"fight" + 0.008*"french" + 0.007*"soldier" + 0.007*"kill"
topic #21 (0.033): 0.009*"police" + 0.009*"say" + 0.007*"death" + 0.007*"murder" + 0.006*"wife" + 0.005*"father" + 0.005*"prison" + 0.005*"arrest" + 0.005*"son" + 0.005*"marry"
topic diff=0.177933, rho=0.136291
PROGRESS: pass 3, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.033): 0.007*"provide" + 0.007*"market" + 0.006*"information" + 0.006*"consumer" + 0.005*"product" + 0.005*"economic" + 0.005*"cost" + 0.005*"increase" + 0.005*"issue" + 0.004*"public"
topic #29 (0.033): 0.039*"village" + 0.036*"russian" + 0.015*"cricket" + 0.015*"county" + 0.014*"population" + 0.014*"irish" + 0.012*"wicket" + 0.012*"polish" + 0.012*"rural" + 0.011*"central"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"kill" + 0.007*"try" + 0.006*"help" + 0.005*"reveal" + 0.005*"want" + 0.005*"turn" + 0.005*"episode"
topic #24 (0.033): 0.019*"university" + 0.019*"college" + 0.018*"award" + 0.017*"student" + 0.015*"research" + 0.014*"study" + 0.013*"science" + 0.011*"education" + 0.010*"program" + 0.008*"graduate"
topic #15 (0.033): 0.064*"game" + 0.039*"season" + 0.025*"player" + 0.015*"race" + 0.011*"league" + 0.010*"football" + 0.010*"finish" + 0.009*"point" + 0.009*"basketball" + 0.007*"coach"
topic diff=0.170077, rho=0.136291
PROGRESS: pass 3, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.033): 0.061*"company" + 0.019*"business" + 0.013*"found" + 0.010*"industry" + 0.008*"sell" + 0.007*"firm" + 0.007*"acquire" + 0.007*"union" + 0.006*"development" + 0.006*"worker"
topic #22 (0.033): 0.014*"railway" + 0.014*"line" + 0.013*"bridge" + 0.010*"street" + 0.009*"company" + 0.008*"store" + 0.007*"close" + 0.007*"train" + 0.007*"park" + 0.006*"construction"
topic #23 (0.033): 0.012*"court" + 0.011*"law" + 0.009*"government" + 0.006*"order" + 0.006*"act" + 0.006*"rule" + 0.006*"case" + 0.005*"right" + 0.005*"claim" + 0.005*"king"
topic #26 (0.033): 0.051*"specie" + 0.044*"black" + 0.031*"white" + 0.028*"genus" + 0.024*"describe" + 0.013*"jewish" + 0.012*"portuguese" + 0.012*"african" + 0.012*"brown" + 0.010*"hungarian"
topic #17 (0.033): 0.007*"provide" + 0.007*"market" + 0.006*"information" + 0.005*"consumer" + 0.005*"product" + 0.005*"economic" + 0.005*"cost" + 0.005*"issue" + 0.005*"increase" + 0.005*"need"
topic diff=0.160417, rho=0.136291
PROGRESS: pass 3, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.033): 0.010*"say" + 0.009*"police" + 0.008*"murder" + 0.007*"death" + 0.007*"wife" + 0.006*"prison" + 0.006*"father" + 0.005*"arrest" + 0.005*"daughter" + 0.005*"marry"
topic #27 (0.033): 0.013*"system" + 0.008*"datum" + 0.006*"computer" + 0.006*"process" + 0.005*"user" + 0.005*"develop" + 0.005*"code" + 0.005*"software" + 0.005*"application" + 0.005*"information"
topic #8 (0.033): 0.034*"album" + 0.033*"song" + 0.029*"music" + 0.023*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.008*"tour" + 0.007*"video" + 0.007*"chart"
topic #0 (0.033): 0.015*"displaystyle" + 0.007*"cell" + 0.007*"example" + 0.007*"define" + 0.006*"function" + 0.006*"case" + 0.006*"point" + 0.005*"protein" + 0.005*"term" + 0.004*"result"
topic #11 (0.033): 0.010*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"kill" + 0.007*"try" + 0.006*"help" + 0.005*"want" + 0.005*"reveal" + 0.005*"turn" + 0.004*"say"
topic diff=0.160958, rho=0.136291
PROGRESS: pass 3, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.033): 0.039*"village" + 0.035*"russian" + 0.017*"irish" + 0.016*"cricket" + 0.016*"county" + 0.015*"population" + 0.012*"polish" + 0.012*"rural" + 0.012*"central" + 0.010*"test"
topic #1 (0.033): 0.098*"film" + 0.018*"star" + 0.013*"direct" + 0.010*"character" + 0.009*"movie" + 0.008*"story" + 0.008*"award" + 0.007*"director" + 0.006*"role" + 0.006*"production"
topic #16 (0.033): 0.018*"unit" + 0.016*"army" + 0.015*"military" + 0.014*"aircraft" + 0.014*"operation" + 0.014*"war" + 0.013*"force" + 0.013*"air" + 0.013*"command" + 0.010*"fly"
topic #0 (0.033): 0.014*"displaystyle" + 0.007*"example" + 0.007*"cell" + 0.007*"define" + 0.006*"function" + 0.006*"case" + 0.006*"point" + 0.005*"term" + 0.005*"protein" + 0.005*"theory"
topic #20 (0.033): 0.030*"book" + 0.027*"publish" + 0.010*"author" + 0.009*"language" + 0.008*"novel" + 0.008*"study" + 0.008*"writer" + 0.007*"history" + 0.007*"magazine" + 0.006*"editor"
topic diff=0.152423, rho=0.136291
PROGRESS: pass 3, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.033): 0.019*"government" + 0.015*"german" + 0.011*"war" + 0.011*"country" + 0.009*"korean" + 0.007*"military" + 0.007*"international" + 0.006*"political" + 0.006*"organization" + 0.006*"japanese"
topic #5 (0.033): 0.009*"life" + 0.009*"woman" + 0.006*"accord" + 0.005*"say" + 0.005*"tradition" + 0.004*"practice" + 0.004*"religious" + 0.004*"christian" + 0.004*"believe" + 0.004*"death"
topic #11 (0.033): 0.010*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"try" + 0.007*"kill" + 0.006*"help" + 0.005*"want" + 0.005*"reveal" + 0.005*"turn" + 0.005*"decide"
topic #1 (0.033): 0.099*"film" + 0.018*"star" + 0.013*"direct" + 0.010*"character" + 0.010*"movie" + 0.008*"story" + 0.008*"award" + 0.007*"director" + 0.006*"role" + 0.006*"production"
topic #4 (0.033): 0.017*"army" + 0.017*"battle" + 0.015*"force" + 0.015*"war" + 0.011*"attack" + 0.009*"troop" + 0.008*"french" + 0.008*"fight" + 0.008*"capture" + 0.008*"soldier"
topic diff=0.154544, rho=0.136291
PROGRESS: pass 3, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.033): 0.037*"ship" + 0.030*"island" + 0.011*"sea" + 0.011*"boat" + 0.011*"port" + 0.010*"crew" + 0.009*"vessel" + 0.008*"coast" + 0.008*"sail" + 0.008*"navy"
topic #0 (0.033): 0.013*"displaystyle" + 0.007*"cell" + 0.007*"example" + 0.007*"function" + 0.006*"define" + 0.006*"case" + 0.005*"protein" + 0.005*"point" + 0.005*"term" + 0.005*"theory"
topic #19 (0.033): 0.028*"building" + 0.021*"church" + 0.017*"house" + 0.010*"design" + 0.009*"site" + 0.008*"th_century" + 0.006*"stone" + 0.006*"wall" + 0.005*"room" + 0.005*"tower"
topic #16 (0.033): 0.018*"unit" + 0.017*"army" + 0.015*"military" + 0.015*"operation" + 0.014*"aircraft" + 0.013*"war" + 0.013*"command" + 0.013*"air" + 0.013*"force" + 0.010*"training"
topic #28 (0.033): 0.023*"town" + 0.023*"population" + 0.021*"village" + 0.020*"county" + 0.017*"road" + 0.015*"river" + 0.013*"north" + 0.013*"age" + 0.011*"route" + 0.011*"south"
topic diff=0.141379, rho=0.136291
PROGRESS: pass 3, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 10
PROGRESS: pass 3, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.033): 0.051*"specie" + 0.044*"black" + 0.035*"white" + 0.026*"genus" + 0.024*"describe" + 0.016*"african" + 0.013*"brown" + 0.012*"hungarian" + 0.011*"blue" + 0.011*"portuguese"
topic #3 (0.033): 0.013*"design" + 0.009*"car" + 0.009*"engine" + 0.009*"system" + 0.008*"power" + 0.008*"model" + 0.008*"vehicle" + 0.005*"produce" + 0.005*"type" + 0.005*"low"
topic #10 (0.033): 0.010*"child" + 0.008*"increase" + 0.006*"cause" + 0.005*"treatment" + 0.005*"drug" + 0.005*"individual" + 0.005*"social" + 0.005*"effect" + 0.004*"result" + 0.004*"patient"
topic #20 (0.033): 0.033*"book" + 0.028*"publish" + 0.010*"author" + 0.009*"language" + 0.009*"novel" + 0.008*"study" + 0.008*"history" + 0.008*"writer" + 0.007*"magazine" + 0.007*"editor"
topic #0 (0.033): 0.013*"displaystyle" + 0.007*"cell" + 0.007*"example" + 0.007*"function" + 0.006*"define" + 0.006*"case" + 0.005*"point" + 0.005*"protein" + 0.005*"term" + 0.005*"theory"
topic diff=0.135783, rho=0.136291
PROGRESS: pass 3, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.033): 0.016*"line" + 0.015*"railway" + 0.013*"bridge" + 0.010*"street" + 0.008*"store" + 0.008*"close" + 0.007*"company" + 0.007*"park" + 0.007*"train" + 0.007*"road"
topic #18 (0.033): 0.062*"company" + 0.020*"business" + 0.013*"found" + 0.011*"industry" + 0.009*"sell" + 0.007*"acquire" + 0.007*"firm" + 0.007*"union" + 0.006*"development" + 0.006*"corporation"
topic #10 (0.033): 0.010*"child" + 0.008*"increase" + 0.007*"cause" + 0.005*"treatment" + 0.005*"social" + 0.005*"individual" + 0.005*"drug" + 0.005*"effect" + 0.004*"result" + 0.004*"patient"
topic #19 (0.033): 0.028*"building" + 0.023*"church" + 0.017*"house" + 0.010*"design" + 0.009*"site" + 0.008*"th_century" + 0.006*"stone" + 0.006*"wall" + 0.005*"style" + 0.005*"room"
topic #4 (0.033): 0.018*"battle" + 0.017*"army" + 0.015*"war" + 0.015*"force" + 0.012*"attack" + 0.009*"fight" + 0.009*"troop" + 0.008*"french" + 0.008*"soldier" + 0.007*"kill"
topic diff=0.134025, rho=0.136291
PROGRESS: pass 3, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.033): 0.018*"unit" + 0.016*"army" + 0.015*"aircraft" + 0.014*"military" + 0.014*"operation" + 0.013*"war" + 0.013*"air" + 0.012*"command" + 0.012*"force" + 0.010*"fly"
topic #20 (0.033): 0.032*"book" + 0.028*"publish" + 0.010*"author" + 0.009*"language" + 0.008*"novel" + 0.008*"magazine" + 0.008*"study" + 0.008*"writer" + 0.007*"history" + 0.007*"editor"
topic #14 (0.033): 0.011*"specie" + 0.009*"water" + 0.007*"plant" + 0.006*"forest" + 0.006*"tree" + 0.005*"park" + 0.005*"bird" + 0.005*"animal" + 0.004*"grow" + 0.004*"range"
topic #12 (0.033): 0.071*"station" + 0.019*"radio" + 0.017*"line" + 0.012*"operate" + 0.011*"network" + 0.010*"channel" + 0.010*"broadcast" + 0.009*"bus" + 0.009*"local" + 0.008*"train"
topic #3 (0.033): 0.013*"design" + 0.010*"engine" + 0.009*"car" + 0.009*"power" + 0.008*"system" + 0.008*"vehicle" + 0.007*"model" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic diff=0.131281, rho=0.136291
PROGRESS: pass 3, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 10
PROGRESS: pass 3, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.033): 0.073*"station" + 0.018*"line" + 0.018*"radio" + 0.012*"operate" + 0.011*"network" + 0.010*"channel" + 0.010*"broadcast" + 0.009*"bus" + 0.008*"local" + 0.008*"program"
topic #1 (0.033): 0.098*"film" + 0.020*"star" + 0.013*"direct" + 0.010*"character" + 0.009*"movie" + 0.008*"story" + 0.008*"award" + 0.006*"role" + 0.006*"director" + 0.006*"production"
topic #3 (0.033): 0.013*"design" + 0.010*"engine" + 0.009*"power" + 0.009*"car" + 0.008*"system" + 0.007*"model" + 0.007*"vehicle" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #16 (0.033): 0.018*"unit" + 0.016*"army" + 0.015*"aircraft" + 0.014*"operation" + 0.014*"air" + 0.013*"military" + 0.013*"war" + 0.013*"command" + 0.012*"force" + 0.010*"fly"
topic #9 (0.033): 0.026*"series" + 0.015*"episode" + 0.014*"television" + 0.008*"appear" + 0.008*"award" + 0.007*"role" + 0.007*"tv" + 0.006*"character" + 0.006*"actor" + 0.005*"air"
topic diff=0.132090, rho=0.136291
PROGRESS: pass 3, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 10
PROGRESS: pass 3, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 11
PROGRESS: pass 3, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 12
PROGRESS: pass 3, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 13
PROGRESS: pass 3, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 13
PROGRESS: pass 3, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 14
PROGRESS: pass 3, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 14
PROGRESS: pass 3, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 15
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.063*"game" + 0.041*"season" + 0.023*"player" + 0.014*"race" + 0.011*"football" + 0.011*"league" + 0.010*"point" + 0.010*"finish" + 0.009*"basketball" + 0.008*"coach"
topic #27 (0.033): 0.014*"system" + 0.008*"datum" + 0.006*"computer" + 0.006*"process" + 0.006*"develop" + 0.005*"software" + 0.005*"user" + 0.005*"code" + 0.005*"application" + 0.005*"control"
topic #9 (0.033): 0.025*"series" + 0.015*"television" + 0.015*"episode" + 0.008*"appear" + 0.008*"award" + 0.008*"role" + 0.007*"tv" + 0.006*"character" + 0.006*"actor" + 0.005*"air"
topic #2 (0.033): 0.033*"club" + 0.025*"season" + 0.018*"championship" + 0.017*"match" + 0.017*"league" + 0.016*"final" + 0.015*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic #18 (0.033): 0.063*"company" + 0.020*"business" + 0.013*"found" + 0.010*"industry" + 0.009*"sell" + 0.007*"acquire" + 0.007*"firm" + 0.007*"union" + 0.006*"development" + 0.006*"corporation"
topic diff=0.125229, rho=0.136291
-8.291 per-word bound, 313.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.033): 0.042*"russian" + 0.041*"village" + 0.022*"irish" + 0.017*"county" + 0.015*"population" + 0.014*"cricket" + 0.014*"polish" + 0.012*"central" + 0.011*"rural" + 0.011*"danish"
topic #25 (0.033): 0.027*"election" + 0.019*"party" + 0.018*"elect" + 0.016*"vote" + 0.011*"candidate" + 0.010*"president" + 0.009*"seat" + 0.009*"appoint" + 0.009*"general" + 0.008*"government"
topic #24 (0.033): 0.021*"university" + 0.020*"student" + 0.019*"college" + 0.018*"award" + 0.015*"research" + 0.014*"study" + 0.013*"science" + 0.011*"education" + 0.011*"program" + 0.009*"graduate"
topic #9 (0.033): 0.025*"series" + 0.014*"episode" + 0.014*"television" + 0.008*"appear" + 0.008*"role" + 0.007*"award" + 0.007*"tv" + 0.006*"character" + 0.006*"actor" + 0.005*"air"
topic #20 (0.033): 0.032*"book" + 0.027*"publish" + 0.010*"author" + 0.010*"language" + 0.008*"novel" + 0.008*"magazine" + 0.008*"writer" + 0.008*"history" + 0.007*"study" + 0.007*"editor"
topic diff=0.118266, rho=0.136291
-8.291 per-word bound, 313.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 10000 documents into a model of 49835 documents
topic #19 (0.033): 0.028*"building" + 0.024*"church" + 0.018*"house" + 0.010*"design" + 0.009*"site" + 0.009*"th_century" + 0.006*"stone" + 0.006*"wall" + 0.005*"style" + 0.005*"tower"
topic #14 (0.033): 0.011*"specie" + 0.008*"water" + 0.007*"plant" + 0.006*"tree" + 0.005*"forest" + 0.005*"animal" + 0.005*"bird" + 0.005*"park" + 0.004*"grow" + 0.004*"fish"
topic #20 (0.033): 0.032*"book" + 0.027*"publish" + 0.010*"author" + 0.010*"language" + 0.008*"novel" + 0.008*"magazine" + 0.008*"writer" + 0.008*"history" + 0.008*"study" + 0.007*"editor"
topic #28 (0.033): 0.024*"population" + 0.023*"town" + 0.021*"village" + 0.020*"county" + 0.016*"road" + 0.016*"river" + 0.014*"north" + 0.012*"age" + 0.012*"south" + 0.011*"km"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"try" + 0.006*"kill" + 0.006*"help" + 0.005*"want" + 0.005*"reveal" + 0.005*"say" + 0.005*"turn"
topic diff=0.105962, rho=0.136291
-8.290 per-word bound, 313.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #18 (0.033): 0.065*"company" + 0.020*"business" + 0.013*"found" + 0.011*"industry" + 0.010*"sell" + 0.008*"firm" + 0.007*"union" + 0.007*"acquire" + 0.006*"product" + 0.006*"development"
topic #7 (0.033): 0.043*"art" + 0.022*"museum" + 0.021*"artist" + 0.015*"painting" + 0.010*"exhibition" + 0.009*"festival" + 0.008*"paint" + 0.008*"study" + 0.008*"collection" + 0.007*"bishop"
topic #4 (0.033): 0.019*"battle" + 0.017*"army" + 0.016*"war" + 0.014*"force" + 0.012*"attack" + 0.009*"troop" + 0.009*"fight" + 0.008*"french" + 0.008*"soldier" + 0.007*"capture"
topic #1 (0.033): 0.103*"film" + 0.021*"star" + 0.014*"direct" + 0.011*"movie" + 0.010*"character" + 0.008*"story" + 0.008*"award" + 0.007*"director" + 0.007*"role" + 0.006*"produce"
topic #25 (0.033): 0.027*"election" + 0.019*"elect" + 0.019*"party" + 0.016*"vote" + 0.010*"president" + 0.010*"candidate" + 0.009*"general" + 0.009*"appoint" + 0.009*"seat" + 0.008*"government"
topic diff=0.125379, rho=0.136291
-8.127 per-word bound, 279.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.033): 0.065*"company" + 0.020*"business" + 0.013*"found" + 0.011*"industry" + 0.010*"sell" + 0.008*"firm" + 0.007*"acquire" + 0.007*"development" + 0.007*"union" + 0.006*"product"
topic #5 (0.033): 0.009*"woman" + 0.008*"life" + 0.006*"accord" + 0.005*"christian" + 0.005*"say" + 0.005*"religious" + 0.005*"tradition" + 0.005*"practice" + 0.004*"word" + 0.004*"wine"
topic #17 (0.033): 0.008*"provide" + 0.006*"consumer" + 0.006*"information" + 0.006*"market" + 0.005*"economic" + 0.005*"public" + 0.005*"issue" + 0.005*"cost" + 0.005*"increase" + 0.005*"product"
topic #25 (0.033): 0.027*"election" + 0.019*"elect" + 0.019*"party" + 0.016*"vote" + 0.010*"president" + 0.010*"candidate" + 0.009*"general" + 0.009*"seat" + 0.009*"appoint" + 0.008*"government"
topic #9 (0.033): 0.025*"series" + 0.015*"episode" + 0.014*"television" + 0.008*"appear" + 0.008*"role" + 0.007*"award" + 0.007*"tv" + 0.007*"character" + 0.006*"actor" + 0.006*"host"
topic diff=0.111398, rho=0.135043
PROGRESS: pass 4, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.007*"kill" + 0.006*"help" + 0.005*"reveal" + 0.005*"want" + 0.005*"turn" + 0.005*"decide"
topic #14 (0.033): 0.011*"specie" + 0.008*"water" + 0.007*"plant" + 0.006*"tree" + 0.005*"forest" + 0.005*"bird" + 0.005*"animal" + 0.005*"grow" + 0.005*"park" + 0.004*"fish"
topic #18 (0.033): 0.064*"company" + 0.019*"business" + 0.013*"found" + 0.011*"industry" + 0.010*"sell" + 0.008*"firm" + 0.007*"acquire" + 0.006*"development" + 0.006*"union" + 0.006*"product"
topic #17 (0.033): 0.008*"provide" + 0.006*"information" + 0.006*"market" + 0.006*"consumer" + 0.005*"public" + 0.005*"economic" + 0.005*"cost" + 0.005*"issue" + 0.005*"increase" + 0.005*"need"
topic #15 (0.033): 0.064*"game" + 0.041*"season" + 0.026*"player" + 0.014*"race" + 0.011*"football" + 0.011*"league" + 0.010*"finish" + 0.009*"basketball" + 0.009*"point" + 0.008*"coach"
topic diff=0.107696, rho=0.135043
PROGRESS: pass 4, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.033): 0.065*"company" + 0.020*"business" + 0.013*"found" + 0.011*"industry" + 0.011*"sell" + 0.007*"firm" + 0.007*"acquire" + 0.006*"product" + 0.006*"development" + 0.006*"union"
topic #19 (0.033): 0.030*"building" + 0.020*"church" + 0.017*"house" + 0.010*"design" + 0.009*"site" + 0.007*"th_century" + 0.007*"stone" + 0.007*"wall" + 0.006*"room" + 0.005*"floor"
topic #12 (0.033): 0.074*"station" + 0.018*"radio" + 0.016*"line" + 0.012*"operate" + 0.012*"network" + 0.011*"channel" + 0.010*"broadcast" + 0.009*"air" + 0.009*"local" + 0.008*"grade"
topic #1 (0.033): 0.102*"film" + 0.020*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"character" + 0.009*"story" + 0.008*"award" + 0.007*"director" + 0.006*"role" + 0.006*"produce"
topic #25 (0.033): 0.027*"election" + 0.019*"elect" + 0.018*"party" + 0.016*"vote" + 0.010*"candidate" + 0.010*"president" + 0.010*"appoint" + 0.010*"seat" + 0.009*"general" + 0.008*"government"
topic diff=0.101517, rho=0.135043
PROGRESS: pass 4, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.033): 0.014*"court" + 0.013*"law" + 0.009*"government" + 0.007*"rule" + 0.007*"act" + 0.007*"right" + 0.006*"order" + 0.006*"case" + 0.005*"claim" + 0.005*"king"
topic #28 (0.033): 0.024*"population" + 0.024*"town" + 0.022*"village" + 0.020*"county" + 0.015*"river" + 0.015*"road" + 0.014*"north" + 0.013*"age" + 0.011*"km" + 0.011*"south"
topic #24 (0.033): 0.021*"student" + 0.020*"college" + 0.020*"university" + 0.017*"award" + 0.015*"study" + 0.014*"research" + 0.013*"science" + 0.012*"education" + 0.011*"program" + 0.009*"graduate"
topic #15 (0.033): 0.066*"game" + 0.042*"season" + 0.026*"player" + 0.014*"race" + 0.011*"league" + 0.011*"football" + 0.010*"basketball" + 0.009*"finish" + 0.009*"point" + 0.008*"coach"
topic #22 (0.033): 0.015*"line" + 0.014*"railway" + 0.013*"bridge" + 0.011*"street" + 0.010*"park" + 0.008*"store" + 0.008*"close" + 0.007*"train" + 0.007*"road" + 0.007*"construction"
topic diff=0.103375, rho=0.135043
PROGRESS: pass 4, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.033): 0.015*"displaystyle" + 0.007*"cell" + 0.007*"example" + 0.006*"function" + 0.006*"protein" + 0.006*"define" + 0.006*"case" + 0.005*"point" + 0.005*"theory" + 0.005*"term"
topic #27 (0.033): 0.015*"system" + 0.008*"datum" + 0.007*"computer" + 0.006*"user" + 0.006*"code" + 0.005*"process" + 0.005*"develop" + 0.005*"software" + 0.005*"application" + 0.005*"information"
topic #17 (0.033): 0.008*"provide" + 0.007*"information" + 0.006*"market" + 0.006*"public" + 0.005*"economic" + 0.005*"increase" + 0.005*"issue" + 0.005*"value" + 0.005*"policy" + 0.005*"consumer"
topic #22 (0.033): 0.016*"line" + 0.014*"railway" + 0.012*"bridge" + 0.011*"street" + 0.010*"park" + 0.008*"store" + 0.008*"close" + 0.007*"train" + 0.007*"road" + 0.007*"construction"
topic #25 (0.033): 0.027*"election" + 0.019*"elect" + 0.019*"party" + 0.016*"vote" + 0.011*"candidate" + 0.010*"seat" + 0.010*"president" + 0.010*"appoint" + 0.009*"general" + 0.009*"government"
topic diff=0.097952, rho=0.135043
PROGRESS: pass 4, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.033): 0.018*"unit" + 0.016*"operation" + 0.016*"army" + 0.016*"military" + 0.016*"aircraft" + 0.013*"air" + 0.013*"war" + 0.013*"command" + 0.012*"force" + 0.010*"fly"
topic #24 (0.033): 0.022*"student" + 0.021*"college" + 0.020*"university" + 0.016*"award" + 0.015*"study" + 0.014*"research" + 0.013*"science" + 0.012*"education" + 0.011*"program" + 0.009*"graduate"
topic #1 (0.033): 0.103*"film" + 0.020*"star" + 0.014*"direct" + 0.011*"movie" + 0.010*"character" + 0.009*"story" + 0.009*"award" + 0.007*"director" + 0.007*"role" + 0.006*"production"
topic #4 (0.033): 0.017*"battle" + 0.017*"army" + 0.017*"war" + 0.016*"force" + 0.012*"attack" + 0.009*"troop" + 0.009*"french" + 0.008*"fight" + 0.008*"soldier" + 0.008*"capture"
topic #7 (0.033): 0.043*"art" + 0.021*"artist" + 0.021*"museum" + 0.016*"painting" + 0.011*"exhibition" + 0.008*"paint" + 0.008*"festival" + 0.008*"study" + 0.008*"collection" + 0.007*"bishop"
topic diff=0.101234, rho=0.135043
PROGRESS: pass 4, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.034*"book" + 0.029*"publish" + 0.010*"author" + 0.010*"language" + 0.010*"novel" + 0.008*"writer" + 0.008*"magazine" + 0.008*"history" + 0.007*"study" + 0.007*"editor"
topic #18 (0.033): 0.065*"company" + 0.020*"business" + 0.012*"found" + 0.011*"industry" + 0.011*"sell" + 0.008*"firm" + 0.008*"acquire" + 0.007*"product" + 0.007*"corporation" + 0.006*"development"
topic #8 (0.033): 0.035*"album" + 0.035*"song" + 0.030*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #6 (0.033): 0.039*"ship" + 0.031*"island" + 0.012*"sea" + 0.011*"port" + 0.011*"boat" + 0.010*"crew" + 0.010*"vessel" + 0.008*"navy" + 0.008*"coast" + 0.008*"sail"
topic #4 (0.033): 0.018*"battle" + 0.018*"army" + 0.017*"war" + 0.016*"force" + 0.012*"attack" + 0.009*"troop" + 0.009*"french" + 0.008*"fight" + 0.008*"soldier" + 0.007*"capture"
topic diff=0.090630, rho=0.135043
PROGRESS: pass 4, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.033): 0.016*"line" + 0.015*"railway" + 0.012*"bridge" + 0.011*"street" + 0.010*"park" + 0.008*"store" + 0.008*"close" + 0.008*"road" + 0.008*"train" + 0.006*"construction"
topic #18 (0.033): 0.065*"company" + 0.020*"business" + 0.013*"found" + 0.011*"industry" + 0.011*"sell" + 0.008*"acquire" + 0.007*"firm" + 0.007*"product" + 0.007*"corporation" + 0.006*"market"
topic #13 (0.033): 0.020*"government" + 0.016*"german" + 0.013*"country" + 0.012*"war" + 0.008*"political" + 0.008*"korean" + 0.007*"international" + 0.006*"military" + 0.006*"chinese" + 0.006*"organization"
topic #25 (0.033): 0.027*"election" + 0.019*"elect" + 0.019*"party" + 0.016*"vote" + 0.011*"candidate" + 0.011*"president" + 0.011*"seat" + 0.009*"appoint" + 0.009*"general" + 0.009*"government"
topic #28 (0.033): 0.024*"town" + 0.024*"population" + 0.021*"village" + 0.020*"county" + 0.016*"river" + 0.016*"road" + 0.014*"north" + 0.013*"age" + 0.011*"south" + 0.011*"municipality"
topic diff=0.089149, rho=0.135043
PROGRESS: pass 4, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.033): 0.011*"specie" + 0.009*"water" + 0.008*"plant" + 0.006*"tree" + 0.006*"forest" + 0.005*"animal" + 0.005*"bird" + 0.005*"grow" + 0.005*"park" + 0.005*"fish"
topic #21 (0.033): 0.011*"say" + 0.010*"police" + 0.009*"death" + 0.008*"wife" + 0.008*"son" + 0.008*"father" + 0.007*"murder" + 0.007*"marry" + 0.006*"daughter" + 0.006*"child"
topic #13 (0.033): 0.020*"government" + 0.019*"german" + 0.013*"country" + 0.012*"war" + 0.008*"political" + 0.007*"korean" + 0.007*"international" + 0.006*"military" + 0.006*"chinese" + 0.006*"organization"
topic #20 (0.033): 0.033*"book" + 0.029*"publish" + 0.010*"author" + 0.010*"language" + 0.010*"novel" + 0.008*"writer" + 0.008*"magazine" + 0.008*"history" + 0.007*"study" + 0.007*"editor"
topic #10 (0.033): 0.009*"child" + 0.008*"increase" + 0.008*"cause" + 0.007*"treatment" + 0.006*"patient" + 0.005*"individual" + 0.005*"effect" + 0.005*"result" + 0.005*"drug" + 0.005*"social"
topic diff=0.088580, rho=0.135043
PROGRESS: pass 4, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 10
PROGRESS: pass 4, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #28 (0.033): 0.024*"town" + 0.024*"population" + 0.021*"village" + 0.019*"county" + 0.018*"river" + 0.015*"road" + 0.014*"north" + 0.012*"age" + 0.012*"south" + 0.011*"km"
topic #1 (0.033): 0.102*"film" + 0.020*"star" + 0.014*"direct" + 0.011*"character" + 0.010*"movie" + 0.009*"award" + 0.009*"story" + 0.007*"role" + 0.007*"director" + 0.006*"produce"
topic #12 (0.033): 0.077*"station" + 0.021*"radio" + 0.017*"line" + 0.013*"operate" + 0.012*"network" + 0.012*"channel" + 0.011*"broadcast" + 0.010*"bus" + 0.009*"air" + 0.009*"local"
topic #8 (0.033): 0.036*"album" + 0.034*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #25 (0.033): 0.028*"election" + 0.021*"party" + 0.020*"elect" + 0.016*"vote" + 0.011*"candidate" + 0.011*"president" + 0.010*"seat" + 0.009*"appoint" + 0.009*"general" + 0.009*"government"
topic diff=0.086741, rho=0.135043
PROGRESS: pass 4, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.033): 0.022*"student" + 0.020*"university" + 0.019*"college" + 0.017*"award" + 0.015*"research" + 0.015*"study" + 0.012*"science" + 0.012*"education" + 0.011*"program" + 0.009*"graduate"
topic #17 (0.033): 0.008*"provide" + 0.006*"information" + 0.006*"public" + 0.005*"market" + 0.005*"economic" + 0.005*"consumer" + 0.005*"increase" + 0.005*"cost" + 0.005*"policy" + 0.005*"issue"
topic #29 (0.033): 0.040*"village" + 0.039*"russian" + 0.028*"irish" + 0.018*"county" + 0.016*"cricket" + 0.014*"population" + 0.013*"polish" + 0.012*"central" + 0.012*"rural" + 0.012*"swedish"
topic #22 (0.033): 0.017*"line" + 0.015*"railway" + 0.013*"bridge" + 0.012*"street" + 0.010*"park" + 0.008*"road" + 0.008*"store" + 0.008*"close" + 0.008*"train" + 0.007*"construction"
topic #27 (0.033): 0.015*"system" + 0.008*"datum" + 0.006*"computer" + 0.006*"process" + 0.006*"develop" + 0.005*"code" + 0.005*"application" + 0.005*"software" + 0.005*"control" + 0.005*"user"
topic diff=0.089208, rho=0.135043
PROGRESS: pass 4, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.033): 0.037*"album" + 0.035*"song" + 0.030*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #26 (0.033): 0.056*"black" + 0.052*"specie" + 0.041*"white" + 0.026*"genus" + 0.024*"describe" + 0.016*"color" + 0.015*"brown" + 0.014*"blue" + 0.013*"african" + 0.012*"red"
topic #13 (0.033): 0.023*"german" + 0.019*"government" + 0.013*"country" + 0.011*"war" + 0.008*"political" + 0.007*"international" + 0.007*"korean" + 0.006*"chinese" + 0.006*"military" + 0.006*"organization"
topic #2 (0.033): 0.033*"club" + 0.025*"season" + 0.019*"championship" + 0.018*"match" + 0.017*"league" + 0.016*"final" + 0.015*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic #18 (0.033): 0.064*"company" + 0.021*"business" + 0.013*"found" + 0.012*"sell" + 0.011*"industry" + 0.007*"acquire" + 0.007*"product" + 0.007*"firm" + 0.007*"corporation" + 0.006*"union"
topic diff=0.088379, rho=0.135043
PROGRESS: pass 4, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.034*"book" + 0.028*"publish" + 0.011*"author" + 0.010*"language" + 0.009*"novel" + 0.008*"magazine" + 0.008*"writer" + 0.008*"history" + 0.007*"study" + 0.007*"editor"
topic #9 (0.033): 0.026*"series" + 0.015*"episode" + 0.014*"television" + 0.009*"appear" + 0.008*"award" + 0.008*"role" + 0.007*"tv" + 0.007*"character" + 0.006*"actor" + 0.006*"theatre"
topic #28 (0.033): 0.024*"population" + 0.024*"town" + 0.022*"village" + 0.019*"county" + 0.017*"river" + 0.015*"road" + 0.014*"north" + 0.012*"age" + 0.011*"south" + 0.011*"km"
topic #16 (0.033): 0.018*"unit" + 0.016*"army" + 0.016*"aircraft" + 0.015*"military" + 0.014*"air" + 0.014*"operation" + 0.013*"command" + 0.012*"war" + 0.011*"force" + 0.010*"fly"
topic #4 (0.033): 0.019*"battle" + 0.017*"war" + 0.017*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"fight" + 0.009*"troop" + 0.009*"french" + 0.008*"soldier" + 0.007*"kill"
topic diff=0.081730, rho=0.135043
PROGRESS: pass 4, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 10
PROGRESS: pass 4, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #27 (0.033): 0.016*"system" + 0.009*"datum" + 0.006*"computer" + 0.006*"user" + 0.006*"process" + 0.006*"code" + 0.006*"develop" + 0.005*"software" + 0.005*"application" + 0.005*"technology"
topic #12 (0.033): 0.077*"station" + 0.020*"radio" + 0.017*"line" + 0.013*"channel" + 0.012*"operate" + 0.012*"network" + 0.011*"broadcast" + 0.010*"air" + 0.010*"airport" + 0.009*"bus"
topic #9 (0.033): 0.027*"series" + 0.016*"episode" + 0.015*"television" + 0.009*"appear" + 0.008*"role" + 0.008*"award" + 0.007*"tv" + 0.007*"character" + 0.006*"actor" + 0.006*"season"
topic #16 (0.033): 0.018*"unit" + 0.016*"army" + 0.015*"military" + 0.015*"aircraft" + 0.014*"operation" + 0.014*"air" + 0.013*"command" + 0.013*"war" + 0.011*"force" + 0.010*"training"
topic #4 (0.033): 0.019*"battle" + 0.017*"army" + 0.017*"war" + 0.015*"force" + 0.013*"attack" + 0.009*"fight" + 0.009*"troop" + 0.009*"french" + 0.008*"soldier" + 0.007*"capture"
topic diff=0.079270, rho=0.135043
-8.235 per-word bound, 301.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.106*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"character" + 0.009*"award" + 0.009*"story" + 0.008*"director" + 0.007*"role" + 0.006*"produce"
topic #0 (0.033): 0.012*"displaystyle" + 0.008*"cell" + 0.008*"protein" + 0.007*"example" + 0.007*"function" + 0.006*"point" + 0.006*"case" + 0.005*"term" + 0.005*"define" + 0.005*"gene"
topic #2 (0.033): 0.033*"club" + 0.025*"season" + 0.018*"championship" + 0.017*"match" + 0.017*"league" + 0.015*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic #23 (0.033): 0.014*"law" + 0.013*"court" + 0.009*"government" + 0.008*"act" + 0.008*"rule" + 0.007*"right" + 0.007*"order" + 0.006*"case" + 0.006*"claim" + 0.006*"king"
topic #17 (0.033): 0.008*"provide" + 0.006*"public" + 0.006*"economic" + 0.006*"cost" + 0.006*"information" + 0.005*"issue" + 0.005*"market" + 0.005*"report" + 0.005*"policy" + 0.005*"increase"
topic diff=0.077281, rho=0.135043
-8.237 per-word bound, 301.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #10 (0.033): 0.009*"increase" + 0.008*"cause" + 0.008*"child" + 0.007*"treatment" + 0.006*"patient" + 0.006*"effect" + 0.005*"result" + 0.005*"individual" + 0.005*"woman" + 0.005*"drug"
topic #5 (0.033): 0.009*"woman" + 0.008*"life" + 0.006*"accord" + 0.005*"christian" + 0.005*"religious" + 0.005*"tradition" + 0.005*"say" + 0.004*"word" + 0.004*"mean" + 0.004*"practice"
topic #15 (0.033): 0.065*"game" + 0.042*"season" + 0.025*"player" + 0.016*"race" + 0.012*"football" + 0.011*"league" + 0.010*"point" + 0.010*"basketball" + 0.009*"finish" + 0.009*"coach"
topic #27 (0.033): 0.017*"system" + 0.008*"datum" + 0.007*"computer" + 0.006*"code" + 0.006*"user" + 0.006*"process" + 0.006*"develop" + 0.005*"application" + 0.005*"software" + 0.005*"technology"
topic #23 (0.033): 0.014*"law" + 0.014*"court" + 0.009*"government" + 0.008*"act" + 0.008*"rule" + 0.007*"right" + 0.007*"order" + 0.006*"case" + 0.006*"king" + 0.006*"claim"
topic diff=0.074672, rho=0.135043
-8.205 per-word bound, 295.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.033*"book" + 0.029*"publish" + 0.011*"language" + 0.010*"author" + 0.009*"novel" + 0.008*"history" + 0.008*"magazine" + 0.008*"writer" + 0.007*"study" + 0.007*"publication"
topic #12 (0.033): 0.075*"station" + 0.019*"radio" + 0.016*"line" + 0.014*"network" + 0.013*"channel" + 0.013*"operate" + 0.011*"broadcast" + 0.010*"bus" + 0.010*"air" + 0.009*"airport"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"help" + 0.006*"kill" + 0.005*"want" + 0.005*"turn" + 0.005*"say" + 0.005*"reveal"
topic #2 (0.033): 0.031*"club" + 0.024*"season" + 0.019*"championship" + 0.017*"match" + 0.016*"league" + 0.016*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic #27 (0.033): 0.016*"system" + 0.008*"datum" + 0.007*"computer" + 0.006*"user" + 0.006*"code" + 0.006*"develop" + 0.006*"process" + 0.005*"application" + 0.005*"software" + 0.005*"information"
topic diff=0.076701, rho=0.133828
PROGRESS: pass 5, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.033*"book" + 0.028*"publish" + 0.011*"language" + 0.010*"author" + 0.010*"novel" + 0.008*"writer" + 0.008*"history" + 0.008*"magazine" + 0.007*"study" + 0.007*"editor"
topic #5 (0.033): 0.008*"woman" + 0.008*"life" + 0.006*"accord" + 0.005*"christian" + 0.005*"tradition" + 0.005*"practice" + 0.005*"religious" + 0.005*"say" + 0.004*"word" + 0.004*"mean"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"kill" + 0.006*"help" + 0.005*"want" + 0.005*"reveal" + 0.005*"turn" + 0.005*"say"
topic #13 (0.033): 0.019*"government" + 0.018*"german" + 0.013*"country" + 0.012*"war" + 0.008*"political" + 0.008*"international" + 0.006*"military" + 0.006*"chinese" + 0.005*"organization" + 0.005*"korean"
topic #18 (0.033): 0.066*"company" + 0.021*"business" + 0.013*"found" + 0.012*"sell" + 0.011*"industry" + 0.007*"acquire" + 0.007*"firm" + 0.007*"market" + 0.007*"product" + 0.006*"corporation"
topic diff=0.075448, rho=0.133828
PROGRESS: pass 5, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.033): 0.066*"company" + 0.021*"business" + 0.013*"found" + 0.013*"sell" + 0.011*"industry" + 0.007*"acquire" + 0.007*"product" + 0.007*"market" + 0.007*"firm" + 0.006*"sale"
topic #6 (0.033): 0.038*"ship" + 0.033*"island" + 0.012*"sea" + 0.011*"port" + 0.010*"boat" + 0.010*"crew" + 0.010*"vessel" + 0.009*"coast" + 0.008*"sail" + 0.008*"fleet"
topic #26 (0.033): 0.056*"specie" + 0.050*"black" + 0.037*"white" + 0.029*"genus" + 0.025*"describe" + 0.015*"blue" + 0.014*"color" + 0.014*"brown" + 0.014*"portuguese" + 0.012*"red"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"kill" + 0.006*"help" + 0.005*"want" + 0.005*"say" + 0.005*"reveal" + 0.005*"turn"
topic #5 (0.033): 0.008*"woman" + 0.008*"life" + 0.007*"accord" + 0.005*"christian" + 0.005*"tradition" + 0.005*"say" + 0.005*"religious" + 0.005*"practice" + 0.004*"word" + 0.004*"mean"
topic diff=0.071635, rho=0.133828
PROGRESS: pass 5, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.033): 0.031*"club" + 0.024*"season" + 0.019*"championship" + 0.018*"match" + 0.016*"league" + 0.016*"final" + 0.013*"football" + 0.013*"event" + 0.011*"compete" + 0.011*"round"
topic #20 (0.033): 0.033*"book" + 0.028*"publish" + 0.011*"language" + 0.010*"author" + 0.009*"novel" + 0.008*"writer" + 0.008*"history" + 0.008*"magazine" + 0.007*"study" + 0.007*"editor"
topic #29 (0.033): 0.040*"village" + 0.038*"russian" + 0.024*"irish" + 0.018*"county" + 0.017*"cricket" + 0.015*"population" + 0.014*"polish" + 0.013*"swedish" + 0.013*"rural" + 0.012*"central"
topic #4 (0.033): 0.018*"war" + 0.018*"battle" + 0.017*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"troop" + 0.009*"french" + 0.008*"fight" + 0.008*"soldier" + 0.008*"capture"
topic #6 (0.033): 0.037*"ship" + 0.033*"island" + 0.012*"sea" + 0.011*"port" + 0.011*"boat" + 0.010*"crew" + 0.010*"vessel" + 0.009*"coast" + 0.008*"sail" + 0.008*"fleet"
topic diff=0.073531, rho=0.133828
PROGRESS: pass 5, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.033): 0.036*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"help" + 0.006*"kill" + 0.006*"want" + 0.005*"say" + 0.005*"reveal" + 0.005*"turn"
topic #3 (0.033): 0.012*"design" + 0.010*"car" + 0.010*"engine" + 0.009*"power" + 0.008*"system" + 0.007*"model" + 0.007*"vehicle" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #24 (0.033): 0.023*"student" + 0.022*"college" + 0.019*"university" + 0.016*"award" + 0.015*"study" + 0.014*"research" + 0.013*"science" + 0.013*"education" + 0.011*"program" + 0.009*"graduate"
topic #17 (0.033): 0.008*"provide" + 0.007*"information" + 0.006*"public" + 0.006*"economic" + 0.005*"market" + 0.005*"policy" + 0.005*"increase" + 0.005*"issue" + 0.005*"cost" + 0.005*"report"
topic diff=0.069571, rho=0.133828
PROGRESS: pass 5, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.033): 0.020*"government" + 0.016*"german" + 0.013*"country" + 0.011*"war" + 0.009*"korean" + 0.008*"political" + 0.007*"international" + 0.006*"military" + 0.006*"chinese" + 0.006*"organization"
topic #0 (0.033): 0.014*"displaystyle" + 0.008*"cell" + 0.007*"example" + 0.006*"function" + 0.006*"protein" + 0.006*"define" + 0.005*"case" + 0.005*"point" + 0.005*"theory" + 0.005*"term"
topic #12 (0.033): 0.079*"station" + 0.021*"radio" + 0.016*"line" + 0.013*"operate" + 0.013*"network" + 0.013*"channel" + 0.012*"bus" + 0.011*"broadcast" + 0.010*"air" + 0.009*"local"
topic #5 (0.033): 0.009*"life" + 0.008*"woman" + 0.006*"accord" + 0.005*"tradition" + 0.005*"christian" + 0.005*"religious" + 0.005*"say" + 0.004*"practice" + 0.004*"word" + 0.004*"refer"
topic #15 (0.033): 0.068*"game" + 0.042*"season" + 0.025*"player" + 0.014*"race" + 0.012*"league" + 0.012*"football" + 0.010*"basketball" + 0.010*"point" + 0.009*"finish" + 0.008*"coach"
topic diff=0.072967, rho=0.133828
PROGRESS: pass 5, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.033): 0.040*"village" + 0.037*"russian" + 0.028*"irish" + 0.019*"county" + 0.017*"cricket" + 0.015*"polish" + 0.015*"population" + 0.014*"swedish" + 0.013*"rural" + 0.012*"central"
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"engine" + 0.009*"power" + 0.008*"vehicle" + 0.008*"model" + 0.008*"system" + 0.005*"produce" + 0.005*"type" + 0.005*"light"
topic #17 (0.033): 0.008*"provide" + 0.007*"information" + 0.006*"public" + 0.006*"economic" + 0.006*"consumer" + 0.005*"policy" + 0.005*"increase" + 0.005*"market" + 0.005*"issue" + 0.005*"cost"
topic #5 (0.033): 0.008*"life" + 0.007*"woman" + 0.006*"accord" + 0.005*"tradition" + 0.005*"religious" + 0.005*"christian" + 0.005*"say" + 0.004*"mean" + 0.004*"practice" + 0.004*"temple"
topic #13 (0.033): 0.020*"government" + 0.015*"german" + 0.013*"country" + 0.011*"war" + 0.009*"political" + 0.008*"korean" + 0.007*"international" + 0.006*"military" + 0.006*"chinese" + 0.006*"organization"
topic diff=0.064895, rho=0.133828
PROGRESS: pass 5, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.033): 0.031*"building" + 0.022*"church" + 0.018*"house" + 0.010*"site" + 0.010*"design" + 0.008*"th_century" + 0.006*"stone" + 0.006*"wall" + 0.006*"tower" + 0.006*"room"
topic #9 (0.033): 0.029*"series" + 0.016*"episode" + 0.015*"television" + 0.010*"award" + 0.009*"appear" + 0.008*"role" + 0.007*"tv" + 0.007*"season" + 0.006*"theatre" + 0.006*"character"
topic #1 (0.033): 0.105*"film" + 0.020*"star" + 0.015*"direct" + 0.011*"character" + 0.011*"movie" + 0.009*"award" + 0.009*"story" + 0.007*"director" + 0.007*"role" + 0.006*"production"
topic #8 (0.033): 0.036*"album" + 0.036*"song" + 0.031*"music" + 0.024*"band" + 0.016*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #15 (0.033): 0.067*"game" + 0.041*"season" + 0.026*"player" + 0.016*"race" + 0.012*"football" + 0.012*"league" + 0.010*"basketball" + 0.010*"point" + 0.010*"finish" + 0.009*"coach"
topic diff=0.064540, rho=0.133828
PROGRESS: pass 5, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.107*"film" + 0.020*"star" + 0.015*"direct" + 0.011*"character" + 0.011*"movie" + 0.009*"award" + 0.009*"story" + 0.007*"director" + 0.007*"role" + 0.006*"production"
topic #8 (0.033): 0.036*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #25 (0.033): 0.029*"election" + 0.021*"party" + 0.021*"elect" + 0.017*"vote" + 0.012*"candidate" + 0.011*"seat" + 0.011*"president" + 0.010*"appoint" + 0.010*"general" + 0.009*"government"
topic #19 (0.033): 0.031*"building" + 0.022*"church" + 0.018*"house" + 0.011*"site" + 0.010*"design" + 0.008*"th_century" + 0.006*"stone" + 0.006*"wall" + 0.006*"tower" + 0.006*"room"
topic #16 (0.033): 0.018*"unit" + 0.017*"aircraft" + 0.016*"army" + 0.015*"operation" + 0.015*"military" + 0.013*"air" + 0.013*"command" + 0.013*"war" + 0.012*"force" + 0.011*"fly"
topic diff=0.065185, rho=0.133828
PROGRESS: pass 5, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.068*"game" + 0.041*"season" + 0.026*"player" + 0.014*"race" + 0.012*"football" + 0.011*"league" + 0.010*"point" + 0.010*"basketball" + 0.009*"finish" + 0.009*"coach"
topic #25 (0.033): 0.029*"election" + 0.022*"party" + 0.020*"elect" + 0.017*"vote" + 0.011*"candidate" + 0.011*"president" + 0.011*"seat" + 0.010*"government" + 0.010*"general" + 0.009*"appoint"
topic #12 (0.033): 0.081*"station" + 0.023*"radio" + 0.016*"line" + 0.013*"channel" + 0.013*"network" + 0.013*"operate" + 0.012*"broadcast" + 0.010*"bus" + 0.010*"air" + 0.009*"airport"
topic #10 (0.033): 0.009*"cause" + 0.009*"increase" + 0.008*"treatment" + 0.008*"child" + 0.007*"patient" + 0.006*"effect" + 0.006*"result" + 0.006*"individual" + 0.005*"woman" + 0.005*"drug"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"try" + 0.006*"kill" + 0.006*"help" + 0.006*"want" + 0.005*"say" + 0.005*"turn" + 0.005*"reveal"
topic diff=0.063163, rho=0.133828
PROGRESS: pass 5, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.008*"life" + 0.008*"woman" + 0.006*"accord" + 0.005*"christian" + 0.005*"religious" + 0.005*"tradition" + 0.005*"say" + 0.005*"word" + 0.005*"practice" + 0.004*"refer"
topic #22 (0.033): 0.018*"line" + 0.015*"railway" + 0.013*"bridge" + 0.012*"street" + 0.012*"park" + 0.010*"road" + 0.008*"train" + 0.008*"close" + 0.007*"store" + 0.007*"construction"
topic #12 (0.033): 0.083*"station" + 0.023*"radio" + 0.017*"line" + 0.013*"network" + 0.013*"channel" + 0.013*"operate" + 0.012*"broadcast" + 0.010*"air" + 0.010*"bus" + 0.010*"airport"
topic #28 (0.033): 0.024*"population" + 0.024*"town" + 0.022*"village" + 0.020*"county" + 0.019*"river" + 0.015*"road" + 0.015*"north" + 0.012*"age" + 0.012*"south" + 0.011*"km"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"help" + 0.006*"kill" + 0.006*"want" + 0.005*"turn" + 0.005*"say" + 0.005*"reveal"
topic diff=0.066127, rho=0.133828
PROGRESS: pass 5, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.008*"try" + 0.006*"help" + 0.006*"kill" + 0.006*"want" + 0.005*"say" + 0.005*"turn" + 0.005*"decide"
topic #0 (0.033): 0.013*"displaystyle" + 0.008*"cell" + 0.008*"protein" + 0.007*"example" + 0.006*"function" + 0.005*"point" + 0.005*"case" + 0.005*"define" + 0.005*"gene" + 0.005*"type"
topic #21 (0.033): 0.011*"say" + 0.010*"son" + 0.010*"death" + 0.010*"police" + 0.009*"father" + 0.009*"wife" + 0.008*"marry" + 0.008*"daughter" + 0.008*"child" + 0.007*"murder"
topic #5 (0.033): 0.008*"woman" + 0.008*"life" + 0.006*"accord" + 0.006*"christian" + 0.005*"religious" + 0.005*"tradition" + 0.005*"say" + 0.005*"word" + 0.005*"practice" + 0.005*"mean"
topic #29 (0.033): 0.043*"russian" + 0.041*"village" + 0.032*"irish" + 0.018*"county" + 0.016*"cricket" + 0.015*"population" + 0.015*"swedish" + 0.013*"polish" + 0.013*"central" + 0.012*"danish"
topic diff=0.066883, rho=0.133828
PROGRESS: pass 5, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.035*"book" + 0.029*"publish" + 0.011*"author" + 0.011*"language" + 0.010*"novel" + 0.008*"magazine" + 0.008*"writer" + 0.008*"history" + 0.007*"editor" + 0.007*"publication"
topic #18 (0.033): 0.065*"company" + 0.022*"business" + 0.014*"sell" + 0.013*"found" + 0.011*"industry" + 0.008*"product" + 0.008*"market" + 0.007*"acquire" + 0.007*"firm" + 0.007*"corporation"
topic #22 (0.033): 0.018*"line" + 0.015*"railway" + 0.014*"bridge" + 0.013*"park" + 0.012*"street" + 0.010*"road" + 0.008*"train" + 0.008*"close" + 0.007*"store" + 0.007*"construction"
topic #6 (0.033): 0.039*"ship" + 0.032*"island" + 0.013*"sea" + 0.013*"port" + 0.011*"crew" + 0.010*"boat" + 0.010*"coast" + 0.010*"vessel" + 0.009*"fleet" + 0.009*"navy"
topic #12 (0.033): 0.081*"station" + 0.023*"radio" + 0.016*"line" + 0.014*"channel" + 0.013*"network" + 0.013*"broadcast" + 0.013*"operate" + 0.011*"air" + 0.010*"bus" + 0.010*"airport"
topic diff=0.061741, rho=0.133828
PROGRESS: pass 5, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 10
PROGRESS: pass 5, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.033): 0.028*"series" + 0.017*"episode" + 0.015*"television" + 0.009*"appear" + 0.009*"award" + 0.009*"role" + 0.007*"tv" + 0.007*"season" + 0.007*"theatre" + 0.007*"character"
topic #2 (0.033): 0.032*"club" + 0.025*"season" + 0.019*"championship" + 0.018*"match" + 0.017*"league" + 0.016*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic #0 (0.033): 0.012*"displaystyle" + 0.009*"cell" + 0.008*"protein" + 0.007*"function" + 0.007*"example" + 0.006*"point" + 0.005*"case" + 0.005*"define" + 0.005*"gene" + 0.005*"type"
topic #1 (0.033): 0.109*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"role" + 0.007*"produce"
topic #8 (0.033): 0.038*"album" + 0.035*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic diff=0.059400, rho=0.133828
-8.257 per-word bound, 306.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.033): 0.018*"line" + 0.014*"railway" + 0.013*"park" + 0.013*"bridge" + 0.011*"street" + 0.010*"road" + 0.008*"close" + 0.008*"train" + 0.007*"store" + 0.007*"construction"
topic #27 (0.033): 0.018*"system" + 0.009*"datum" + 0.007*"computer" + 0.007*"code" + 0.006*"user" + 0.006*"develop" + 0.006*"process" + 0.006*"software" + 0.006*"application" + 0.006*"technology"
topic #9 (0.033): 0.029*"series" + 0.017*"episode" + 0.015*"television" + 0.010*"appear" + 0.009*"award" + 0.009*"role" + 0.007*"tv" + 0.007*"season" + 0.007*"theatre" + 0.006*"character"
topic #1 (0.033): 0.109*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"produce" + 0.007*"role"
topic #21 (0.033): 0.011*"say" + 0.011*"police" + 0.010*"son" + 0.010*"death" + 0.010*"wife" + 0.009*"father" + 0.009*"daughter" + 0.008*"marry" + 0.008*"child" + 0.007*"murder"
topic diff=0.057343, rho=0.133828
-8.261 per-word bound, 306.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #28 (0.033): 0.025*"population" + 0.024*"town" + 0.022*"village" + 0.021*"county" + 0.016*"river" + 0.015*"road" + 0.014*"north" + 0.013*"age" + 0.012*"south" + 0.012*"km"
topic #0 (0.033): 0.012*"displaystyle" + 0.009*"cell" + 0.007*"protein" + 0.007*"function" + 0.007*"example" + 0.006*"define" + 0.006*"point" + 0.005*"gene" + 0.005*"case" + 0.005*"term"
topic #8 (0.033): 0.037*"album" + 0.036*"song" + 0.032*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #1 (0.033): 0.110*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"role" + 0.007*"produce"
topic #29 (0.033): 0.042*"russian" + 0.042*"village" + 0.031*"irish" + 0.019*"county" + 0.016*"swedish" + 0.016*"polish" + 0.015*"cricket" + 0.015*"population" + 0.013*"central" + 0.012*"danish"
topic diff=0.055963, rho=0.133828
-8.223 per-word bound, 298.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.033): 0.043*"art" + 0.022*"museum" + 0.022*"artist" + 0.016*"painting" + 0.011*"exhibition" + 0.008*"paint" + 0.008*"collection" + 0.008*"study" + 0.008*"festival" + 0.007*"bishop"
topic #18 (0.033): 0.067*"company" + 0.021*"business" + 0.014*"sell" + 0.013*"found" + 0.011*"industry" + 0.009*"market" + 0.008*"product" + 0.007*"acquire" + 0.007*"firm" + 0.007*"production"
topic #27 (0.033): 0.018*"system" + 0.009*"datum" + 0.007*"computer" + 0.007*"user" + 0.006*"code" + 0.006*"develop" + 0.006*"process" + 0.005*"application" + 0.005*"information" + 0.005*"software"
topic #15 (0.033): 0.067*"game" + 0.042*"season" + 0.026*"player" + 0.015*"race" + 0.012*"football" + 0.012*"league" + 0.010*"point" + 0.010*"finish" + 0.010*"basketball" + 0.009*"coach"
topic #17 (0.033): 0.008*"provide" + 0.006*"public" + 0.006*"information" + 0.006*"economic" + 0.005*"report" + 0.005*"issue" + 0.005*"cost" + 0.005*"consumer" + 0.005*"increase" + 0.005*"policy"
topic diff=0.059235, rho=0.132645
PROGRESS: pass 6, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.033): 0.014*"displaystyle" + 0.008*"cell" + 0.007*"example" + 0.007*"function" + 0.007*"protein" + 0.006*"define" + 0.006*"point" + 0.005*"case" + 0.005*"gene" + 0.005*"theory"
topic #20 (0.033): 0.033*"book" + 0.029*"publish" + 0.012*"language" + 0.011*"author" + 0.010*"novel" + 0.008*"writer" + 0.008*"history" + 0.008*"magazine" + 0.007*"editor" + 0.007*"publication"
topic #21 (0.033): 0.011*"son" + 0.011*"death" + 0.011*"say" + 0.010*"police" + 0.010*"wife" + 0.010*"father" + 0.009*"daughter" + 0.009*"marry" + 0.009*"child" + 0.007*"murder"
topic #8 (0.033): 0.037*"album" + 0.036*"song" + 0.031*"music" + 0.025*"band" + 0.015*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #13 (0.033): 0.019*"government" + 0.018*"german" + 0.014*"country" + 0.012*"war" + 0.009*"political" + 0.008*"international" + 0.006*"military" + 0.006*"chinese" + 0.005*"movement" + 0.005*"organization"
topic diff=0.058843, rho=0.132645
PROGRESS: pass 6, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.066*"game" + 0.042*"season" + 0.026*"player" + 0.014*"race" + 0.012*"league" + 0.012*"football" + 0.011*"basketball" + 0.010*"finish" + 0.010*"point" + 0.009*"coach"
topic #6 (0.033): 0.038*"ship" + 0.034*"island" + 0.012*"sea" + 0.011*"port" + 0.011*"boat" + 0.010*"crew" + 0.010*"vessel" + 0.010*"coast" + 0.009*"fleet" + 0.009*"sail"
topic #8 (0.033): 0.037*"album" + 0.036*"song" + 0.031*"music" + 0.024*"band" + 0.015*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #23 (0.033): 0.015*"court" + 0.015*"law" + 0.008*"government" + 0.008*"act" + 0.008*"rule" + 0.007*"order" + 0.007*"right" + 0.007*"case" + 0.006*"king" + 0.006*"claim"
topic #9 (0.033): 0.029*"series" + 0.017*"episode" + 0.016*"television" + 0.009*"appear" + 0.009*"award" + 0.008*"role" + 0.008*"tv" + 0.007*"theatre" + 0.007*"season" + 0.007*"host"
topic diff=0.055917, rho=0.132645
PROGRESS: pass 6, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.033): 0.015*"court" + 0.015*"law" + 0.008*"government" + 0.008*"rule" + 0.008*"act" + 0.008*"right" + 0.007*"order" + 0.007*"case" + 0.006*"king" + 0.006*"claim"
topic #11 (0.033): 0.011*"get" + 0.009*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"help" + 0.006*"kill" + 0.006*"say" + 0.006*"want" + 0.005*"reveal" + 0.005*"turn"
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"information" + 0.006*"economic" + 0.006*"policy" + 0.005*"report" + 0.005*"issue" + 0.005*"increase" + 0.005*"cost" + 0.005*"project"
topic #13 (0.033): 0.020*"government" + 0.017*"german" + 0.014*"country" + 0.012*"war" + 0.008*"political" + 0.008*"korean" + 0.008*"international" + 0.006*"military" + 0.006*"chinese" + 0.005*"organization"
topic #16 (0.033): 0.018*"unit" + 0.017*"aircraft" + 0.017*"army" + 0.016*"military" + 0.015*"operation" + 0.013*"air" + 0.013*"war" + 0.012*"command" + 0.012*"force" + 0.011*"fly"
topic diff=0.057633, rho=0.132645
PROGRESS: pass 6, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.109*"film" + 0.020*"star" + 0.015*"direct" + 0.011*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"role" + 0.007*"production"
topic #8 (0.033): 0.037*"album" + 0.036*"song" + 0.032*"music" + 0.024*"band" + 0.015*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #3 (0.033): 0.012*"design" + 0.011*"car" + 0.010*"engine" + 0.009*"power" + 0.007*"model" + 0.007*"vehicle" + 0.007*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #29 (0.033): 0.039*"village" + 0.039*"russian" + 0.032*"irish" + 0.018*"county" + 0.017*"cricket" + 0.016*"swedish" + 0.015*"polish" + 0.015*"population" + 0.013*"rural" + 0.013*"central"
topic #27 (0.033): 0.017*"system" + 0.009*"datum" + 0.008*"computer" + 0.007*"user" + 0.006*"code" + 0.006*"develop" + 0.006*"application" + 0.006*"process" + 0.006*"information" + 0.006*"software"
topic diff=0.054553, rho=0.132645
PROGRESS: pass 6, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.069*"game" + 0.043*"season" + 0.026*"player" + 0.014*"race" + 0.012*"league" + 0.012*"football" + 0.010*"basketball" + 0.010*"point" + 0.009*"finish" + 0.008*"coach"
topic #6 (0.033): 0.039*"ship" + 0.034*"island" + 0.013*"sea" + 0.012*"boat" + 0.011*"port" + 0.010*"crew" + 0.009*"vessel" + 0.009*"coast" + 0.009*"navy" + 0.009*"sail"
topic #14 (0.033): 0.011*"specie" + 0.009*"water" + 0.008*"plant" + 0.006*"animal" + 0.006*"forest" + 0.006*"tree" + 0.006*"bird" + 0.005*"fish" + 0.005*"grow" + 0.004*"range"
topic #29 (0.033): 0.039*"village" + 0.038*"russian" + 0.032*"irish" + 0.019*"county" + 0.017*"cricket" + 0.017*"swedish" + 0.016*"polish" + 0.014*"population" + 0.013*"central" + 0.012*"rural"
topic #23 (0.033): 0.015*"court" + 0.015*"law" + 0.009*"rule" + 0.009*"act" + 0.008*"government" + 0.007*"right" + 0.007*"case" + 0.007*"order" + 0.006*"king" + 0.005*"claim"
topic diff=0.058105, rho=0.132645
PROGRESS: pass 6, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.033): 0.035*"book" + 0.030*"publish" + 0.011*"author" + 0.011*"language" + 0.011*"novel" + 0.009*"writer" + 0.008*"magazine" + 0.008*"history" + 0.007*"publication" + 0.007*"editor"
topic #11 (0.033): 0.011*"get" + 0.009*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"help" + 0.006*"say" + 0.006*"want" + 0.006*"kill" + 0.005*"reveal" + 0.005*"turn"
topic #2 (0.033): 0.032*"club" + 0.023*"season" + 0.019*"championship" + 0.017*"match" + 0.017*"league" + 0.016*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.010*"player"
topic #0 (0.033): 0.013*"displaystyle" + 0.008*"cell" + 0.007*"example" + 0.007*"function" + 0.006*"protein" + 0.005*"define" + 0.005*"case" + 0.005*"point" + 0.005*"theory" + 0.005*"gene"
topic #4 (0.033): 0.018*"war" + 0.017*"battle" + 0.017*"army" + 0.016*"force" + 0.012*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.008*"soldier" + 0.008*"capture"
topic diff=0.050138, rho=0.132645
PROGRESS: pass 6, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.033): 0.036*"album" + 0.036*"song" + 0.031*"music" + 0.024*"band" + 0.016*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"chart" + 0.008*"tour" + 0.007*"video"
topic #14 (0.033): 0.011*"specie" + 0.009*"water" + 0.008*"plant" + 0.006*"animal" + 0.006*"tree" + 0.006*"bird" + 0.006*"forest" + 0.005*"fish" + 0.005*"grow" + 0.004*"range"
topic #21 (0.033): 0.011*"son" + 0.011*"say" + 0.011*"father" + 0.010*"death" + 0.010*"wife" + 0.010*"police" + 0.009*"daughter" + 0.009*"marry" + 0.009*"child" + 0.007*"murder"
topic #29 (0.033): 0.040*"village" + 0.037*"russian" + 0.031*"irish" + 0.020*"county" + 0.017*"cricket" + 0.016*"swedish" + 0.015*"polish" + 0.014*"population" + 0.012*"danish" + 0.012*"rural"
topic #2 (0.033): 0.032*"club" + 0.024*"season" + 0.019*"championship" + 0.017*"match" + 0.017*"league" + 0.015*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic diff=0.052463, rho=0.132645
PROGRESS: pass 6, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.008*"life" + 0.007*"woman" + 0.006*"accord" + 0.006*"christian" + 0.005*"religious" + 0.005*"tradition" + 0.005*"say" + 0.005*"mean" + 0.004*"refer" + 0.004*"word"
topic #27 (0.033): 0.017*"system" + 0.009*"datum" + 0.007*"computer" + 0.006*"code" + 0.006*"user" + 0.006*"software" + 0.006*"process" + 0.006*"develop" + 0.006*"application" + 0.005*"technology"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"help" + 0.006*"kill" + 0.006*"want" + 0.006*"say" + 0.005*"turn" + 0.005*"reveal"
topic #10 (0.033): 0.010*"cause" + 0.009*"increase" + 0.008*"patient" + 0.008*"treatment" + 0.007*"child" + 0.006*"effect" + 0.006*"result" + 0.006*"disease" + 0.006*"woman" + 0.006*"individual"
topic #16 (0.033): 0.019*"unit" + 0.017*"aircraft" + 0.016*"army" + 0.015*"operation" + 0.015*"military" + 0.013*"air" + 0.013*"command" + 0.012*"war" + 0.011*"force" + 0.011*"fly"
topic diff=0.052505, rho=0.132645
PROGRESS: pass 6, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.007*"life" + 0.007*"woman" + 0.007*"accord" + 0.005*"christian" + 0.005*"religious" + 0.005*"tradition" + 0.005*"say" + 0.005*"mean" + 0.005*"word" + 0.005*"refer"
topic #7 (0.033): 0.043*"art" + 0.022*"artist" + 0.021*"museum" + 0.017*"painting" + 0.010*"exhibition" + 0.008*"collection" + 0.008*"paint" + 0.008*"opera" + 0.008*"study" + 0.007*"festival"
topic #9 (0.033): 0.030*"series" + 0.017*"episode" + 0.016*"television" + 0.011*"award" + 0.010*"appear" + 0.009*"role" + 0.008*"season" + 0.007*"theatre" + 0.007*"tv" + 0.006*"air"
topic #28 (0.033): 0.025*"population" + 0.024*"town" + 0.022*"village" + 0.020*"county" + 0.019*"river" + 0.014*"north" + 0.014*"road" + 0.013*"age" + 0.012*"south" + 0.012*"municipality"
topic #21 (0.033): 0.011*"son" + 0.011*"say" + 0.010*"death" + 0.010*"father" + 0.010*"police" + 0.010*"wife" + 0.009*"daughter" + 0.009*"child" + 0.009*"marry" + 0.007*"murder"
topic diff=0.049892, rho=0.132645
PROGRESS: pass 6, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.033): 0.031*"building" + 0.022*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.005*"main"
topic #5 (0.033): 0.008*"life" + 0.007*"woman" + 0.007*"accord" + 0.006*"christian" + 0.005*"religious" + 0.005*"tradition" + 0.005*"word" + 0.005*"say" + 0.005*"mean" + 0.005*"refer"
topic #24 (0.033): 0.024*"student" + 0.021*"college" + 0.020*"university" + 0.016*"award" + 0.016*"study" + 0.015*"research" + 0.013*"science" + 0.012*"education" + 0.011*"program" + 0.010*"graduate"
topic #25 (0.033): 0.030*"election" + 0.022*"party" + 0.021*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.011*"seat" + 0.011*"president" + 0.010*"general" + 0.010*"appoint" + 0.010*"government"
topic #1 (0.033): 0.109*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.007*"director" + 0.007*"role" + 0.007*"produce"
topic diff=0.055246, rho=0.132645
PROGRESS: pass 6, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.033): 0.022*"german" + 0.019*"government" + 0.014*"country" + 0.011*"war" + 0.009*"political" + 0.007*"international" + 0.006*"korean" + 0.006*"chinese" + 0.006*"movement" + 0.006*"military"
topic #22 (0.033): 0.019*"line" + 0.015*"railway" + 0.014*"park" + 0.013*"bridge" + 0.012*"street" + 0.012*"road" + 0.008*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"store"
topic #20 (0.033): 0.035*"book" + 0.030*"publish" + 0.012*"language" + 0.011*"author" + 0.010*"novel" + 0.009*"magazine" + 0.009*"writer" + 0.008*"history" + 0.008*"editor" + 0.007*"publication"
topic #6 (0.033): 0.040*"ship" + 0.034*"island" + 0.013*"sea" + 0.011*"port" + 0.011*"coast" + 0.011*"boat" + 0.011*"crew" + 0.010*"vessel" + 0.009*"fleet" + 0.008*"navy"
topic #21 (0.033): 0.011*"son" + 0.011*"say" + 0.011*"father" + 0.010*"death" + 0.010*"wife" + 0.010*"child" + 0.009*"police" + 0.009*"marry" + 0.009*"daughter" + 0.007*"murder"
topic diff=0.051320, rho=0.132645
PROGRESS: pass 6, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.033): 0.012*"displaystyle" + 0.009*"cell" + 0.009*"protein" + 0.007*"example" + 0.006*"function" + 0.005*"gene" + 0.005*"point" + 0.005*"case" + 0.005*"define" + 0.005*"structure"
topic #7 (0.033): 0.045*"art" + 0.022*"artist" + 0.022*"museum" + 0.016*"painting" + 0.010*"exhibition" + 0.008*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"festival" + 0.007*"opera"
topic #23 (0.033): 0.016*"law" + 0.014*"court" + 0.009*"act" + 0.008*"rule" + 0.008*"government" + 0.008*"right" + 0.007*"case" + 0.007*"order" + 0.006*"claim" + 0.005*"king"
topic #29 (0.033): 0.044*"russian" + 0.041*"village" + 0.033*"irish" + 0.019*"county" + 0.016*"swedish" + 0.016*"polish" + 0.016*"cricket" + 0.015*"population" + 0.013*"danish" + 0.013*"central"
topic #25 (0.033): 0.030*"election" + 0.022*"party" + 0.021*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.011*"president" + 0.011*"seat" + 0.010*"general" + 0.010*"appoint" + 0.010*"government"
topic diff=0.050112, rho=0.132645
PROGRESS: pass 6, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.033): 0.083*"station" + 0.023*"radio" + 0.015*"channel" + 0.014*"line" + 0.014*"network" + 0.013*"broadcast" + 0.013*"operate" + 0.012*"air" + 0.011*"airport" + 0.010*"bus"
topic #24 (0.033): 0.024*"student" + 0.021*"university" + 0.020*"college" + 0.016*"award" + 0.016*"study" + 0.014*"research" + 0.013*"science" + 0.012*"education" + 0.011*"program" + 0.010*"graduate"
topic #9 (0.033): 0.029*"series" + 0.017*"episode" + 0.015*"television" + 0.010*"appear" + 0.010*"award" + 0.009*"role" + 0.008*"season" + 0.007*"tv" + 0.007*"theatre" + 0.007*"actor"
topic #5 (0.033): 0.007*"life" + 0.007*"woman" + 0.007*"accord" + 0.006*"christian" + 0.006*"religious" + 0.005*"tradition" + 0.005*"word" + 0.005*"mean" + 0.005*"say" + 0.005*"refer"
topic #18 (0.033): 0.066*"company" + 0.022*"business" + 0.015*"sell" + 0.012*"found" + 0.011*"industry" + 0.009*"market" + 0.008*"product" + 0.007*"sale" + 0.007*"acquire" + 0.007*"firm"
topic diff=0.048526, rho=0.132645
-8.263 per-word bound, 307.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #13 (0.033): 0.020*"german" + 0.019*"government" + 0.014*"country" + 0.011*"war" + 0.009*"political" + 0.007*"international" + 0.006*"military" + 0.006*"movement" + 0.006*"chinese" + 0.006*"korean"
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"economic" + 0.006*"report" + 0.006*"cost" + 0.006*"issue" + 0.006*"policy" + 0.006*"information" + 0.006*"increase" + 0.005*"system"
topic #16 (0.033): 0.019*"unit" + 0.017*"army" + 0.016*"aircraft" + 0.015*"military" + 0.014*"operation" + 0.013*"air" + 0.013*"command" + 0.012*"war" + 0.011*"force" + 0.010*"division"
topic #4 (0.033): 0.018*"battle" + 0.017*"war" + 0.016*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"french" + 0.009*"troop" + 0.009*"fight" + 0.008*"soldier" + 0.007*"kill"
topic #15 (0.033): 0.066*"game" + 0.042*"season" + 0.025*"player" + 0.015*"race" + 0.013*"football" + 0.012*"league" + 0.010*"basketball" + 0.010*"point" + 0.010*"finish" + 0.009*"coach"
topic diff=0.044925, rho=0.132645
-8.265 per-word bound, 307.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #5 (0.033): 0.007*"life" + 0.007*"woman" + 0.007*"accord" + 0.006*"christian" + 0.006*"religious" + 0.005*"tradition" + 0.005*"word" + 0.005*"mean" + 0.005*"say" + 0.005*"refer"
topic #6 (0.033): 0.039*"ship" + 0.032*"island" + 0.013*"sea" + 0.012*"port" + 0.011*"boat" + 0.011*"vessel" + 0.010*"coast" + 0.010*"crew" + 0.009*"fleet" + 0.009*"navy"
topic #23 (0.033): 0.016*"law" + 0.015*"court" + 0.009*"act" + 0.008*"rule" + 0.008*"government" + 0.007*"right" + 0.007*"order" + 0.007*"case" + 0.006*"king" + 0.006*"claim"
topic #10 (0.033): 0.010*"cause" + 0.009*"increase" + 0.007*"treatment" + 0.007*"patient" + 0.007*"child" + 0.006*"effect" + 0.006*"woman" + 0.006*"result" + 0.006*"disease" + 0.006*"drug"
topic #29 (0.033): 0.042*"russian" + 0.041*"village" + 0.032*"irish" + 0.020*"county" + 0.017*"swedish" + 0.016*"polish" + 0.016*"cricket" + 0.015*"population" + 0.013*"central" + 0.012*"danish"
topic diff=0.046978, rho=0.132645
-8.218 per-word bound, 297.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 10
PROGRESS: pass 7, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.113*"film" + 0.021*"star" + 0.016*"direct" + 0.012*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"role" + 0.007*"festival"
topic #15 (0.033): 0.068*"game" + 0.042*"season" + 0.026*"player" + 0.014*"race" + 0.012*"football" + 0.012*"league" + 0.010*"point" + 0.010*"basketball" + 0.010*"finish" + 0.009*"coach"
topic #13 (0.033): 0.019*"government" + 0.018*"german" + 0.014*"country" + 0.012*"war" + 0.009*"political" + 0.007*"international" + 0.006*"military" + 0.006*"movement" + 0.006*"chinese" + 0.005*"organization"
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"power" + 0.010*"engine" + 0.008*"model" + 0.007*"vehicle" + 0.007*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"type"
topic #14 (0.033): 0.010*"specie" + 0.008*"water" + 0.008*"plant" + 0.006*"tree" + 0.006*"animal" + 0.006*"forest" + 0.005*"bird" + 0.005*"grow" + 0.005*"fish" + 0.004*"range"
topic diff=0.048660, rho=0.131494
PROGRESS: pass 7, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.033): 0.018*"war" + 0.017*"battle" + 0.016*"army" + 0.015*"force" + 0.012*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.007*"capture" + 0.007*"soldier"
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"engine" + 0.010*"power" + 0.007*"model" + 0.007*"vehicle" + 0.007*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #16 (0.033): 0.019*"unit" + 0.017*"aircraft" + 0.016*"army" + 0.016*"military" + 0.014*"operation" + 0.012*"war" + 0.012*"air" + 0.012*"command" + 0.011*"force" + 0.010*"fly"
topic #14 (0.033): 0.010*"specie" + 0.009*"water" + 0.008*"plant" + 0.007*"tree" + 0.006*"animal" + 0.006*"forest" + 0.005*"bird" + 0.005*"grow" + 0.005*"fish" + 0.004*"range"
topic #8 (0.033): 0.037*"album" + 0.036*"song" + 0.032*"music" + 0.025*"band" + 0.016*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic diff=0.049113, rho=0.131494
PROGRESS: pass 7, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.110*"film" + 0.020*"star" + 0.016*"direct" + 0.012*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.007*"director" + 0.007*"produce" + 0.007*"festival"
topic #18 (0.033): 0.067*"company" + 0.021*"business" + 0.015*"sell" + 0.012*"found" + 0.011*"industry" + 0.010*"market" + 0.008*"product" + 0.008*"sale" + 0.007*"acquire" + 0.007*"firm"
topic #14 (0.033): 0.010*"specie" + 0.008*"water" + 0.008*"plant" + 0.006*"tree" + 0.006*"animal" + 0.006*"forest" + 0.005*"bird" + 0.005*"grow" + 0.004*"fish" + 0.004*"range"
topic #21 (0.033): 0.013*"son" + 0.011*"death" + 0.011*"father" + 0.011*"child" + 0.011*"wife" + 0.010*"say" + 0.010*"daughter" + 0.010*"marry" + 0.009*"police" + 0.007*"mother"
topic #6 (0.033): 0.039*"ship" + 0.035*"island" + 0.013*"sea" + 0.011*"port" + 0.011*"boat" + 0.010*"crew" + 0.010*"coast" + 0.010*"vessel" + 0.009*"fleet" + 0.009*"sail"
topic diff=0.046696, rho=0.131494
PROGRESS: pass 7, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.033): 0.016*"court" + 0.016*"law" + 0.009*"act" + 0.009*"rule" + 0.008*"government" + 0.008*"right" + 0.007*"order" + 0.007*"case" + 0.006*"claim" + 0.006*"king"
topic #6 (0.033): 0.038*"ship" + 0.035*"island" + 0.013*"sea" + 0.012*"boat" + 0.012*"port" + 0.011*"crew" + 0.010*"coast" + 0.010*"vessel" + 0.009*"fleet" + 0.008*"sail"
topic #28 (0.033): 0.026*"population" + 0.024*"town" + 0.023*"village" + 0.021*"county" + 0.017*"river" + 0.014*"age" + 0.014*"north" + 0.013*"road" + 0.012*"km" + 0.012*"municipality"
topic #22 (0.033): 0.019*"line" + 0.016*"park" + 0.015*"railway" + 0.013*"bridge" + 0.012*"road" + 0.012*"street" + 0.008*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"hotel"
topic #27 (0.033): 0.018*"system" + 0.009*"datum" + 0.008*"computer" + 0.007*"user" + 0.006*"code" + 0.006*"process" + 0.006*"develop" + 0.006*"information" + 0.006*"application" + 0.006*"software"
topic diff=0.048307, rho=0.131494
PROGRESS: pass 7, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.008*"life" + 0.007*"accord" + 0.006*"woman" + 0.006*"tradition" + 0.006*"christian" + 0.005*"religious" + 0.005*"word" + 0.005*"refer" + 0.005*"say" + 0.005*"mean"
topic #2 (0.033): 0.032*"club" + 0.023*"season" + 0.019*"championship" + 0.018*"match" + 0.016*"league" + 0.015*"final" + 0.014*"football" + 0.013*"event" + 0.011*"compete" + 0.010*"player"
topic #14 (0.033): 0.010*"specie" + 0.009*"water" + 0.008*"plant" + 0.006*"animal" + 0.006*"tree" + 0.006*"forest" + 0.006*"bird" + 0.005*"fish" + 0.005*"grow" + 0.004*"range"
topic #25 (0.033): 0.030*"election" + 0.022*"party" + 0.021*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.012*"seat" + 0.011*"president" + 0.010*"government" + 0.010*"general" + 0.010*"appoint"
topic #11 (0.033): 0.011*"get" + 0.009*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"say" + 0.006*"help" + 0.006*"want" + 0.006*"kill" + 0.005*"reveal" + 0.005*"turn"
topic diff=0.045346, rho=0.131494
PROGRESS: pass 7, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.033): 0.018*"unit" + 0.017*"aircraft" + 0.017*"army" + 0.016*"operation" + 0.016*"military" + 0.013*"air" + 0.013*"command" + 0.012*"war" + 0.012*"force" + 0.011*"fly"
topic #26 (0.033): 0.055*"specie" + 0.052*"black" + 0.041*"white" + 0.027*"genus" + 0.024*"describe" + 0.017*"blue" + 0.017*"red" + 0.016*"brown" + 0.015*"color" + 0.014*"african"
topic #5 (0.033): 0.008*"life" + 0.007*"accord" + 0.006*"woman" + 0.006*"tradition" + 0.006*"christian" + 0.006*"religious" + 0.005*"word" + 0.005*"say" + 0.005*"refer" + 0.005*"mean"
topic #0 (0.033): 0.014*"displaystyle" + 0.008*"cell" + 0.007*"example" + 0.007*"function" + 0.006*"protein" + 0.006*"define" + 0.005*"case" + 0.005*"gene" + 0.005*"theory" + 0.005*"point"
topic #4 (0.033): 0.018*"war" + 0.017*"battle" + 0.017*"army" + 0.016*"force" + 0.012*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.008*"soldier" + 0.008*"capture"
topic diff=0.049070, rho=0.131494
PROGRESS: pass 7, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
PROGRESS: pass 7, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.033): 0.041*"village" + 0.037*"russian" + 0.032*"irish" + 0.020*"county" + 0.017*"cricket" + 0.017*"swedish" + 0.016*"polish" + 0.014*"population" + 0.013*"rural" + 0.012*"danish"
topic #20 (0.033): 0.036*"book" + 0.031*"publish" + 0.011*"language" + 0.011*"author" + 0.011*"novel" + 0.009*"writer" + 0.008*"magazine" + 0.008*"history" + 0.007*"editor" + 0.007*"publication"
topic #25 (0.033): 0.030*"election" + 0.021*"party" + 0.021*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.012*"seat" + 0.011*"president" + 0.010*"general" + 0.010*"government" + 0.010*"appoint"
topic #19 (0.033): 0.032*"building" + 0.022*"church" + 0.019*"house" + 0.011*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.006*"tower"
topic #21 (0.033): 0.013*"son" + 0.012*"father" + 0.011*"death" + 0.010*"say" + 0.010*"wife" + 0.010*"child" + 0.010*"daughter" + 0.010*"marry" + 0.009*"police" + 0.007*"brother"
topic diff=0.041888, rho=0.131494
PROGRESS: pass 7, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.013*"design" + 0.011*"car" + 0.010*"engine" + 0.009*"power" + 0.008*"vehicle" + 0.008*"model" + 0.007*"system" + 0.005*"produce" + 0.005*"type" + 0.005*"light"
topic #19 (0.033): 0.032*"building" + 0.022*"church" + 0.019*"house" + 0.011*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"tower" + 0.006*"room"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.006*"say" + 0.006*"help" + 0.006*"want" + 0.005*"kill" + 0.005*"turn" + 0.005*"reveal"
topic #7 (0.033): 0.042*"art" + 0.022*"museum" + 0.022*"artist" + 0.015*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"opera" + 0.008*"study" + 0.007*"festival"
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"information" + 0.006*"economic" + 0.006*"policy" + 0.006*"increase" + 0.005*"issue" + 0.005*"report" + 0.005*"cost" + 0.005*"consumer"
topic diff=0.043405, rho=0.131494
PROGRESS: pass 7, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.033): 0.031*"election" + 0.022*"party" + 0.022*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.012*"seat" + 0.011*"president" + 0.010*"general" + 0.010*"government" + 0.010*"appoint"
topic #26 (0.033): 0.056*"specie" + 0.049*"black" + 0.039*"white" + 0.028*"genus" + 0.024*"describe" + 0.016*"red" + 0.016*"blue" + 0.016*"color" + 0.015*"brown" + 0.014*"african"
topic #16 (0.033): 0.019*"unit" + 0.018*"aircraft" + 0.017*"army" + 0.015*"operation" + 0.015*"military" + 0.013*"command" + 0.013*"air" + 0.012*"war" + 0.011*"force" + 0.011*"fly"
topic #23 (0.033): 0.017*"law" + 0.015*"court" + 0.009*"act" + 0.009*"rule" + 0.008*"right" + 0.008*"government" + 0.008*"case" + 0.007*"order" + 0.006*"king" + 0.006*"claim"
topic #19 (0.033): 0.032*"building" + 0.022*"church" + 0.019*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"tower" + 0.006*"room"
topic diff=0.044364, rho=0.131494
PROGRESS: pass 7, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.033): 0.025*"student" + 0.021*"university" + 0.021*"college" + 0.016*"study" + 0.016*"award" + 0.014*"research" + 0.013*"science" + 0.012*"education" + 0.011*"program" + 0.010*"graduate"
topic #3 (0.033): 0.013*"design" + 0.010*"engine" + 0.010*"car" + 0.009*"power" + 0.008*"vehicle" + 0.007*"model" + 0.007*"system" + 0.006*"light" + 0.006*"produce" + 0.005*"low"
topic #4 (0.033): 0.018*"war" + 0.018*"battle" + 0.016*"army" + 0.016*"force" + 0.013*"attack" + 0.009*"fight" + 0.009*"french" + 0.009*"troop" + 0.008*"soldier" + 0.007*"capture"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.007*"back" + 0.007*"try" + 0.006*"say" + 0.006*"help" + 0.006*"want" + 0.006*"kill" + 0.005*"turn" + 0.005*"reveal"
topic #15 (0.033): 0.069*"game" + 0.041*"season" + 0.027*"player" + 0.013*"race" + 0.012*"football" + 0.012*"league" + 0.010*"point" + 0.010*"basketball" + 0.009*"coach" + 0.009*"finish"
topic diff=0.042080, rho=0.131494
PROGRESS: pass 7, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.033): 0.032*"building" + 0.022*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.006*"tower"
topic #24 (0.033): 0.024*"student" + 0.021*"university" + 0.020*"college" + 0.016*"study" + 0.016*"award" + 0.015*"research" + 0.013*"education" + 0.013*"science" + 0.011*"program" + 0.010*"graduate"
topic #1 (0.033): 0.109*"film" + 0.023*"star" + 0.015*"direct" + 0.011*"character" + 0.011*"movie" + 0.009*"award" + 0.009*"story" + 0.007*"director" + 0.007*"produce" + 0.007*"role"
topic #2 (0.033): 0.033*"club" + 0.024*"season" + 0.019*"championship" + 0.017*"match" + 0.017*"league" + 0.016*"final" + 0.015*"football" + 0.013*"event" + 0.011*"compete" + 0.010*"player"
topic #3 (0.033): 0.013*"design" + 0.010*"engine" + 0.010*"car" + 0.009*"power" + 0.008*"vehicle" + 0.008*"model" + 0.007*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic diff=0.045330, rho=0.131494
PROGRESS: pass 7, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.033): 0.040*"ship" + 0.034*"island" + 0.014*"sea" + 0.011*"port" + 0.011*"coast" + 0.011*"boat" + 0.011*"crew" + 0.010*"vessel" + 0.009*"fleet" + 0.009*"navy"
topic #13 (0.033): 0.022*"german" + 0.019*"government" + 0.014*"country" + 0.011*"war" + 0.009*"political" + 0.007*"international" + 0.006*"korean" + 0.006*"movement" + 0.006*"chinese" + 0.006*"military"
topic #25 (0.033): 0.031*"election" + 0.023*"party" + 0.021*"elect" + 0.019*"vote" + 0.012*"candidate" + 0.011*"seat" + 0.011*"president" + 0.010*"general" + 0.010*"appoint" + 0.010*"government"
topic #0 (0.033): 0.013*"displaystyle" + 0.008*"cell" + 0.008*"protein" + 0.007*"example" + 0.006*"function" + 0.006*"gene" + 0.005*"point" + 0.005*"define" + 0.005*"case" + 0.005*"type"
topic #20 (0.033): 0.036*"book" + 0.030*"publish" + 0.012*"language" + 0.012*"author" + 0.010*"novel" + 0.009*"magazine" + 0.009*"writer" + 0.008*"editor" + 0.008*"history" + 0.007*"publication"
topic diff=0.047336, rho=0.131494
PROGRESS: pass 7, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 10
PROGRESS: pass 7, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.033): 0.039*"album" + 0.036*"song" + 0.032*"music" + 0.025*"band" + 0.015*"single" + 0.011*"track" + 0.011*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #7 (0.033): 0.045*"art" + 0.022*"museum" + 0.022*"artist" + 0.016*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"festival"
topic #2 (0.033): 0.033*"club" + 0.024*"season" + 0.019*"championship" + 0.018*"match" + 0.017*"league" + 0.016*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.010*"player"
topic #21 (0.033): 0.013*"son" + 0.012*"father" + 0.011*"death" + 0.011*"child" + 0.011*"marry" + 0.010*"wife" + 0.010*"say" + 0.010*"daughter" + 0.010*"police" + 0.008*"brother"
topic #25 (0.033): 0.031*"election" + 0.022*"party" + 0.021*"elect" + 0.019*"vote" + 0.012*"candidate" + 0.011*"president" + 0.011*"seat" + 0.010*"general" + 0.010*"appoint" + 0.010*"government"
topic diff=0.043069, rho=0.131494
PROGRESS: pass 7, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.033): 0.112*"film" + 0.022*"star" + 0.016*"direct" + 0.012*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"produce" + 0.007*"role"
topic #13 (0.033): 0.020*"german" + 0.019*"government" + 0.014*"country" + 0.011*"war" + 0.009*"political" + 0.007*"international" + 0.006*"movement" + 0.006*"military" + 0.006*"korean" + 0.005*"chinese"
topic #19 (0.033): 0.031*"building" + 0.025*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.007*"wall" + 0.006*"tower" + 0.006*"room"
topic #22 (0.033): 0.021*"line" + 0.016*"park" + 0.015*"railway" + 0.014*"road" + 0.013*"bridge" + 0.012*"street" + 0.009*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"track"
topic #7 (0.033): 0.045*"art" + 0.023*"museum" + 0.022*"artist" + 0.015*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"festival" + 0.007*"opera"
topic diff=0.041465, rho=0.131494
-8.262 per-word bound, 306.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.033): 0.025*"student" + 0.021*"college" + 0.020*"university" + 0.016*"study" + 0.015*"award" + 0.014*"research" + 0.013*"science" + 0.013*"education" + 0.011*"program" + 0.010*"graduate"
topic #22 (0.033): 0.021*"line" + 0.016*"park" + 0.015*"railway" + 0.014*"road" + 0.012*"bridge" + 0.012*"street" + 0.009*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"track"
topic #6 (0.033): 0.038*"ship" + 0.034*"island" + 0.013*"sea" + 0.012*"port" + 0.011*"boat" + 0.011*"coast" + 0.010*"crew" + 0.010*"vessel" + 0.009*"fleet" + 0.009*"navy"
topic #16 (0.033): 0.019*"unit" + 0.018*"army" + 0.016*"aircraft" + 0.015*"military" + 0.014*"operation" + 0.013*"command" + 0.013*"air" + 0.012*"war" + 0.012*"division" + 0.011*"force"
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"economic" + 0.006*"report" + 0.006*"policy" + 0.006*"increase" + 0.006*"cost" + 0.006*"issue" + 0.005*"government" + 0.005*"information"
topic diff=0.039079, rho=0.131494
-8.263 per-word bound, 307.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #19 (0.033): 0.031*"building" + 0.023*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.007*"wall" + 0.006*"tower" + 0.006*"room"
topic #9 (0.033): 0.030*"series" + 0.018*"episode" + 0.015*"television" + 0.011*"award" + 0.010*"appear" + 0.009*"role" + 0.008*"season" + 0.007*"theatre" + 0.007*"tv" + 0.007*"actor"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"try" + 0.007*"say" + 0.006*"help" + 0.006*"kill" + 0.006*"want" + 0.005*"turn" + 0.005*"reveal"
topic #1 (0.033): 0.114*"film" + 0.021*"star" + 0.016*"direct" + 0.012*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"produce" + 0.007*"role"
topic #27 (0.033): 0.020*"system" + 0.009*"datum" + 0.008*"computer" + 0.007*"code" + 0.007*"user" + 0.006*"develop" + 0.006*"information" + 0.006*"technology" + 0.006*"process" + 0.006*"application"
topic diff=0.038662, rho=0.131494
-8.225 per-word bound, 299.2 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.033): 0.038*"ship" + 0.034*"island" + 0.013*"sea" + 0.012*"port" + 0.011*"boat" + 0.010*"coast" + 0.010*"vessel" + 0.010*"crew" + 0.009*"fleet" + 0.009*"navy"
topic #22 (0.033): 0.020*"line" + 0.016*"park" + 0.015*"railway" + 0.014*"road" + 0.013*"bridge" + 0.012*"street" + 0.008*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"track"
topic #15 (0.033): 0.068*"game" + 0.042*"season" + 0.027*"player" + 0.013*"race" + 0.012*"football" + 0.012*"league" + 0.010*"basketball" + 0.010*"point" + 0.009*"finish" + 0.009*"coach"
topic #2 (0.033): 0.031*"club" + 0.024*"season" + 0.019*"championship" + 0.017*"match" + 0.016*"league" + 0.016*"final" + 0.014*"football" + 0.013*"event" + 0.011*"compete" + 0.011*"player"
topic #7 (0.033): 0.044*"art" + 0.023*"museum" + 0.022*"artist" + 0.016*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"festival" + 0.007*"opera"
topic diff=0.042551, rho=0.130371
PROGRESS: pass 8, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.033): 0.043*"art" + 0.023*"museum" + 0.022*"artist" + 0.016*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"festival"
topic #8 (0.033): 0.038*"album" + 0.037*"song" + 0.032*"music" + 0.026*"band" + 0.016*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #4 (0.033): 0.018*"war" + 0.017*"battle" + 0.016*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.007*"capture" + 0.007*"soldier"
topic #23 (0.033): 0.016*"law" + 0.015*"court" + 0.009*"act" + 0.008*"rule" + 0.008*"right" + 0.008*"government" + 0.007*"case" + 0.007*"order" + 0.006*"king" + 0.006*"claim"
topic #5 (0.033): 0.007*"life" + 0.007*"accord" + 0.006*"woman" + 0.006*"christian" + 0.006*"religious" + 0.005*"tradition" + 0.005*"word" + 0.005*"mean" + 0.005*"practice" + 0.005*"refer"
topic diff=0.042832, rho=0.130371
PROGRESS: pass 8, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.033): 0.031*"election" + 0.022*"party" + 0.022*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.011*"seat" + 0.011*"president" + 0.011*"general" + 0.010*"appoint" + 0.010*"government"
topic #28 (0.033): 0.027*"population" + 0.024*"town" + 0.024*"village" + 0.021*"county" + 0.017*"river" + 0.015*"age" + 0.014*"north" + 0.012*"road" + 0.012*"km" + 0.012*"municipality"
topic #16 (0.033): 0.018*"unit" + 0.017*"army" + 0.017*"aircraft" + 0.016*"military" + 0.015*"operation" + 0.012*"air" + 0.012*"war" + 0.012*"command" + 0.011*"force" + 0.010*"division"
topic #29 (0.033): 0.040*"village" + 0.040*"russian" + 0.029*"irish" + 0.018*"county" + 0.018*"swedish" + 0.017*"cricket" + 0.015*"polish" + 0.015*"population" + 0.012*"rural" + 0.012*"wicket"
topic #8 (0.033): 0.037*"album" + 0.036*"song" + 0.032*"music" + 0.025*"band" + 0.016*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic diff=0.040590, rho=0.130371
PROGRESS: pass 8, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.071*"game" + 0.043*"season" + 0.027*"player" + 0.013*"race" + 0.012*"league" + 0.012*"football" + 0.011*"basketball" + 0.009*"point" + 0.009*"finish" + 0.009*"coach"
topic #7 (0.033): 0.043*"art" + 0.023*"museum" + 0.022*"artist" + 0.016*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"festival"
topic #20 (0.033): 0.035*"book" + 0.030*"publish" + 0.012*"language" + 0.011*"author" + 0.010*"novel" + 0.009*"writer" + 0.008*"magazine" + 0.008*"history" + 0.007*"publication" + 0.007*"editor"
topic #14 (0.033): 0.010*"specie" + 0.009*"water" + 0.008*"plant" + 0.007*"tree" + 0.006*"animal" + 0.006*"forest" + 0.006*"bird" + 0.005*"grow" + 0.005*"fish" + 0.004*"common"
topic #9 (0.033): 0.029*"series" + 0.017*"episode" + 0.016*"television" + 0.010*"award" + 0.010*"appear" + 0.009*"role" + 0.008*"season" + 0.008*"theatre" + 0.007*"tv" + 0.007*"host"
topic diff=0.042577, rho=0.130371
PROGRESS: pass 8, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.070*"game" + 0.043*"season" + 0.027*"player" + 0.013*"race" + 0.013*"league" + 0.012*"football" + 0.011*"basketball" + 0.009*"point" + 0.009*"finish" + 0.009*"coach"
topic #22 (0.033): 0.022*"line" + 0.018*"park" + 0.015*"railway" + 0.014*"road" + 0.012*"bridge" + 0.012*"street" + 0.009*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"operate"
topic #19 (0.033): 0.032*"building" + 0.021*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.007*"wall" + 0.006*"tower" + 0.006*"room"
topic #16 (0.033): 0.019*"unit" + 0.018*"aircraft" + 0.017*"army" + 0.016*"military" + 0.015*"operation" + 0.013*"air" + 0.013*"command" + 0.012*"war" + 0.012*"force" + 0.011*"fly"
topic #9 (0.033): 0.029*"series" + 0.017*"episode" + 0.016*"television" + 0.011*"award" + 0.010*"appear" + 0.009*"role" + 0.008*"season" + 0.008*"theatre" + 0.007*"tv" + 0.007*"host"
topic diff=0.039762, rho=0.130371
PROGRESS: pass 8, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"engine" + 0.009*"power" + 0.009*"vehicle" + 0.008*"model" + 0.007*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #28 (0.033): 0.027*"population" + 0.024*"town" + 0.023*"village" + 0.021*"county" + 0.017*"river" + 0.015*"age" + 0.014*"north" + 0.012*"road" + 0.012*"km" + 0.012*"municipality"
topic #15 (0.033): 0.070*"game" + 0.043*"season" + 0.026*"player" + 0.013*"league" + 0.012*"race" + 0.012*"football" + 0.011*"basketball" + 0.010*"point" + 0.009*"finish" + 0.009*"coach"
topic #19 (0.033): 0.032*"building" + 0.021*"church" + 0.020*"house" + 0.011*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"tower" + 0.006*"room"
topic #24 (0.033): 0.025*"student" + 0.022*"college" + 0.020*"university" + 0.016*"study" + 0.014*"award" + 0.014*"research" + 0.013*"education" + 0.013*"science" + 0.011*"program" + 0.010*"graduate"
topic diff=0.043406, rho=0.130371
PROGRESS: pass 8, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.013*"design" + 0.011*"car" + 0.010*"engine" + 0.009*"power" + 0.008*"vehicle" + 0.008*"model" + 0.007*"system" + 0.005*"produce" + 0.005*"low" + 0.005*"light"
topic #9 (0.033): 0.030*"series" + 0.018*"episode" + 0.016*"television" + 0.011*"award" + 0.010*"appear" + 0.009*"role" + 0.009*"season" + 0.008*"theatre" + 0.007*"tv" + 0.007*"host"
topic #1 (0.033): 0.111*"film" + 0.021*"star" + 0.015*"direct" + 0.012*"movie" + 0.011*"character" + 0.009*"award" + 0.009*"story" + 0.008*"director" + 0.007*"produce" + 0.007*"production"
topic #29 (0.033): 0.041*"village" + 0.038*"russian" + 0.033*"irish" + 0.020*"county" + 0.018*"swedish" + 0.018*"cricket" + 0.016*"polish" + 0.014*"population" + 0.013*"rural" + 0.013*"danish"
topic #5 (0.033): 0.007*"life" + 0.007*"accord" + 0.006*"religious" + 0.006*"tradition" + 0.006*"woman" + 0.006*"christian" + 0.005*"word" + 0.005*"refer" + 0.005*"mean" + 0.005*"great"
topic diff=0.036792, rho=0.130371
PROGRESS: pass 8, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.033): 0.021*"line" + 0.017*"park" + 0.016*"railway" + 0.015*"road" + 0.012*"bridge" + 0.012*"street" + 0.009*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"rail"
topic #15 (0.033): 0.069*"game" + 0.042*"season" + 0.027*"player" + 0.013*"race" + 0.012*"football" + 0.012*"league" + 0.010*"basketball" + 0.010*"point" + 0.009*"finish" + 0.009*"coach"
topic #18 (0.033): 0.066*"company" + 0.022*"business" + 0.016*"sell" + 0.012*"industry" + 0.012*"found" + 0.011*"market" + 0.009*"product" + 0.008*"sale" + 0.008*"acquire" + 0.007*"firm"
topic #14 (0.033): 0.010*"specie" + 0.009*"water" + 0.008*"plant" + 0.006*"animal" + 0.006*"tree" + 0.006*"forest" + 0.006*"bird" + 0.005*"fish" + 0.005*"grow" + 0.004*"range"
topic #9 (0.033): 0.030*"series" + 0.018*"episode" + 0.016*"television" + 0.012*"award" + 0.010*"appear" + 0.009*"role" + 0.009*"season" + 0.008*"theatre" + 0.007*"tv" + 0.006*"host"
topic diff=0.038200, rho=0.130371
PROGRESS: pass 8, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.033): 0.043*"art" + 0.022*"artist" + 0.022*"museum" + 0.016*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"opera" + 0.008*"study" + 0.007*"festival"
topic #9 (0.033): 0.030*"series" + 0.018*"episode" + 0.016*"television" + 0.012*"award" + 0.010*"appear" + 0.009*"role" + 0.009*"season" + 0.008*"theatre" + 0.007*"tv" + 0.006*"host"
topic #24 (0.033): 0.024*"student" + 0.021*"college" + 0.020*"university" + 0.016*"study" + 0.015*"award" + 0.013*"research" + 0.013*"science" + 0.013*"education" + 0.011*"program" + 0.010*"graduate"
topic #18 (0.033): 0.065*"company" + 0.022*"business" + 0.016*"sell" + 0.012*"industry" + 0.012*"found" + 0.010*"market" + 0.009*"product" + 0.008*"sale" + 0.008*"acquire" + 0.008*"firm"
topic #22 (0.033): 0.021*"line" + 0.017*"park" + 0.016*"railway" + 0.015*"road" + 0.013*"bridge" + 0.012*"street" + 0.009*"train" + 0.007*"close" + 0.007*"construction" + 0.006*"rail"
topic diff=0.039169, rho=0.130371
PROGRESS: pass 8, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.033): 0.032*"building" + 0.022*"church" + 0.020*"house" + 0.013*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.006*"tower"
topic #12 (0.033): 0.088*"station" + 0.028*"radio" + 0.016*"channel" + 0.015*"network" + 0.014*"broadcast" + 0.013*"operate" + 0.012*"air" + 0.011*"bus" + 0.011*"line" + 0.011*"airport"
topic #28 (0.033): 0.026*"population" + 0.025*"town" + 0.023*"village" + 0.021*"county" + 0.020*"river" + 0.014*"north" + 0.013*"age" + 0.012*"km" + 0.012*"municipality" + 0.012*"road"
topic #14 (0.033): 0.010*"specie" + 0.010*"water" + 0.008*"plant" + 0.006*"tree" + 0.006*"forest" + 0.006*"animal" + 0.006*"bird" + 0.005*"grow" + 0.005*"fish" + 0.005*"range"
topic #6 (0.033): 0.039*"ship" + 0.035*"island" + 0.014*"sea" + 0.012*"port" + 0.011*"boat" + 0.010*"coast" + 0.010*"crew" + 0.009*"vessel" + 0.008*"navy" + 0.008*"fleet"
topic diff=0.037070, rho=0.130371
PROGRESS: pass 8, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.033): 0.018*"law" + 0.015*"court" + 0.010*"act" + 0.009*"rule" + 0.009*"right" + 0.008*"case" + 0.008*"government" + 0.007*"order" + 0.006*"claim" + 0.005*"king"
topic #7 (0.033): 0.045*"art" + 0.023*"artist" + 0.022*"museum" + 0.016*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"opera" + 0.008*"study" + 0.007*"festival"
topic #26 (0.033): 0.057*"specie" + 0.047*"black" + 0.038*"white" + 0.027*"genus" + 0.024*"describe" + 0.017*"red" + 0.016*"color" + 0.016*"blue" + 0.015*"brown" + 0.013*"african"
topic #25 (0.033): 0.032*"election" + 0.023*"party" + 0.022*"elect" + 0.019*"vote" + 0.013*"candidate" + 0.012*"seat" + 0.011*"president" + 0.010*"government" + 0.010*"general" + 0.010*"appoint"
topic #15 (0.033): 0.071*"game" + 0.042*"season" + 0.026*"player" + 0.013*"football" + 0.012*"league" + 0.012*"race" + 0.010*"point" + 0.009*"basketball" + 0.009*"finish" + 0.009*"coach"
topic diff=0.040344, rho=0.130371
PROGRESS: pass 8, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #12 (0.033): 0.089*"station" + 0.027*"radio" + 0.016*"channel" + 0.015*"broadcast" + 0.015*"network" + 0.013*"air" + 0.013*"operate" + 0.011*"airport" + 0.011*"line" + 0.010*"bus"
topic #9 (0.033): 0.030*"series" + 0.018*"episode" + 0.016*"television" + 0.011*"award" + 0.010*"appear" + 0.010*"role" + 0.008*"season" + 0.008*"theatre" + 0.007*"tv" + 0.007*"actor"
topic #20 (0.033): 0.036*"book" + 0.031*"publish" + 0.012*"language" + 0.012*"author" + 0.010*"novel" + 0.009*"magazine" + 0.009*"writer" + 0.008*"editor" + 0.008*"history" + 0.007*"publication"
topic #2 (0.033): 0.034*"club" + 0.024*"season" + 0.020*"championship" + 0.018*"match" + 0.017*"league" + 0.016*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.011*"player"
topic #28 (0.033): 0.026*"population" + 0.025*"town" + 0.023*"village" + 0.020*"county" + 0.019*"river" + 0.015*"north" + 0.013*"age" + 0.012*"km" + 0.012*"municipality" + 0.012*"road"
topic diff=0.042570, rho=0.130371
PROGRESS: pass 8, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #27 (0.033): 0.020*"system" + 0.009*"datum" + 0.008*"computer" + 0.006*"technology" + 0.006*"user" + 0.006*"information" + 0.006*"code" + 0.006*"software" + 0.006*"develop" + 0.006*"application"
topic #16 (0.033): 0.018*"unit" + 0.018*"army" + 0.017*"aircraft" + 0.015*"military" + 0.014*"operation" + 0.014*"air" + 0.013*"command" + 0.012*"war" + 0.011*"force" + 0.010*"fly"
topic #25 (0.033): 0.031*"election" + 0.023*"party" + 0.022*"elect" + 0.019*"vote" + 0.012*"candidate" + 0.012*"president" + 0.011*"seat" + 0.010*"general" + 0.010*"government" + 0.010*"appoint"
topic #15 (0.033): 0.066*"game" + 0.043*"season" + 0.025*"player" + 0.013*"football" + 0.012*"league" + 0.012*"race" + 0.010*"basketball" + 0.010*"point" + 0.010*"coach" + 0.009*"finish"
topic #29 (0.033): 0.044*"russian" + 0.040*"village" + 0.035*"irish" + 0.020*"county" + 0.018*"swedish" + 0.016*"polish" + 0.016*"cricket" + 0.015*"population" + 0.014*"danish" + 0.012*"central"
topic diff=0.038725, rho=0.130371
PROGRESS: pass 8, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.033): 0.018*"battle" + 0.017*"war" + 0.016*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"french" + 0.009*"fight" + 0.009*"troop" + 0.008*"soldier" + 0.007*"capture"
topic #13 (0.033): 0.020*"german" + 0.019*"government" + 0.014*"country" + 0.011*"war" + 0.009*"political" + 0.007*"international" + 0.006*"movement" + 0.006*"military" + 0.006*"korean" + 0.005*"chinese"
topic #7 (0.033): 0.045*"art" + 0.023*"museum" + 0.023*"artist" + 0.015*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"festival"
topic #19 (0.033): 0.031*"building" + 0.025*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.007*"wall" + 0.006*"tower" + 0.006*"room"
topic #20 (0.033): 0.036*"book" + 0.031*"publish" + 0.012*"language" + 0.011*"author" + 0.011*"novel" + 0.009*"magazine" + 0.009*"writer" + 0.008*"history" + 0.007*"editor" + 0.007*"publication"
topic diff=0.037023, rho=0.130371
-8.264 per-word bound, 307.4 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.033): 0.032*"building" + 0.024*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"tower" + 0.006*"side"
topic #27 (0.033): 0.020*"system" + 0.009*"datum" + 0.008*"computer" + 0.007*"code" + 0.007*"user" + 0.006*"technology" + 0.006*"information" + 0.006*"develop" + 0.006*"software" + 0.006*"application"
topic #29 (0.033): 0.043*"russian" + 0.041*"village" + 0.036*"irish" + 0.020*"county" + 0.019*"swedish" + 0.016*"polish" + 0.016*"cricket" + 0.015*"population" + 0.013*"danish" + 0.012*"central"
topic #25 (0.033): 0.031*"election" + 0.024*"party" + 0.022*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.012*"president" + 0.011*"seat" + 0.011*"general" + 0.010*"government" + 0.010*"appoint"
topic #16 (0.033): 0.019*"unit" + 0.018*"army" + 0.016*"aircraft" + 0.015*"military" + 0.014*"operation" + 0.013*"command" + 0.013*"air" + 0.012*"division" + 0.012*"war" + 0.011*"force"
topic diff=0.034779, rho=0.130371
-8.266 per-word bound, 307.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #7 (0.033): 0.045*"art" + 0.023*"museum" + 0.022*"artist" + 0.016*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"festival"
topic #21 (0.033): 0.015*"son" + 0.013*"father" + 0.012*"death" + 0.012*"child" + 0.012*"daughter" + 0.012*"marry" + 0.011*"wife" + 0.010*"police" + 0.009*"say" + 0.008*"brother"
topic #24 (0.033): 0.025*"student" + 0.022*"college" + 0.021*"university" + 0.016*"study" + 0.015*"award" + 0.014*"research" + 0.013*"education" + 0.013*"science" + 0.011*"program" + 0.010*"graduate"
topic #26 (0.033): 0.057*"specie" + 0.053*"black" + 0.041*"white" + 0.029*"genus" + 0.024*"describe" + 0.016*"red" + 0.016*"brown" + 0.016*"color" + 0.015*"blue" + 0.013*"african"
topic #16 (0.033): 0.019*"unit" + 0.018*"army" + 0.016*"aircraft" + 0.015*"military" + 0.015*"operation" + 0.013*"command" + 0.013*"air" + 0.012*"war" + 0.012*"division" + 0.011*"force"
topic diff=0.034502, rho=0.130371
-8.228 per-word bound, 299.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.033): 0.031*"election" + 0.023*"party" + 0.022*"elect" + 0.019*"vote" + 0.012*"candidate" + 0.011*"president" + 0.011*"seat" + 0.011*"general" + 0.010*"government" + 0.010*"appoint"
topic #24 (0.033): 0.024*"student" + 0.021*"university" + 0.021*"college" + 0.016*"study" + 0.015*"award" + 0.014*"research" + 0.013*"education" + 0.013*"science" + 0.011*"program" + 0.010*"graduate"
topic #11 (0.033): 0.011*"get" + 0.008*"tell" + 0.008*"back" + 0.007*"say" + 0.007*"try" + 0.006*"help" + 0.006*"want" + 0.005*"kill" + 0.005*"turn" + 0.005*"way"
topic #2 (0.033): 0.031*"club" + 0.024*"season" + 0.020*"championship" + 0.017*"match" + 0.016*"league" + 0.016*"final" + 0.014*"football" + 0.013*"event" + 0.011*"compete" + 0.011*"player"
topic #14 (0.033): 0.009*"specie" + 0.009*"water" + 0.008*"plant" + 0.007*"tree" + 0.006*"animal" + 0.006*"forest" + 0.005*"bird" + 0.005*"grow" + 0.005*"fish" + 0.004*"range"
topic diff=0.038475, rho=0.129277
PROGRESS: pass 9, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.033): 0.044*"art" + 0.023*"museum" + 0.022*"artist" + 0.016*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"festival"
topic #12 (0.033): 0.087*"station" + 0.024*"radio" + 0.017*"channel" + 0.016*"network" + 0.014*"broadcast" + 0.013*"operate" + 0.013*"air" + 0.011*"airport" + 0.011*"news" + 0.010*"bus"
topic #5 (0.033): 0.007*"accord" + 0.007*"life" + 0.006*"christian" + 0.006*"woman" + 0.006*"religious" + 0.005*"tradition" + 0.005*"church" + 0.005*"word" + 0.005*"mean" + 0.005*"refer"
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"engine" + 0.010*"power" + 0.007*"model" + 0.007*"vehicle" + 0.007*"system" + 0.005*"light" + 0.005*"produce" + 0.005*"low"
topic #1 (0.033): 0.113*"film" + 0.021*"star" + 0.016*"direct" + 0.012*"movie" + 0.011*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"production" + 0.007*"produce"
topic diff=0.038826, rho=0.129277
PROGRESS: pass 9, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.033): 0.068*"game" + 0.043*"season" + 0.027*"player" + 0.012*"league" + 0.012*"football" + 0.012*"race" + 0.011*"basketball" + 0.009*"point" + 0.009*"finish" + 0.009*"coach"
topic #19 (0.033): 0.033*"building" + 0.022*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.007*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.006*"tower"
topic #10 (0.033): 0.011*"cause" + 0.009*"patient" + 0.008*"increase" + 0.008*"treatment" + 0.007*"effect" + 0.007*"result" + 0.007*"disease" + 0.006*"study" + 0.006*"child" + 0.006*"health"
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"policy" + 0.006*"report" + 0.006*"economic" + 0.006*"increase" + 0.006*"government" + 0.006*"issue" + 0.005*"information" + 0.005*"need"
topic #4 (0.033): 0.018*"war" + 0.017*"battle" + 0.016*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.007*"soldier" + 0.007*"capture"
topic diff=0.036749, rho=0.129277
PROGRESS: pass 9, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.012*"design" + 0.011*"car" + 0.010*"engine" + 0.010*"power" + 0.007*"model" + 0.007*"vehicle" + 0.007*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #4 (0.033): 0.018*"war" + 0.017*"battle" + 0.016*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.008*"soldier" + 0.007*"capture"
topic #0 (0.033): 0.015*"displaystyle" + 0.009*"cell" + 0.007*"protein" + 0.007*"function" + 0.007*"example" + 0.006*"define" + 0.005*"gene" + 0.005*"point" + 0.005*"structure" + 0.005*"case"
topic #12 (0.033): 0.089*"station" + 0.026*"radio" + 0.017*"channel" + 0.016*"network" + 0.014*"broadcast" + 0.013*"operate" + 0.013*"air" + 0.012*"airport" + 0.011*"news" + 0.010*"bus"
topic #27 (0.033): 0.019*"system" + 0.009*"datum" + 0.008*"computer" + 0.007*"user" + 0.007*"information" + 0.006*"code" + 0.006*"technology" + 0.006*"application" + 0.006*"software" + 0.006*"develop"
topic diff=0.038791, rho=0.129277
PROGRESS: pass 9, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 10
PROGRESS: pass 9, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.033): 0.031*"election" + 0.023*"party" + 0.022*"elect" + 0.019*"vote" + 0.012*"candidate" + 0.012*"seat" + 0.011*"president" + 0.011*"government" + 0.011*"general" + 0.010*"appoint"
topic #14 (0.033): 0.010*"specie" + 0.009*"water" + 0.008*"plant" + 0.007*"animal" + 0.006*"tree" + 0.006*"forest" + 0.006*"bird" + 0.005*"fish" + 0.005*"grow" + 0.004*"range"
topic #8 (0.033): 0.038*"album" + 0.037*"song" + 0.033*"music" + 0.025*"band" + 0.016*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #11 (0.033): 0.011*"get" + 0.009*"tell" + 0.008*"back" + 0.008*"say" + 0.007*"try" + 0.006*"want" + 0.006*"help" + 0.005*"kill" + 0.005*"reveal" + 0.005*"turn"
topic #15 (0.033): 0.070*"game" + 0.043*"season" + 0.027*"player" + 0.013*"league" + 0.012*"football" + 0.012*"race" + 0.011*"basketball" + 0.009*"coach" + 0.009*"point" + 0.009*"finish"
topic diff=0.035943, rho=0.129277
PROGRESS: pass 9, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"engine" + 0.009*"power" + 0.009*"vehicle" + 0.008*"model" + 0.006*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"low"
topic #26 (0.033): 0.056*"specie" + 0.051*"black" + 0.040*"white" + 0.027*"genus" + 0.023*"describe" + 0.017*"red" + 0.017*"blue" + 0.016*"brown" + 0.015*"color" + 0.013*"african"
topic #20 (0.033): 0.036*"book" + 0.031*"publish" + 0.011*"author" + 0.011*"language" + 0.011*"novel" + 0.009*"writer" + 0.008*"magazine" + 0.008*"history" + 0.007*"publication" + 0.007*"editor"
topic #17 (0.033): 0.008*"provide" + 0.007*"public" + 0.006*"economic" + 0.006*"policy" + 0.006*"increase" + 0.006*"government" + 0.006*"information" + 0.006*"report" + 0.005*"consumer" + 0.005*"issue"
topic #0 (0.033): 0.014*"displaystyle" + 0.008*"cell" + 0.007*"example" + 0.007*"function" + 0.007*"protein" + 0.005*"define" + 0.005*"gene" + 0.005*"theory" + 0.005*"structure" + 0.005*"point"
topic diff=0.039493, rho=0.129277
PROGRESS: pass 9, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #27 (0.033): 0.020*"system" + 0.009*"datum" + 0.008*"computer" + 0.007*"information" + 0.007*"code" + 0.006*"user" + 0.006*"software" + 0.006*"application" + 0.006*"version" + 0.006*"develop"
topic #22 (0.033): 0.023*"line" + 0.018*"park" + 0.017*"road" + 0.016*"railway" + 0.012*"bridge" + 0.012*"street" + 0.009*"train" + 0.008*"close" + 0.007*"construction" + 0.006*"rail"
topic #19 (0.033): 0.032*"building" + 0.022*"church" + 0.020*"house" + 0.012*"site" + 0.010*"design" + 0.008*"th_century" + 0.007*"stone" + 0.006*"wall" + 0.006*"room" + 0.006*"tower"
topic #11 (0.033): 0.011*"get" + 0.009*"tell" + 0.008*"back" + 0.007*"say" + 0.007*"try" + 0.006*"help" + 0.006*"want" + 0.005*"kill" + 0.005*"turn" + 0.005*"way"
topic #7 (0.033): 0.044*"art" + 0.023*"museum" + 0.022*"artist" + 0.015*"painting" + 0.011*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"study" + 0.007*"opera" + 0.007*"french"
topic diff=0.033276, rho=0.129277
PROGRESS: pass 9, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.033): 0.007*"life" + 0.007*"accord" + 0.006*"religious" + 0.006*"christian" + 0.006*"tradition" + 0.005*"church" + 0.005*"woman" + 0.005*"refer" + 0.005*"word" + 0.005*"great"
topic #28 (0.033): 0.027*"population" + 0.025*"town" + 0.023*"village" + 0.021*"county" + 0.020*"river" + 0.014*"north" + 0.014*"age" + 0.012*"km" + 0.012*"municipality" + 0.011*"south"
topic #3 (0.033): 0.013*"design" + 0.011*"car" + 0.010*"engine" + 0.009*"power" + 0.008*"vehicle" + 0.008*"model" + 0.006*"system" + 0.005*"produce" + 0.005*"type" + 0.005*"low"
topic #25 (0.033): 0.031*"election" + 0.022*"party" + 0.022*"elect" + 0.018*"vote" + 0.012*"candidate" + 0.012*"seat" + 0.012*"president" + 0.011*"government" + 0.010*"general" + 0.010*"appoint"
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"increase" + 0.006*"economic" + 0.006*"policy" + 0.006*"government" + 0.006*"report" + 0.006*"information" + 0.005*"issue" + 0.005*"system"
topic diff=0.035848, rho=0.129277
PROGRESS: pass 9, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 4000 documents into a model of 49835 documents
topic #15 (0.033): 0.069*"game" + 0.042*"season" + 0.026*"player" + 0.013*"football" + 0.012*"league" + 0.012*"race" + 0.010*"basketball" + 0.010*"point" + 0.010*"coach" + 0.009*"finish"
topic #9 (0.033): 0.031*"series" + 0.018*"episode" + 0.016*"television" + 0.013*"award" + 0.010*"appear" + 0.010*"role" + 0.009*"season" + 0.008*"theatre" + 0.007*"tv" + 0.006*"host"
topic #1 (0.033): 0.113*"film" + 0.021*"star" + 0.016*"direct" + 0.012*"movie" + 0.012*"character" + 0.010*"award" + 0.009*"story" + 0.008*"director" + 0.007*"role" + 0.007*"produce"
topic #7 (0.033): 0.043*"art" + 0.022*"museum" + 0.022*"artist" + 0.016*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.008*"paint" + 0.008*"opera" + 0.008*"study" + 0.007*"french"
topic #21 (0.033): 0.015*"son" + 0.013*"father" + 0.012*"child" + 0.012*"death" + 0.012*"wife" + 0.011*"marry" + 0.011*"daughter" + 0.010*"police" + 0.009*"say" + 0.008*"brother"
topic diff=0.032376, rho=0.129277
PROGRESS: pass 9, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.033): 0.057*"specie" + 0.047*"black" + 0.039*"white" + 0.027*"genus" + 0.024*"describe" + 0.017*"red" + 0.017*"color" + 0.016*"blue" + 0.014*"brown" + 0.013*"african"
topic #1 (0.033): 0.112*"film" + 0.021*"star" + 0.016*"direct" + 0.012*"character" + 0.012*"movie" + 0.010*"award" + 0.010*"story" + 0.007*"director" + 0.007*"produce" + 0.007*"role"
topic #15 (0.033): 0.070*"game" + 0.042*"season" + 0.027*"player" + 0.012*"league" + 0.012*"football" + 0.011*"race" + 0.010*"point" + 0.010*"basketball" + 0.009*"coach" + 0.009*"finish"
topic #7 (0.033): 0.045*"art" + 0.023*"artist" + 0.022*"museum" + 0.017*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.009*"paint" + 0.008*"opera" + 0.008*"study" + 0.007*"festival"
topic #18 (0.033): 0.065*"company" + 0.022*"business" + 0.016*"sell" + 0.012*"industry" + 0.011*"found" + 0.010*"market" + 0.009*"product" + 0.008*"sale" + 0.008*"acquire" + 0.007*"purchase"
topic diff=0.038135, rho=0.129277
PROGRESS: pass 9, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"engine" + 0.009*"power" + 0.008*"vehicle" + 0.007*"model" + 0.007*"system" + 0.005*"light" + 0.005*"produce" + 0.005*"low"
topic #9 (0.033): 0.031*"series" + 0.018*"episode" + 0.016*"television" + 0.012*"award" + 0.010*"appear" + 0.009*"role" + 0.009*"season" + 0.008*"theatre" + 0.007*"tv" + 0.006*"actor"
topic #6 (0.033): 0.039*"ship" + 0.035*"island" + 0.014*"sea" + 0.011*"boat" + 0.011*"port" + 0.011*"crew" + 0.011*"coast" + 0.010*"vessel" + 0.010*"fleet" + 0.009*"navy"
topic #15 (0.033): 0.070*"game" + 0.043*"season" + 0.026*"player" + 0.013*"football" + 0.012*"league" + 0.011*"race" + 0.010*"point" + 0.010*"basketball" + 0.010*"coach" + 0.009*"finish"
topic #4 (0.033): 0.017*"war" + 0.017*"battle" + 0.016*"force" + 0.015*"army" + 0.013*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.008*"soldier" + 0.007*"capture"
topic diff=0.037153, rho=0.129277
PROGRESS: pass 9, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.033): 0.033*"club" + 0.024*"season" + 0.020*"championship" + 0.018*"match" + 0.017*"league" + 0.016*"final" + 0.014*"football" + 0.012*"event" + 0.011*"compete" + 0.010*"player"
topic #1 (0.033): 0.113*"film" + 0.023*"star" + 0.016*"direct" + 0.012*"movie" + 0.012*"character" + 0.010*"award" + 0.010*"story" + 0.008*"director" + 0.007*"produce" + 0.007*"role"
topic #22 (0.033): 0.023*"line" + 0.017*"road" + 0.017*"park" + 0.015*"railway" + 0.013*"bridge" + 0.012*"street" + 0.010*"train" + 0.007*"close" + 0.007*"construction" + 0.007*"track"
topic #8 (0.033): 0.039*"album" + 0.037*"song" + 0.032*"music" + 0.025*"band" + 0.016*"single" + 0.011*"track" + 0.011*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #16 (0.033): 0.018*"unit" + 0.018*"army" + 0.017*"aircraft" + 0.015*"military" + 0.014*"operation" + 0.014*"air" + 0.013*"command" + 0.012*"war" + 0.011*"force" + 0.010*"fly"
topic diff=0.036529, rho=0.129277
PROGRESS: pass 9, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.033): 0.058*"specie" + 0.054*"black" + 0.041*"white" + 0.027*"genus" + 0.024*"describe" + 0.016*"brown" + 0.016*"red" + 0.015*"blue" + 0.015*"color" + 0.013*"african"
topic #14 (0.033): 0.009*"specie" + 0.009*"water" + 0.008*"plant" + 0.007*"tree" + 0.006*"animal" + 0.006*"forest" + 0.005*"bird" + 0.005*"grow" + 0.005*"fish" + 0.005*"range"
topic #9 (0.033): 0.029*"series" + 0.018*"episode" + 0.015*"television" + 0.012*"award" + 0.010*"appear" + 0.010*"role" + 0.008*"season" + 0.008*"theatre" + 0.007*"actor" + 0.007*"tv"
topic #7 (0.033): 0.046*"art" + 0.023*"museum" + 0.023*"artist" + 0.016*"painting" + 0.010*"exhibition" + 0.009*"collection" + 0.009*"paint" + 0.008*"study" + 0.007*"opera" + 0.006*"festival"
topic #3 (0.033): 0.013*"design" + 0.010*"car" + 0.010*"power" + 0.010*"engine" + 0.007*"model" + 0.007*"vehicle" + 0.006*"system" + 0.005*"produce" + 0.005*"light" + 0.005*"type"
topic diff=0.034633, rho=0.129277
PROGRESS: pass 9, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"economic" + 0.006*"policy" + 0.006*"report" + 0.006*"increase" + 0.006*"government" + 0.006*"issue" + 0.005*"cost" + 0.005*"system"
topic #6 (0.033): 0.039*"ship" + 0.034*"island" + 0.014*"sea" + 0.012*"port" + 0.011*"boat" + 0.011*"coast" + 0.011*"crew" + 0.010*"vessel" + 0.010*"fleet" + 0.009*"navy"
topic #8 (0.033): 0.039*"album" + 0.038*"song" + 0.032*"music" + 0.025*"band" + 0.016*"single" + 0.011*"track" + 0.011*"perform" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #16 (0.033): 0.019*"unit" + 0.018*"army" + 0.016*"aircraft" + 0.015*"military" + 0.014*"operation" + 0.013*"command" + 0.013*"air" + 0.012*"division" + 0.012*"war" + 0.011*"force"
topic #13 (0.033): 0.019*"german" + 0.019*"government" + 0.014*"country" + 0.011*"war" + 0.010*"political" + 0.007*"international" + 0.006*"movement" + 0.006*"military" + 0.005*"korean" + 0.005*"chinese"
topic diff=0.032555, rho=0.129277
-8.266 per-word bound, 307.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #17 (0.033): 0.009*"provide" + 0.007*"public" + 0.006*"report" + 0.006*"increase" + 0.006*"economic" + 0.006*"policy" + 0.006*"government" + 0.006*"issue" + 0.006*"cost" + 0.005*"plan"
topic #22 (0.033): 0.023*"line" + 0.018*"road" + 0.017*"park" + 0.015*"railway" + 0.012*"bridge" + 0.012*"street" + 0.009*"train" + 0.007*"close" + 0.007*"construction" + 0.006*"track"
topic #0 (0.033): 0.012*"displaystyle" + 0.009*"cell" + 0.008*"protein" + 0.007*"function" + 0.007*"example" + 0.006*"gene" + 0.006*"point" + 0.005*"define" + 0.005*"structure" + 0.005*"case"
topic #29 (0.033): 0.043*"russian" + 0.042*"village" + 0.036*"irish" + 0.020*"county" + 0.019*"swedish" + 0.017*"polish" + 0.016*"cricket" + 0.015*"population" + 0.013*"danish" + 0.012*"central"
topic #20 (0.033): 0.036*"book" + 0.031*"publish" + 0.012*"language" + 0.012*"author" + 0.011*"novel" + 0.009*"writer" + 0.009*"magazine" + 0.008*"history" + 0.007*"publication" + 0.007*"editor"
topic diff=0.031821, rho=0.129277
-8.266 per-word bound, 307.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 2835 documents into a model of 49835 documents
topic #9 (0.033): 0.030*"series" + 0.019*"episode" + 0.016*"television" + 0.012*"award" + 0.010*"appear" + 0.009*"role" + 0.008*"season" + 0.008*"theatre" + 0.007*"tv" + 0.007*"actor"
topic #13 (0.033): 0.019*"government" + 0.017*"german" + 0.014*"country" + 0.011*"war" + 0.009*"political" + 0.007*"international" + 0.006*"military" + 0.006*"movement" + 0.005*"chinese" + 0.005*"leader"
topic #23 (0.033): 0.018*"law" + 0.017*"court" + 0.010*"act" + 0.009*"rule" + 0.008*"right" + 0.008*"case" + 0.008*"order" + 0.007*"government" + 0.006*"claim" + 0.005*"king"
topic #4 (0.033): 0.017*"battle" + 0.017*"war" + 0.015*"army" + 0.015*"force" + 0.013*"attack" + 0.009*"french" + 0.009*"troop" + 0.008*"fight" + 0.008*"king" + 0.007*"soldier"
topic #18 (0.033): 0.067*"company" + 0.021*"business" + 0.017*"sell" + 0.012*"industry" + 0.011*"found" + 0.011*"market" + 0.009*"product" + 0.008*"sale" + 0.008*"production" + 0.008*"purchase"
topic diff=0.034898, rho=0.129277
-8.209 per-word bound, 295.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=26262, num_topics=30, decay=0.5, chunksize=1000) in 1374.51s', 'datetime': '2022-02-07T00:19:54.588557', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows
1 batches submitted to accumulate stats from 64 documents (2187 virtual)
2 batches submitted to accumulate stats from 128 documents (6627 virtual)
3 batches submitted to accumulate stats from 192 documents (7985 virtual)
5 batches submitted to accumulate stats from 320 documents (9015 virtual)
6 batches submitted to accumulate stats from 384 documents (13105 virtual)
9 batches submitted to accumulate stats from 576 documents (14218 virtual)
10 batches submitted to accumulate stats from 640 documents (16303 virtual)
11 batches submitted to accumulate stats from 704 documents (17034 virtual)
12 batches submitted to accumulate stats from 768 documents (21467 virtual)
13 batches submitted to accumulate stats from 832 documents (24262 virtual)
14 batches submitted to accumulate stats from 896 documents (24287 virtual)
16 batches submitted to accumulate stats from 1024 documents (25616 virtual)
17 batches submitted to accumulate stats from 1088 documents (26250 virtual)
18 batches submitted to accumulate stats from 1152 documents (30416 virtual)
19 batches submitted to accumulate stats from 1216 documents (30915 virtual)
20 batches submitted to accumulate stats from 1280 documents (33823 virtual)
21 batches submitted to accumulate stats from 1344 documents (35208 virtual)
22 batches submitted to accumulate stats from 1408 documents (36746 virtual)
23 batches submitted to accumulate stats from 1472 documents (41705 virtual)
24 batches submitted to accumulate stats from 1536 documents (43538 virtual)
25 batches submitted to accumulate stats from 1600 documents (45108 virtual)
27 batches submitted to accumulate stats from 1728 documents (45827 virtual)
28 batches submitted to accumulate stats from 1792 documents (49506 virtual)
29 batches submitted to accumulate stats from 1856 documents (56945 virtual)
30 batches submitted to accumulate stats from 1920 documents (57350 virtual)
32 batches submitted to accumulate stats from 2048 documents (57166 virtual)
33 batches submitted to accumulate stats from 2112 documents (58359 virtual)
35 batches submitted to accumulate stats from 2240 documents (62518 virtual)
36 batches submitted to accumulate stats from 2304 documents (63944 virtual)
37 batches submitted to accumulate stats from 2368 documents (64842 virtual)
39 batches submitted to accumulate stats from 2496 documents (68851 virtual)
40 batches submitted to accumulate stats from 2560 documents (70671 virtual)
42 batches submitted to accumulate stats from 2688 documents (70319 virtual)
43 batches submitted to accumulate stats from 2752 documents (74922 virtual)
44 batches submitted to accumulate stats from 2816 documents (74950 virtual)
45 batches submitted to accumulate stats from 2880 documents (77532 virtual)
46 batches submitted to accumulate stats from 2944 documents (81898 virtual)
48 batches submitted to accumulate stats from 3072 documents (87860 virtual)
50 batches submitted to accumulate stats from 3200 documents (87343 virtual)
51 batches submitted to accumulate stats from 3264 documents (88847 virtual)
52 batches submitted to accumulate stats from 3328 documents (92294 virtual)
53 batches submitted to accumulate stats from 3392 documents (93455 virtual)
54 batches submitted to accumulate stats from 3456 documents (96208 virtual)
55 batches submitted to accumulate stats from 3520 documents (96895 virtual)
56 batches submitted to accumulate stats from 3584 documents (99237 virtual)
57 batches submitted to accumulate stats from 3648 documents (103125 virtual)
58 batches submitted to accumulate stats from 3712 documents (105779 virtual)
59 batches submitted to accumulate stats from 3776 documents (107144 virtual)
60 batches submitted to accumulate stats from 3840 documents (107543 virtual)
62 batches submitted to accumulate stats from 3968 documents (108243 virtual)
64 batches submitted to accumulate stats from 4096 documents (109052 virtual)
65 batches submitted to accumulate stats from 4160 documents (110619 virtual)
66 batches submitted to accumulate stats from 4224 documents (110741 virtual)
68 batches submitted to accumulate stats from 4352 documents (112520 virtual)
69 batches submitted to accumulate stats from 4416 documents (124670 virtual)
70 batches submitted to accumulate stats from 4480 documents (126349 virtual)
71 batches submitted to accumulate stats from 4544 documents (130710 virtual)
72 batches submitted to accumulate stats from 4608 documents (134849 virtual)
76 batches submitted to accumulate stats from 4864 documents (136222 virtual)
77 batches submitted to accumulate stats from 4928 documents (139707 virtual)
78 batches submitted to accumulate stats from 4992 documents (141474 virtual)
79 batches submitted to accumulate stats from 5056 documents (142777 virtual)
80 batches submitted to accumulate stats from 5120 documents (142821 virtual)
81 batches submitted to accumulate stats from 5184 documents (145115 virtual)
84 batches submitted to accumulate stats from 5376 documents (144724 virtual)
85 batches submitted to accumulate stats from 5440 documents (145335 virtual)
86 batches submitted to accumulate stats from 5504 documents (147655 virtual)
87 batches submitted to accumulate stats from 5568 documents (148536 virtual)
89 batches submitted to accumulate stats from 5696 documents (148991 virtual)
90 batches submitted to accumulate stats from 5760 documents (151277 virtual)
91 batches submitted to accumulate stats from 5824 documents (153794 virtual)
92 batches submitted to accumulate stats from 5888 documents (154187 virtual)
93 batches submitted to accumulate stats from 5952 documents (157276 virtual)
94 batches submitted to accumulate stats from 6016 documents (161280 virtual)
95 batches submitted to accumulate stats from 6080 documents (164057 virtual)
97 batches submitted to accumulate stats from 6208 documents (165903 virtual)
98 batches submitted to accumulate stats from 6272 documents (170228 virtual)
99 batches submitted to accumulate stats from 6336 documents (170782 virtual)
100 batches submitted to accumulate stats from 6400 documents (171501 virtual)
102 batches submitted to accumulate stats from 6528 documents (175383 virtual)
105 batches submitted to accumulate stats from 6720 documents (174139 virtual)
106 batches submitted to accumulate stats from 6784 documents (176659 virtual)
107 batches submitted to accumulate stats from 6848 documents (177476 virtual)
108 batches submitted to accumulate stats from 6912 documents (178444 virtual)
110 batches submitted to accumulate stats from 7040 documents (179419 virtual)
112 batches submitted to accumulate stats from 7168 documents (178725 virtual)
113 batches submitted to accumulate stats from 7232 documents (181673 virtual)
115 batches submitted to accumulate stats from 7360 documents (185883 virtual)
116 batches submitted to accumulate stats from 7424 documents (188930 virtual)
118 batches submitted to accumulate stats from 7552 documents (190805 virtual)
119 batches submitted to accumulate stats from 7616 documents (193623 virtual)
123 batches submitted to accumulate stats from 7872 documents (194920 virtual)
125 batches submitted to accumulate stats from 8000 documents (196939 virtual)
126 batches submitted to accumulate stats from 8064 documents (197837 virtual)
127 batches submitted to accumulate stats from 8128 documents (200141 virtual)
129 batches submitted to accumulate stats from 8256 documents (203748 virtual)
130 batches submitted to accumulate stats from 8320 documents (207556 virtual)
131 batches submitted to accumulate stats from 8384 documents (208597 virtual)
133 batches submitted to accumulate stats from 8512 documents (208102 virtual)
134 batches submitted to accumulate stats from 8576 documents (208702 virtual)
135 batches submitted to accumulate stats from 8640 documents (210953 virtual)
136 batches submitted to accumulate stats from 8704 documents (211057 virtual)
139 batches submitted to accumulate stats from 8896 documents (221806 virtual)
140 batches submitted to accumulate stats from 8960 documents (223320 virtual)
142 batches submitted to accumulate stats from 9088 documents (226146 virtual)
143 batches submitted to accumulate stats from 9152 documents (228356 virtual)
144 batches submitted to accumulate stats from 9216 documents (233879 virtual)
145 batches submitted to accumulate stats from 9280 documents (238989 virtual)
148 batches submitted to accumulate stats from 9472 documents (239223 virtual)
149 batches submitted to accumulate stats from 9536 documents (243073 virtual)
150 batches submitted to accumulate stats from 9600 documents (245711 virtual)
151 batches submitted to accumulate stats from 9664 documents (247861 virtual)
154 batches submitted to accumulate stats from 9856 documents (251021 virtual)
155 batches submitted to accumulate stats from 9920 documents (251375 virtual)
156 batches submitted to accumulate stats from 9984 documents (254402 virtual)
157 batches submitted to accumulate stats from 10048 documents (257626 virtual)
160 batches submitted to accumulate stats from 10240 documents (257321 virtual)
161 batches submitted to accumulate stats from 10304 documents (258945 virtual)
163 batches submitted to accumulate stats from 10432 documents (264912 virtual)
165 batches submitted to accumulate stats from 10560 documents (264666 virtual)
168 batches submitted to accumulate stats from 10752 documents (264566 virtual)
169 batches submitted to accumulate stats from 10816 documents (264622 virtual)
171 batches submitted to accumulate stats from 10944 documents (268008 virtual)
172 batches submitted to accumulate stats from 11008 documents (269586 virtual)
173 batches submitted to accumulate stats from 11072 documents (271284 virtual)
174 batches submitted to accumulate stats from 11136 documents (277811 virtual)
175 batches submitted to accumulate stats from 11200 documents (279644 virtual)
176 batches submitted to accumulate stats from 11264 documents (281844 virtual)
178 batches submitted to accumulate stats from 11392 documents (286373 virtual)
179 batches submitted to accumulate stats from 11456 documents (289608 virtual)
180 batches submitted to accumulate stats from 11520 documents (293559 virtual)
181 batches submitted to accumulate stats from 11584 documents (294034 virtual)
182 batches submitted to accumulate stats from 11648 documents (294057 virtual)
184 batches submitted to accumulate stats from 11776 documents (295606 virtual)
185 batches submitted to accumulate stats from 11840 documents (296868 virtual)
186 batches submitted to accumulate stats from 11904 documents (299999 virtual)
189 batches submitted to accumulate stats from 12096 documents (299068 virtual)
192 batches submitted to accumulate stats from 12288 documents (302234 virtual)
193 batches submitted to accumulate stats from 12352 documents (303997 virtual)
194 batches submitted to accumulate stats from 12416 documents (306470 virtual)
195 batches submitted to accumulate stats from 12480 documents (307995 virtual)
196 batches submitted to accumulate stats from 12544 documents (310490 virtual)
197 batches submitted to accumulate stats from 12608 documents (313742 virtual)
198 batches submitted to accumulate stats from 12672 documents (315993 virtual)
199 batches submitted to accumulate stats from 12736 documents (320952 virtual)
202 batches submitted to accumulate stats from 12928 documents (321180 virtual)
206 batches submitted to accumulate stats from 13184 documents (321753 virtual)
208 batches submitted to accumulate stats from 13312 documents (323304 virtual)
209 batches submitted to accumulate stats from 13376 documents (323933 virtual)
210 batches submitted to accumulate stats from 13440 documents (325931 virtual)
211 batches submitted to accumulate stats from 13504 documents (327077 virtual)
213 batches submitted to accumulate stats from 13632 documents (331708 virtual)
214 batches submitted to accumulate stats from 13696 documents (332164 virtual)
215 batches submitted to accumulate stats from 13760 documents (335745 virtual)
216 batches submitted to accumulate stats from 13824 documents (338923 virtual)
217 batches submitted to accumulate stats from 13888 documents (341936 virtual)
218 batches submitted to accumulate stats from 13952 documents (342991 virtual)
219 batches submitted to accumulate stats from 14016 documents (347112 virtual)
220 batches submitted to accumulate stats from 14080 documents (348187 virtual)
221 batches submitted to accumulate stats from 14144 documents (349897 virtual)
222 batches submitted to accumulate stats from 14208 documents (352367 virtual)
225 batches submitted to accumulate stats from 14400 documents (356989 virtual)
227 batches submitted to accumulate stats from 14528 documents (360288 virtual)
228 batches submitted to accumulate stats from 14592 documents (360515 virtual)
230 batches submitted to accumulate stats from 14720 documents (361856 virtual)
231 batches submitted to accumulate stats from 14784 documents (364003 virtual)
232 batches submitted to accumulate stats from 14848 documents (365926 virtual)
233 batches submitted to accumulate stats from 14912 documents (366736 virtual)
234 batches submitted to accumulate stats from 14976 documents (369269 virtual)
235 batches submitted to accumulate stats from 15040 documents (369313 virtual)
236 batches submitted to accumulate stats from 15104 documents (369460 virtual)
238 batches submitted to accumulate stats from 15232 documents (369392 virtual)
239 batches submitted to accumulate stats from 15296 documents (372047 virtual)
240 batches submitted to accumulate stats from 15360 documents (372411 virtual)
241 batches submitted to accumulate stats from 15424 documents (373482 virtual)
244 batches submitted to accumulate stats from 15616 documents (375111 virtual)
245 batches submitted to accumulate stats from 15680 documents (375303 virtual)
246 batches submitted to accumulate stats from 15744 documents (378055 virtual)
247 batches submitted to accumulate stats from 15808 documents (379420 virtual)
248 batches submitted to accumulate stats from 15872 documents (384222 virtual)
249 batches submitted to accumulate stats from 15936 documents (384988 virtual)
251 batches submitted to accumulate stats from 16064 documents (388124 virtual)
252 batches submitted to accumulate stats from 16128 documents (392736 virtual)
253 batches submitted to accumulate stats from 16192 documents (401885 virtual)
255 batches submitted to accumulate stats from 16320 documents (405070 virtual)
256 batches submitted to accumulate stats from 16384 documents (407476 virtual)
257 batches submitted to accumulate stats from 16448 documents (419581 virtual)
258 batches submitted to accumulate stats from 16512 documents (425381 virtual)
259 batches submitted to accumulate stats from 16576 documents (425959 virtual)
260 batches submitted to accumulate stats from 16640 documents (429063 virtual)
261 batches submitted to accumulate stats from 16704 documents (430026 virtual)
262 batches submitted to accumulate stats from 16768 documents (431838 virtual)
263 batches submitted to accumulate stats from 16832 documents (433290 virtual)
264 batches submitted to accumulate stats from 16896 documents (433733 virtual)
265 batches submitted to accumulate stats from 16960 documents (436257 virtual)
266 batches submitted to accumulate stats from 17024 documents (439993 virtual)
267 batches submitted to accumulate stats from 17088 documents (441161 virtual)
268 batches submitted to accumulate stats from 17152 documents (443391 virtual)
269 batches submitted to accumulate stats from 17216 documents (447340 virtual)
270 batches submitted to accumulate stats from 17280 documents (450489 virtual)
271 batches submitted to accumulate stats from 17344 documents (453102 virtual)
272 batches submitted to accumulate stats from 17408 documents (458624 virtual)
273 batches submitted to accumulate stats from 17472 documents (461622 virtual)
275 batches submitted to accumulate stats from 17600 documents (462618 virtual)
276 batches submitted to accumulate stats from 17664 documents (466362 virtual)
277 batches submitted to accumulate stats from 17728 documents (467587 virtual)
278 batches submitted to accumulate stats from 17792 documents (468133 virtual)
279 batches submitted to accumulate stats from 17856 documents (469341 virtual)
280 batches submitted to accumulate stats from 17920 documents (470728 virtual)
284 batches submitted to accumulate stats from 18176 documents (467693 virtual)
286 batches submitted to accumulate stats from 18304 documents (468294 virtual)
287 batches submitted to accumulate stats from 18368 documents (470597 virtual)
288 batches submitted to accumulate stats from 18432 documents (473697 virtual)
289 batches submitted to accumulate stats from 18496 documents (477377 virtual)
290 batches submitted to accumulate stats from 18560 documents (478951 virtual)
291 batches submitted to accumulate stats from 18624 documents (479547 virtual)
292 batches submitted to accumulate stats from 18688 documents (481199 virtual)
294 batches submitted to accumulate stats from 18816 documents (483087 virtual)
295 batches submitted to accumulate stats from 18880 documents (483705 virtual)
296 batches submitted to accumulate stats from 18944 documents (484609 virtual)
299 batches submitted to accumulate stats from 19136 documents (485265 virtual)
300 batches submitted to accumulate stats from 19200 documents (486815 virtual)
301 batches submitted to accumulate stats from 19264 documents (490651 virtual)
302 batches submitted to accumulate stats from 19328 documents (493365 virtual)
303 batches submitted to accumulate stats from 19392 documents (494501 virtual)
304 batches submitted to accumulate stats from 19456 documents (494766 virtual)
305 batches submitted to accumulate stats from 19520 documents (501932 virtual)
306 batches submitted to accumulate stats from 19584 documents (504730 virtual)
307 batches submitted to accumulate stats from 19648 documents (504884 virtual)
311 batches submitted to accumulate stats from 19904 documents (503901 virtual)
312 batches submitted to accumulate stats from 19968 documents (505744 virtual)
313 batches submitted to accumulate stats from 20032 documents (506517 virtual)
314 batches submitted to accumulate stats from 20096 documents (507704 virtual)
315 batches submitted to accumulate stats from 20160 documents (508657 virtual)
316 batches submitted to accumulate stats from 20224 documents (510087 virtual)
317 batches submitted to accumulate stats from 20288 documents (520469 virtual)
318 batches submitted to accumulate stats from 20352 documents (523095 virtual)
320 batches submitted to accumulate stats from 20480 documents (532043 virtual)
321 batches submitted to accumulate stats from 20544 documents (532796 virtual)
322 batches submitted to accumulate stats from 20608 documents (533061 virtual)
323 batches submitted to accumulate stats from 20672 documents (534586 virtual)
324 batches submitted to accumulate stats from 20736 documents (535444 virtual)
325 batches submitted to accumulate stats from 20800 documents (538408 virtual)
326 batches submitted to accumulate stats from 20864 documents (540154 virtual)
327 batches submitted to accumulate stats from 20928 documents (541050 virtual)
328 batches submitted to accumulate stats from 20992 documents (542414 virtual)
329 batches submitted to accumulate stats from 21056 documents (543074 virtual)
331 batches submitted to accumulate stats from 21184 documents (544269 virtual)
332 batches submitted to accumulate stats from 21248 documents (544297 virtual)
333 batches submitted to accumulate stats from 21312 documents (545353 virtual)
334 batches submitted to accumulate stats from 21376 documents (547866 virtual)
335 batches submitted to accumulate stats from 21440 documents (549078 virtual)
336 batches submitted to accumulate stats from 21504 documents (552974 virtual)
337 batches submitted to accumulate stats from 21568 documents (558077 virtual)
339 batches submitted to accumulate stats from 21696 documents (557924 virtual)
340 batches submitted to accumulate stats from 21760 documents (558283 virtual)
343 batches submitted to accumulate stats from 21952 documents (561500 virtual)
344 batches submitted to accumulate stats from 22016 documents (562537 virtual)
346 batches submitted to accumulate stats from 22144 documents (561875 virtual)
348 batches submitted to accumulate stats from 22272 documents (564270 virtual)
349 batches submitted to accumulate stats from 22336 documents (567336 virtual)
350 batches submitted to accumulate stats from 22400 documents (568724 virtual)
351 batches submitted to accumulate stats from 22464 documents (569566 virtual)
352 batches submitted to accumulate stats from 22528 documents (571523 virtual)
353 batches submitted to accumulate stats from 22592 documents (574252 virtual)
354 batches submitted to accumulate stats from 22656 documents (576542 virtual)
356 batches submitted to accumulate stats from 22784 documents (579284 virtual)
357 batches submitted to accumulate stats from 22848 documents (580855 virtual)
359 batches submitted to accumulate stats from 22976 documents (582216 virtual)
360 batches submitted to accumulate stats from 23040 documents (583934 virtual)
361 batches submitted to accumulate stats from 23104 documents (591229 virtual)
364 batches submitted to accumulate stats from 23296 documents (590200 virtual)
365 batches submitted to accumulate stats from 23360 documents (593831 virtual)
366 batches submitted to accumulate stats from 23424 documents (595197 virtual)
368 batches submitted to accumulate stats from 23552 documents (594849 virtual)
369 batches submitted to accumulate stats from 23616 documents (595773 virtual)
370 batches submitted to accumulate stats from 23680 documents (596630 virtual)
371 batches submitted to accumulate stats from 23744 documents (599581 virtual)
373 batches submitted to accumulate stats from 23872 documents (601429 virtual)
376 batches submitted to accumulate stats from 24064 documents (602491 virtual)
378 batches submitted to accumulate stats from 24192 documents (607094 virtual)
380 batches submitted to accumulate stats from 24320 documents (610969 virtual)
381 batches submitted to accumulate stats from 24384 documents (612280 virtual)
382 batches submitted to accumulate stats from 24448 documents (615641 virtual)
383 batches submitted to accumulate stats from 24512 documents (616033 virtual)
384 batches submitted to accumulate stats from 24576 documents (625096 virtual)
385 batches submitted to accumulate stats from 24640 documents (626600 virtual)
386 batches submitted to accumulate stats from 24704 documents (627480 virtual)
387 batches submitted to accumulate stats from 24768 documents (628102 virtual)
388 batches submitted to accumulate stats from 24832 documents (630652 virtual)
389 batches submitted to accumulate stats from 24896 documents (631980 virtual)
390 batches submitted to accumulate stats from 24960 documents (638277 virtual)
391 batches submitted to accumulate stats from 25024 documents (643494 virtual)
392 batches submitted to accumulate stats from 25088 documents (645704 virtual)
393 batches submitted to accumulate stats from 25152 documents (648696 virtual)
394 batches submitted to accumulate stats from 25216 documents (650196 virtual)
396 batches submitted to accumulate stats from 25344 documents (648662 virtual)
398 batches submitted to accumulate stats from 25472 documents (649278 virtual)
399 batches submitted to accumulate stats from 25536 documents (650679 virtual)
400 batches submitted to accumulate stats from 25600 documents (654353 virtual)
402 batches submitted to accumulate stats from 25728 documents (654827 virtual)
403 batches submitted to accumulate stats from 25792 documents (657295 virtual)
405 batches submitted to accumulate stats from 25920 documents (656431 virtual)
407 batches submitted to accumulate stats from 26048 documents (658012 virtual)
409 batches submitted to accumulate stats from 26176 documents (659661 virtual)
410 batches submitted to accumulate stats from 26240 documents (660363 virtual)
412 batches submitted to accumulate stats from 26368 documents (660402 virtual)
413 batches submitted to accumulate stats from 26432 documents (660819 virtual)
414 batches submitted to accumulate stats from 26496 documents (663013 virtual)
416 batches submitted to accumulate stats from 26624 documents (664340 virtual)
417 batches submitted to accumulate stats from 26688 documents (669711 virtual)
418 batches submitted to accumulate stats from 26752 documents (670163 virtual)
419 batches submitted to accumulate stats from 26816 documents (670594 virtual)
420 batches submitted to accumulate stats from 26880 documents (679307 virtual)
421 batches submitted to accumulate stats from 26944 documents (687116 virtual)
423 batches submitted to accumulate stats from 27072 documents (686917 virtual)
424 batches submitted to accumulate stats from 27136 documents (689262 virtual)
425 batches submitted to accumulate stats from 27200 documents (691194 virtual)
426 batches submitted to accumulate stats from 27264 documents (692509 virtual)
427 batches submitted to accumulate stats from 27328 documents (693842 virtual)
429 batches submitted to accumulate stats from 27456 documents (694660 virtual)
430 batches submitted to accumulate stats from 27520 documents (698156 virtual)
431 batches submitted to accumulate stats from 27584 documents (703361 virtual)
432 batches submitted to accumulate stats from 27648 documents (704051 virtual)
435 batches submitted to accumulate stats from 27840 documents (701988 virtual)
436 batches submitted to accumulate stats from 27904 documents (702162 virtual)
437 batches submitted to accumulate stats from 27968 documents (704247 virtual)
439 batches submitted to accumulate stats from 28096 documents (704663 virtual)
440 batches submitted to accumulate stats from 28160 documents (704809 virtual)
441 batches submitted to accumulate stats from 28224 documents (705194 virtual)
442 batches submitted to accumulate stats from 28288 documents (706167 virtual)
443 batches submitted to accumulate stats from 28352 documents (710646 virtual)
444 batches submitted to accumulate stats from 28416 documents (713572 virtual)
445 batches submitted to accumulate stats from 28480 documents (716820 virtual)
446 batches submitted to accumulate stats from 28544 documents (719287 virtual)
450 batches submitted to accumulate stats from 28800 documents (723005 virtual)
452 batches submitted to accumulate stats from 28928 documents (722345 virtual)
453 batches submitted to accumulate stats from 28992 documents (725969 virtual)
454 batches submitted to accumulate stats from 29056 documents (727137 virtual)
455 batches submitted to accumulate stats from 29120 documents (729885 virtual)
456 batches submitted to accumulate stats from 29184 documents (731883 virtual)
457 batches submitted to accumulate stats from 29248 documents (736251 virtual)
458 batches submitted to accumulate stats from 29312 documents (737834 virtual)
459 batches submitted to accumulate stats from 29376 documents (740383 virtual)
460 batches submitted to accumulate stats from 29440 documents (742256 virtual)
461 batches submitted to accumulate stats from 29504 documents (746013 virtual)
462 batches submitted to accumulate stats from 29568 documents (752182 virtual)
463 batches submitted to accumulate stats from 29632 documents (754138 virtual)
464 batches submitted to accumulate stats from 29696 documents (754221 virtual)
465 batches submitted to accumulate stats from 29760 documents (757297 virtual)
466 batches submitted to accumulate stats from 29824 documents (762586 virtual)
467 batches submitted to accumulate stats from 29888 documents (763935 virtual)
468 batches submitted to accumulate stats from 29952 documents (772748 virtual)
469 batches submitted to accumulate stats from 30016 documents (773864 virtual)
470 batches submitted to accumulate stats from 30080 documents (775012 virtual)
474 batches submitted to accumulate stats from 30336 documents (774442 virtual)
476 batches submitted to accumulate stats from 30464 documents (774334 virtual)
477 batches submitted to accumulate stats from 30528 documents (778956 virtual)
480 batches submitted to accumulate stats from 30720 documents (778897 virtual)
481 batches submitted to accumulate stats from 30784 documents (780189 virtual)
482 batches submitted to accumulate stats from 30848 documents (782395 virtual)
483 batches submitted to accumulate stats from 30912 documents (788938 virtual)
484 batches submitted to accumulate stats from 30976 documents (792277 virtual)
485 batches submitted to accumulate stats from 31040 documents (796051 virtual)
488 batches submitted to accumulate stats from 31232 documents (803127 virtual)
489 batches submitted to accumulate stats from 31296 documents (809246 virtual)
491 batches submitted to accumulate stats from 31424 documents (810359 virtual)
492 batches submitted to accumulate stats from 31488 documents (813296 virtual)
493 batches submitted to accumulate stats from 31552 documents (817247 virtual)
494 batches submitted to accumulate stats from 31616 documents (818563 virtual)
495 batches submitted to accumulate stats from 31680 documents (823800 virtual)
496 batches submitted to accumulate stats from 31744 documents (825078 virtual)
499 batches submitted to accumulate stats from 31936 documents (829701 virtual)
500 batches submitted to accumulate stats from 32000 documents (830802 virtual)
501 batches submitted to accumulate stats from 32064 documents (835664 virtual)
503 batches submitted to accumulate stats from 32192 documents (836433 virtual)
504 batches submitted to accumulate stats from 32256 documents (841702 virtual)
505 batches submitted to accumulate stats from 32320 documents (850226 virtual)
506 batches submitted to accumulate stats from 32384 documents (854252 virtual)
507 batches submitted to accumulate stats from 32448 documents (857196 virtual)
508 batches submitted to accumulate stats from 32512 documents (858586 virtual)
509 batches submitted to accumulate stats from 32576 documents (859946 virtual)
511 batches submitted to accumulate stats from 32704 documents (861724 virtual)
512 batches submitted to accumulate stats from 32768 documents (862828 virtual)
513 batches submitted to accumulate stats from 32832 documents (864188 virtual)
514 batches submitted to accumulate stats from 32896 documents (865890 virtual)
515 batches submitted to accumulate stats from 32960 documents (869422 virtual)
516 batches submitted to accumulate stats from 33024 documents (870009 virtual)
517 batches submitted to accumulate stats from 33088 documents (871458 virtual)
518 batches submitted to accumulate stats from 33152 documents (872683 virtual)
519 batches submitted to accumulate stats from 33216 documents (873698 virtual)
520 batches submitted to accumulate stats from 33280 documents (874061 virtual)
521 batches submitted to accumulate stats from 33344 documents (874636 virtual)
522 batches submitted to accumulate stats from 33408 documents (875061 virtual)
523 batches submitted to accumulate stats from 33472 documents (881877 virtual)
528 batches submitted to accumulate stats from 33792 documents (893057 virtual)
529 batches submitted to accumulate stats from 33856 documents (894144 virtual)
530 batches submitted to accumulate stats from 33920 documents (894921 virtual)
531 batches submitted to accumulate stats from 33984 documents (900180 virtual)
532 batches submitted to accumulate stats from 34048 documents (903151 virtual)
533 batches submitted to accumulate stats from 34112 documents (904133 virtual)
534 batches submitted to accumulate stats from 34176 documents (905894 virtual)
535 batches submitted to accumulate stats from 34240 documents (906561 virtual)
536 batches submitted to accumulate stats from 34304 documents (906626 virtual)
537 batches submitted to accumulate stats from 34368 documents (907168 virtual)
538 batches submitted to accumulate stats from 34432 documents (909712 virtual)
539 batches submitted to accumulate stats from 34496 documents (911264 virtual)
540 batches submitted to accumulate stats from 34560 documents (916368 virtual)
541 batches submitted to accumulate stats from 34624 documents (918911 virtual)
544 batches submitted to accumulate stats from 34816 documents (918761 virtual)
545 batches submitted to accumulate stats from 34880 documents (922222 virtual)
546 batches submitted to accumulate stats from 34944 documents (924062 virtual)
548 batches submitted to accumulate stats from 35072 documents (925076 virtual)
550 batches submitted to accumulate stats from 35200 documents (925242 virtual)
555 batches submitted to accumulate stats from 35520 documents (929956 virtual)
557 batches submitted to accumulate stats from 35648 documents (933955 virtual)
558 batches submitted to accumulate stats from 35712 documents (938179 virtual)
559 batches submitted to accumulate stats from 35776 documents (940370 virtual)
560 batches submitted to accumulate stats from 35840 documents (942034 virtual)
562 batches submitted to accumulate stats from 35968 documents (945260 virtual)
564 batches submitted to accumulate stats from 36096 documents (946562 virtual)
565 batches submitted to accumulate stats from 36160 documents (947934 virtual)
567 batches submitted to accumulate stats from 36288 documents (947249 virtual)
568 batches submitted to accumulate stats from 36352 documents (951298 virtual)
570 batches submitted to accumulate stats from 36480 documents (952080 virtual)
571 batches submitted to accumulate stats from 36544 documents (953291 virtual)
572 batches submitted to accumulate stats from 36608 documents (956694 virtual)
574 batches submitted to accumulate stats from 36736 documents (954610 virtual)
575 batches submitted to accumulate stats from 36800 documents (963842 virtual)
577 batches submitted to accumulate stats from 36928 documents (970235 virtual)
578 batches submitted to accumulate stats from 36992 documents (974455 virtual)
579 batches submitted to accumulate stats from 37056 documents (979062 virtual)
580 batches submitted to accumulate stats from 37120 documents (981318 virtual)
581 batches submitted to accumulate stats from 37184 documents (988340 virtual)
582 batches submitted to accumulate stats from 37248 documents (988556 virtual)
584 batches submitted to accumulate stats from 37376 documents (988497 virtual)
587 batches submitted to accumulate stats from 37568 documents (991297 virtual)
588 batches submitted to accumulate stats from 37632 documents (992229 virtual)
589 batches submitted to accumulate stats from 37696 documents (993177 virtual)
590 batches submitted to accumulate stats from 37760 documents (995582 virtual)
591 batches submitted to accumulate stats from 37824 documents (998876 virtual)
594 batches submitted to accumulate stats from 38016 documents (1000176 virtual)
595 batches submitted to accumulate stats from 38080 documents (1001868 virtual)
596 batches submitted to accumulate stats from 38144 documents (1005675 virtual)
597 batches submitted to accumulate stats from 38208 documents (1008389 virtual)
598 batches submitted to accumulate stats from 38272 documents (1009170 virtual)
599 batches submitted to accumulate stats from 38336 documents (1013522 virtual)
600 batches submitted to accumulate stats from 38400 documents (1014555 virtual)
602 batches submitted to accumulate stats from 38528 documents (1014441 virtual)
603 batches submitted to accumulate stats from 38592 documents (1020830 virtual)
606 batches submitted to accumulate stats from 38784 documents (1021494 virtual)
609 batches submitted to accumulate stats from 38976 documents (1021145 virtual)
610 batches submitted to accumulate stats from 39040 documents (1023549 virtual)
611 batches submitted to accumulate stats from 39104 documents (1026503 virtual)
612 batches submitted to accumulate stats from 39168 documents (1028692 virtual)
613 batches submitted to accumulate stats from 39232 documents (1032960 virtual)
614 batches submitted to accumulate stats from 39296 documents (1036413 virtual)
615 batches submitted to accumulate stats from 39360 documents (1037947 virtual)
616 batches submitted to accumulate stats from 39424 documents (1040852 virtual)
617 batches submitted to accumulate stats from 39488 documents (1042283 virtual)
619 batches submitted to accumulate stats from 39616 documents (1041678 virtual)
621 batches submitted to accumulate stats from 39744 documents (1042856 virtual)
622 batches submitted to accumulate stats from 39808 documents (1046423 virtual)
624 batches submitted to accumulate stats from 39936 documents (1050247 virtual)
625 batches submitted to accumulate stats from 40000 documents (1051370 virtual)
626 batches submitted to accumulate stats from 40064 documents (1053468 virtual)
627 batches submitted to accumulate stats from 40128 documents (1055039 virtual)
629 batches submitted to accumulate stats from 40256 documents (1055363 virtual)
630 batches submitted to accumulate stats from 40320 documents (1056407 virtual)
632 batches submitted to accumulate stats from 40448 documents (1055653 virtual)
633 batches submitted to accumulate stats from 40512 documents (1058177 virtual)
634 batches submitted to accumulate stats from 40576 documents (1059913 virtual)
635 batches submitted to accumulate stats from 40640 documents (1060860 virtual)
637 batches submitted to accumulate stats from 40768 documents (1061398 virtual)
638 batches submitted to accumulate stats from 40832 documents (1062024 virtual)
640 batches submitted to accumulate stats from 40960 documents (1063416 virtual)
641 batches submitted to accumulate stats from 41024 documents (1070293 virtual)
642 batches submitted to accumulate stats from 41088 documents (1073775 virtual)
643 batches submitted to accumulate stats from 41152 documents (1085608 virtual)
645 batches submitted to accumulate stats from 41280 documents (1086479 virtual)
648 batches submitted to accumulate stats from 41472 documents (1087333 virtual)
649 batches submitted to accumulate stats from 41536 documents (1089883 virtual)
650 batches submitted to accumulate stats from 41600 documents (1097070 virtual)
651 batches submitted to accumulate stats from 41664 documents (1103417 virtual)
653 batches submitted to accumulate stats from 41792 documents (1103676 virtual)
655 batches submitted to accumulate stats from 41920 documents (1102148 virtual)
656 batches submitted to accumulate stats from 41984 documents (1107825 virtual)
659 batches submitted to accumulate stats from 42176 documents (1112642 virtual)
660 batches submitted to accumulate stats from 42240 documents (1113250 virtual)
664 batches submitted to accumulate stats from 42496 documents (1113988 virtual)
665 batches submitted to accumulate stats from 42560 documents (1116581 virtual)
666 batches submitted to accumulate stats from 42624 documents (1117642 virtual)
667 batches submitted to accumulate stats from 42688 documents (1122936 virtual)
668 batches submitted to accumulate stats from 42752 documents (1123129 virtual)
670 batches submitted to accumulate stats from 42880 documents (1124641 virtual)
671 batches submitted to accumulate stats from 42944 documents (1125279 virtual)
672 batches submitted to accumulate stats from 43008 documents (1128589 virtual)
673 batches submitted to accumulate stats from 43072 documents (1132321 virtual)
674 batches submitted to accumulate stats from 43136 documents (1133450 virtual)
675 batches submitted to accumulate stats from 43200 documents (1136191 virtual)
677 batches submitted to accumulate stats from 43328 documents (1140194 virtual)
678 batches submitted to accumulate stats from 43392 documents (1141122 virtual)
679 batches submitted to accumulate stats from 43456 documents (1141607 virtual)
682 batches submitted to accumulate stats from 43648 documents (1141138 virtual)
683 batches submitted to accumulate stats from 43712 documents (1143253 virtual)
684 batches submitted to accumulate stats from 43776 documents (1144252 virtual)
685 batches submitted to accumulate stats from 43840 documents (1147809 virtual)
687 batches submitted to accumulate stats from 43968 documents (1148734 virtual)
690 batches submitted to accumulate stats from 44160 documents (1148779 virtual)
691 batches submitted to accumulate stats from 44224 documents (1149803 virtual)
692 batches submitted to accumulate stats from 44288 documents (1155159 virtual)
693 batches submitted to accumulate stats from 44352 documents (1156272 virtual)
694 batches submitted to accumulate stats from 44416 documents (1164235 virtual)
695 batches submitted to accumulate stats from 44480 documents (1166692 virtual)
696 batches submitted to accumulate stats from 44544 documents (1167237 virtual)
697 batches submitted to accumulate stats from 44608 documents (1167581 virtual)
698 batches submitted to accumulate stats from 44672 documents (1171654 virtual)
699 batches submitted to accumulate stats from 44736 documents (1177113 virtual)
701 batches submitted to accumulate stats from 44864 documents (1178768 virtual)
702 batches submitted to accumulate stats from 44928 documents (1181886 virtual)
706 batches submitted to accumulate stats from 45184 documents (1185355 virtual)
707 batches submitted to accumulate stats from 45248 documents (1189159 virtual)
708 batches submitted to accumulate stats from 45312 documents (1189353 virtual)
709 batches submitted to accumulate stats from 45376 documents (1191990 virtual)
710 batches submitted to accumulate stats from 45440 documents (1192576 virtual)
711 batches submitted to accumulate stats from 45504 documents (1196804 virtual)
712 batches submitted to accumulate stats from 45568 documents (1201233 virtual)
713 batches submitted to accumulate stats from 45632 documents (1211436 virtual)
715 batches submitted to accumulate stats from 45760 documents (1212820 virtual)
716 batches submitted to accumulate stats from 45824 documents (1218268 virtual)
717 batches submitted to accumulate stats from 45888 documents (1221872 virtual)
718 batches submitted to accumulate stats from 45952 documents (1232605 virtual)
719 batches submitted to accumulate stats from 46016 documents (1236209 virtual)
721 batches submitted to accumulate stats from 46144 documents (1237498 virtual)
722 batches submitted to accumulate stats from 46208 documents (1237583 virtual)
723 batches submitted to accumulate stats from 46272 documents (1238150 virtual)
724 batches submitted to accumulate stats from 46336 documents (1242873 virtual)
726 batches submitted to accumulate stats from 46464 documents (1245323 virtual)
727 batches submitted to accumulate stats from 46528 documents (1249200 virtual)
729 batches submitted to accumulate stats from 46656 documents (1251837 virtual)
730 batches submitted to accumulate stats from 46720 documents (1260056 virtual)
731 batches submitted to accumulate stats from 46784 documents (1265556 virtual)
732 batches submitted to accumulate stats from 46848 documents (1267961 virtual)
733 batches submitted to accumulate stats from 46912 documents (1274109 virtual)
734 batches submitted to accumulate stats from 46976 documents (1274702 virtual)
735 batches submitted to accumulate stats from 47040 documents (1278169 virtual)
736 batches submitted to accumulate stats from 47104 documents (1279667 virtual)
737 batches submitted to accumulate stats from 47168 documents (1280031 virtual)
738 batches submitted to accumulate stats from 47232 documents (1280940 virtual)
740 batches submitted to accumulate stats from 47360 documents (1281019 virtual)
741 batches submitted to accumulate stats from 47424 documents (1282279 virtual)
742 batches submitted to accumulate stats from 47488 documents (1282315 virtual)
743 batches submitted to accumulate stats from 47552 documents (1285883 virtual)
747 batches submitted to accumulate stats from 47808 documents (1291490 virtual)
748 batches submitted to accumulate stats from 47872 documents (1292485 virtual)
749 batches submitted to accumulate stats from 47936 documents (1299063 virtual)
750 batches submitted to accumulate stats from 48000 documents (1301399 virtual)
752 batches submitted to accumulate stats from 48128 documents (1304356 virtual)
753 batches submitted to accumulate stats from 48192 documents (1307888 virtual)
756 batches submitted to accumulate stats from 48384 documents (1310493 virtual)
758 batches submitted to accumulate stats from 48512 documents (1309566 virtual)
759 batches submitted to accumulate stats from 48576 documents (1312454 virtual)
760 batches submitted to accumulate stats from 48640 documents (1315819 virtual)
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
3 accumulators retrieved from output queue
accumulated word occurrence stats for 3848293 virtual documents
K=30, Coherence Score: 0.6369310647874664
Starting K=40
using symmetric alpha at 0.025
using symmetric eta at 0.025
using serial LDA version on this node
running online LDA training, 40 topics, 10 passes over the supplied corpus of 49835 documents, updating every 3000 documents, evaluating every ~49835 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 3 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.025): 0.005*"census" + 0.004*"county" + 0.003*"building" + 0.003*"study" + 0.003*"award" + 0.002*"australian" + 0.002*"field" + 0.002*"change" + 0.002*"say" + 0.002*"child"
topic #38 (0.025): 0.006*"station" + 0.004*"student" + 0.003*"study" + 0.003*"book" + 0.003*"line" + 0.002*"village" + 0.002*"life" + 0.002*"gold" + 0.002*"album" + 0.002*"child"
topic #1 (0.025): 0.011*"film" + 0.004*"series" + 0.004*"game" + 0.003*"player" + 0.003*"specie" + 0.003*"character" + 0.003*"final" + 0.003*"art" + 0.003*"season" + 0.003*"award"
topic #6 (0.025): 0.004*"season" + 0.003*"study" + 0.003*"child" + 0.003*"building" + 0.002*"island" + 0.002*"church" + 0.002*"gun" + 0.002*"support" + 0.002*"publish" + 0.002*"north"
topic #11 (0.025): 0.006*"park" + 0.005*"game" + 0.005*"season" + 0.003*"character" + 0.003*"series" + 0.003*"episode" + 0.003*"player" + 0.003*"house" + 0.002*"get" + 0.002*"offer"
topic diff=18.591995, rho=1.000000
PROGRESS: pass 0, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.025): 0.005*"game" + 0.005*"season" + 0.005*"park" + 0.003*"series" + 0.003*"character" + 0.003*"episode" + 0.002*"player" + 0.002*"get" + 0.002*"version" + 0.002*"system"
topic #34 (0.025): 0.005*"game" + 0.003*"student" + 0.003*"population" + 0.003*"ship" + 0.002*"mill" + 0.002*"land" + 0.002*"house" + 0.002*"change" + 0.002*"series" + 0.002*"company"
topic #27 (0.025): 0.005*"language" + 0.004*"company" + 0.003*"film" + 0.003*"series" + 0.003*"town" + 0.003*"house" + 0.002*"war" + 0.002*"appear" + 0.002*"force" + 0.002*"government"
topic #21 (0.025): 0.006*"film" + 0.005*"game" + 0.003*"single" + 0.003*"house" + 0.003*"season" + 0.003*"song" + 0.003*"album" + 0.002*"election" + 0.002*"court" + 0.002*"party"
topic #25 (0.025): 0.005*"season" + 0.005*"woman" + 0.004*"film" + 0.004*"club" + 0.003*"company" + 0.003*"game" + 0.003*"music" + 0.003*"house" + 0.003*"international" + 0.003*"church"
topic diff=6.536592, rho=0.500000
PROGRESS: pass 0, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.025): 0.004*"government" + 0.004*"son" + 0.003*"child" + 0.003*"party" + 0.003*"album" + 0.003*"election" + 0.003*"say" + 0.002*"title" + 0.002*"king" + 0.002*"force"
topic #32 (0.025): 0.005*"album" + 0.005*"society" + 0.003*"village" + 0.003*"island" + 0.003*"law" + 0.003*"research" + 0.003*"specie" + 0.002*"study" + 0.002*"publish" + 0.002*"change"
topic #29 (0.025): 0.005*"single" + 0.005*"season" + 0.004*"village" + 0.004*"final" + 0.004*"title" + 0.004*"championship" + 0.003*"specie" + 0.003*"company" + 0.003*"captain" + 0.003*"club"
topic #20 (0.025): 0.006*"book" + 0.004*"sterling" + 0.004*"building" + 0.004*"publish" + 0.004*"say" + 0.003*"company" + 0.003*"station" + 0.002*"film" + 0.002*"story" + 0.002*"set"
topic #36 (0.025): 0.016*"displaystyle" + 0.003*"function" + 0.003*"test" + 0.003*"system" + 0.003*"set" + 0.003*"season" + 0.003*"game" + 0.003*"research" + 0.003*"produce" + 0.002*"series"
topic diff=2.050341, rho=0.377964
PROGRESS: pass 0, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.016*"film" + 0.006*"company" + 0.005*"highway" + 0.004*"french" + 0.004*"line" + 0.003*"north" + 0.003*"street" + 0.003*"wine" + 0.003*"route" + 0.003*"direct"
topic #9 (0.025): 0.007*"series" + 0.007*"song" + 0.006*"film" + 0.006*"album" + 0.005*"episode" + 0.005*"television" + 0.005*"band" + 0.004*"award" + 0.004*"music" + 0.004*"produce"
topic #36 (0.025): 0.012*"displaystyle" + 0.004*"system" + 0.004*"research" + 0.004*"test" + 0.003*"function" + 0.003*"example" + 0.003*"datum" + 0.003*"set" + 0.002*"theory" + 0.002*"produce"
topic #21 (0.025): 0.005*"film" + 0.004*"election" + 0.004*"court" + 0.004*"party" + 0.004*"game" + 0.003*"vote" + 0.003*"house" + 0.003*"say" + 0.003*"season" + 0.003*"order"
topic #22 (0.025): 0.005*"company" + 0.004*"house" + 0.004*"building" + 0.003*"line" + 0.003*"album" + 0.003*"track" + 0.003*"song" + 0.003*"system" + 0.002*"station" + 0.002*"road"
topic diff=0.850841, rho=0.316228
PROGRESS: pass 0, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.025): 0.007*"airport" + 0.006*"regiment" + 0.006*"define" + 0.004*"category" + 0.004*"game" + 0.004*"rules_player" + 0.004*"non_fifa" + 0.003*"event" + 0.003*"nationality" + 0.003*"verb"
topic #26 (0.025): 0.009*"hotel" + 0.008*"election" + 0.005*"council" + 0.005*"elect" + 0.004*"seat" + 0.004*"government" + 0.003*"party" + 0.003*"south" + 0.003*"casino" + 0.002*"local"
topic #28 (0.025): 0.013*"age" + 0.011*"population" + 0.010*"road" + 0.009*"town" + 0.008*"county" + 0.008*"route" + 0.008*"household" + 0.007*"km" + 0.006*"village" + 0.006*"census"
topic #30 (0.025): 0.007*"company" + 0.005*"game" + 0.004*"design" + 0.003*"town" + 0.003*"fish" + 0.003*"building" + 0.002*"site" + 0.002*"produce" + 0.002*"castle" + 0.002*"river"
topic #4 (0.025): 0.013*"game" + 0.007*"player" + 0.005*"film" + 0.005*"line" + 0.004*"series" + 0.004*"route" + 0.004*"season" + 0.003*"army" + 0.003*"battle" + 0.003*"war"
topic diff=0.807693, rho=0.277350
PROGRESS: pass 0, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.007*"korean" + 0.007*"government" + 0.005*"college" + 0.005*"film" + 0.004*"war" + 0.004*"university" + 0.004*"program" + 0.004*"student" + 0.004*"japanese" + 0.003*"health"
topic #33 (0.025): 0.017*"film" + 0.009*"highway" + 0.007*"company" + 0.005*"french" + 0.004*"line" + 0.004*"route" + 0.003*"street" + 0.003*"north" + 0.003*"road" + 0.003*"wine"
topic #25 (0.025): 0.009*"woman" + 0.007*"election" + 0.006*"elect" + 0.004*"appoint" + 0.004*"church" + 0.004*"child" + 0.004*"general" + 0.004*"company" + 0.004*"house" + 0.003*"president"
topic #0 (0.025): 0.007*"airport" + 0.006*"define" + 0.006*"regiment" + 0.005*"category" + 0.004*"rules_player" + 0.004*"non_fifa" + 0.004*"game" + 0.004*"nationality" + 0.003*"event" + 0.003*"ottawa"
topic #38 (0.025): 0.018*"station" + 0.008*"student" + 0.005*"book" + 0.004*"line" + 0.004*"card" + 0.004*"life" + 0.003*"publish" + 0.003*"radio" + 0.003*"study" + 0.003*"gold"
topic diff=0.792393, rho=0.250000
PROGRESS: pass 0, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.025): 0.016*"game" + 0.007*"player" + 0.006*"line" + 0.005*"battle" + 0.004*"series" + 0.004*"film" + 0.004*"army" + 0.003*"route" + 0.003*"war" + 0.003*"point"
topic #20 (0.025): 0.019*"book" + 0.014*"publish" + 0.006*"novel" + 0.005*"author" + 0.004*"story" + 0.004*"poet" + 0.004*"magazine" + 0.004*"poem" + 0.004*"study" + 0.004*"writer"
topic #31 (0.025): 0.005*"match" + 0.004*"award" + 0.004*"stadium" + 0.003*"cricket" + 0.003*"station" + 0.003*"mall" + 0.003*"text" + 0.003*"study" + 0.003*"game" + 0.002*"club"
topic #15 (0.025): 0.036*"game" + 0.015*"player" + 0.014*"season" + 0.009*"basketball" + 0.005*"conference" + 0.005*"point" + 0.005*"finish" + 0.005*"tournament" + 0.004*"final" + 0.004*"lose"
topic #12 (0.025): 0.019*"station" + 0.012*"line" + 0.011*"railway" + 0.004*"community" + 0.004*"operate" + 0.004*"bus" + 0.004*"train" + 0.004*"public" + 0.003*"act" + 0.003*"law"
topic diff=0.747700, rho=0.229416
PROGRESS: pass 0, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.039*"game" + 0.017*"player" + 0.014*"season" + 0.010*"basketball" + 0.005*"point" + 0.005*"conference" + 0.005*"tournament" + 0.004*"finish" + 0.004*"final" + 0.004*"lose"
topic #16 (0.025): 0.009*"war" + 0.009*"army" + 0.008*"german" + 0.008*"unit" + 0.006*"squadron" + 0.006*"air" + 0.006*"air_force" + 0.005*"command" + 0.005*"operation" + 0.005*"force"
topic #20 (0.025): 0.021*"book" + 0.016*"publish" + 0.006*"novel" + 0.006*"author" + 0.005*"magazine" + 0.004*"editor" + 0.004*"writer" + 0.004*"story" + 0.004*"poet" + 0.004*"study"
topic #38 (0.025): 0.023*"station" + 0.007*"student" + 0.006*"radio" + 0.005*"book" + 0.004*"card" + 0.004*"line" + 0.004*"life" + 0.003*"study" + 0.003*"publish" + 0.003*"jewish"
topic #11 (0.025): 0.012*"game" + 0.007*"character" + 0.005*"version" + 0.005*"series" + 0.005*"season" + 0.004*"cork" + 0.003*"munster" + 0.003*"siren" + 0.003*"player" + 0.003*"park"
topic diff=0.724211, rho=0.213201
PROGRESS: pass 0, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.025): 0.014*"game" + 0.008*"character" + 0.006*"cork" + 0.005*"series" + 0.005*"version" + 0.004*"season" + 0.004*"player" + 0.004*"system" + 0.004*"munster" + 0.003*"design"
topic #26 (0.025): 0.017*"election" + 0.010*"hotel" + 0.010*"council" + 0.008*"elect" + 0.008*"seat" + 0.005*"party" + 0.005*"government" + 0.005*"candidate" + 0.004*"constituency" + 0.004*"ward"
topic #39 (0.025): 0.008*"finish" + 0.008*"race" + 0.006*"season" + 0.006*"championship" + 0.004*"game" + 0.004*"cell" + 0.004*"woman" + 0.004*"yard" + 0.003*"football" + 0.003*"stage"
topic #2 (0.025): 0.036*"season" + 0.023*"club" + 0.018*"league" + 0.018*"game" + 0.016*"football" + 0.013*"player" + 0.012*"match" + 0.012*"championship" + 0.011*"final" + 0.010*"score"
topic #22 (0.025): 0.009*"line" + 0.008*"company" + 0.005*"building" + 0.005*"train" + 0.005*"street" + 0.005*"track" + 0.004*"operate" + 0.004*"station" + 0.004*"tunnel" + 0.004*"house"
topic diff=0.707287, rho=0.200000
PROGRESS: pass 0, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.025): 0.012*"airport" + 0.008*"vowel" + 0.008*"define" + 0.007*"regiment" + 0.005*"word" + 0.005*"category" + 0.005*"rules_player" + 0.005*"nationality" + 0.005*"verb" + 0.004*"non_fifa"
topic #30 (0.025): 0.009*"company" + 0.005*"fire" + 0.004*"fish" + 0.004*"design" + 0.004*"castle" + 0.003*"produce" + 0.003*"town" + 0.003*"mine" + 0.003*"tornado" + 0.003*"site"
topic #8 (0.025): 0.025*"album" + 0.023*"song" + 0.020*"music" + 0.016*"band" + 0.011*"single" + 0.007*"perform" + 0.007*"track" + 0.006*"tour" + 0.005*"chart" + 0.005*"video"
topic #14 (0.025): 0.010*"park" + 0.006*"island" + 0.005*"site" + 0.005*"specie" + 0.005*"river" + 0.004*"water" + 0.004*"forest" + 0.004*"plant" + 0.004*"village" + 0.004*"land"
topic #28 (0.025): 0.015*"population" + 0.015*"town" + 0.015*"county" + 0.014*"road" + 0.012*"river" + 0.011*"age" + 0.011*"village" + 0.010*"north" + 0.009*"route" + 0.009*"km"
topic diff=0.696054, rho=0.188982
PROGRESS: pass 0, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.025): 0.015*"festival" + 0.009*"film" + 0.006*"album" + 0.006*"season" + 0.006*"freiburg" + 0.006*"sign" + 0.005*"music" + 0.004*"art" + 0.003*"career" + 0.003*"game"
topic #10 (0.025): 0.009*"son" + 0.008*"king" + 0.008*"government" + 0.006*"force" + 0.006*"military" + 0.005*"french" + 0.005*"war" + 0.004*"army" + 0.004*"death" + 0.004*"party"
topic #22 (0.025): 0.012*"line" + 0.009*"company" + 0.006*"street" + 0.006*"train" + 0.006*"building" + 0.005*"track" + 0.005*"operate" + 0.005*"bridge" + 0.005*"station" + 0.004*"construction"
topic #4 (0.025): 0.017*"game" + 0.007*"battle" + 0.007*"player" + 0.007*"fight" + 0.006*"line" + 0.005*"attack" + 0.005*"army" + 0.004*"battalion" + 0.004*"force" + 0.004*"series"
topic #9 (0.025): 0.016*"series" + 0.011*"episode" + 0.010*"television" + 0.008*"award" + 0.008*"film" + 0.005*"produce" + 0.005*"tv" + 0.005*"appear" + 0.005*"broadcast" + 0.005*"channel"
topic diff=0.682727, rho=0.179605
PROGRESS: pass 0, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.025): 0.022*"village" + 0.021*"population" + 0.014*"municipality" + 0.011*"company" + 0.010*"club" + 0.008*"census" + 0.007*"rural" + 0.007*"central" + 0.007*"country" + 0.006*"government"
topic #23 (0.025): 0.009*"law" + 0.009*"government" + 0.007*"church" + 0.006*"court" + 0.005*"act" + 0.005*"party" + 0.004*"right" + 0.004*"public" + 0.004*"case" + 0.004*"say"
topic #7 (0.025): 0.016*"festival" + 0.009*"film" + 0.006*"season" + 0.006*"sign" + 0.006*"album" + 0.005*"freiburg" + 0.005*"music" + 0.004*"art" + 0.004*"tamil" + 0.004*"career"
topic #2 (0.025): 0.038*"season" + 0.024*"club" + 0.019*"league" + 0.018*"game" + 0.017*"football" + 0.013*"match" + 0.013*"championship" + 0.013*"player" + 0.012*"final" + 0.010*"score"
topic #4 (0.025): 0.016*"game" + 0.008*"battle" + 0.007*"fight" + 0.007*"player" + 0.006*"line" + 0.005*"army" + 0.005*"attack" + 0.004*"battalion" + 0.004*"enemy" + 0.004*"cavalry"
topic diff=0.657233, rho=0.171499
PROGRESS: pass 0, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #31 (0.025): 0.009*"stadium" + 0.006*"cricket" + 0.006*"match" + 0.005*"text" + 0.005*"mall" + 0.004*"verse" + 0.004*"award" + 0.003*"wicket" + 0.003*"manuscript" + 0.003*"ground"
topic #8 (0.025): 0.028*"album" + 0.025*"song" + 0.021*"music" + 0.017*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.005*"chart" + 0.005*"video"
topic #11 (0.025): 0.019*"game" + 0.010*"character" + 0.008*"cork" + 0.007*"version" + 0.005*"player" + 0.005*"color" + 0.005*"series" + 0.004*"graph" + 0.004*"server" + 0.004*"system"
topic #24 (0.025): 0.026*"art" + 0.011*"museum" + 0.010*"artist" + 0.010*"award" + 0.006*"painting" + 0.006*"study" + 0.006*"exhibition" + 0.005*"public" + 0.005*"college" + 0.005*"hospital"
topic #25 (0.025): 0.012*"woman" + 0.011*"election" + 0.010*"elect" + 0.007*"appoint" + 0.006*"president" + 0.006*"party" + 0.006*"vote" + 0.005*"general" + 0.005*"child" + 0.005*"representative"
topic diff=0.636836, rho=0.164399
PROGRESS: pass 0, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.072*"game" + 0.024*"player" + 0.017*"basketball" + 0.016*"season" + 0.011*"conference" + 0.008*"point" + 0.006*"tournament" + 0.006*"seed" + 0.005*"raider" + 0.005*"lose"
topic #22 (0.025): 0.013*"line" + 0.010*"company" + 0.007*"bridge" + 0.006*"street" + 0.006*"train" + 0.006*"construction" + 0.005*"operate" + 0.005*"building" + 0.005*"railway" + 0.005*"track"
topic #8 (0.025): 0.028*"album" + 0.025*"song" + 0.021*"music" + 0.016*"band" + 0.011*"single" + 0.008*"perform" + 0.007*"track" + 0.006*"tour" + 0.005*"chart" + 0.005*"video"
topic #28 (0.025): 0.018*"county" + 0.017*"town" + 0.016*"population" + 0.015*"road" + 0.013*"village" + 0.012*"north" + 0.012*"river" + 0.012*"age" + 0.010*"south" + 0.010*"km"
topic #33 (0.025): 0.014*"company" + 0.013*"film" + 0.011*"wine" + 0.008*"highway" + 0.008*"store" + 0.006*"french" + 0.006*"production" + 0.004*"street" + 0.004*"line" + 0.004*"produce"
topic diff=0.613790, rho=0.158114
-8.604 per-word bound, 389.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.025): 0.018*"series" + 0.013*"episode" + 0.012*"television" + 0.008*"award" + 0.008*"film" + 0.007*"tv" + 0.006*"channel" + 0.005*"appear" + 0.005*"produce" + 0.005*"broadcast"
topic #19 (0.025): 0.024*"building" + 0.018*"church" + 0.015*"house" + 0.009*"design" + 0.007*"th_century" + 0.005*"street" + 0.005*"style" + 0.005*"stone" + 0.005*"wall" + 0.005*"main"
topic #3 (0.025): 0.010*"design" + 0.010*"system" + 0.009*"engine" + 0.008*"power" + 0.007*"model" + 0.005*"vehicle" + 0.005*"car" + 0.005*"type" + 0.004*"control" + 0.004*"standard"
topic #2 (0.025): 0.039*"season" + 0.024*"club" + 0.019*"league" + 0.018*"game" + 0.017*"football" + 0.013*"match" + 0.013*"championship" + 0.013*"player" + 0.012*"final" + 0.010*"score"
topic #26 (0.025): 0.028*"election" + 0.013*"seat" + 0.012*"council" + 0.012*"party" + 0.011*"elect" + 0.010*"hotel" + 0.008*"constituency" + 0.007*"candidate" + 0.006*"ward" + 0.006*"government"
topic diff=0.598785, rho=0.152499
-8.593 per-word bound, 386.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #33 (0.025): 0.015*"company" + 0.013*"film" + 0.012*"wine" + 0.010*"store" + 0.008*"highway" + 0.007*"french" + 0.007*"production" + 0.004*"street" + 0.004*"produce" + 0.003*"line"
topic #9 (0.025): 0.019*"series" + 0.013*"episode" + 0.012*"television" + 0.008*"award" + 0.008*"film" + 0.007*"tv" + 0.006*"channel" + 0.005*"produce" + 0.005*"appear" + 0.005*"broadcast"
topic #7 (0.025): 0.023*"festival" + 0.008*"film" + 0.006*"sign" + 0.006*"san_jose" + 0.005*"season" + 0.005*"music" + 0.004*"album" + 0.004*"jenkin" + 0.004*"telugu" + 0.004*"blackpool"
topic #22 (0.025): 0.013*"line" + 0.010*"company" + 0.008*"bridge" + 0.006*"street" + 0.006*"train" + 0.006*"construction" + 0.006*"operate" + 0.005*"railway" + 0.005*"building" + 0.005*"track"
topic #10 (0.025): 0.011*"king" + 0.011*"son" + 0.007*"french" + 0.006*"war" + 0.006*"government" + 0.006*"force" + 0.006*"military" + 0.006*"death" + 0.005*"army" + 0.004*"daughter"
topic diff=0.579613, rho=0.147442
-8.580 per-word bound, 382.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #39 (0.025): 0.008*"yard" + 0.005*"finish" + 0.005*"cause" + 0.005*"cell" + 0.005*"chess" + 0.005*"disease" + 0.004*"woman" + 0.004*"treatment" + 0.004*"touchdown" + 0.004*"virus"
topic #5 (0.025): 0.022*"flag" + 0.010*"event" + 0.005*"woman" + 0.005*"life" + 0.004*"say" + 0.004*"color" + 0.003*"ceremony" + 0.003*"death" + 0.003*"order" + 0.003*"ritual"
topic #28 (0.025): 0.020*"county" + 0.020*"town" + 0.017*"population" + 0.015*"road" + 0.014*"village" + 0.013*"age" + 0.012*"north" + 0.012*"river" + 0.011*"south" + 0.011*"km"
topic #4 (0.025): 0.012*"game" + 0.012*"battle" + 0.012*"fight" + 0.008*"line" + 0.008*"attack" + 0.006*"army" + 0.006*"division" + 0.006*"player" + 0.006*"regiment" + 0.005*"enemy"
topic #1 (0.025): 0.069*"film" + 0.013*"star" + 0.009*"character" + 0.008*"direct" + 0.007*"story" + 0.005*"series" + 0.004*"role" + 0.004*"director" + 0.004*"appear" + 0.004*"earth"
topic diff=0.565557, rho=0.141421
-8.339 per-word bound, 323.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 10
PROGRESS: pass 1, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.025): 0.026*"festival" + 0.008*"film" + 0.007*"sign" + 0.005*"san_jose" + 0.005*"season" + 0.005*"music" + 0.005*"jenkin" + 0.004*"tamil" + 0.004*"saint" + 0.004*"international"
topic #35 (0.025): 0.057*"race" + 0.015*"car" + 0.012*"event" + 0.012*"championship" + 0.011*"driver" + 0.010*"point" + 0.009*"finish" + 0.008*"horse" + 0.008*"racing" + 0.008*"series"
topic #11 (0.025): 0.026*"game" + 0.011*"character" + 0.010*"version" + 0.009*"player" + 0.007*"cork" + 0.006*"housemate" + 0.005*"level" + 0.005*"series" + 0.004*"system" + 0.004*"color"
topic #31 (0.025): 0.011*"wicket" + 0.011*"cricket" + 0.009*"match" + 0.007*"text" + 0.007*"stadium" + 0.007*"somerset" + 0.005*"class" + 0.004*"czech" + 0.004*"test" + 0.004*"cricketer"
topic #24 (0.025): 0.030*"art" + 0.016*"museum" + 0.013*"award" + 0.012*"artist" + 0.008*"painting" + 0.007*"exhibition" + 0.007*"study" + 0.006*"hospital" + 0.005*"public" + 0.005*"collection"
topic diff=0.547514, rho=0.138896
PROGRESS: pass 1, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #28 (0.025): 0.020*"town" + 0.019*"county" + 0.017*"population" + 0.016*"road" + 0.014*"village" + 0.014*"age" + 0.013*"river" + 0.013*"north" + 0.011*"south" + 0.011*"km"
topic #33 (0.025): 0.017*"company" + 0.014*"wine" + 0.012*"film" + 0.010*"store" + 0.008*"highway" + 0.008*"french" + 0.007*"production" + 0.005*"restaurant" + 0.004*"produce" + 0.004*"theatre"
topic #1 (0.025): 0.071*"film" + 0.013*"star" + 0.010*"character" + 0.008*"direct" + 0.008*"story" + 0.005*"series" + 0.004*"role" + 0.004*"earth" + 0.004*"director" + 0.004*"appear"
topic #38 (0.025): 0.037*"station" + 0.015*"radio" + 0.009*"jewish" + 0.007*"broadcast" + 0.007*"christian" + 0.005*"air" + 0.005*"local" + 0.005*"format" + 0.004*"student" + 0.004*"programming"
topic #12 (0.025): 0.060*"station" + 0.034*"line" + 0.027*"railway" + 0.014*"train" + 0.011*"operate" + 0.011*"platform" + 0.010*"bus" + 0.007*"passenger" + 0.007*"rail" + 0.005*"railway_station"
topic diff=0.542640, rho=0.138896
PROGRESS: pass 1, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #11 (0.025): 0.032*"game" + 0.012*"version" + 0.012*"character" + 0.010*"player" + 0.006*"cork" + 0.005*"system" + 0.005*"series" + 0.005*"level" + 0.005*"server" + 0.005*"video"
topic #39 (0.025): 0.009*"virus" + 0.008*"yard" + 0.006*"patient" + 0.006*"cause" + 0.005*"treatment" + 0.005*"cell" + 0.005*"disease" + 0.005*"chess" + 0.005*"finish" + 0.004*"infection"
topic #24 (0.025): 0.033*"art" + 0.017*"museum" + 0.014*"award" + 0.013*"artist" + 0.009*"painting" + 0.008*"exhibition" + 0.007*"study" + 0.006*"hospital" + 0.006*"collection" + 0.005*"college"
topic #7 (0.025): 0.029*"festival" + 0.007*"film" + 0.007*"sign" + 0.005*"music" + 0.005*"season" + 0.004*"jenkin" + 0.004*"san_jose" + 0.004*"saint" + 0.004*"international" + 0.004*"tamil"
topic #10 (0.025): 0.012*"son" + 0.011*"king" + 0.008*"war" + 0.006*"death" + 0.006*"force" + 0.006*"french" + 0.006*"military" + 0.006*"army" + 0.006*"government" + 0.005*"brother"
topic diff=0.530044, rho=0.138896
PROGRESS: pass 1, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.025): 0.008*"tell" + 0.007*"say" + 0.006*"get" + 0.006*"kill" + 0.005*"try" + 0.005*"murder" + 0.005*"back" + 0.004*"police" + 0.004*"death" + 0.004*"father"
topic #23 (0.025): 0.012*"government" + 0.010*"law" + 0.010*"court" + 0.006*"act" + 0.005*"right" + 0.005*"case" + 0.005*"say" + 0.005*"party" + 0.005*"report" + 0.004*"public"
topic #9 (0.025): 0.020*"series" + 0.014*"episode" + 0.013*"television" + 0.008*"award" + 0.008*"tv" + 0.007*"film" + 0.006*"appear" + 0.006*"produce" + 0.005*"channel" + 0.005*"broadcast"
topic #32 (0.025): 0.016*"research" + 0.014*"study" + 0.012*"society" + 0.010*"science" + 0.008*"social" + 0.006*"professor" + 0.005*"scientific" + 0.005*"publish" + 0.005*"university" + 0.005*"theory"
topic #22 (0.025): 0.013*"line" + 0.011*"bridge" + 0.009*"company" + 0.009*"street" + 0.008*"construction" + 0.007*"operate" + 0.007*"tunnel" + 0.006*"train" + 0.006*"plan" + 0.006*"car"
topic diff=0.530851, rho=0.138896
PROGRESS: pass 1, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #36 (0.025): 0.010*"displaystyle" + 0.009*"system" + 0.006*"example" + 0.005*"datum" + 0.005*"value" + 0.005*"function" + 0.005*"method" + 0.004*"term" + 0.004*"point" + 0.004*"result"
topic #4 (0.025): 0.014*"battle" + 0.013*"fight" + 0.010*"game" + 0.010*"attack" + 0.008*"line" + 0.007*"enemy" + 0.007*"army" + 0.006*"regiment" + 0.006*"division" + 0.005*"force"
topic #1 (0.025): 0.074*"film" + 0.013*"star" + 0.010*"character" + 0.009*"direct" + 0.008*"story" + 0.005*"series" + 0.005*"role" + 0.004*"director" + 0.004*"earth" + 0.004*"appear"
topic #22 (0.025): 0.014*"line" + 0.011*"bridge" + 0.010*"street" + 0.009*"company" + 0.007*"operate" + 0.007*"construction" + 0.006*"tunnel" + 0.006*"car" + 0.006*"plan" + 0.005*"train"
topic #32 (0.025): 0.016*"research" + 0.014*"study" + 0.012*"society" + 0.010*"science" + 0.008*"social" + 0.006*"professor" + 0.005*"scientific" + 0.005*"life" + 0.005*"theory" + 0.005*"publish"
topic diff=0.516831, rho=0.138896
PROGRESS: pass 1, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.025): 0.014*"line" + 0.012*"bridge" + 0.010*"street" + 0.009*"company" + 0.008*"construction" + 0.007*"operate" + 0.007*"tunnel" + 0.006*"car" + 0.006*"road" + 0.006*"plan"
topic #36 (0.025): 0.010*"displaystyle" + 0.009*"system" + 0.006*"example" + 0.005*"value" + 0.005*"datum" + 0.004*"term" + 0.004*"method" + 0.004*"function" + 0.004*"process" + 0.004*"result"
topic #6 (0.025): 0.049*"specie" + 0.020*"genus" + 0.015*"describe" + 0.005*"length" + 0.005*"species" + 0.005*"ice" + 0.005*"island" + 0.005*"beetle" + 0.004*"marine" + 0.004*"occur"
topic #13 (0.025): 0.014*"university" + 0.012*"student" + 0.012*"college" + 0.011*"program" + 0.009*"korean" + 0.008*"government" + 0.008*"department" + 0.007*"organization" + 0.007*"health" + 0.007*"education"
topic #30 (0.025): 0.009*"mine" + 0.008*"company" + 0.007*"fish" + 0.007*"coal" + 0.007*"fire" + 0.006*"produce" + 0.005*"tornado" + 0.004*"black" + 0.004*"iron" + 0.004*"gold"
topic diff=0.518104, rho=0.138896
PROGRESS: pass 1, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.025): 0.012*"flag" + 0.008*"event" + 0.006*"life" + 0.006*"woman" + 0.005*"buddhist" + 0.005*"say" + 0.004*"soul" + 0.004*"tradition" + 0.004*"death" + 0.004*"buddha"
topic #4 (0.025): 0.016*"battle" + 0.014*"fight" + 0.010*"attack" + 0.008*"game" + 0.008*"army" + 0.008*"line" + 0.007*"enemy" + 0.007*"division" + 0.006*"force" + 0.005*"cavalry"
topic #23 (0.025): 0.013*"government" + 0.010*"law" + 0.009*"court" + 0.007*"act" + 0.005*"right" + 0.005*"case" + 0.005*"say" + 0.005*"political" + 0.005*"report" + 0.005*"party"
topic #22 (0.025): 0.013*"line" + 0.012*"bridge" + 0.010*"street" + 0.008*"company" + 0.008*"construction" + 0.007*"operate" + 0.006*"car" + 0.006*"tunnel" + 0.006*"road" + 0.006*"plan"
topic #0 (0.025): 0.017*"word" + 0.017*"airport" + 0.014*"define" + 0.013*"language" + 0.011*"vowel" + 0.011*"nationality" + 0.010*"rules_player" + 0.010*"non_fifa" + 0.009*"wrestling" + 0.008*"dialect"
topic diff=0.498043, rho=0.138896
PROGRESS: pass 1, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.025): 0.011*"flag" + 0.008*"event" + 0.007*"life" + 0.006*"woman" + 0.005*"say" + 0.004*"buddhist" + 0.004*"soul" + 0.004*"tradition" + 0.004*"chinese" + 0.004*"accord"
topic #3 (0.025): 0.013*"design" + 0.010*"engine" + 0.010*"system" + 0.008*"power" + 0.008*"model" + 0.008*"vehicle" + 0.006*"car" + 0.005*"type" + 0.005*"produce" + 0.005*"control"
topic #25 (0.025): 0.013*"elect" + 0.012*"election" + 0.011*"woman" + 0.010*"president" + 0.009*"appoint" + 0.009*"vote" + 0.008*"party" + 0.007*"general" + 0.006*"law" + 0.006*"politician"
topic #39 (0.025): 0.008*"disease" + 0.008*"cause" + 0.008*"virus" + 0.007*"treatment" + 0.007*"patient" + 0.006*"yard" + 0.006*"infection" + 0.006*"chess" + 0.006*"cell" + 0.005*"symptom"
topic #4 (0.025): 0.017*"battle" + 0.016*"fight" + 0.011*"attack" + 0.008*"enemy" + 0.008*"army" + 0.008*"game" + 0.007*"line" + 0.006*"division" + 0.006*"force" + 0.005*"cavalry"
topic diff=0.487756, rho=0.138896
PROGRESS: pass 1, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.015*"university" + 0.012*"college" + 0.012*"student" + 0.012*"program" + 0.009*"department" + 0.008*"government" + 0.008*"organization" + 0.007*"education" + 0.007*"korean" + 0.007*"health"
topic #2 (0.025): 0.039*"season" + 0.026*"club" + 0.020*"league" + 0.019*"football" + 0.016*"championship" + 0.014*"game" + 0.014*"match" + 0.014*"final" + 0.012*"player" + 0.010*"score"
topic #26 (0.025): 0.045*"election" + 0.025*"seat" + 0.023*"party" + 0.018*"council" + 0.017*"elect" + 0.014*"candidate" + 0.012*"hotel" + 0.012*"constituency" + 0.011*"vote" + 0.009*"ward"
topic #21 (0.025): 0.008*"tell" + 0.007*"say" + 0.007*"get" + 0.006*"kill" + 0.005*"try" + 0.005*"back" + 0.004*"death" + 0.004*"friend" + 0.004*"police" + 0.004*"father"
topic #34 (0.025): 0.029*"student" + 0.014*"college" + 0.009*"grade" + 0.006*"education" + 0.006*"child" + 0.006*"campus" + 0.006*"class" + 0.004*"land" + 0.004*"diocese" + 0.004*"british"
topic diff=0.479758, rho=0.138896
PROGRESS: pass 1, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.016*"university" + 0.013*"student" + 0.012*"program" + 0.012*"college" + 0.009*"department" + 0.008*"government" + 0.008*"organization" + 0.007*"education" + 0.007*"health" + 0.007*"korean"
topic #6 (0.025): 0.054*"specie" + 0.021*"genus" + 0.016*"describe" + 0.006*"species" + 0.005*"length" + 0.005*"beetle" + 0.005*"endemic" + 0.005*"island" + 0.005*"occur" + 0.004*"contain"
topic #1 (0.025): 0.077*"film" + 0.014*"star" + 0.009*"character" + 0.009*"direct" + 0.008*"story" + 0.005*"series" + 0.005*"role" + 0.005*"appear" + 0.005*"movie" + 0.004*"director"
topic #18 (0.025): 0.041*"village" + 0.040*"population" + 0.026*"municipality" + 0.015*"census" + 0.012*"rural" + 0.011*"central" + 0.010*"region" + 0.009*"total" + 0.009*"country" + 0.008*"worker"
topic #3 (0.025): 0.013*"design" + 0.011*"engine" + 0.010*"system" + 0.009*"power" + 0.008*"vehicle" + 0.008*"model" + 0.005*"car" + 0.005*"produce" + 0.005*"type" + 0.005*"control"
topic diff=0.472890, rho=0.138896
PROGRESS: pass 1, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.086*"game" + 0.029*"player" + 0.025*"season" + 0.016*"basketball" + 0.013*"conference" + 0.011*"point" + 0.008*"yard" + 0.007*"field" + 0.006*"ball" + 0.006*"raider"
topic #0 (0.025): 0.023*"language" + 0.022*"word" + 0.014*"airport" + 0.012*"vowel" + 0.011*"dialect" + 0.011*"define" + 0.008*"speak" + 0.008*"nationality" + 0.007*"verb" + 0.007*"refer"
topic #10 (0.025): 0.012*"son" + 0.012*"king" + 0.008*"war" + 0.007*"french" + 0.007*"death" + 0.006*"force" + 0.006*"military" + 0.006*"army" + 0.005*"brother" + 0.005*"government"
topic #12 (0.025): 0.073*"station" + 0.048*"line" + 0.033*"railway" + 0.022*"train" + 0.014*"operate" + 0.013*"bus" + 0.011*"platform" + 0.010*"passenger" + 0.009*"rail" + 0.007*"branch"
topic #37 (0.025): 0.028*"ship" + 0.014*"island" + 0.009*"force" + 0.008*"sea" + 0.007*"vessel" + 0.007*"port" + 0.007*"coast" + 0.007*"navy" + 0.007*"crew" + 0.006*"fleet"
topic diff=0.463924, rho=0.138896
PROGRESS: pass 1, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.025): 0.033*"album" + 0.030*"song" + 0.026*"music" + 0.021*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.006*"video"
topic #20 (0.025): 0.034*"book" + 0.027*"publish" + 0.010*"author" + 0.010*"novel" + 0.009*"magazine" + 0.009*"writer" + 0.008*"editor" + 0.007*"story" + 0.006*"publication" + 0.006*"newspaper"
topic #31 (0.025): 0.017*"cricket" + 0.013*"stadium" + 0.013*"match" + 0.009*"wicket" + 0.008*"test" + 0.008*"wale" + 0.008*"class" + 0.008*"text" + 0.007*"verse" + 0.006*"ground"
topic #16 (0.025): 0.030*"german" + 0.019*"army" + 0.018*"war" + 0.014*"unit" + 0.013*"aircraft" + 0.013*"air" + 0.012*"command" + 0.012*"military" + 0.011*"force" + 0.010*"operation"
topic #5 (0.025): 0.008*"flag" + 0.008*"woman" + 0.007*"event" + 0.007*"life" + 0.005*"say" + 0.004*"temple" + 0.004*"tradition" + 0.004*"accord" + 0.004*"buddhist" + 0.004*"soul"
topic diff=0.446175, rho=0.138896
PROGRESS: pass 1, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.025): 0.043*"village" + 0.043*"population" + 0.027*"municipality" + 0.015*"census" + 0.012*"central" + 0.011*"rural" + 0.011*"region" + 0.010*"total" + 0.010*"country" + 0.008*"worker"
topic #23 (0.025): 0.013*"government" + 0.011*"law" + 0.009*"court" + 0.007*"act" + 0.006*"right" + 0.006*"case" + 0.005*"report" + 0.005*"say" + 0.005*"party" + 0.005*"political"
topic #34 (0.025): 0.031*"student" + 0.015*"college" + 0.009*"grade" + 0.008*"education" + 0.007*"child" + 0.006*"class" + 0.006*"campus" + 0.005*"teacher" + 0.005*"diocese" + 0.004*"attend"
topic #39 (0.025): 0.010*"treatment" + 0.010*"disease" + 0.010*"cause" + 0.009*"patient" + 0.007*"virus" + 0.006*"infection" + 0.006*"skin" + 0.005*"symptom" + 0.005*"occur" + 0.005*"cell"
topic #24 (0.025): 0.040*"art" + 0.019*"museum" + 0.018*"artist" + 0.018*"award" + 0.013*"painting" + 0.009*"exhibition" + 0.008*"study" + 0.007*"collection" + 0.006*"paint" + 0.006*"design"
topic diff=0.431609, rho=0.138896
PROGRESS: pass 1, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #32 (0.025): 0.018*"research" + 0.017*"study" + 0.012*"society" + 0.011*"science" + 0.010*"social" + 0.007*"professor" + 0.006*"theory" + 0.006*"scientific" + 0.005*"human" + 0.005*"publish"
topic #31 (0.025): 0.017*"cricket" + 0.015*"match" + 0.013*"stadium" + 0.010*"test" + 0.010*"wicket" + 0.009*"class" + 0.009*"wale" + 0.007*"welsh" + 0.007*"text" + 0.007*"ground"
topic #21 (0.025): 0.008*"tell" + 0.008*"say" + 0.008*"get" + 0.006*"kill" + 0.006*"try" + 0.005*"back" + 0.005*"death" + 0.005*"life" + 0.004*"friend" + 0.004*"mother"
topic #26 (0.025): 0.051*"election" + 0.031*"party" + 0.024*"seat" + 0.018*"elect" + 0.016*"council" + 0.015*"candidate" + 0.014*"vote" + 0.012*"constituency" + 0.011*"parliament" + 0.010*"labour"
topic #9 (0.025): 0.024*"series" + 0.017*"episode" + 0.016*"television" + 0.009*"tv" + 0.009*"award" + 0.007*"film" + 0.007*"appear" + 0.006*"role" + 0.006*"air" + 0.006*"actor"
topic diff=0.417175, rho=0.138896
-8.398 per-word bound, 337.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.017*"university" + 0.013*"program" + 0.013*"student" + 0.012*"college" + 0.010*"department" + 0.009*"health" + 0.008*"education" + 0.008*"organization" + 0.007*"medical" + 0.007*"government"
topic #38 (0.025): 0.046*"station" + 0.027*"radio" + 0.014*"jewish" + 0.012*"broadcast" + 0.010*"christian" + 0.008*"local" + 0.008*"air" + 0.008*"format" + 0.007*"network" + 0.007*"channel"
topic #24 (0.025): 0.041*"art" + 0.020*"museum" + 0.019*"award" + 0.018*"artist" + 0.013*"painting" + 0.009*"exhibition" + 0.008*"study" + 0.007*"collection" + 0.007*"paint" + 0.006*"design"
topic #26 (0.025): 0.052*"election" + 0.034*"party" + 0.025*"seat" + 0.019*"elect" + 0.017*"council" + 0.015*"candidate" + 0.015*"vote" + 0.012*"parliament" + 0.012*"constituency" + 0.009*"labour"
topic #27 (0.025): 0.013*"protein" + 0.012*"cell" + 0.009*"gene" + 0.007*"structure" + 0.006*"human" + 0.005*"increase" + 0.004*"effect" + 0.004*"result" + 0.004*"contain" + 0.004*"molecule"
topic diff=0.407315, rho=0.138896
-8.399 per-word bound, 337.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #1 (0.025): 0.087*"film" + 0.016*"star" + 0.011*"direct" + 0.009*"character" + 0.008*"story" + 0.006*"movie" + 0.005*"role" + 0.005*"director" + 0.005*"appear" + 0.005*"series"
topic #20 (0.025): 0.035*"book" + 0.028*"publish" + 0.011*"novel" + 0.010*"author" + 0.009*"magazine" + 0.009*"writer" + 0.008*"editor" + 0.007*"story" + 0.007*"newspaper" + 0.007*"publication"
topic #8 (0.025): 0.033*"album" + 0.031*"song" + 0.027*"music" + 0.020*"band" + 0.013*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.006*"video"
topic #31 (0.025): 0.019*"cricket" + 0.016*"match" + 0.013*"stadium" + 0.011*"class" + 0.010*"test" + 0.010*"wicket" + 0.009*"wale" + 0.009*"welsh" + 0.007*"ground" + 0.007*"text"
topic #24 (0.025): 0.041*"art" + 0.020*"museum" + 0.019*"award" + 0.018*"artist" + 0.013*"painting" + 0.009*"exhibition" + 0.008*"study" + 0.007*"collection" + 0.006*"paint" + 0.006*"design"
topic diff=0.397125, rho=0.138896
-8.396 per-word bound, 336.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #5 (0.025): 0.021*"flag" + 0.007*"woman" + 0.006*"life" + 0.006*"event" + 0.005*"tradition" + 0.005*"accord" + 0.005*"temple" + 0.005*"color" + 0.004*"say" + 0.004*"ritual"
topic #9 (0.025): 0.025*"series" + 0.017*"episode" + 0.016*"television" + 0.009*"award" + 0.009*"tv" + 0.006*"role" + 0.006*"appear" + 0.006*"film" + 0.006*"air" + 0.006*"actor"
topic #32 (0.025): 0.017*"research" + 0.017*"study" + 0.011*"society" + 0.011*"science" + 0.009*"social" + 0.007*"professor" + 0.006*"theory" + 0.006*"idea" + 0.006*"scientific" + 0.005*"publish"
topic #11 (0.025): 0.063*"game" + 0.023*"player" + 0.016*"version" + 0.014*"character" + 0.009*"video" + 0.007*"system" + 0.007*"file" + 0.006*"level" + 0.006*"available" + 0.006*"mode"
topic #30 (0.025): 0.013*"mine" + 0.013*"coal" + 0.009*"fire" + 0.007*"produce" + 0.007*"gold" + 0.007*"black" + 0.007*"fish" + 0.006*"mining" + 0.006*"company" + 0.005*"steel"
topic diff=0.390494, rho=0.138896
-8.218 per-word bound, 297.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #35 (0.025): 0.059*"race" + 0.032*"event" + 0.017*"finish" + 0.016*"championship" + 0.014*"car" + 0.012*"compete" + 0.011*"point" + 0.011*"driver" + 0.009*"horse" + 0.009*"winner"
topic #37 (0.025): 0.030*"ship" + 0.015*"island" + 0.009*"vessel" + 0.009*"sea" + 0.008*"port" + 0.008*"navy" + 0.008*"force" + 0.007*"boat" + 0.007*"fleet" + 0.007*"coast"
topic #19 (0.025): 0.033*"building" + 0.018*"church" + 0.017*"house" + 0.010*"design" + 0.008*"stone" + 0.008*"wall" + 0.007*"site" + 0.006*"th_century" + 0.006*"room" + 0.006*"street"
topic #4 (0.025): 0.021*"battle" + 0.018*"fight" + 0.016*"attack" + 0.010*"army" + 0.009*"division" + 0.008*"force" + 0.008*"line" + 0.008*"enemy" + 0.006*"troop" + 0.006*"regiment"
topic #18 (0.025): 0.047*"village" + 0.045*"population" + 0.025*"municipality" + 0.018*"census" + 0.014*"central" + 0.014*"region" + 0.012*"total" + 0.011*"rural" + 0.010*"country" + 0.009*"province"
topic diff=0.369403, rho=0.137575
PROGRESS: pass 2, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #17 (0.025): 0.027*"company" + 0.010*"business" + 0.008*"market" + 0.007*"provide" + 0.006*"product" + 0.006*"development" + 0.006*"public" + 0.006*"project" + 0.006*"management" + 0.005*"purchase"
topic #4 (0.025): 0.021*"battle" + 0.019*"fight" + 0.017*"attack" + 0.010*"army" + 0.009*"division" + 0.008*"force" + 0.008*"enemy" + 0.008*"line" + 0.006*"troop" + 0.006*"defeat"
topic #18 (0.025): 0.048*"village" + 0.044*"population" + 0.026*"municipality" + 0.017*"census" + 0.014*"region" + 0.014*"central" + 0.012*"rural" + 0.011*"total" + 0.010*"country" + 0.009*"province"
topic #12 (0.025): 0.076*"station" + 0.053*"line" + 0.035*"railway" + 0.025*"train" + 0.017*"operate" + 0.015*"bus" + 0.013*"platform" + 0.011*"passenger" + 0.011*"rail" + 0.008*"route"
topic #30 (0.025): 0.013*"mine" + 0.011*"coal" + 0.009*"fire" + 0.008*"gold" + 0.007*"produce" + 0.006*"black" + 0.006*"fish" + 0.006*"mining" + 0.006*"company" + 0.005*"wear"
topic diff=0.354397, rho=0.137575
PROGRESS: pass 2, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.056*"election" + 0.035*"party" + 0.027*"seat" + 0.021*"elect" + 0.018*"vote" + 0.018*"candidate" + 0.016*"council" + 0.013*"parliament" + 0.012*"constituency" + 0.011*"labour"
topic #35 (0.025): 0.057*"race" + 0.035*"event" + 0.017*"finish" + 0.017*"championship" + 0.014*"compete" + 0.013*"car" + 0.011*"point" + 0.009*"driver" + 0.009*"horse" + 0.009*"metre"
topic #25 (0.025): 0.014*"elect" + 0.012*"president" + 0.012*"election" + 0.011*"woman" + 0.011*"appoint" + 0.009*"vote" + 0.008*"general" + 0.008*"law" + 0.007*"party" + 0.006*"politician"
topic #3 (0.025): 0.014*"design" + 0.010*"engine" + 0.010*"system" + 0.010*"power" + 0.008*"model" + 0.008*"vehicle" + 0.006*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #33 (0.025): 0.030*"company" + 0.018*"theatre" + 0.017*"production" + 0.016*"store" + 0.015*"wine" + 0.014*"theater" + 0.014*"french" + 0.009*"restaurant" + 0.008*"produce" + 0.007*"coffee"
topic diff=0.337176, rho=0.137575
PROGRESS: pass 2, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.025): 0.033*"album" + 0.032*"song" + 0.028*"music" + 0.023*"band" + 0.014*"single" + 0.010*"perform" + 0.009*"track" + 0.007*"tour" + 0.007*"chart" + 0.006*"video"
topic #2 (0.025): 0.037*"season" + 0.028*"club" + 0.021*"league" + 0.019*"football" + 0.017*"championship" + 0.016*"match" + 0.015*"final" + 0.013*"game" + 0.012*"player" + 0.011*"score"
topic #27 (0.025): 0.013*"cell" + 0.011*"protein" + 0.008*"gene" + 0.007*"human" + 0.007*"structure" + 0.005*"increase" + 0.004*"process" + 0.004*"nm" + 0.004*"effect" + 0.004*"result"
topic #28 (0.025): 0.026*"county" + 0.025*"town" + 0.018*"road" + 0.018*"river" + 0.016*"age" + 0.016*"population" + 0.015*"north" + 0.013*"south" + 0.013*"km" + 0.013*"village"
topic #11 (0.025): 0.073*"game" + 0.025*"player" + 0.016*"version" + 0.015*"character" + 0.009*"video" + 0.007*"system" + 0.007*"computer" + 0.006*"level" + 0.006*"available" + 0.006*"user"
topic diff=0.330803, rho=0.137575
PROGRESS: pass 2, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.025): 0.013*"design" + 0.011*"engine" + 0.010*"system" + 0.010*"power" + 0.009*"model" + 0.008*"vehicle" + 0.006*"car" + 0.006*"produce" + 0.005*"standard" + 0.005*"production"
topic #6 (0.025): 0.058*"specie" + 0.021*"genus" + 0.017*"describe" + 0.007*"length" + 0.006*"plant" + 0.006*"species" + 0.006*"occur" + 0.005*"endemic" + 0.005*"yellow" + 0.005*"beetle"
topic #38 (0.025): 0.047*"station" + 0.029*"radio" + 0.015*"jewish" + 0.013*"broadcast" + 0.012*"christian" + 0.010*"channel" + 0.009*"air" + 0.009*"network" + 0.009*"local" + 0.008*"news"
topic #19 (0.025): 0.032*"building" + 0.019*"church" + 0.018*"house" + 0.010*"design" + 0.008*"wall" + 0.007*"stone" + 0.007*"site" + 0.007*"th_century" + 0.006*"room" + 0.006*"tower"
topic #26 (0.025): 0.058*"election" + 0.038*"party" + 0.029*"seat" + 0.021*"vote" + 0.021*"elect" + 0.021*"candidate" + 0.018*"council" + 0.012*"parliament" + 0.012*"constituency" + 0.011*"hotel"
topic diff=0.314252, rho=0.137575
PROGRESS: pass 2, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.018*"university" + 0.014*"program" + 0.014*"student" + 0.013*"college" + 0.010*"department" + 0.010*"korean" + 0.009*"education" + 0.009*"organization" + 0.008*"health" + 0.008*"government"
topic #15 (0.025): 0.070*"game" + 0.036*"season" + 0.024*"player" + 0.018*"basketball" + 0.012*"conference" + 0.010*"point" + 0.010*"yard" + 0.008*"baseball" + 0.007*"field" + 0.007*"hit"
topic #35 (0.025): 0.052*"race" + 0.036*"event" + 0.017*"finish" + 0.016*"championship" + 0.015*"compete" + 0.012*"car" + 0.011*"driver" + 0.010*"point" + 0.009*"metre" + 0.009*"winner"
topic #36 (0.025): 0.011*"displaystyle" + 0.011*"system" + 0.007*"example" + 0.006*"datum" + 0.006*"value" + 0.005*"method" + 0.005*"term" + 0.005*"point" + 0.005*"function" + 0.005*"process"
topic #3 (0.025): 0.013*"design" + 0.010*"engine" + 0.010*"system" + 0.010*"power" + 0.009*"model" + 0.009*"vehicle" + 0.006*"car" + 0.006*"produce" + 0.005*"standard" + 0.005*"production"
topic diff=0.312189, rho=0.137575
PROGRESS: pass 2, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #37 (0.025): 0.031*"ship" + 0.018*"island" + 0.009*"sea" + 0.008*"port" + 0.008*"boat" + 0.008*"navy" + 0.008*"vessel" + 0.007*"crew" + 0.007*"fleet" + 0.007*"force"
topic #15 (0.025): 0.068*"game" + 0.037*"season" + 0.024*"player" + 0.017*"basketball" + 0.012*"conference" + 0.011*"point" + 0.009*"yard" + 0.008*"baseball" + 0.007*"hit" + 0.007*"field"
topic #10 (0.025): 0.013*"king" + 0.013*"son" + 0.009*"war" + 0.007*"french" + 0.007*"death" + 0.006*"military" + 0.006*"force" + 0.006*"father" + 0.006*"brother" + 0.005*"army"
topic #21 (0.025): 0.009*"say" + 0.008*"tell" + 0.008*"get" + 0.006*"kill" + 0.006*"try" + 0.005*"back" + 0.005*"death" + 0.005*"life" + 0.005*"mother" + 0.005*"friend"
topic #32 (0.025): 0.018*"research" + 0.017*"study" + 0.011*"society" + 0.011*"science" + 0.010*"social" + 0.007*"professor" + 0.006*"theory" + 0.006*"scientific" + 0.005*"idea" + 0.005*"life"
topic diff=0.291526, rho=0.137575
PROGRESS: pass 2, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.025): 0.028*"series" + 0.018*"episode" + 0.018*"television" + 0.011*"award" + 0.009*"tv" + 0.007*"appear" + 0.007*"role" + 0.007*"air" + 0.006*"season" + 0.006*"film"
topic #16 (0.025): 0.024*"german" + 0.021*"war" + 0.021*"army" + 0.017*"military" + 0.016*"unit" + 0.014*"aircraft" + 0.013*"operation" + 0.013*"force" + 0.012*"air" + 0.012*"command"
topic #6 (0.025): 0.059*"specie" + 0.021*"genus" + 0.017*"describe" + 0.007*"length" + 0.007*"plant" + 0.006*"species" + 0.006*"occur" + 0.006*"endemic" + 0.005*"cm" + 0.005*"mm"
topic #25 (0.025): 0.014*"elect" + 0.013*"president" + 0.012*"woman" + 0.011*"appoint" + 0.010*"election" + 0.008*"vote" + 0.008*"law" + 0.008*"general" + 0.007*"party" + 0.007*"politician"
topic #4 (0.025): 0.023*"battle" + 0.020*"fight" + 0.018*"attack" + 0.012*"army" + 0.011*"force" + 0.008*"enemy" + 0.008*"division" + 0.007*"japanese" + 0.007*"troop" + 0.007*"line"
topic diff=0.282021, rho=0.137575
PROGRESS: pass 2, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.018*"university" + 0.014*"program" + 0.013*"student" + 0.013*"college" + 0.011*"department" + 0.009*"organization" + 0.009*"education" + 0.009*"hospital" + 0.008*"medical" + 0.008*"international"
topic #0 (0.025): 0.046*"language" + 0.025*"word" + 0.012*"dialect" + 0.012*"refer" + 0.012*"define" + 0.010*"speak" + 0.010*"mean" + 0.009*"speaker" + 0.008*"vowel" + 0.008*"japanese"
topic #26 (0.025): 0.061*"election" + 0.041*"party" + 0.030*"seat" + 0.024*"vote" + 0.023*"elect" + 0.021*"candidate" + 0.019*"council" + 0.012*"constituency" + 0.012*"parliament" + 0.010*"assembly"
topic #15 (0.025): 0.066*"game" + 0.038*"season" + 0.023*"player" + 0.017*"basketball" + 0.012*"conference" + 0.011*"point" + 0.009*"yard" + 0.008*"baseball" + 0.008*"college" + 0.007*"hit"
topic #29 (0.025): 0.025*"danish" + 0.024*"commune" + 0.023*"russian" + 0.022*"dutch" + 0.021*"village" + 0.017*"german" + 0.016*"denmark" + 0.014*"ranger" + 0.014*"flower" + 0.014*"north"
topic diff=0.270261, rho=0.137575
PROGRESS: pass 2, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #32 (0.025): 0.018*"research" + 0.018*"study" + 0.011*"society" + 0.011*"science" + 0.010*"social" + 0.007*"professor" + 0.006*"scientific" + 0.006*"theory" + 0.006*"idea" + 0.005*"human"
topic #25 (0.025): 0.014*"elect" + 0.014*"president" + 0.012*"woman" + 0.011*"appoint" + 0.010*"election" + 0.008*"vote" + 0.008*"law" + 0.008*"general" + 0.007*"office" + 0.007*"party"
topic #26 (0.025): 0.061*"election" + 0.043*"party" + 0.029*"seat" + 0.024*"vote" + 0.022*"elect" + 0.021*"candidate" + 0.018*"council" + 0.013*"parliament" + 0.012*"constituency" + 0.012*"labour"
topic #15 (0.025): 0.065*"game" + 0.037*"season" + 0.022*"player" + 0.016*"basketball" + 0.013*"conference" + 0.011*"point" + 0.010*"yard" + 0.008*"field" + 0.008*"baseball" + 0.007*"college"
topic #17 (0.025): 0.028*"company" + 0.011*"business" + 0.008*"market" + 0.008*"provide" + 0.007*"product" + 0.006*"project" + 0.006*"development" + 0.006*"public" + 0.006*"industry" + 0.005*"management"
topic diff=0.266991, rho=0.137575
PROGRESS: pass 2, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.025): 0.013*"king" + 0.013*"son" + 0.009*"war" + 0.007*"death" + 0.007*"french" + 0.006*"military" + 0.006*"brother" + 0.006*"force" + 0.006*"father" + 0.005*"daughter"
topic #5 (0.025): 0.008*"flag" + 0.008*"life" + 0.007*"woman" + 0.007*"temple" + 0.006*"accord" + 0.006*"tradition" + 0.005*"religious" + 0.005*"say" + 0.004*"ancient" + 0.004*"event"
topic #16 (0.025): 0.026*"german" + 0.021*"war" + 0.020*"army" + 0.016*"military" + 0.015*"unit" + 0.015*"aircraft" + 0.014*"air" + 0.012*"command" + 0.012*"force" + 0.012*"operation"
topic #39 (0.025): 0.013*"treatment" + 0.013*"patient" + 0.012*"cause" + 0.011*"disease" + 0.008*"virus" + 0.008*"skin" + 0.007*"drug" + 0.006*"infection" + 0.006*"risk" + 0.006*"occur"
topic #22 (0.025): 0.019*"bridge" + 0.017*"street" + 0.011*"construction" + 0.010*"park" + 0.010*"road" + 0.009*"line" + 0.008*"plan" + 0.007*"site" + 0.007*"railroad" + 0.006*"tunnel"
topic diff=0.261920, rho=0.137575
PROGRESS: pass 2, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.013*"mine" + 0.011*"gold" + 0.010*"fire" + 0.010*"black" + 0.008*"produce" + 0.007*"wear" + 0.007*"tornado" + 0.006*"coal" + 0.006*"mining" + 0.006*"fish"
topic #8 (0.025): 0.036*"album" + 0.033*"song" + 0.029*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.006*"singer"
topic #16 (0.025): 0.033*"german" + 0.022*"war" + 0.021*"army" + 0.016*"military" + 0.015*"unit" + 0.014*"aircraft" + 0.014*"air" + 0.013*"command" + 0.013*"force" + 0.011*"operation"
topic #27 (0.025): 0.013*"protein" + 0.012*"cell" + 0.009*"gene" + 0.007*"human" + 0.007*"structure" + 0.005*"increase" + 0.005*"effect" + 0.005*"enzyme" + 0.004*"socket" + 0.004*"process"
topic #39 (0.025): 0.013*"treatment" + 0.013*"patient" + 0.012*"cause" + 0.012*"disease" + 0.007*"drug" + 0.007*"virus" + 0.007*"skin" + 0.006*"body" + 0.006*"infection" + 0.006*"blood"
topic diff=0.246895, rho=0.137575
PROGRESS: pass 2, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.063*"election" + 0.048*"party" + 0.028*"seat" + 0.027*"vote" + 0.022*"elect" + 0.021*"candidate" + 0.017*"council" + 0.014*"parliament" + 0.012*"constituency" + 0.011*"labour"
topic #19 (0.025): 0.033*"building" + 0.026*"church" + 0.019*"house" + 0.010*"design" + 0.008*"site" + 0.008*"th_century" + 0.007*"stone" + 0.007*"wall" + 0.006*"tower" + 0.006*"room"
topic #13 (0.025): 0.020*"university" + 0.015*"program" + 0.014*"student" + 0.012*"college" + 0.011*"department" + 0.009*"organization" + 0.009*"education" + 0.008*"international" + 0.008*"hospital" + 0.008*"health"
topic #3 (0.025): 0.014*"design" + 0.011*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.007*"vehicle" + 0.006*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #31 (0.025): 0.021*"match" + 0.020*"cricket" + 0.015*"stadium" + 0.013*"wale" + 0.013*"test" + 0.012*"class" + 0.010*"wicket" + 0.010*"welsh" + 0.009*"ground" + 0.007*"somerset"
topic diff=0.234985, rho=0.137575
PROGRESS: pass 2, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #31 (0.025): 0.022*"match" + 0.020*"cricket" + 0.015*"stadium" + 0.014*"test" + 0.014*"wale" + 0.013*"class" + 0.011*"welsh" + 0.010*"wicket" + 0.009*"ground" + 0.007*"australian"
topic #9 (0.025): 0.029*"series" + 0.020*"episode" + 0.018*"television" + 0.010*"award" + 0.009*"tv" + 0.008*"role" + 0.008*"appear" + 0.007*"actor" + 0.007*"season" + 0.006*"air"
topic #3 (0.025): 0.014*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.008*"model" + 0.007*"vehicle" + 0.006*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #17 (0.025): 0.029*"company" + 0.011*"business" + 0.008*"market" + 0.007*"provide" + 0.007*"product" + 0.006*"development" + 0.006*"project" + 0.006*"sell" + 0.006*"management" + 0.006*"public"
topic #27 (0.025): 0.014*"cell" + 0.013*"protein" + 0.009*"gene" + 0.007*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"effect" + 0.004*"contain" + 0.004*"result" + 0.004*"process"
topic diff=0.225169, rho=0.137575
-8.325 per-word bound, 320.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.025): 0.018*"bridge" + 0.016*"street" + 0.012*"park" + 0.012*"construction" + 0.011*"road" + 0.008*"plan" + 0.008*"line" + 0.007*"site" + 0.006*"close" + 0.006*"complete"
topic #36 (0.025): 0.011*"system" + 0.009*"displaystyle" + 0.007*"example" + 0.006*"point" + 0.006*"datum" + 0.005*"function" + 0.005*"term" + 0.005*"method" + 0.005*"value" + 0.005*"model"
topic #6 (0.025): 0.059*"specie" + 0.021*"genus" + 0.017*"describe" + 0.008*"plant" + 0.007*"length" + 0.006*"brown" + 0.006*"flower" + 0.006*"occur" + 0.006*"plate" + 0.005*"endemic"
topic #28 (0.025): 0.028*"county" + 0.025*"town" + 0.020*"river" + 0.019*"road" + 0.017*"north" + 0.017*"age" + 0.015*"population" + 0.015*"south" + 0.014*"km" + 0.012*"village"
topic #20 (0.025): 0.038*"book" + 0.032*"publish" + 0.012*"novel" + 0.011*"author" + 0.010*"magazine" + 0.010*"writer" + 0.008*"story" + 0.008*"editor" + 0.008*"publication" + 0.007*"newspaper"
topic diff=0.219500, rho=0.137575
-8.328 per-word bound, 321.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.013*"mine" + 0.011*"gold" + 0.010*"fire" + 0.009*"black" + 0.008*"produce" + 0.007*"wear" + 0.007*"fish" + 0.006*"coal" + 0.006*"mining" + 0.006*"iron"
topic #38 (0.025): 0.049*"station" + 0.031*"radio" + 0.017*"broadcast" + 0.016*"jewish" + 0.014*"channel" + 0.012*"network" + 0.011*"christian" + 0.011*"news" + 0.010*"air" + 0.010*"local"
topic #5 (0.025): 0.007*"life" + 0.007*"temple" + 0.007*"woman" + 0.007*"tradition" + 0.007*"accord" + 0.006*"flag" + 0.005*"religious" + 0.005*"say" + 0.004*"text" + 0.004*"ritual"
topic #19 (0.025): 0.033*"building" + 0.025*"church" + 0.020*"house" + 0.010*"design" + 0.008*"site" + 0.008*"th_century" + 0.008*"stone" + 0.007*"wall" + 0.006*"tower" + 0.006*"style"
topic #10 (0.025): 0.015*"king" + 0.014*"son" + 0.009*"war" + 0.008*"death" + 0.007*"french" + 0.006*"daughter" + 0.006*"father" + 0.006*"brother" + 0.005*"force" + 0.005*"military"
topic diff=0.219887, rho=0.137575
-8.332 per-word bound, 322.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #11 (0.025): 0.073*"game" + 0.026*"player" + 0.016*"version" + 0.013*"character" + 0.010*"video" + 0.009*"user" + 0.009*"file" + 0.008*"computer" + 0.008*"system" + 0.006*"available"
topic #16 (0.025): 0.028*"german" + 0.023*"war" + 0.021*"army" + 0.018*"military" + 0.016*"unit" + 0.013*"aircraft" + 0.013*"operation" + 0.012*"force" + 0.012*"air" + 0.012*"command"
topic #8 (0.025): 0.035*"album" + 0.033*"song" + 0.030*"music" + 0.023*"band" + 0.014*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.006*"video"
topic #17 (0.025): 0.030*"company" + 0.011*"business" + 0.008*"market" + 0.008*"provide" + 0.006*"development" + 0.006*"product" + 0.006*"public" + 0.006*"management" + 0.006*"project" + 0.006*"industry"
topic #27 (0.025): 0.016*"cell" + 0.012*"protein" + 0.009*"gene" + 0.007*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"effect" + 0.005*"process" + 0.004*"result" + 0.004*"nm"
topic diff=0.203616, rho=0.137575
-8.241 per-word bound, 302.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.025): 0.099*"film" + 0.018*"star" + 0.013*"direct" + 0.009*"story" + 0.009*"movie" + 0.009*"character" + 0.006*"director" + 0.006*"role" + 0.005*"appear" + 0.005*"earth"
topic #26 (0.025): 0.065*"election" + 0.049*"party" + 0.030*"vote" + 0.027*"seat" + 0.025*"elect" + 0.022*"candidate" + 0.017*"council" + 0.015*"parliament" + 0.013*"labour" + 0.011*"constituency"
topic #12 (0.025): 0.074*"station" + 0.064*"line" + 0.040*"railway" + 0.028*"train" + 0.018*"operate" + 0.018*"bus" + 0.014*"airport" + 0.014*"rail" + 0.012*"passenger" + 0.011*"platform"
topic #14 (0.025): 0.009*"water" + 0.009*"park" + 0.008*"forest" + 0.007*"tree" + 0.007*"site" + 0.006*"animal" + 0.006*"plant" + 0.006*"bird" + 0.004*"region" + 0.004*"land"
topic #39 (0.025): 0.015*"patient" + 0.013*"treatment" + 0.012*"cause" + 0.011*"disease" + 0.007*"drug" + 0.006*"medical" + 0.006*"virus" + 0.006*"occur" + 0.006*"risk" + 0.006*"increase"
topic diff=0.197964, rho=0.136291
PROGRESS: pass 3, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.020*"university" + 0.015*"program" + 0.013*"student" + 0.013*"college" + 0.012*"department" + 0.010*"education" + 0.009*"organization" + 0.009*"international" + 0.009*"hospital" + 0.008*"medical"
topic #3 (0.025): 0.014*"design" + 0.011*"engine" + 0.011*"power" + 0.010*"system" + 0.009*"vehicle" + 0.009*"model" + 0.006*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"control"
topic #23 (0.025): 0.016*"government" + 0.011*"law" + 0.010*"court" + 0.008*"act" + 0.007*"report" + 0.006*"case" + 0.006*"right" + 0.006*"say" + 0.005*"political" + 0.005*"issue"
topic #14 (0.025): 0.009*"water" + 0.009*"park" + 0.008*"forest" + 0.008*"tree" + 0.007*"site" + 0.006*"animal" + 0.006*"bird" + 0.006*"plant" + 0.004*"region" + 0.004*"land"
topic #36 (0.025): 0.011*"displaystyle" + 0.011*"system" + 0.008*"example" + 0.006*"point" + 0.006*"datum" + 0.005*"method" + 0.005*"value" + 0.005*"function" + 0.005*"term" + 0.005*"model"
topic diff=0.188384, rho=0.136291
PROGRESS: pass 3, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.025): 0.009*"water" + 0.009*"park" + 0.008*"forest" + 0.008*"tree" + 0.007*"site" + 0.006*"animal" + 0.006*"bird" + 0.006*"plant" + 0.004*"region" + 0.004*"land"
topic #16 (0.025): 0.025*"german" + 0.024*"war" + 0.021*"army" + 0.018*"military" + 0.016*"unit" + 0.014*"aircraft" + 0.013*"operation" + 0.013*"force" + 0.012*"air" + 0.012*"command"
topic #7 (0.025): 0.053*"festival" + 0.024*"irish" + 0.021*"swedish" + 0.017*"finnish" + 0.012*"hungarian" + 0.011*"finland" + 0.011*"saint" + 0.010*"nhl" + 0.009*"sign" + 0.009*"san_jose"
topic #23 (0.025): 0.016*"government" + 0.011*"law" + 0.011*"court" + 0.008*"act" + 0.006*"report" + 0.006*"case" + 0.006*"right" + 0.006*"say" + 0.005*"political" + 0.005*"issue"
topic #37 (0.025): 0.034*"ship" + 0.023*"island" + 0.010*"sea" + 0.009*"port" + 0.009*"vessel" + 0.009*"fleet" + 0.009*"navy" + 0.009*"boat" + 0.008*"crew" + 0.008*"coast"
topic diff=0.177628, rho=0.136291
PROGRESS: pass 3, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.025): 0.034*"season" + 0.033*"club" + 0.022*"league" + 0.020*"football" + 0.018*"match" + 0.017*"championship" + 0.016*"final" + 0.014*"player" + 0.014*"game" + 0.011*"goal"
topic #34 (0.025): 0.034*"student" + 0.024*"college" + 0.014*"grade" + 0.014*"child" + 0.011*"education" + 0.010*"teacher" + 0.009*"bishop" + 0.008*"class" + 0.007*"attend" + 0.006*"boy"
topic #3 (0.025): 0.014*"design" + 0.011*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.009*"vehicle" + 0.007*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #27 (0.025): 0.014*"cell" + 0.011*"protein" + 0.009*"gene" + 0.007*"human" + 0.007*"structure" + 0.005*"increase" + 0.005*"process" + 0.005*"effect" + 0.004*"material" + 0.004*"result"
topic #32 (0.025): 0.018*"study" + 0.018*"research" + 0.012*"society" + 0.011*"social" + 0.011*"science" + 0.007*"professor" + 0.007*"theory" + 0.006*"scientific" + 0.006*"idea" + 0.005*"human"
topic diff=0.176150, rho=0.136291
PROGRESS: pass 3, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 10
PROGRESS: pass 3, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.025): 0.015*"president" + 0.014*"elect" + 0.013*"appoint" + 0.012*"woman" + 0.009*"law" + 0.008*"general" + 0.008*"election" + 0.008*"office" + 0.007*"politician" + 0.007*"minister"
topic #20 (0.025): 0.038*"book" + 0.032*"publish" + 0.013*"novel" + 0.011*"author" + 0.011*"writer" + 0.010*"magazine" + 0.009*"story" + 0.008*"editor" + 0.008*"publication" + 0.007*"newspaper"
topic #27 (0.025): 0.013*"cell" + 0.011*"protein" + 0.009*"gene" + 0.007*"structure" + 0.007*"human" + 0.006*"increase" + 0.005*"process" + 0.005*"effect" + 0.005*"chemical" + 0.005*"nm"
topic #0 (0.025): 0.050*"language" + 0.026*"word" + 0.014*"refer" + 0.013*"define" + 0.012*"japanese" + 0.011*"dialect" + 0.010*"speak" + 0.010*"mean" + 0.010*"nationality" + 0.010*"vowel"
topic #35 (0.025): 0.048*"race" + 0.043*"event" + 0.022*"compete" + 0.020*"championship" + 0.019*"finish" + 0.010*"car" + 0.010*"point" + 0.009*"metre" + 0.009*"winner" + 0.009*"sport"
topic diff=0.166528, rho=0.136291
PROGRESS: pass 3, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #36 (0.025): 0.011*"system" + 0.011*"displaystyle" + 0.008*"example" + 0.006*"datum" + 0.006*"value" + 0.006*"point" + 0.005*"method" + 0.005*"term" + 0.005*"function" + 0.005*"process"
topic #18 (0.025): 0.050*"village" + 0.048*"population" + 0.028*"municipality" + 0.021*"region" + 0.017*"census" + 0.016*"town" + 0.013*"central" + 0.013*"province" + 0.012*"rural" + 0.011*"total"
topic #34 (0.025): 0.036*"student" + 0.025*"college" + 0.014*"child" + 0.013*"grade" + 0.012*"education" + 0.010*"teacher" + 0.009*"bishop" + 0.008*"class" + 0.007*"attend" + 0.006*"boy"
topic #1 (0.025): 0.100*"film" + 0.019*"star" + 0.013*"direct" + 0.010*"story" + 0.009*"movie" + 0.008*"character" + 0.006*"director" + 0.006*"role" + 0.005*"appear" + 0.005*"earth"
topic #3 (0.025): 0.014*"design" + 0.011*"engine" + 0.010*"power" + 0.010*"vehicle" + 0.010*"system" + 0.010*"model" + 0.007*"car" + 0.006*"produce" + 0.005*"standard" + 0.005*"control"
topic diff=0.168213, rho=0.136291
PROGRESS: pass 3, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.025): 0.046*"russian" + 0.033*"dutch" + 0.024*"commune" + 0.024*"polish" + 0.024*"german" + 0.023*"danish" + 0.022*"village" + 0.016*"netherland" + 0.016*"north" + 0.016*"denmark"
topic #39 (0.025): 0.014*"patient" + 0.013*"treatment" + 0.012*"cause" + 0.012*"disease" + 0.009*"virus" + 0.009*"drug" + 0.007*"medical" + 0.006*"infection" + 0.006*"chess" + 0.006*"blood"
topic #5 (0.025): 0.008*"life" + 0.007*"flag" + 0.007*"tradition" + 0.007*"temple" + 0.006*"accord" + 0.006*"woman" + 0.005*"religious" + 0.005*"say" + 0.005*"ancient" + 0.004*"buddhist"
topic #7 (0.025): 0.052*"festival" + 0.034*"irish" + 0.022*"swedish" + 0.015*"finnish" + 0.014*"hungarian" + 0.011*"finland" + 0.010*"saint" + 0.010*"nhl" + 0.009*"sign" + 0.009*"tamil"
topic #4 (0.025): 0.024*"battle" + 0.021*"attack" + 0.018*"fight" + 0.015*"force" + 0.014*"army" + 0.009*"troop" + 0.008*"division" + 0.008*"japanese" + 0.008*"enemy" + 0.008*"soldier"
topic diff=0.153836, rho=0.136291
PROGRESS: pass 3, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.025): 0.041*"book" + 0.033*"publish" + 0.014*"novel" + 0.012*"author" + 0.011*"writer" + 0.010*"magazine" + 0.009*"story" + 0.008*"editor" + 0.008*"publication" + 0.007*"newspaper"
topic #2 (0.025): 0.035*"club" + 0.034*"season" + 0.023*"league" + 0.021*"football" + 0.017*"championship" + 0.017*"match" + 0.016*"final" + 0.014*"player" + 0.013*"game" + 0.012*"goal"
topic #29 (0.025): 0.048*"russian" + 0.034*"dutch" + 0.026*"german" + 0.025*"commune" + 0.024*"polish" + 0.022*"village" + 0.021*"danish" + 0.016*"netherland" + 0.016*"north" + 0.015*"denmark"
topic #9 (0.025): 0.032*"series" + 0.020*"episode" + 0.018*"television" + 0.012*"award" + 0.009*"tv" + 0.009*"season" + 0.008*"appear" + 0.008*"role" + 0.007*"air" + 0.007*"host"
topic #7 (0.025): 0.049*"festival" + 0.036*"irish" + 0.022*"swedish" + 0.016*"hungarian" + 0.015*"finnish" + 0.014*"freiburg" + 0.011*"finland" + 0.010*"saint" + 0.009*"nhl" + 0.009*"canadian"
topic diff=0.151536, rho=0.136291
PROGRESS: pass 3, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.025): 0.058*"specie" + 0.020*"genus" + 0.017*"describe" + 0.009*"plant" + 0.007*"white" + 0.007*"flower" + 0.007*"length" + 0.006*"brown" + 0.006*"endemic" + 0.006*"occur"
topic #12 (0.025): 0.072*"station" + 0.063*"line" + 0.039*"railway" + 0.029*"train" + 0.019*"operate" + 0.018*"bus" + 0.015*"airport" + 0.014*"rail" + 0.013*"passenger" + 0.013*"route"
topic #2 (0.025): 0.035*"club" + 0.034*"season" + 0.023*"league" + 0.021*"football" + 0.017*"championship" + 0.017*"final" + 0.017*"match" + 0.014*"player" + 0.013*"game" + 0.012*"goal"
topic #33 (0.025): 0.036*"company" + 0.027*"theatre" + 0.027*"french" + 0.021*"production" + 0.018*"store" + 0.015*"wine" + 0.014*"theater" + 0.013*"opera" + 0.011*"produce" + 0.010*"restaurant"
topic #19 (0.025): 0.035*"building" + 0.024*"church" + 0.020*"house" + 0.010*"design" + 0.009*"site" + 0.008*"th_century" + 0.007*"wall" + 0.007*"stone" + 0.006*"room" + 0.006*"tower"
topic diff=0.144498, rho=0.136291
PROGRESS: pass 3, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.025): 0.027*"german" + 0.024*"war" + 0.021*"army" + 0.018*"military" + 0.016*"unit" + 0.015*"aircraft" + 0.013*"air" + 0.013*"operation" + 0.013*"force" + 0.012*"command"
topic #3 (0.025): 0.014*"design" + 0.012*"engine" + 0.011*"power" + 0.010*"system" + 0.009*"vehicle" + 0.009*"model" + 0.007*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"control"
topic #27 (0.025): 0.014*"cell" + 0.011*"protein" + 0.009*"gene" + 0.007*"human" + 0.007*"structure" + 0.006*"increase" + 0.005*"chemical" + 0.005*"effect" + 0.005*"enzyme" + 0.005*"process"
topic #25 (0.025): 0.016*"president" + 0.014*"elect" + 0.013*"appoint" + 0.012*"woman" + 0.010*"law" + 0.008*"office" + 0.008*"general" + 0.007*"election" + 0.007*"representative" + 0.007*"position"
topic #21 (0.025): 0.009*"say" + 0.008*"tell" + 0.008*"get" + 0.006*"kill" + 0.006*"try" + 0.005*"back" + 0.005*"friend" + 0.005*"death" + 0.005*"life" + 0.005*"want"
topic diff=0.143444, rho=0.136291
PROGRESS: pass 3, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.014*"mine" + 0.013*"gold" + 0.009*"black" + 0.009*"wear" + 0.008*"fire" + 0.008*"produce" + 0.007*"iron" + 0.007*"tornado" + 0.006*"mining" + 0.006*"fish"
topic #28 (0.025): 0.030*"county" + 0.025*"town" + 0.025*"river" + 0.019*"road" + 0.018*"north" + 0.016*"age" + 0.015*"south" + 0.014*"km" + 0.014*"population" + 0.014*"route"
topic #12 (0.025): 0.076*"station" + 0.066*"line" + 0.038*"railway" + 0.029*"train" + 0.018*"operate" + 0.017*"bus" + 0.016*"airport" + 0.013*"passenger" + 0.013*"rail" + 0.012*"route"
topic #21 (0.025): 0.009*"say" + 0.008*"get" + 0.008*"tell" + 0.006*"try" + 0.006*"kill" + 0.006*"back" + 0.005*"friend" + 0.005*"life" + 0.005*"death" + 0.005*"child"
topic #16 (0.025): 0.026*"german" + 0.024*"war" + 0.021*"army" + 0.018*"military" + 0.016*"unit" + 0.016*"aircraft" + 0.014*"air" + 0.013*"command" + 0.013*"operation" + 0.012*"force"
topic diff=0.142240, rho=0.136291
PROGRESS: pass 3, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.053*"game" + 0.047*"season" + 0.017*"player" + 0.014*"basketball" + 0.012*"conference" + 0.010*"point" + 0.010*"football" + 0.009*"yard" + 0.009*"college" + 0.009*"league"
topic #34 (0.025): 0.035*"student" + 0.024*"college" + 0.015*"child" + 0.012*"education" + 0.011*"grade" + 0.010*"teacher" + 0.010*"bishop" + 0.008*"class" + 0.008*"attend" + 0.007*"girl"
topic #5 (0.025): 0.008*"life" + 0.007*"accord" + 0.007*"temple" + 0.007*"tradition" + 0.006*"woman" + 0.006*"religious" + 0.005*"flag" + 0.005*"say" + 0.005*"ancient" + 0.005*"spiritual"
topic #22 (0.025): 0.019*"street" + 0.019*"bridge" + 0.018*"park" + 0.015*"road" + 0.012*"construction" + 0.009*"plan" + 0.008*"site" + 0.007*"close" + 0.007*"facility" + 0.006*"project"
topic #28 (0.025): 0.029*"county" + 0.026*"town" + 0.024*"river" + 0.019*"road" + 0.018*"north" + 0.017*"age" + 0.015*"south" + 0.014*"km" + 0.014*"population" + 0.013*"route"
topic diff=0.136834, rho=0.136291
PROGRESS: pass 3, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.013*"mine" + 0.013*"gold" + 0.012*"black" + 0.009*"wear" + 0.008*"produce" + 0.008*"fire" + 0.007*"coal" + 0.007*"mining" + 0.006*"iron" + 0.006*"fish"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.012*"college" + 0.011*"department" + 0.010*"international" + 0.010*"education" + 0.009*"organization" + 0.008*"hospital" + 0.008*"director"
topic #11 (0.025): 0.073*"game" + 0.026*"player" + 0.015*"version" + 0.013*"character" + 0.011*"computer" + 0.010*"video" + 0.010*"user" + 0.009*"system" + 0.008*"software" + 0.008*"card"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.007*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #23 (0.025): 0.015*"government" + 0.011*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.006*"right" + 0.006*"case" + 0.006*"say" + 0.005*"political" + 0.005*"claim"
topic diff=0.131991, rho=0.136291
PROGRESS: pass 3, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.025): 0.030*"german" + 0.025*"war" + 0.022*"army" + 0.020*"military" + 0.016*"unit" + 0.014*"aircraft" + 0.014*"air" + 0.013*"command" + 0.012*"operation" + 0.012*"force"
topic #36 (0.025): 0.011*"system" + 0.009*"displaystyle" + 0.008*"example" + 0.006*"point" + 0.006*"datum" + 0.006*"function" + 0.005*"term" + 0.005*"value" + 0.005*"method" + 0.005*"process"
topic #32 (0.025): 0.018*"study" + 0.018*"research" + 0.011*"society" + 0.011*"social" + 0.010*"science" + 0.007*"theory" + 0.007*"professor" + 0.006*"scientific" + 0.006*"idea" + 0.005*"human"
topic #33 (0.025): 0.038*"company" + 0.027*"french" + 0.027*"theatre" + 0.023*"production" + 0.020*"store" + 0.014*"wine" + 0.012*"theater" + 0.012*"produce" + 0.011*"restaurant" + 0.010*"sell"
topic #0 (0.025): 0.056*"language" + 0.028*"word" + 0.016*"refer" + 0.013*"japanese" + 0.012*"speak" + 0.012*"mean" + 0.011*"define" + 0.011*"dialect" + 0.010*"chinese" + 0.009*"english"
topic diff=0.123784, rho=0.136291
-8.326 per-word bound, 321.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.025): 0.050*"art" + 0.025*"museum" + 0.023*"award" + 0.023*"artist" + 0.016*"painting" + 0.010*"exhibition" + 0.009*"design" + 0.009*"collection" + 0.009*"paint" + 0.009*"study"
topic #29 (0.025): 0.064*"russian" + 0.039*"dutch" + 0.039*"german" + 0.029*"polish" + 0.022*"commune" + 0.022*"danish" + 0.020*"village" + 0.018*"netherland" + 0.017*"soviet" + 0.016*"denmark"
topic #33 (0.025): 0.038*"company" + 0.027*"french" + 0.027*"theatre" + 0.023*"production" + 0.020*"store" + 0.014*"wine" + 0.012*"theater" + 0.012*"produce" + 0.011*"restaurant" + 0.010*"sell"
topic #28 (0.025): 0.030*"county" + 0.025*"town" + 0.023*"river" + 0.019*"age" + 0.019*"north" + 0.018*"road" + 0.015*"population" + 0.015*"south" + 0.015*"km" + 0.012*"route"
topic #6 (0.025): 0.058*"specie" + 0.020*"genus" + 0.017*"describe" + 0.010*"plant" + 0.009*"white" + 0.008*"brown" + 0.008*"flower" + 0.007*"length" + 0.006*"black" + 0.006*"occur"
topic diff=0.120561, rho=0.136291
-8.331 per-word bound, 321.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #5 (0.025): 0.007*"life" + 0.007*"temple" + 0.007*"tradition" + 0.007*"accord" + 0.006*"religious" + 0.006*"woman" + 0.005*"ancient" + 0.005*"say" + 0.005*"text" + 0.005*"flag"
topic #27 (0.025): 0.017*"cell" + 0.013*"protein" + 0.010*"gene" + 0.008*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"process" + 0.005*"effect" + 0.005*"chemical" + 0.005*"material"
topic #8 (0.025): 0.037*"album" + 0.035*"song" + 0.031*"music" + 0.023*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.006*"singer"
topic #11 (0.025): 0.072*"game" + 0.026*"player" + 0.016*"version" + 0.012*"character" + 0.010*"computer" + 0.010*"user" + 0.010*"video" + 0.009*"system" + 0.009*"file" + 0.008*"software"
topic #3 (0.025): 0.015*"design" + 0.011*"power" + 0.010*"engine" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.007*"car" + 0.005*"produce" + 0.005*"type" + 0.005*"standard"
topic diff=0.118789, rho=0.136291
-8.331 per-word bound, 321.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #28 (0.025): 0.031*"county" + 0.026*"town" + 0.023*"river" + 0.018*"age" + 0.018*"road" + 0.018*"north" + 0.016*"km" + 0.015*"south" + 0.015*"population" + 0.013*"highway"
topic #29 (0.025): 0.063*"russian" + 0.038*"german" + 0.038*"dutch" + 0.037*"polish" + 0.023*"danish" + 0.022*"commune" + 0.021*"village" + 0.019*"soviet" + 0.018*"netherland" + 0.015*"north"
topic #39 (0.025): 0.013*"patient" + 0.012*"treatment" + 0.012*"cause" + 0.011*"disease" + 0.008*"drug" + 0.007*"medical" + 0.007*"risk" + 0.006*"increase" + 0.006*"occur" + 0.006*"virus"
topic #25 (0.025): 0.016*"president" + 0.014*"elect" + 0.014*"appoint" + 0.011*"woman" + 0.010*"law" + 0.009*"office" + 0.009*"general" + 0.008*"secretary" + 0.007*"position" + 0.007*"minister"
topic #31 (0.025): 0.025*"match" + 0.022*"australian" + 0.021*"cricket" + 0.016*"wale" + 0.016*"test" + 0.015*"australia" + 0.014*"class" + 0.012*"stadium" + 0.012*"welsh" + 0.011*"wicket"
topic diff=0.125436, rho=0.136291
-8.165 per-word bound, 287.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #34 (0.025): 0.033*"student" + 0.024*"college" + 0.017*"child" + 0.015*"grade" + 0.013*"education" + 0.012*"teacher" + 0.010*"bishop" + 0.008*"attend" + 0.008*"girl" + 0.007*"class"
topic #18 (0.025): 0.054*"village" + 0.049*"population" + 0.025*"municipality" + 0.022*"region" + 0.018*"town" + 0.017*"census" + 0.015*"province" + 0.015*"central" + 0.012*"total" + 0.011*"rural"
topic #4 (0.025): 0.024*"battle" + 0.022*"attack" + 0.018*"fight" + 0.015*"force" + 0.014*"army" + 0.010*"troop" + 0.009*"division" + 0.008*"fire" + 0.008*"war" + 0.007*"soldier"
topic #8 (0.025): 0.036*"album" + 0.035*"song" + 0.031*"music" + 0.025*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.007*"chart" + 0.007*"video"
topic #14 (0.025): 0.010*"water" + 0.008*"forest" + 0.008*"tree" + 0.008*"park" + 0.007*"site" + 0.007*"animal" + 0.006*"bird" + 0.005*"plant" + 0.005*"region" + 0.004*"mountain"
topic diff=0.113811, rho=0.135043
PROGRESS: pass 4, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.025): 0.015*"design" + 0.011*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.007*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"control"
topic #6 (0.025): 0.057*"specie" + 0.021*"genus" + 0.016*"describe" + 0.010*"plant" + 0.008*"white" + 0.008*"flower" + 0.007*"length" + 0.007*"brown" + 0.007*"yellow" + 0.006*"grow"
topic #10 (0.025): 0.015*"son" + 0.015*"king" + 0.009*"war" + 0.008*"death" + 0.006*"brother" + 0.006*"french" + 0.006*"father" + 0.006*"daughter" + 0.006*"prince" + 0.005*"royal"
topic #15 (0.025): 0.053*"game" + 0.049*"season" + 0.018*"player" + 0.015*"basketball" + 0.011*"football" + 0.010*"league" + 0.010*"yard" + 0.010*"conference" + 0.010*"coach" + 0.009*"baseball"
topic #7 (0.025): 0.051*"festival" + 0.037*"irish" + 0.030*"swedish" + 0.021*"canadian" + 0.019*"finnish" + 0.014*"hungarian" + 0.013*"saint" + 0.012*"finland" + 0.012*"nhl" + 0.010*"montreal"
topic diff=0.109315, rho=0.135043
PROGRESS: pass 4, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #31 (0.025): 0.026*"match" + 0.024*"australian" + 0.022*"cricket" + 0.016*"test" + 0.016*"australia" + 0.014*"wicket" + 0.014*"wale" + 0.014*"class" + 0.012*"stadium" + 0.011*"welsh"
topic #27 (0.025): 0.015*"cell" + 0.012*"protein" + 0.009*"gene" + 0.008*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"material" + 0.005*"contain" + 0.005*"process" + 0.005*"reaction"
topic #0 (0.025): 0.053*"language" + 0.027*"word" + 0.014*"japanese" + 0.014*"refer" + 0.013*"define" + 0.012*"dialect" + 0.011*"speak" + 0.010*"mean" + 0.010*"nationality" + 0.009*"english"
topic #3 (0.025): 0.015*"design" + 0.011*"engine" + 0.011*"power" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.007*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #32 (0.025): 0.018*"study" + 0.016*"research" + 0.012*"society" + 0.011*"social" + 0.010*"science" + 0.007*"theory" + 0.007*"professor" + 0.006*"idea" + 0.005*"scientific" + 0.005*"human"
topic diff=0.102367, rho=0.135043
PROGRESS: pass 4, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.025): 0.026*"war" + 0.022*"army" + 0.021*"military" + 0.021*"german" + 0.017*"unit" + 0.015*"aircraft" + 0.014*"operation" + 0.014*"force" + 0.013*"air" + 0.012*"command"
topic #2 (0.025): 0.036*"club" + 0.033*"season" + 0.022*"league" + 0.019*"match" + 0.018*"football" + 0.017*"final" + 0.016*"championship" + 0.015*"player" + 0.014*"game" + 0.012*"goal"
topic #39 (0.025): 0.016*"patient" + 0.012*"cause" + 0.012*"treatment" + 0.012*"disease" + 0.009*"virus" + 0.008*"drug" + 0.008*"medical" + 0.006*"chess" + 0.006*"increase" + 0.006*"body"
topic #4 (0.025): 0.025*"battle" + 0.022*"attack" + 0.018*"fight" + 0.017*"force" + 0.015*"army" + 0.009*"troop" + 0.008*"war" + 0.008*"kill" + 0.008*"division" + 0.008*"soldier"
topic #7 (0.025): 0.051*"festival" + 0.036*"irish" + 0.029*"swedish" + 0.025*"canadian" + 0.018*"finnish" + 0.018*"hungarian" + 0.012*"saint" + 0.012*"nhl" + 0.011*"finland" + 0.011*"quebec"
topic diff=0.103244, rho=0.135043
PROGRESS: pass 4, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #21 (0.025): 0.010*"say" + 0.008*"tell" + 0.008*"get" + 0.006*"kill" + 0.006*"try" + 0.006*"back" + 0.005*"death" + 0.005*"life" + 0.005*"want" + 0.005*"help"
topic #30 (0.025): 0.014*"gold" + 0.013*"mine" + 0.012*"black" + 0.009*"coal" + 0.009*"produce" + 0.008*"wear" + 0.007*"iron" + 0.007*"fire" + 0.006*"fish" + 0.006*"mining"
topic #9 (0.025): 0.032*"series" + 0.021*"episode" + 0.018*"television" + 0.011*"award" + 0.009*"season" + 0.009*"appear" + 0.009*"role" + 0.009*"tv" + 0.007*"character" + 0.007*"actor"
topic #12 (0.025): 0.073*"station" + 0.067*"line" + 0.038*"railway" + 0.028*"train" + 0.020*"operate" + 0.018*"bus" + 0.017*"airport" + 0.015*"route" + 0.013*"rail" + 0.013*"passenger"
topic #26 (0.025): 0.070*"election" + 0.051*"party" + 0.038*"vote" + 0.031*"seat" + 0.027*"elect" + 0.027*"candidate" + 0.018*"council" + 0.017*"parliament" + 0.011*"constituency" + 0.010*"assembly"
topic diff=0.096094, rho=0.135043
PROGRESS: pass 4, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #4 (0.025): 0.023*"battle" + 0.021*"attack" + 0.018*"force" + 0.018*"fight" + 0.015*"army" + 0.010*"troop" + 0.009*"japanese" + 0.009*"war" + 0.008*"soldier" + 0.008*"kill"
topic #33 (0.025): 0.040*"company" + 0.031*"french" + 0.028*"theatre" + 0.021*"production" + 0.018*"store" + 0.015*"theater" + 0.013*"opera" + 0.011*"wine" + 0.011*"produce" + 0.010*"restaurant"
topic #17 (0.025): 0.030*"company" + 0.011*"business" + 0.009*"market" + 0.008*"product" + 0.007*"provide" + 0.007*"industry" + 0.006*"public" + 0.006*"purchase" + 0.006*"consumer" + 0.006*"sell"
topic #36 (0.025): 0.012*"displaystyle" + 0.012*"system" + 0.008*"example" + 0.006*"datum" + 0.006*"value" + 0.006*"point" + 0.005*"method" + 0.005*"function" + 0.005*"term" + 0.005*"process"
topic #31 (0.025): 0.030*"australian" + 0.025*"match" + 0.022*"cricket" + 0.020*"australia" + 0.016*"test" + 0.015*"wale" + 0.013*"stadium" + 0.013*"class" + 0.012*"welsh" + 0.011*"wicket"
topic diff=0.104032, rho=0.135043
PROGRESS: pass 4, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #16 (0.025): 0.026*"war" + 0.022*"army" + 0.021*"military" + 0.021*"german" + 0.017*"unit" + 0.015*"aircraft" + 0.014*"operation" + 0.013*"force" + 0.013*"command" + 0.013*"air"
topic #0 (0.025): 0.053*"language" + 0.025*"word" + 0.016*"japanese" + 0.015*"refer" + 0.011*"define" + 0.011*"mean" + 0.011*"chinese" + 0.010*"speak" + 0.010*"dialect" + 0.009*"english"
topic #24 (0.025): 0.050*"art" + 0.026*"museum" + 0.023*"artist" + 0.022*"award" + 0.015*"painting" + 0.012*"exhibition" + 0.010*"design" + 0.010*"collection" + 0.009*"paint" + 0.009*"study"
topic #4 (0.025): 0.023*"battle" + 0.021*"attack" + 0.018*"fight" + 0.017*"force" + 0.015*"army" + 0.010*"troop" + 0.009*"japanese" + 0.009*"war" + 0.008*"soldier" + 0.008*"fire"
topic #17 (0.025): 0.029*"company" + 0.011*"business" + 0.009*"market" + 0.007*"provide" + 0.007*"product" + 0.007*"industry" + 0.006*"public" + 0.006*"purchase" + 0.006*"sell" + 0.006*"development"
topic diff=0.089801, rho=0.135043
PROGRESS: pass 4, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #27 (0.025): 0.015*"cell" + 0.012*"protein" + 0.009*"gene" + 0.007*"structure" + 0.007*"human" + 0.006*"increase" + 0.005*"enzyme" + 0.005*"chemical" + 0.005*"effect" + 0.005*"reaction"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.013*"college" + 0.011*"department" + 0.010*"international" + 0.010*"education" + 0.010*"hospital" + 0.010*"organization" + 0.008*"study"
topic #14 (0.025): 0.011*"water" + 0.009*"forest" + 0.008*"park" + 0.007*"tree" + 0.007*"animal" + 0.007*"bird" + 0.006*"site" + 0.005*"plant" + 0.005*"mountain" + 0.005*"region"
topic #21 (0.025): 0.010*"say" + 0.008*"tell" + 0.008*"get" + 0.006*"try" + 0.006*"kill" + 0.006*"back" + 0.005*"death" + 0.005*"life" + 0.005*"help" + 0.005*"friend"
topic #18 (0.025): 0.052*"village" + 0.049*"population" + 0.028*"municipality" + 0.024*"region" + 0.021*"town" + 0.016*"census" + 0.015*"province" + 0.012*"central" + 0.012*"rural" + 0.010*"total"
topic diff=0.089277, rho=0.135043
PROGRESS: pass 4, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.052*"game" + 0.051*"season" + 0.017*"player" + 0.015*"basketball" + 0.014*"football" + 0.012*"league" + 0.011*"conference" + 0.011*"coach" + 0.010*"point" + 0.010*"baseball"
topic #23 (0.025): 0.017*"government" + 0.011*"law" + 0.009*"court" + 0.008*"act" + 0.007*"right" + 0.007*"case" + 0.007*"report" + 0.007*"say" + 0.005*"political" + 0.005*"issue"
topic #20 (0.025): 0.042*"book" + 0.035*"publish" + 0.014*"novel" + 0.012*"author" + 0.011*"writer" + 0.011*"magazine" + 0.009*"story" + 0.009*"editor" + 0.008*"publication" + 0.007*"newspaper"
topic #0 (0.025): 0.053*"language" + 0.026*"word" + 0.017*"japanese" + 0.015*"refer" + 0.012*"chinese" + 0.011*"mean" + 0.011*"speak" + 0.011*"define" + 0.010*"dialect" + 0.009*"english"
topic #6 (0.025): 0.057*"specie" + 0.020*"genus" + 0.017*"describe" + 0.011*"plant" + 0.010*"white" + 0.008*"flower" + 0.007*"brown" + 0.007*"black" + 0.007*"length" + 0.007*"yellow"
topic diff=0.089219, rho=0.135043
PROGRESS: pass 4, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.014*"gold" + 0.013*"mine" + 0.013*"black" + 0.010*"wear" + 0.009*"produce" + 0.007*"iron" + 0.007*"fire" + 0.007*"mining" + 0.007*"coal" + 0.006*"fish"
topic #26 (0.025): 0.071*"election" + 0.056*"party" + 0.037*"vote" + 0.029*"seat" + 0.028*"elect" + 0.026*"candidate" + 0.018*"council" + 0.016*"parliament" + 0.012*"assembly" + 0.011*"labour"
topic #11 (0.025): 0.073*"game" + 0.028*"player" + 0.014*"version" + 0.014*"character" + 0.011*"computer" + 0.010*"card" + 0.009*"video" + 0.009*"user" + 0.009*"software" + 0.009*"system"
topic #25 (0.025): 0.018*"president" + 0.015*"appoint" + 0.014*"elect" + 0.012*"woman" + 0.011*"law" + 0.009*"office" + 0.008*"general" + 0.008*"position" + 0.008*"secretary" + 0.007*"chairman"
topic #28 (0.025): 0.032*"county" + 0.027*"river" + 0.025*"town" + 0.019*"north" + 0.018*"age" + 0.018*"road" + 0.016*"km" + 0.015*"south" + 0.015*"population" + 0.014*"highway"
topic diff=0.086812, rho=0.135043
PROGRESS: pass 4, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.038*"company" + 0.030*"french" + 0.029*"theatre" + 0.022*"production" + 0.018*"store" + 0.014*"wine" + 0.013*"opera" + 0.013*"theater" + 0.012*"produce" + 0.011*"restaurant"
topic #32 (0.025): 0.017*"study" + 0.017*"research" + 0.012*"social" + 0.011*"society" + 0.009*"science" + 0.007*"theory" + 0.007*"professor" + 0.006*"idea" + 0.006*"scientific" + 0.005*"human"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.012*"college" + 0.012*"department" + 0.011*"international" + 0.010*"education" + 0.010*"organization" + 0.009*"hospital" + 0.009*"research"
topic #26 (0.025): 0.071*"election" + 0.056*"party" + 0.039*"vote" + 0.029*"seat" + 0.028*"elect" + 0.027*"candidate" + 0.017*"council" + 0.016*"parliament" + 0.011*"assembly" + 0.011*"constituency"
topic #16 (0.025): 0.026*"war" + 0.021*"army" + 0.020*"german" + 0.020*"military" + 0.017*"unit" + 0.016*"aircraft" + 0.014*"air" + 0.014*"operation" + 0.013*"command" + 0.012*"force"
topic diff=0.087727, rho=0.135043
PROGRESS: pass 4, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.071*"election" + 0.056*"party" + 0.040*"vote" + 0.028*"seat" + 0.027*"elect" + 0.027*"candidate" + 0.017*"council" + 0.016*"parliament" + 0.011*"assembly" + 0.011*"labour"
topic #18 (0.025): 0.052*"village" + 0.049*"population" + 0.027*"municipality" + 0.024*"region" + 0.021*"town" + 0.016*"province" + 0.015*"census" + 0.013*"central" + 0.012*"rural" + 0.010*"country"
topic #38 (0.025): 0.052*"station" + 0.039*"radio" + 0.019*"broadcast" + 0.019*"channel" + 0.017*"jewish" + 0.016*"network" + 0.013*"air" + 0.012*"news" + 0.011*"program" + 0.011*"tv"
topic #8 (0.025): 0.038*"album" + 0.036*"song" + 0.031*"music" + 0.025*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"singer"
topic #32 (0.025): 0.017*"study" + 0.017*"research" + 0.012*"social" + 0.011*"society" + 0.009*"science" + 0.007*"theory" + 0.007*"professor" + 0.006*"idea" + 0.006*"scientific" + 0.005*"human"
topic diff=0.087727, rho=0.135043
PROGRESS: pass 4, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.038*"company" + 0.031*"french" + 0.027*"theatre" + 0.022*"production" + 0.019*"store" + 0.015*"wine" + 0.013*"theater" + 0.012*"produce" + 0.012*"opera" + 0.011*"sell"
topic #28 (0.025): 0.032*"county" + 0.025*"river" + 0.025*"town" + 0.019*"age" + 0.019*"north" + 0.018*"road" + 0.016*"km" + 0.015*"population" + 0.015*"south" + 0.013*"highway"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.008*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #15 (0.025): 0.051*"season" + 0.050*"game" + 0.016*"player" + 0.015*"football" + 0.015*"basketball" + 0.012*"league" + 0.011*"coach" + 0.011*"conference" + 0.009*"point" + 0.009*"college"
topic #17 (0.025): 0.031*"company" + 0.012*"business" + 0.009*"market" + 0.007*"provide" + 0.007*"product" + 0.006*"industry" + 0.006*"sell" + 0.006*"public" + 0.006*"increase" + 0.006*"management"
topic diff=0.084611, rho=0.135043
PROGRESS: pass 4, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #35 (0.025): 0.045*"race" + 0.044*"event" + 0.026*"compete" + 0.024*"championship" + 0.020*"finish" + 0.012*"sport" + 0.012*"woman" + 0.011*"competition" + 0.010*"point" + 0.009*"athlete"
topic #0 (0.025): 0.056*"language" + 0.027*"word" + 0.018*"japanese" + 0.017*"refer" + 0.013*"chinese" + 0.012*"mean" + 0.012*"speak" + 0.010*"dialect" + 0.010*"english" + 0.010*"define"
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.010*"court" + 0.008*"act" + 0.007*"report" + 0.006*"case" + 0.006*"right" + 0.006*"say" + 0.005*"issue" + 0.005*"claim"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.012*"college" + 0.012*"department" + 0.011*"international" + 0.010*"education" + 0.009*"organization" + 0.009*"hospital" + 0.009*"research"
topic #27 (0.025): 0.016*"cell" + 0.014*"protein" + 0.010*"gene" + 0.008*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"effect" + 0.005*"process" + 0.005*"contain" + 0.005*"chemical"
topic diff=0.078223, rho=0.135043
-8.288 per-word bound, 312.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.008*"model" + 0.008*"car" + 0.007*"vehicle" + 0.005*"produce" + 0.005*"type" + 0.005*"control"
topic #20 (0.025): 0.043*"book" + 0.036*"publish" + 0.014*"novel" + 0.013*"author" + 0.012*"magazine" + 0.011*"writer" + 0.010*"story" + 0.009*"editor" + 0.009*"publication" + 0.008*"newspaper"
topic #10 (0.025): 0.016*"king" + 0.015*"son" + 0.009*"war" + 0.008*"death" + 0.006*"father" + 0.006*"daughter" + 0.006*"brother" + 0.006*"french" + 0.006*"royal" + 0.006*"kingdom"
topic #12 (0.025): 0.071*"station" + 0.069*"line" + 0.037*"railway" + 0.028*"train" + 0.020*"operate" + 0.020*"airport" + 0.017*"bus" + 0.014*"route" + 0.014*"passenger" + 0.013*"rail"
topic #11 (0.025): 0.072*"game" + 0.025*"player" + 0.015*"version" + 0.012*"computer" + 0.011*"character" + 0.011*"user" + 0.010*"video" + 0.009*"software" + 0.009*"system" + 0.008*"card"
topic diff=0.075608, rho=0.135043
-8.293 per-word bound, 313.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #9 (0.025): 0.035*"series" + 0.023*"episode" + 0.018*"television" + 0.012*"award" + 0.009*"role" + 0.009*"appear" + 0.009*"season" + 0.009*"tv" + 0.008*"character" + 0.007*"actor"
topic #17 (0.025): 0.031*"company" + 0.012*"business" + 0.009*"market" + 0.007*"provide" + 0.007*"product" + 0.006*"industry" + 0.006*"sell" + 0.006*"increase" + 0.006*"public" + 0.006*"development"
topic #32 (0.025): 0.017*"study" + 0.016*"research" + 0.011*"society" + 0.011*"social" + 0.009*"science" + 0.007*"theory" + 0.006*"professor" + 0.006*"idea" + 0.005*"scientific" + 0.005*"human"
topic #1 (0.025): 0.109*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"story" + 0.008*"character" + 0.007*"director" + 0.007*"role" + 0.006*"produce" + 0.006*"actor"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.008*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic diff=0.073769, rho=0.135043
-8.257 per-word bound, 306.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 10
PROGRESS: pass 5, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.015*"gold" + 0.014*"black" + 0.012*"mine" + 0.010*"wear" + 0.009*"produce" + 0.007*"coal" + 0.006*"iron" + 0.006*"mining" + 0.006*"steel" + 0.005*"fire"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.013*"college" + 0.011*"department" + 0.010*"international" + 0.010*"education" + 0.009*"hospital" + 0.009*"organization" + 0.009*"study"
topic #24 (0.025): 0.051*"art" + 0.027*"museum" + 0.024*"artist" + 0.023*"award" + 0.017*"painting" + 0.012*"exhibition" + 0.011*"design" + 0.009*"collection" + 0.009*"paint" + 0.008*"study"
topic #12 (0.025): 0.072*"station" + 0.068*"line" + 0.039*"railway" + 0.028*"train" + 0.020*"operate" + 0.019*"airport" + 0.018*"bus" + 0.014*"rail" + 0.013*"passenger" + 0.013*"route"
topic #38 (0.025): 0.050*"station" + 0.035*"radio" + 0.021*"channel" + 0.019*"broadcast" + 0.018*"network" + 0.016*"jewish" + 0.014*"news" + 0.013*"air" + 0.013*"tv" + 0.011*"program"
topic diff=0.076678, rho=0.133828
PROGRESS: pass 5, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.025): 0.012*"water" + 0.009*"forest" + 0.008*"tree" + 0.007*"animal" + 0.007*"park" + 0.007*"site" + 0.006*"bird" + 0.006*"mountain" + 0.005*"plant" + 0.005*"region"
topic #31 (0.025): 0.045*"australian" + 0.028*"australia" + 0.024*"match" + 0.020*"cricket" + 0.015*"test" + 0.014*"wicket" + 0.014*"wale" + 0.013*"class" + 0.011*"stadium" + 0.011*"somerset"
topic #5 (0.025): 0.007*"life" + 0.007*"tradition" + 0.007*"accord" + 0.007*"temple" + 0.006*"religious" + 0.006*"flag" + 0.006*"woman" + 0.005*"ancient" + 0.005*"say" + 0.005*"text"
topic #10 (0.025): 0.015*"king" + 0.015*"son" + 0.009*"war" + 0.008*"death" + 0.006*"brother" + 0.006*"french" + 0.006*"daughter" + 0.006*"father" + 0.006*"royal" + 0.005*"kingdom"
topic #27 (0.025): 0.015*"cell" + 0.012*"protein" + 0.010*"gene" + 0.008*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"material" + 0.005*"chemical" + 0.005*"process" + 0.005*"energy"
topic diff=0.074284, rho=0.133828
PROGRESS: pass 5, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.025): 0.054*"language" + 0.027*"word" + 0.019*"japanese" + 0.015*"refer" + 0.012*"chinese" + 0.012*"define" + 0.011*"speak" + 0.011*"dialect" + 0.010*"mean" + 0.010*"english"
topic #6 (0.025): 0.058*"specie" + 0.019*"genus" + 0.017*"describe" + 0.011*"plant" + 0.011*"white" + 0.009*"flower" + 0.008*"brown" + 0.007*"length" + 0.007*"black" + 0.006*"yellow"
topic #18 (0.025): 0.054*"village" + 0.049*"population" + 0.027*"municipality" + 0.024*"region" + 0.021*"town" + 0.016*"census" + 0.014*"province" + 0.013*"central" + 0.011*"rural" + 0.011*"total"
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.010*"court" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.007*"right" + 0.006*"say" + 0.005*"issue" + 0.005*"claim"
topic #28 (0.025): 0.033*"county" + 0.024*"town" + 0.024*"river" + 0.022*"age" + 0.018*"north" + 0.017*"road" + 0.016*"population" + 0.016*"km" + 0.015*"south" + 0.012*"highway"
topic diff=0.068868, rho=0.133828
PROGRESS: pass 5, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #2 (0.025): 0.038*"club" + 0.032*"season" + 0.022*"league" + 0.020*"match" + 0.018*"football" + 0.017*"final" + 0.015*"player" + 0.015*"championship" + 0.014*"game" + 0.013*"goal"
topic #1 (0.025): 0.109*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"story" + 0.008*"character" + 0.007*"director" + 0.007*"role" + 0.005*"produce" + 0.005*"production"
topic #32 (0.025): 0.017*"study" + 0.016*"research" + 0.011*"social" + 0.011*"society" + 0.009*"science" + 0.007*"theory" + 0.006*"professor" + 0.006*"idea" + 0.005*"human" + 0.005*"scientific"
topic #21 (0.025): 0.011*"say" + 0.008*"get" + 0.008*"tell" + 0.006*"kill" + 0.006*"back" + 0.006*"try" + 0.005*"death" + 0.005*"life" + 0.005*"help" + 0.005*"want"
topic #27 (0.025): 0.015*"cell" + 0.012*"protein" + 0.009*"gene" + 0.008*"structure" + 0.007*"human" + 0.005*"increase" + 0.005*"material" + 0.005*"process" + 0.005*"contain" + 0.005*"chemical"
topic diff=0.071263, rho=0.133828
PROGRESS: pass 5, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.071*"election" + 0.055*"party" + 0.039*"vote" + 0.029*"seat" + 0.029*"elect" + 0.027*"candidate" + 0.018*"parliament" + 0.017*"council" + 0.012*"assembly" + 0.011*"government"
topic #12 (0.025): 0.071*"station" + 0.069*"line" + 0.038*"railway" + 0.028*"train" + 0.020*"operate" + 0.019*"airport" + 0.017*"route" + 0.017*"bus" + 0.014*"rail" + 0.013*"passenger"
topic #4 (0.025): 0.024*"battle" + 0.022*"attack" + 0.018*"force" + 0.018*"fight" + 0.015*"army" + 0.010*"troop" + 0.009*"war" + 0.009*"kill" + 0.009*"fire" + 0.009*"soldier"
topic #13 (0.025): 0.020*"university" + 0.016*"program" + 0.014*"student" + 0.013*"college" + 0.011*"department" + 0.011*"international" + 0.010*"education" + 0.010*"organization" + 0.009*"science" + 0.009*"hospital"
topic #0 (0.025): 0.051*"language" + 0.025*"word" + 0.021*"japanese" + 0.015*"refer" + 0.013*"chinese" + 0.011*"mean" + 0.011*"speak" + 0.011*"define" + 0.010*"term" + 0.010*"dialect"
topic diff=0.067185, rho=0.133828
PROGRESS: pass 5, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.071*"election" + 0.054*"party" + 0.040*"vote" + 0.029*"seat" + 0.028*"elect" + 0.027*"candidate" + 0.018*"council" + 0.017*"parliament" + 0.011*"assembly" + 0.011*"government"
topic #1 (0.025): 0.110*"film" + 0.021*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"story" + 0.008*"character" + 0.007*"director" + 0.007*"role" + 0.006*"production" + 0.006*"produce"
topic #15 (0.025): 0.053*"season" + 0.052*"game" + 0.016*"player" + 0.016*"basketball" + 0.015*"football" + 0.014*"league" + 0.011*"coach" + 0.010*"baseball" + 0.010*"conference" + 0.009*"point"
topic #29 (0.025): 0.095*"german" + 0.072*"russian" + 0.044*"polish" + 0.037*"dutch" + 0.026*"soviet" + 0.024*"commune" + 0.022*"danish" + 0.020*"netherland" + 0.020*"village" + 0.016*"north"
topic #22 (0.025): 0.027*"park" + 0.020*"street" + 0.020*"road" + 0.017*"bridge" + 0.012*"construction" + 0.009*"plan" + 0.007*"project" + 0.007*"close" + 0.007*"site" + 0.007*"facility"
topic diff=0.070369, rho=0.133828
PROGRESS: pass 5, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 10
PROGRESS: pass 5, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #37 (0.025): 0.037*"ship" + 0.028*"island" + 0.011*"sea" + 0.011*"port" + 0.011*"boat" + 0.010*"crew" + 0.009*"navy" + 0.009*"coast" + 0.009*"fleet" + 0.009*"vessel"
topic #39 (0.025): 0.015*"patient" + 0.013*"treatment" + 0.013*"disease" + 0.013*"cause" + 0.009*"drug" + 0.009*"medical" + 0.008*"virus" + 0.007*"body" + 0.006*"blood" + 0.006*"increase"
topic #6 (0.025): 0.057*"specie" + 0.019*"genus" + 0.016*"describe" + 0.012*"white" + 0.012*"plant" + 0.009*"flower" + 0.009*"brown" + 0.008*"black" + 0.007*"length" + 0.007*"yellow"
topic #3 (0.025): 0.015*"design" + 0.012*"engine" + 0.011*"power" + 0.010*"system" + 0.010*"vehicle" + 0.010*"model" + 0.008*"car" + 0.006*"produce" + 0.006*"type" + 0.005*"control"
topic #30 (0.025): 0.015*"gold" + 0.015*"black" + 0.012*"mine" + 0.009*"wear" + 0.009*"produce" + 0.007*"iron" + 0.006*"mining" + 0.006*"coal" + 0.005*"steel" + 0.005*"fish"
topic diff=0.062455, rho=0.133828
PROGRESS: pass 5, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.070*"election" + 0.055*"party" + 0.038*"vote" + 0.030*"seat" + 0.029*"elect" + 0.026*"candidate" + 0.018*"council" + 0.017*"parliament" + 0.011*"assembly" + 0.011*"constituency"
topic #21 (0.025): 0.010*"say" + 0.008*"get" + 0.008*"tell" + 0.006*"try" + 0.006*"back" + 0.006*"kill" + 0.005*"life" + 0.005*"friend" + 0.005*"want" + 0.005*"death"
topic #5 (0.025): 0.008*"life" + 0.007*"tradition" + 0.007*"accord" + 0.007*"temple" + 0.007*"religious" + 0.005*"ancient" + 0.005*"flag" + 0.005*"woman" + 0.005*"say" + 0.005*"text"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.013*"college" + 0.011*"international" + 0.011*"department" + 0.010*"education" + 0.010*"hospital" + 0.010*"organization" + 0.009*"science"
topic #37 (0.025): 0.037*"ship" + 0.029*"island" + 0.012*"sea" + 0.011*"boat" + 0.011*"port" + 0.010*"crew" + 0.009*"coast" + 0.009*"vessel" + 0.009*"navy" + 0.009*"fleet"
topic diff=0.065081, rho=0.133828
PROGRESS: pass 5, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.025): 0.113*"german" + 0.070*"russian" + 0.041*"polish" + 0.041*"dutch" + 0.029*"soviet" + 0.023*"commune" + 0.022*"danish" + 0.021*"netherland" + 0.019*"village" + 0.016*"west"
topic #25 (0.025): 0.019*"president" + 0.016*"appoint" + 0.014*"elect" + 0.012*"law" + 0.012*"woman" + 0.010*"office" + 0.009*"general" + 0.009*"position" + 0.008*"secretary" + 0.008*"chairman"
topic #2 (0.025): 0.039*"club" + 0.033*"season" + 0.023*"league" + 0.019*"football" + 0.019*"match" + 0.017*"final" + 0.016*"player" + 0.015*"championship" + 0.014*"game" + 0.013*"goal"
topic #37 (0.025): 0.038*"ship" + 0.029*"island" + 0.012*"sea" + 0.011*"port" + 0.011*"boat" + 0.009*"crew" + 0.009*"coast" + 0.009*"navy" + 0.009*"vessel" + 0.008*"fleet"
topic #0 (0.025): 0.053*"language" + 0.026*"word" + 0.021*"japanese" + 0.016*"chinese" + 0.016*"refer" + 0.012*"mean" + 0.011*"speak" + 0.010*"define" + 0.010*"dialect" + 0.009*"term"
topic diff=0.062586, rho=0.133828
PROGRESS: pass 5, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.025): 0.052*"language" + 0.026*"word" + 0.020*"japanese" + 0.016*"refer" + 0.015*"chinese" + 0.011*"mean" + 0.011*"speak" + 0.010*"dialect" + 0.009*"define" + 0.009*"term"
topic #9 (0.025): 0.037*"series" + 0.023*"episode" + 0.018*"television" + 0.013*"award" + 0.010*"season" + 0.010*"role" + 0.010*"appear" + 0.008*"tv" + 0.007*"air" + 0.007*"character"
topic #12 (0.025): 0.071*"station" + 0.068*"line" + 0.038*"railway" + 0.030*"train" + 0.020*"operate" + 0.017*"bus" + 0.017*"airport" + 0.015*"route" + 0.015*"rail" + 0.013*"passenger"
topic #10 (0.025): 0.015*"king" + 0.015*"son" + 0.008*"death" + 0.008*"war" + 0.007*"brother" + 0.007*"father" + 0.006*"daughter" + 0.006*"french" + 0.006*"royal" + 0.005*"great"
topic #17 (0.025): 0.032*"company" + 0.012*"business" + 0.009*"market" + 0.008*"product" + 0.007*"provide" + 0.007*"industry" + 0.006*"purchase" + 0.006*"sell" + 0.006*"public" + 0.006*"development"
topic diff=0.061807, rho=0.133828
PROGRESS: pass 5, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #38 (0.025): 0.053*"station" + 0.040*"radio" + 0.019*"broadcast" + 0.019*"channel" + 0.018*"network" + 0.017*"jewish" + 0.013*"news" + 0.013*"air" + 0.011*"program" + 0.011*"television"
topic #0 (0.025): 0.053*"language" + 0.027*"word" + 0.019*"japanese" + 0.016*"refer" + 0.016*"chinese" + 0.011*"mean" + 0.011*"speak" + 0.010*"dialect" + 0.009*"term" + 0.009*"example"
topic #24 (0.025): 0.053*"art" + 0.026*"museum" + 0.025*"artist" + 0.020*"award" + 0.018*"painting" + 0.011*"exhibition" + 0.011*"design" + 0.010*"collection" + 0.010*"paint" + 0.008*"study"
topic #33 (0.025): 0.040*"company" + 0.034*"french" + 0.030*"theatre" + 0.021*"production" + 0.019*"store" + 0.015*"opera" + 0.014*"wine" + 0.014*"theater" + 0.012*"produce" + 0.011*"sell"
topic #8 (0.025): 0.038*"album" + 0.037*"song" + 0.032*"music" + 0.025*"band" + 0.016*"single" + 0.011*"perform" + 0.011*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"singer"
topic diff=0.065436, rho=0.133828
PROGRESS: pass 5, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #28 (0.025): 0.034*"county" + 0.028*"river" + 0.024*"town" + 0.020*"north" + 0.020*"age" + 0.017*"road" + 0.016*"km" + 0.015*"south" + 0.015*"population" + 0.014*"route"
topic #17 (0.025): 0.031*"company" + 0.012*"business" + 0.009*"market" + 0.007*"product" + 0.007*"provide" + 0.006*"industry" + 0.006*"sell" + 0.006*"increase" + 0.006*"purchase" + 0.006*"public"
topic #33 (0.025): 0.039*"company" + 0.034*"french" + 0.030*"theatre" + 0.022*"production" + 0.018*"store" + 0.013*"wine" + 0.013*"opera" + 0.013*"theater" + 0.012*"produce" + 0.011*"restaurant"
topic #20 (0.025): 0.044*"book" + 0.036*"publish" + 0.014*"novel" + 0.013*"author" + 0.012*"magazine" + 0.012*"writer" + 0.010*"story" + 0.010*"editor" + 0.009*"publication" + 0.008*"newspaper"
topic #39 (0.025): 0.013*"patient" + 0.013*"cause" + 0.013*"treatment" + 0.013*"disease" + 0.009*"medical" + 0.009*"drug" + 0.007*"body" + 0.007*"increase" + 0.006*"blood" + 0.006*"virus"
topic diff=0.064613, rho=0.133828
PROGRESS: pass 5, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.007*"right" + 0.007*"say" + 0.005*"issue" + 0.005*"claim"
topic #19 (0.025): 0.038*"building" + 0.024*"church" + 0.023*"house" + 0.010*"design" + 0.010*"site" + 0.008*"th_century" + 0.008*"stone" + 0.008*"wall" + 0.007*"room" + 0.007*"tower"
topic #8 (0.025): 0.039*"album" + 0.036*"song" + 0.031*"music" + 0.025*"band" + 0.015*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"singer"
topic #39 (0.025): 0.013*"treatment" + 0.013*"patient" + 0.013*"cause" + 0.012*"disease" + 0.009*"drug" + 0.009*"medical" + 0.007*"body" + 0.007*"health" + 0.006*"increase" + 0.006*"risk"
topic #1 (0.025): 0.110*"film" + 0.023*"star" + 0.015*"direct" + 0.011*"movie" + 0.010*"story" + 0.009*"character" + 0.007*"director" + 0.007*"role" + 0.006*"produce" + 0.006*"actor"
topic diff=0.063516, rho=0.133828
PROGRESS: pass 5, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #20 (0.025): 0.044*"book" + 0.037*"publish" + 0.014*"novel" + 0.013*"author" + 0.012*"magazine" + 0.012*"writer" + 0.010*"story" + 0.009*"editor" + 0.009*"publication" + 0.008*"newspaper"
topic #9 (0.025): 0.036*"series" + 0.024*"episode" + 0.018*"television" + 0.012*"award" + 0.010*"season" + 0.010*"appear" + 0.010*"role" + 0.009*"tv" + 0.008*"character" + 0.008*"actor"
topic #35 (0.025): 0.044*"event" + 0.044*"race" + 0.026*"compete" + 0.026*"championship" + 0.020*"finish" + 0.014*"woman" + 0.013*"sport" + 0.012*"competition" + 0.010*"point" + 0.009*"athlete"
topic #37 (0.025): 0.037*"ship" + 0.028*"island" + 0.012*"sea" + 0.012*"port" + 0.011*"coast" + 0.010*"boat" + 0.010*"crew" + 0.010*"navy" + 0.010*"vessel" + 0.009*"fleet"
topic #31 (0.025): 0.057*"australian" + 0.033*"australia" + 0.022*"match" + 0.018*"cricket" + 0.015*"test" + 0.015*"wale" + 0.013*"sydney" + 0.013*"class" + 0.012*"welsh" + 0.012*"south_wale"
topic diff=0.057500, rho=0.133828
-8.322 per-word bound, 320.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.025): 0.059*"canadian" + 0.054*"irish" + 0.043*"festival" + 0.031*"swedish" + 0.017*"hungarian" + 0.017*"finnish" + 0.014*"quebec" + 0.014*"saint" + 0.013*"nhl" + 0.011*"montreal"
topic #11 (0.025): 0.070*"game" + 0.024*"player" + 0.015*"version" + 0.013*"computer" + 0.011*"user" + 0.011*"character" + 0.010*"video" + 0.010*"software" + 0.010*"system" + 0.008*"card"
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.006*"right" + 0.006*"say" + 0.006*"issue" + 0.006*"claim"
topic #16 (0.025): 0.025*"war" + 0.022*"army" + 0.022*"military" + 0.018*"unit" + 0.015*"aircraft" + 0.014*"operation" + 0.014*"air" + 0.013*"command" + 0.012*"german" + 0.012*"force"
topic #14 (0.025): 0.013*"water" + 0.010*"forest" + 0.008*"tree" + 0.007*"animal" + 0.007*"site" + 0.006*"park" + 0.006*"mountain" + 0.006*"bird" + 0.005*"region" + 0.005*"plant"
topic diff=0.054808, rho=0.133828
-8.324 per-word bound, 320.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4835 documents into a model of 49835 documents
topic #19 (0.025): 0.038*"building" + 0.023*"church" + 0.023*"house" + 0.010*"design" + 0.010*"site" + 0.009*"stone" + 0.008*"wall" + 0.008*"th_century" + 0.007*"room" + 0.007*"tower"
topic #26 (0.025): 0.070*"election" + 0.058*"party" + 0.040*"vote" + 0.029*"elect" + 0.026*"seat" + 0.025*"candidate" + 0.018*"parliament" + 0.016*"council" + 0.011*"government" + 0.011*"labour"
topic #20 (0.025): 0.045*"book" + 0.037*"publish" + 0.014*"novel" + 0.013*"author" + 0.012*"magazine" + 0.012*"writer" + 0.010*"story" + 0.010*"editor" + 0.009*"publication" + 0.008*"newspaper"
topic #30 (0.025): 0.016*"black" + 0.013*"gold" + 0.012*"mine" + 0.010*"wear" + 0.009*"produce" + 0.006*"iron" + 0.006*"coal" + 0.006*"white" + 0.006*"mining" + 0.006*"steel"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.008*"vehicle" + 0.008*"car" + 0.005*"produce" + 0.005*"type" + 0.005*"control"
topic diff=0.054440, rho=0.133828
-8.285 per-word bound, 312.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.022*"university" + 0.016*"program" + 0.015*"student" + 0.013*"college" + 0.011*"department" + 0.011*"international" + 0.010*"education" + 0.010*"research" + 0.010*"science" + 0.010*"study"
topic #5 (0.025): 0.007*"accord" + 0.007*"life" + 0.007*"tradition" + 0.007*"religious" + 0.007*"temple" + 0.006*"christian" + 0.005*"text" + 0.005*"ancient" + 0.005*"say" + 0.005*"flag"
topic #21 (0.025): 0.010*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"try" + 0.006*"back" + 0.006*"kill" + 0.005*"life" + 0.005*"help" + 0.005*"death" + 0.005*"want"
topic #18 (0.025): 0.054*"village" + 0.049*"population" + 0.025*"municipality" + 0.024*"region" + 0.023*"town" + 0.017*"census" + 0.015*"province" + 0.013*"central" + 0.011*"local" + 0.011*"rural"
topic #11 (0.025): 0.070*"game" + 0.027*"player" + 0.014*"version" + 0.013*"computer" + 0.011*"user" + 0.011*"character" + 0.010*"video" + 0.009*"system" + 0.009*"software" + 0.009*"card"
topic diff=0.058368, rho=0.132645
PROGRESS: pass 6, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.025): 0.129*"german" + 0.077*"russian" + 0.043*"polish" + 0.040*"dutch" + 0.029*"soviet" + 0.021*"commune" + 0.020*"netherland" + 0.020*"danish" + 0.019*"village" + 0.015*"north"
topic #13 (0.025): 0.021*"university" + 0.016*"program" + 0.014*"student" + 0.013*"college" + 0.011*"department" + 0.011*"international" + 0.011*"education" + 0.010*"research" + 0.010*"study" + 0.010*"science"
topic #16 (0.025): 0.026*"war" + 0.022*"military" + 0.021*"army" + 0.018*"unit" + 0.016*"aircraft" + 0.014*"operation" + 0.013*"air" + 0.013*"command" + 0.012*"force" + 0.010*"german"
topic #5 (0.025): 0.007*"life" + 0.007*"accord" + 0.007*"tradition" + 0.007*"temple" + 0.007*"religious" + 0.006*"christian" + 0.005*"woman" + 0.005*"ancient" + 0.005*"text" + 0.005*"say"
topic #25 (0.025): 0.018*"president" + 0.016*"appoint" + 0.013*"elect" + 0.012*"woman" + 0.012*"law" + 0.010*"office" + 0.009*"general" + 0.009*"secretary" + 0.008*"position" + 0.008*"minister"
topic diff=0.056792, rho=0.132645
PROGRESS: pass 6, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.052*"season" + 0.051*"game" + 0.017*"player" + 0.016*"basketball" + 0.015*"football" + 0.014*"league" + 0.012*"coach" + 0.009*"baseball" + 0.009*"college" + 0.009*"point"
topic #37 (0.025): 0.038*"ship" + 0.030*"island" + 0.011*"sea" + 0.011*"port" + 0.010*"boat" + 0.010*"coast" + 0.010*"crew" + 0.009*"fleet" + 0.009*"navy" + 0.009*"vessel"
topic #30 (0.025): 0.016*"black" + 0.014*"gold" + 0.011*"mine" + 0.009*"wear" + 0.009*"produce" + 0.007*"iron" + 0.007*"mill" + 0.006*"white" + 0.006*"coal" + 0.005*"steel"
topic #19 (0.025): 0.039*"building" + 0.023*"house" + 0.021*"church" + 0.010*"design" + 0.010*"site" + 0.008*"stone" + 0.008*"wall" + 0.008*"th_century" + 0.007*"room" + 0.007*"tower"
topic #29 (0.025): 0.126*"german" + 0.077*"russian" + 0.043*"polish" + 0.040*"dutch" + 0.029*"soviet" + 0.022*"commune" + 0.020*"netherland" + 0.020*"danish" + 0.019*"village" + 0.014*"north"
topic diff=0.052500, rho=0.132645
PROGRESS: pass 6, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #31 (0.025): 0.058*"australian" + 0.034*"australia" + 0.021*"match" + 0.020*"cricket" + 0.015*"test" + 0.013*"sydney" + 0.013*"wale" + 0.012*"class" + 0.012*"welsh" + 0.012*"wicket"
topic #27 (0.025): 0.015*"cell" + 0.012*"protein" + 0.010*"gene" + 0.008*"structure" + 0.007*"human" + 0.006*"material" + 0.005*"increase" + 0.005*"process" + 0.005*"contain" + 0.005*"chemical"
topic #33 (0.025): 0.041*"company" + 0.036*"french" + 0.029*"theatre" + 0.023*"production" + 0.019*"store" + 0.017*"theater" + 0.013*"opera" + 0.013*"hotel" + 0.012*"produce" + 0.012*"wine"
topic #28 (0.025): 0.036*"county" + 0.026*"river" + 0.024*"town" + 0.023*"age" + 0.019*"north" + 0.017*"km" + 0.017*"population" + 0.015*"road" + 0.015*"south" + 0.012*"route"
topic #35 (0.025): 0.044*"event" + 0.042*"race" + 0.027*"championship" + 0.026*"compete" + 0.020*"finish" + 0.014*"woman" + 0.012*"sport" + 0.012*"competition" + 0.010*"olympic" + 0.009*"point"
topic diff=0.054668, rho=0.132645
PROGRESS: pass 6, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.053*"season" + 0.051*"game" + 0.016*"player" + 0.016*"basketball" + 0.016*"football" + 0.015*"league" + 0.012*"coach" + 0.010*"baseball" + 0.009*"conference" + 0.009*"college"
topic #18 (0.025): 0.051*"village" + 0.048*"population" + 0.025*"municipality" + 0.025*"region" + 0.024*"town" + 0.016*"census" + 0.015*"province" + 0.012*"central" + 0.012*"local" + 0.012*"rural"
topic #38 (0.025): 0.052*"station" + 0.037*"radio" + 0.022*"channel" + 0.020*"broadcast" + 0.019*"network" + 0.016*"jewish" + 0.014*"news" + 0.013*"tv" + 0.013*"air" + 0.013*"television"
topic #34 (0.025): 0.032*"student" + 0.024*"college" + 0.021*"child" + 0.020*"church" + 0.014*"grade" + 0.013*"education" + 0.012*"bishop" + 0.011*"teacher" + 0.009*"attend" + 0.008*"class"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.012*"engine" + 0.010*"system" + 0.009*"model" + 0.009*"vehicle" + 0.009*"car" + 0.005*"produce" + 0.005*"control" + 0.005*"standard"
topic diff=0.051391, rho=0.132645
PROGRESS: pass 6, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.025): 0.014*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"vehicle" + 0.010*"system" + 0.010*"model" + 0.008*"car" + 0.006*"produce" + 0.005*"control" + 0.005*"standard"
topic #27 (0.025): 0.015*"cell" + 0.012*"protein" + 0.010*"gene" + 0.008*"structure" + 0.007*"human" + 0.006*"increase" + 0.005*"material" + 0.005*"process" + 0.005*"chemical" + 0.005*"enzyme"
topic #24 (0.025): 0.054*"art" + 0.027*"museum" + 0.026*"artist" + 0.019*"award" + 0.017*"painting" + 0.013*"exhibition" + 0.011*"design" + 0.010*"collection" + 0.010*"paint" + 0.008*"study"
topic #8 (0.025): 0.037*"album" + 0.037*"song" + 0.034*"music" + 0.024*"band" + 0.016*"single" + 0.010*"track" + 0.010*"perform" + 0.008*"chart" + 0.008*"tour" + 0.007*"video"
topic #32 (0.025): 0.016*"study" + 0.014*"research" + 0.011*"social" + 0.010*"society" + 0.008*"theory" + 0.007*"science" + 0.006*"idea" + 0.005*"life" + 0.005*"professor" + 0.005*"human"
topic diff=0.054701, rho=0.132645
PROGRESS: pass 6, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #6 (0.025): 0.057*"specie" + 0.018*"genus" + 0.016*"describe" + 0.014*"white" + 0.012*"plant" + 0.009*"flower" + 0.009*"brown" + 0.008*"black" + 0.007*"male" + 0.007*"length"
topic #28 (0.025): 0.037*"county" + 0.027*"river" + 0.024*"town" + 0.022*"age" + 0.019*"north" + 0.016*"km" + 0.016*"population" + 0.016*"road" + 0.015*"south" + 0.015*"highway"
topic #39 (0.025): 0.015*"patient" + 0.013*"treatment" + 0.013*"cause" + 0.013*"disease" + 0.010*"drug" + 0.009*"medical" + 0.008*"virus" + 0.007*"body" + 0.007*"health" + 0.006*"blood"
topic #35 (0.025): 0.043*"event" + 0.042*"race" + 0.026*"championship" + 0.026*"compete" + 0.019*"finish" + 0.014*"woman" + 0.013*"sport" + 0.012*"competition" + 0.009*"final" + 0.009*"olympic"
topic #31 (0.025): 0.061*"australian" + 0.037*"australia" + 0.021*"cricket" + 0.021*"match" + 0.015*"sydney" + 0.014*"wale" + 0.014*"test" + 0.013*"melbourne" + 0.013*"south_wale" + 0.012*"class"
topic diff=0.047940, rho=0.132645
PROGRESS: pass 6, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.025): 0.008*"life" + 0.007*"tradition" + 0.007*"accord" + 0.007*"religious" + 0.007*"temple" + 0.006*"christian" + 0.005*"ancient" + 0.005*"text" + 0.005*"say" + 0.005*"religion"
topic #19 (0.025): 0.039*"building" + 0.023*"house" + 0.022*"church" + 0.010*"design" + 0.010*"site" + 0.008*"th_century" + 0.008*"wall" + 0.008*"stone" + 0.007*"tower" + 0.007*"room"
topic #26 (0.025): 0.070*"election" + 0.055*"party" + 0.039*"vote" + 0.029*"elect" + 0.029*"seat" + 0.026*"candidate" + 0.017*"parliament" + 0.017*"council" + 0.011*"assembly" + 0.011*"constituency"
topic #22 (0.025): 0.029*"park" + 0.024*"road" + 0.020*"street" + 0.017*"bridge" + 0.011*"construction" + 0.009*"plan" + 0.007*"project" + 0.007*"site" + 0.007*"close" + 0.007*"facility"
topic #8 (0.025): 0.037*"album" + 0.037*"song" + 0.033*"music" + 0.025*"band" + 0.016*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"chart" + 0.008*"tour" + 0.007*"video"
topic diff=0.050614, rho=0.132645
PROGRESS: pass 6, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #35 (0.025): 0.044*"event" + 0.041*"race" + 0.028*"championship" + 0.027*"compete" + 0.020*"finish" + 0.016*"woman" + 0.013*"sport" + 0.012*"competition" + 0.010*"final" + 0.009*"olympic"
topic #3 (0.025): 0.015*"design" + 0.012*"engine" + 0.012*"power" + 0.010*"system" + 0.010*"vehicle" + 0.009*"model" + 0.008*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"control"
topic #34 (0.025): 0.033*"student" + 0.023*"church" + 0.023*"college" + 0.022*"child" + 0.012*"grade" + 0.012*"education" + 0.011*"bishop" + 0.011*"teacher" + 0.009*"attend" + 0.008*"class"
topic #22 (0.025): 0.028*"park" + 0.023*"road" + 0.020*"street" + 0.018*"bridge" + 0.012*"construction" + 0.009*"plan" + 0.008*"site" + 0.008*"project" + 0.007*"facility" + 0.007*"close"
topic #2 (0.025): 0.041*"club" + 0.033*"season" + 0.023*"league" + 0.019*"match" + 0.019*"football" + 0.017*"final" + 0.016*"player" + 0.014*"championship" + 0.014*"game" + 0.013*"goal"
topic diff=0.049028, rho=0.132645
PROGRESS: pass 6, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.025): 0.065*"canadian" + 0.052*"irish" + 0.042*"festival" + 0.029*"swedish" + 0.018*"hungarian" + 0.018*"finnish" + 0.015*"norwegian" + 0.014*"quebec" + 0.012*"nhl" + 0.011*"freiburg"
topic #13 (0.025): 0.022*"university" + 0.016*"program" + 0.015*"student" + 0.013*"college" + 0.011*"international" + 0.011*"department" + 0.011*"research" + 0.010*"education" + 0.010*"science" + 0.010*"award"
topic #15 (0.025): 0.051*"season" + 0.051*"game" + 0.016*"football" + 0.016*"player" + 0.014*"basketball" + 0.013*"league" + 0.013*"coach" + 0.010*"conference" + 0.010*"point" + 0.010*"college"
topic #22 (0.025): 0.028*"park" + 0.024*"road" + 0.021*"street" + 0.018*"bridge" + 0.012*"construction" + 0.009*"plan" + 0.008*"site" + 0.007*"facility" + 0.007*"project" + 0.007*"close"
topic #9 (0.025): 0.038*"series" + 0.023*"episode" + 0.018*"television" + 0.013*"award" + 0.011*"season" + 0.010*"appear" + 0.010*"role" + 0.008*"tv" + 0.008*"character" + 0.007*"air"
topic diff=0.048036, rho=0.132645
PROGRESS: pass 6, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.025): 0.055*"art" + 0.027*"artist" + 0.026*"museum" + 0.018*"award" + 0.018*"painting" + 0.012*"design" + 0.011*"exhibition" + 0.011*"collection" + 0.010*"paint" + 0.008*"study"
topic #6 (0.025): 0.056*"specie" + 0.018*"genus" + 0.016*"describe" + 0.013*"white" + 0.012*"plant" + 0.009*"flower" + 0.008*"black" + 0.008*"brown" + 0.007*"male" + 0.007*"female"
topic #17 (0.025): 0.031*"company" + 0.012*"business" + 0.009*"market" + 0.008*"product" + 0.007*"provide" + 0.007*"industry" + 0.006*"increase" + 0.006*"sell" + 0.006*"purchase" + 0.006*"sale"
topic #23 (0.025): 0.017*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"right" + 0.007*"case" + 0.007*"say" + 0.006*"claim" + 0.005*"country"
topic #32 (0.025): 0.016*"study" + 0.014*"research" + 0.012*"social" + 0.010*"society" + 0.007*"science" + 0.007*"theory" + 0.006*"idea" + 0.006*"professor" + 0.005*"human" + 0.005*"scientific"
topic diff=0.049357, rho=0.132645
PROGRESS: pass 6, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #35 (0.025): 0.045*"event" + 0.042*"race" + 0.029*"championship" + 0.027*"compete" + 0.019*"finish" + 0.017*"woman" + 0.013*"competition" + 0.013*"sport" + 0.010*"final" + 0.009*"point"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"model" + 0.009*"vehicle" + 0.008*"car" + 0.006*"type" + 0.005*"produce" + 0.005*"control"
topic #28 (0.025): 0.036*"county" + 0.029*"river" + 0.024*"town" + 0.021*"age" + 0.020*"north" + 0.017*"km" + 0.016*"population" + 0.015*"south" + 0.015*"road" + 0.014*"route"
topic #25 (0.025): 0.019*"president" + 0.017*"appoint" + 0.013*"elect" + 0.012*"law" + 0.012*"woman" + 0.011*"office" + 0.009*"general" + 0.009*"position" + 0.008*"secretary" + 0.008*"chairman"
topic #0 (0.025): 0.054*"language" + 0.027*"word" + 0.023*"japanese" + 0.021*"chinese" + 0.016*"refer" + 0.011*"speak" + 0.011*"mean" + 0.010*"dialect" + 0.010*"term" + 0.010*"english"
topic diff=0.052786, rho=0.132645
PROGRESS: pass 6, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.025): 0.061*"canadian" + 0.055*"irish" + 0.042*"festival" + 0.029*"swedish" + 0.019*"norwegian" + 0.018*"hungarian" + 0.017*"finnish" + 0.013*"quebec" + 0.011*"nhl" + 0.010*"finland"
topic #4 (0.025): 0.024*"battle" + 0.022*"attack" + 0.019*"force" + 0.018*"fight" + 0.015*"army" + 0.011*"fire" + 0.010*"war" + 0.010*"kill" + 0.009*"troop" + 0.009*"soldier"
topic #21 (0.025): 0.011*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"back" + 0.006*"try" + 0.006*"kill" + 0.005*"life" + 0.005*"friend" + 0.005*"help" + 0.005*"want"
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.008*"tree" + 0.007*"animal" + 0.007*"mountain" + 0.006*"site" + 0.006*"bird" + 0.006*"park" + 0.006*"region" + 0.005*"range"
topic #36 (0.025): 0.013*"system" + 0.010*"displaystyle" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.006*"function" + 0.006*"set" + 0.005*"term" + 0.005*"value" + 0.005*"method"
topic diff=0.051615, rho=0.132645
PROGRESS: pass 6, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 10
PROGRESS: pass 6, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.040*"company" + 0.037*"french" + 0.029*"theatre" + 0.024*"production" + 0.019*"store" + 0.014*"wine" + 0.013*"theater" + 0.013*"produce" + 0.012*"opera" + 0.011*"sell"
topic #23 (0.025): 0.016*"government" + 0.010*"court" + 0.010*"law" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.007*"say" + 0.006*"right" + 0.006*"issue" + 0.005*"claim"
topic #26 (0.025): 0.070*"election" + 0.057*"party" + 0.040*"vote" + 0.030*"elect" + 0.026*"candidate" + 0.025*"seat" + 0.019*"parliament" + 0.015*"council" + 0.011*"government" + 0.011*"assembly"
topic #30 (0.025): 0.017*"black" + 0.014*"gold" + 0.012*"mine" + 0.010*"wear" + 0.008*"produce" + 0.007*"white" + 0.006*"coal" + 0.006*"iron" + 0.006*"steel" + 0.006*"mining"
topic #11 (0.025): 0.072*"game" + 0.024*"player" + 0.014*"version" + 0.012*"computer" + 0.011*"user" + 0.011*"character" + 0.010*"video" + 0.010*"system" + 0.010*"software" + 0.009*"card"
topic diff=0.045917, rho=0.132645
-8.329 per-word bound, 321.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.006*"right" + 0.006*"say" + 0.006*"issue" + 0.006*"claim"
topic #30 (0.025): 0.017*"black" + 0.013*"gold" + 0.012*"mine" + 0.010*"wear" + 0.009*"produce" + 0.007*"white" + 0.006*"iron" + 0.006*"steel" + 0.006*"mining" + 0.005*"coal"
topic #25 (0.025): 0.019*"president" + 0.017*"appoint" + 0.013*"elect" + 0.012*"law" + 0.012*"woman" + 0.011*"office" + 0.009*"general" + 0.009*"secretary" + 0.009*"position" + 0.008*"governor"
topic #3 (0.025): 0.015*"design" + 0.013*"power" + 0.011*"engine" + 0.010*"system" + 0.008*"car" + 0.008*"model" + 0.008*"vehicle" + 0.005*"produce" + 0.005*"control" + 0.005*"type"
topic #16 (0.025): 0.025*"war" + 0.023*"army" + 0.022*"military" + 0.019*"unit" + 0.015*"aircraft" + 0.015*"operation" + 0.014*"air" + 0.014*"command" + 0.012*"force" + 0.011*"officer"
topic diff=0.043058, rho=0.132645
-8.331 per-word bound, 322.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.051*"season" + 0.051*"game" + 0.016*"player" + 0.016*"football" + 0.015*"basketball" + 0.014*"league" + 0.013*"coach" + 0.010*"baseball" + 0.010*"college" + 0.010*"point"
topic #0 (0.025): 0.055*"language" + 0.027*"word" + 0.023*"japanese" + 0.021*"chinese" + 0.016*"refer" + 0.012*"mean" + 0.011*"speak" + 0.010*"english" + 0.010*"speaker" + 0.010*"term"
topic #25 (0.025): 0.019*"president" + 0.016*"appoint" + 0.013*"elect" + 0.012*"law" + 0.012*"woman" + 0.010*"office" + 0.009*"general" + 0.009*"secretary" + 0.008*"position" + 0.008*"governor"
topic #32 (0.025): 0.015*"study" + 0.013*"research" + 0.011*"social" + 0.010*"society" + 0.008*"theory" + 0.007*"science" + 0.006*"idea" + 0.005*"human" + 0.005*"professor" + 0.005*"life"
topic #24 (0.025): 0.055*"art" + 0.028*"museum" + 0.026*"artist" + 0.017*"award" + 0.017*"painting" + 0.012*"exhibition" + 0.012*"design" + 0.010*"collection" + 0.010*"paint" + 0.008*"study"
topic diff=0.050383, rho=0.132645
-8.336 per-word bound, 323.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #1 (0.025): 0.116*"film" + 0.022*"star" + 0.016*"direct" + 0.012*"movie" + 0.010*"story" + 0.008*"character" + 0.008*"director" + 0.007*"role" + 0.007*"produce" + 0.006*"production"
topic #23 (0.025): 0.017*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.006*"case" + 0.006*"say" + 0.006*"right" + 0.006*"issue" + 0.005*"claim"
topic #20 (0.025): 0.046*"book" + 0.038*"publish" + 0.014*"novel" + 0.014*"author" + 0.013*"magazine" + 0.012*"writer" + 0.010*"story" + 0.010*"editor" + 0.009*"publication" + 0.008*"newspaper"
topic #29 (0.025): 0.156*"german" + 0.079*"russian" + 0.045*"polish" + 0.042*"dutch" + 0.032*"soviet" + 0.021*"commune" + 0.021*"netherland" + 0.020*"danish" + 0.019*"village" + 0.014*"west"
topic #17 (0.025): 0.033*"company" + 0.012*"business" + 0.009*"market" + 0.007*"industry" + 0.007*"provide" + 0.007*"product" + 0.006*"sell" + 0.006*"increase" + 0.006*"management" + 0.006*"public"
topic diff=0.046964, rho=0.132645
-8.245 per-word bound, 303.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #5 (0.025): 0.007*"accord" + 0.007*"tradition" + 0.007*"christian" + 0.007*"religious" + 0.007*"life" + 0.007*"temple" + 0.007*"flag" + 0.006*"text" + 0.006*"ancient" + 0.005*"say"
topic #38 (0.025): 0.051*"station" + 0.036*"radio" + 0.023*"channel" + 0.020*"network" + 0.020*"broadcast" + 0.016*"jewish" + 0.015*"news" + 0.014*"tv" + 0.014*"air" + 0.014*"television"
topic #25 (0.025): 0.019*"president" + 0.016*"appoint" + 0.013*"elect" + 0.012*"law" + 0.011*"woman" + 0.011*"office" + 0.009*"general" + 0.009*"position" + 0.009*"secretary" + 0.008*"minister"
topic #27 (0.025): 0.017*"cell" + 0.013*"protein" + 0.010*"gene" + 0.007*"structure" + 0.007*"human" + 0.006*"material" + 0.005*"energy" + 0.005*"temperature" + 0.005*"chemical" + 0.005*"increase"
topic #35 (0.025): 0.043*"event" + 0.042*"race" + 0.028*"championship" + 0.026*"compete" + 0.020*"finish" + 0.016*"woman" + 0.014*"competition" + 0.013*"sport" + 0.010*"point" + 0.009*"winner"
topic diff=0.047178, rho=0.131494
PROGRESS: pass 7, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #35 (0.025): 0.045*"event" + 0.041*"race" + 0.028*"championship" + 0.026*"compete" + 0.020*"finish" + 0.016*"woman" + 0.014*"competition" + 0.013*"sport" + 0.010*"olympic" + 0.010*"athlete"
topic #31 (0.025): 0.064*"australian" + 0.040*"australia" + 0.021*"match" + 0.020*"cricket" + 0.015*"sydney" + 0.015*"test" + 0.014*"melbourne" + 0.013*"wale" + 0.013*"wicket" + 0.013*"class"
topic #8 (0.025): 0.038*"album" + 0.037*"song" + 0.032*"music" + 0.026*"band" + 0.016*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #26 (0.025): 0.069*"election" + 0.054*"party" + 0.040*"vote" + 0.031*"elect" + 0.025*"seat" + 0.025*"candidate" + 0.019*"parliament" + 0.016*"council" + 0.011*"government" + 0.011*"assembly"
topic #36 (0.025): 0.012*"displaystyle" + 0.012*"system" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.006*"function" + 0.006*"method" + 0.006*"value" + 0.005*"set" + 0.005*"term"
topic diff=0.046067, rho=0.131494
PROGRESS: pass 7, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #31 (0.025): 0.064*"australian" + 0.039*"australia" + 0.021*"match" + 0.020*"cricket" + 0.016*"test" + 0.015*"sydney" + 0.013*"south_wale" + 0.013*"wale" + 0.013*"melbourne" + 0.013*"wicket"
topic #21 (0.025): 0.011*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"back" + 0.006*"try" + 0.006*"kill" + 0.005*"life" + 0.005*"help" + 0.005*"death" + 0.005*"want"
topic #22 (0.025): 0.030*"park" + 0.025*"road" + 0.021*"street" + 0.019*"bridge" + 0.012*"construction" + 0.009*"plan" + 0.008*"project" + 0.007*"facility" + 0.007*"north" + 0.007*"site"
topic #1 (0.025): 0.114*"film" + 0.022*"star" + 0.016*"direct" + 0.012*"movie" + 0.010*"story" + 0.008*"character" + 0.007*"director" + 0.007*"role" + 0.006*"produce" + 0.006*"production"
topic #34 (0.025): 0.031*"student" + 0.029*"church" + 0.024*"child" + 0.021*"college" + 0.014*"grade" + 0.013*"bishop" + 0.013*"education" + 0.011*"teacher" + 0.009*"attend" + 0.007*"class"
topic diff=0.042439, rho=0.131494
PROGRESS: pass 7, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.025): 0.053*"art" + 0.029*"museum" + 0.026*"artist" + 0.018*"painting" + 0.016*"award" + 0.013*"exhibition" + 0.011*"design" + 0.010*"collection" + 0.010*"paint" + 0.008*"study"
topic #20 (0.025): 0.045*"book" + 0.037*"publish" + 0.014*"novel" + 0.013*"author" + 0.012*"magazine" + 0.012*"writer" + 0.010*"story" + 0.009*"editor" + 0.009*"publication" + 0.008*"history"
topic #32 (0.025): 0.016*"study" + 0.013*"research" + 0.011*"social" + 0.010*"society" + 0.007*"theory" + 0.007*"science" + 0.006*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"professor"
topic #33 (0.025): 0.041*"company" + 0.038*"french" + 0.029*"theatre" + 0.022*"production" + 0.019*"store" + 0.018*"theater" + 0.015*"hotel" + 0.014*"opera" + 0.012*"produce" + 0.012*"wine"
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.008*"tree" + 0.007*"animal" + 0.007*"mountain" + 0.006*"bird" + 0.006*"site" + 0.006*"park" + 0.005*"region" + 0.005*"range"
topic diff=0.044688, rho=0.131494
PROGRESS: pass 7, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.025): 0.147*"german" + 0.076*"russian" + 0.048*"polish" + 0.038*"dutch" + 0.032*"soviet" + 0.022*"commune" + 0.020*"netherland" + 0.019*"danish" + 0.018*"village" + 0.015*"north"
topic #25 (0.025): 0.019*"president" + 0.017*"appoint" + 0.012*"law" + 0.012*"elect" + 0.012*"woman" + 0.011*"office" + 0.009*"general" + 0.009*"position" + 0.009*"secretary" + 0.008*"chairman"
topic #5 (0.025): 0.008*"life" + 0.008*"tradition" + 0.007*"accord" + 0.007*"religious" + 0.007*"temple" + 0.006*"christian" + 0.006*"flag" + 0.005*"text" + 0.005*"ancient" + 0.005*"say"
topic #36 (0.025): 0.012*"displaystyle" + 0.012*"system" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.006*"function" + 0.006*"value" + 0.006*"method" + 0.005*"set" + 0.005*"term"
topic #7 (0.025): 0.077*"canadian" + 0.056*"irish" + 0.038*"festival" + 0.032*"swedish" + 0.020*"norwegian" + 0.018*"hungarian" + 0.018*"finnish" + 0.014*"quebec" + 0.012*"finland" + 0.010*"saint"
topic diff=0.041514, rho=0.131494
PROGRESS: pass 7, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #36 (0.025): 0.012*"system" + 0.012*"displaystyle" + 0.008*"example" + 0.007*"datum" + 0.007*"point" + 0.006*"function" + 0.006*"value" + 0.006*"method" + 0.005*"set" + 0.005*"term"
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.007*"tree" + 0.007*"animal" + 0.007*"mountain" + 0.006*"bird" + 0.006*"park" + 0.006*"site" + 0.005*"region" + 0.005*"range"
topic #31 (0.025): 0.064*"australian" + 0.040*"australia" + 0.020*"cricket" + 0.020*"match" + 0.017*"sydney" + 0.015*"test" + 0.015*"south_wale" + 0.014*"wale" + 0.014*"melbourne" + 0.012*"class"
topic #23 (0.025): 0.017*"government" + 0.010*"court" + 0.010*"law" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.007*"say" + 0.006*"right" + 0.005*"issue" + 0.005*"claim"
topic #16 (0.025): 0.025*"war" + 0.023*"military" + 0.022*"army" + 0.018*"unit" + 0.017*"aircraft" + 0.016*"operation" + 0.014*"command" + 0.014*"air" + 0.013*"force" + 0.011*"officer"
topic diff=0.045035, rho=0.131494
PROGRESS: pass 7, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.040*"company" + 0.039*"french" + 0.031*"theatre" + 0.021*"production" + 0.019*"store" + 0.016*"theater" + 0.015*"opera" + 0.014*"hotel" + 0.012*"produce" + 0.011*"wine"
topic #1 (0.025): 0.115*"film" + 0.022*"star" + 0.016*"direct" + 0.012*"movie" + 0.010*"story" + 0.008*"character" + 0.008*"director" + 0.007*"role" + 0.006*"production" + 0.006*"produce"
topic #11 (0.025): 0.070*"game" + 0.024*"player" + 0.014*"version" + 0.012*"character" + 0.012*"computer" + 0.011*"card" + 0.010*"user" + 0.010*"software" + 0.010*"system" + 0.010*"video"
topic #36 (0.025): 0.012*"system" + 0.011*"displaystyle" + 0.008*"example" + 0.007*"datum" + 0.007*"point" + 0.006*"function" + 0.006*"value" + 0.006*"method" + 0.005*"set" + 0.005*"term"
topic #18 (0.025): 0.052*"village" + 0.048*"population" + 0.029*"town" + 0.026*"municipality" + 0.026*"region" + 0.016*"census" + 0.015*"province" + 0.013*"local" + 0.012*"rural" + 0.012*"central"
topic diff=0.038771, rho=0.131494
PROGRESS: pass 7, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.025): 0.017*"government" + 0.010*"law" + 0.009*"court" + 0.009*"act" + 0.007*"case" + 0.007*"right" + 0.007*"report" + 0.006*"say" + 0.005*"issue" + 0.005*"political"
topic #22 (0.025): 0.030*"park" + 0.026*"road" + 0.020*"street" + 0.017*"bridge" + 0.011*"construction" + 0.009*"plan" + 0.008*"project" + 0.007*"facility" + 0.007*"close" + 0.007*"site"
topic #21 (0.025): 0.011*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"back" + 0.006*"try" + 0.006*"kill" + 0.005*"life" + 0.005*"want" + 0.005*"friend" + 0.005*"help"
topic #26 (0.025): 0.069*"election" + 0.054*"party" + 0.039*"vote" + 0.030*"elect" + 0.028*"seat" + 0.026*"candidate" + 0.017*"parliament" + 0.017*"council" + 0.011*"assembly" + 0.011*"government"
topic #28 (0.025): 0.039*"county" + 0.028*"river" + 0.023*"age" + 0.023*"town" + 0.019*"north" + 0.017*"km" + 0.016*"population" + 0.015*"highway" + 0.015*"south" + 0.014*"route"
topic diff=0.041828, rho=0.131494
PROGRESS: pass 7, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.025): 0.020*"president" + 0.017*"appoint" + 0.013*"law" + 0.012*"elect" + 0.012*"woman" + 0.011*"office" + 0.009*"position" + 0.009*"general" + 0.009*"secretary" + 0.008*"chairman"
topic #9 (0.025): 0.039*"series" + 0.024*"episode" + 0.018*"television" + 0.014*"award" + 0.012*"season" + 0.011*"appear" + 0.010*"role" + 0.008*"character" + 0.008*"tv" + 0.007*"air"
topic #4 (0.025): 0.023*"battle" + 0.022*"attack" + 0.020*"force" + 0.018*"fight" + 0.015*"army" + 0.011*"war" + 0.011*"kill" + 0.011*"fire" + 0.010*"troop" + 0.009*"soldier"
topic #23 (0.025): 0.017*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"case" + 0.007*"say" + 0.007*"right" + 0.007*"report" + 0.005*"issue" + 0.005*"police"
topic #0 (0.025): 0.052*"language" + 0.026*"japanese" + 0.025*"word" + 0.022*"chinese" + 0.015*"refer" + 0.011*"mean" + 0.011*"define" + 0.011*"speak" + 0.009*"term" + 0.009*"dialect"
topic diff=0.040544, rho=0.131494
PROGRESS: pass 7, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #9 (0.025): 0.039*"series" + 0.024*"episode" + 0.018*"television" + 0.014*"award" + 0.011*"season" + 0.011*"appear" + 0.010*"role" + 0.008*"character" + 0.008*"tv" + 0.007*"air"
topic #20 (0.025): 0.046*"book" + 0.039*"publish" + 0.015*"novel" + 0.013*"author" + 0.012*"magazine" + 0.012*"writer" + 0.010*"story" + 0.010*"editor" + 0.009*"publication" + 0.008*"history"
topic #34 (0.025): 0.033*"church" + 0.031*"student" + 0.022*"child" + 0.021*"college" + 0.012*"bishop" + 0.012*"grade" + 0.011*"education" + 0.011*"teacher" + 0.009*"attend" + 0.008*"class"
topic #5 (0.025): 0.008*"accord" + 0.007*"life" + 0.007*"religious" + 0.007*"temple" + 0.007*"tradition" + 0.007*"christian" + 0.006*"ancient" + 0.005*"text" + 0.005*"say" + 0.005*"verse"
topic #32 (0.025): 0.015*"study" + 0.012*"research" + 0.011*"social" + 0.009*"society" + 0.008*"theory" + 0.007*"science" + 0.007*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"individual"
topic diff=0.039522, rho=0.131494
PROGRESS: pass 7, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #30 (0.025): 0.016*"black" + 0.014*"gold" + 0.012*"mine" + 0.011*"wear" + 0.009*"produce" + 0.007*"white" + 0.007*"iron" + 0.006*"mining" + 0.006*"tornado" + 0.005*"red"
topic #29 (0.025): 0.159*"german" + 0.078*"russian" + 0.043*"polish" + 0.042*"dutch" + 0.034*"soviet" + 0.023*"commune" + 0.021*"netherland" + 0.020*"danish" + 0.018*"village" + 0.015*"west"
topic #2 (0.025): 0.042*"club" + 0.033*"season" + 0.024*"league" + 0.020*"match" + 0.019*"football" + 0.017*"final" + 0.016*"player" + 0.014*"game" + 0.013*"championship" + 0.013*"goal"
topic #13 (0.025): 0.022*"university" + 0.016*"program" + 0.016*"student" + 0.013*"college" + 0.012*"research" + 0.011*"international" + 0.011*"award" + 0.011*"department" + 0.011*"education" + 0.011*"science"
topic #27 (0.025): 0.016*"cell" + 0.015*"protein" + 0.010*"gene" + 0.007*"human" + 0.007*"structure" + 0.006*"increase" + 0.005*"chemical" + 0.005*"material" + 0.005*"enzyme" + 0.005*"process"
topic diff=0.041137, rho=0.131494
PROGRESS: pass 7, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #22 (0.025): 0.029*"park" + 0.028*"road" + 0.022*"street" + 0.018*"bridge" + 0.011*"construction" + 0.009*"plan" + 0.008*"site" + 0.008*"facility" + 0.007*"project" + 0.007*"close"
topic #1 (0.025): 0.115*"film" + 0.025*"star" + 0.016*"direct" + 0.013*"movie" + 0.010*"story" + 0.008*"character" + 0.008*"director" + 0.007*"role" + 0.007*"produce" + 0.006*"award"
topic #12 (0.025): 0.072*"line" + 0.071*"station" + 0.039*"railway" + 0.027*"train" + 0.019*"operate" + 0.018*"airport" + 0.016*"route" + 0.016*"bus" + 0.014*"rail" + 0.013*"passenger"
topic #5 (0.025): 0.007*"accord" + 0.007*"religious" + 0.007*"life" + 0.007*"christian" + 0.007*"tradition" + 0.007*"temple" + 0.006*"ancient" + 0.005*"text" + 0.005*"religion" + 0.005*"say"
topic #4 (0.025): 0.023*"battle" + 0.022*"attack" + 0.020*"force" + 0.017*"fight" + 0.015*"army" + 0.012*"fire" + 0.011*"war" + 0.010*"kill" + 0.010*"troop" + 0.009*"soldier"
topic diff=0.045025, rho=0.131494
PROGRESS: pass 7, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #32 (0.025): 0.015*"study" + 0.012*"research" + 0.012*"social" + 0.009*"society" + 0.008*"theory" + 0.006*"science" + 0.006*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"develop"
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.007*"say" + 0.007*"right" + 0.006*"issue" + 0.005*"claim"
topic #9 (0.025): 0.038*"series" + 0.024*"episode" + 0.018*"television" + 0.013*"award" + 0.011*"appear" + 0.010*"season" + 0.010*"role" + 0.009*"character" + 0.008*"actor" + 0.008*"tv"
topic #27 (0.025): 0.015*"cell" + 0.015*"protein" + 0.010*"gene" + 0.008*"structure" + 0.007*"human" + 0.005*"material" + 0.005*"increase" + 0.005*"chemical" + 0.005*"contain" + 0.005*"process"
topic #37 (0.025): 0.040*"ship" + 0.029*"island" + 0.013*"port" + 0.012*"sea" + 0.011*"coast" + 0.011*"crew" + 0.010*"boat" + 0.010*"navy" + 0.010*"fleet" + 0.009*"vessel"
topic diff=0.043992, rho=0.131494
PROGRESS: pass 7, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #13 (0.025): 0.022*"university" + 0.016*"program" + 0.016*"student" + 0.014*"college" + 0.012*"research" + 0.012*"award" + 0.011*"international" + 0.011*"science" + 0.011*"study" + 0.011*"department"
topic #34 (0.025): 0.040*"church" + 0.029*"student" + 0.022*"child" + 0.020*"college" + 0.013*"bishop" + 0.011*"education" + 0.011*"teacher" + 0.010*"grade" + 0.009*"attend" + 0.008*"class"
topic #30 (0.025): 0.019*"black" + 0.014*"gold" + 0.012*"mine" + 0.010*"wear" + 0.008*"produce" + 0.007*"white" + 0.006*"coal" + 0.006*"iron" + 0.006*"steel" + 0.006*"mining"
topic #12 (0.025): 0.071*"line" + 0.070*"station" + 0.039*"railway" + 0.028*"train" + 0.020*"airport" + 0.020*"operate" + 0.017*"bus" + 0.015*"route" + 0.014*"passenger" + 0.014*"rail"
topic #31 (0.025): 0.067*"australian" + 0.040*"australia" + 0.019*"match" + 0.018*"sydney" + 0.018*"cricket" + 0.015*"test" + 0.015*"south_wale" + 0.015*"wale" + 0.015*"melbourne" + 0.013*"class"
topic diff=0.038718, rho=0.131494
-8.314 per-word bound, 318.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #3 (0.025): 0.015*"design" + 0.013*"power" + 0.011*"engine" + 0.010*"system" + 0.009*"car" + 0.008*"model" + 0.008*"vehicle" + 0.005*"produce" + 0.005*"control" + 0.005*"type"
topic #8 (0.025): 0.039*"album" + 0.038*"song" + 0.033*"music" + 0.025*"band" + 0.016*"single" + 0.011*"perform" + 0.011*"track" + 0.008*"tour" + 0.008*"chart" + 0.006*"singer"
topic #13 (0.025): 0.021*"university" + 0.016*"student" + 0.016*"program" + 0.014*"college" + 0.012*"research" + 0.012*"award" + 0.011*"science" + 0.011*"international" + 0.011*"department" + 0.011*"study"
topic #2 (0.025): 0.043*"club" + 0.034*"season" + 0.023*"league" + 0.021*"match" + 0.019*"football" + 0.017*"final" + 0.016*"player" + 0.014*"goal" + 0.014*"game" + 0.013*"score"
topic #6 (0.025): 0.055*"specie" + 0.019*"genus" + 0.016*"describe" + 0.014*"white" + 0.013*"plant" + 0.010*"brown" + 0.009*"flower" + 0.009*"black" + 0.008*"female" + 0.008*"male"
topic diff=0.035810, rho=0.131494
-8.318 per-word bound, 319.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3835 documents into a model of 49835 documents
topic #31 (0.025): 0.069*"australian" + 0.043*"australia" + 0.019*"match" + 0.018*"cricket" + 0.018*"sydney" + 0.016*"melbourne" + 0.015*"test" + 0.015*"wale" + 0.015*"south_wale" + 0.014*"class"
topic #2 (0.025): 0.042*"club" + 0.033*"season" + 0.023*"league" + 0.021*"match" + 0.019*"football" + 0.017*"final" + 0.016*"player" + 0.014*"goal" + 0.014*"game" + 0.014*"score"
topic #11 (0.025): 0.065*"game" + 0.024*"player" + 0.014*"version" + 0.012*"computer" + 0.011*"user" + 0.011*"system" + 0.010*"character" + 0.009*"software" + 0.009*"video" + 0.008*"file"
topic #19 (0.025): 0.039*"building" + 0.024*"house" + 0.017*"church" + 0.011*"site" + 0.011*"design" + 0.009*"stone" + 0.008*"wall" + 0.008*"th_century" + 0.007*"room" + 0.007*"tower"
topic #32 (0.025): 0.014*"study" + 0.011*"research" + 0.011*"social" + 0.009*"society" + 0.007*"theory" + 0.006*"science" + 0.006*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"experience"
topic diff=0.040420, rho=0.131494
-8.276 per-word bound, 310.0 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1000 documents into a model of 49835 documents
topic #27 (0.025): 0.018*"cell" + 0.013*"protein" + 0.010*"gene" + 0.007*"structure" + 0.007*"human" + 0.007*"material" + 0.005*"process" + 0.005*"temperature" + 0.005*"increase" + 0.005*"energy"
topic #25 (0.025): 0.019*"president" + 0.017*"appoint" + 0.012*"law" + 0.012*"elect" + 0.012*"office" + 0.011*"woman" + 0.009*"general" + 0.009*"position" + 0.009*"secretary" + 0.008*"governor"
topic #37 (0.025): 0.041*"ship" + 0.030*"island" + 0.012*"port" + 0.012*"sea" + 0.011*"fleet" + 0.011*"coast" + 0.011*"boat" + 0.010*"vessel" + 0.010*"crew" + 0.010*"navy"
topic #20 (0.025): 0.047*"book" + 0.038*"publish" + 0.014*"novel" + 0.013*"author" + 0.013*"magazine" + 0.012*"writer" + 0.011*"story" + 0.010*"editor" + 0.009*"history" + 0.009*"publication"
topic #36 (0.025): 0.012*"system" + 0.010*"displaystyle" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.007*"function" + 0.006*"method" + 0.005*"value" + 0.005*"set" + 0.005*"term"
topic diff=0.045633, rho=0.131494
-8.283 per-word bound, 311.5 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.008*"tree" + 0.007*"mountain" + 0.007*"site" + 0.006*"animal" + 0.006*"bird" + 0.005*"region" + 0.005*"park" + 0.005*"range"
topic #12 (0.025): 0.072*"line" + 0.068*"station" + 0.046*"railway" + 0.028*"train" + 0.020*"operate" + 0.019*"bus" + 0.018*"airport" + 0.017*"rail" + 0.014*"route" + 0.013*"passenger"
topic #36 (0.025): 0.013*"system" + 0.011*"displaystyle" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.007*"function" + 0.006*"set" + 0.006*"method" + 0.006*"value" + 0.005*"term"
topic #35 (0.025): 0.043*"event" + 0.041*"race" + 0.029*"championship" + 0.027*"compete" + 0.020*"finish" + 0.017*"woman" + 0.015*"competition" + 0.013*"sport" + 0.010*"final" + 0.009*"winner"
topic #39 (0.025): 0.015*"patient" + 0.013*"treatment" + 0.013*"cause" + 0.012*"disease" + 0.010*"medical" + 0.009*"drug" + 0.008*"health" + 0.007*"increase" + 0.006*"body" + 0.006*"risk"
topic diff=0.041308, rho=0.130371
PROGRESS: pass 8, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 10
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.025): 0.017*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"say" + 0.006*"case" + 0.006*"right" + 0.006*"issue" + 0.005*"country"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.011*"vehicle" + 0.010*"system" + 0.009*"model" + 0.008*"car" + 0.006*"produce" + 0.005*"control" + 0.005*"type"
topic #22 (0.025): 0.031*"park" + 0.027*"road" + 0.020*"street" + 0.018*"bridge" + 0.012*"construction" + 0.009*"plan" + 0.008*"project" + 0.008*"facility" + 0.007*"north" + 0.007*"site"
topic #26 (0.025): 0.069*"election" + 0.054*"party" + 0.040*"vote" + 0.031*"elect" + 0.025*"candidate" + 0.025*"seat" + 0.019*"parliament" + 0.016*"council" + 0.011*"assembly" + 0.011*"government"
topic #37 (0.025): 0.040*"ship" + 0.032*"island" + 0.012*"sea" + 0.012*"port" + 0.011*"coast" + 0.010*"boat" + 0.010*"fleet" + 0.010*"crew" + 0.009*"navy" + 0.009*"vessel"
topic diff=0.039687, rho=0.130371
PROGRESS: pass 8, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #38 (0.025): 0.051*"station" + 0.036*"radio" + 0.023*"channel" + 0.020*"network" + 0.020*"broadcast" + 0.016*"jewish" + 0.016*"news" + 0.015*"television" + 0.015*"tv" + 0.014*"air"
topic #22 (0.025): 0.031*"park" + 0.027*"road" + 0.021*"street" + 0.018*"bridge" + 0.012*"construction" + 0.009*"plan" + 0.008*"project" + 0.008*"north" + 0.008*"facility" + 0.007*"site"
topic #15 (0.025): 0.052*"season" + 0.051*"game" + 0.016*"player" + 0.015*"football" + 0.015*"basketball" + 0.015*"league" + 0.013*"coach" + 0.009*"career" + 0.009*"baseball" + 0.009*"college"
topic #13 (0.025): 0.022*"university" + 0.016*"program" + 0.015*"student" + 0.015*"college" + 0.012*"research" + 0.012*"award" + 0.011*"education" + 0.011*"international" + 0.011*"science" + 0.011*"study"
topic #7 (0.025): 0.079*"canadian" + 0.047*"irish" + 0.039*"festival" + 0.032*"swedish" + 0.026*"norwegian" + 0.019*"finnish" + 0.016*"hungarian" + 0.014*"finland" + 0.013*"quebec" + 0.011*"belgian"
topic diff=0.036565, rho=0.130371
PROGRESS: pass 8, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #31 (0.025): 0.065*"australian" + 0.039*"australia" + 0.021*"cricket" + 0.019*"match" + 0.017*"sydney" + 0.015*"test" + 0.014*"south_wale" + 0.014*"melbourne" + 0.014*"wale" + 0.013*"class"
topic #18 (0.025): 0.052*"village" + 0.047*"population" + 0.029*"town" + 0.025*"municipality" + 0.025*"region" + 0.015*"census" + 0.014*"province" + 0.013*"local" + 0.013*"central" + 0.011*"rural"
topic #12 (0.025): 0.070*"line" + 0.070*"station" + 0.044*"railway" + 0.027*"train" + 0.021*"operate" + 0.018*"airport" + 0.018*"route" + 0.017*"bus" + 0.016*"rail" + 0.013*"passenger"
topic #33 (0.025): 0.040*"company" + 0.040*"french" + 0.031*"theatre" + 0.021*"production" + 0.019*"store" + 0.018*"theater" + 0.017*"hotel" + 0.014*"opera" + 0.012*"produce" + 0.012*"wine"
topic #32 (0.025): 0.015*"study" + 0.011*"social" + 0.011*"research" + 0.009*"society" + 0.007*"theory" + 0.006*"science" + 0.006*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"individual"
topic diff=0.038577, rho=0.130371
PROGRESS: pass 8, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.025): 0.056*"art" + 0.029*"museum" + 0.027*"artist" + 0.018*"painting" + 0.014*"exhibition" + 0.013*"award" + 0.011*"design" + 0.010*"collection" + 0.010*"paint" + 0.008*"study"
topic #11 (0.025): 0.071*"game" + 0.025*"player" + 0.013*"version" + 0.013*"computer" + 0.011*"user" + 0.011*"card" + 0.010*"system" + 0.010*"character" + 0.010*"video" + 0.009*"software"
topic #0 (0.025): 0.051*"language" + 0.029*"japanese" + 0.024*"word" + 0.022*"chinese" + 0.015*"refer" + 0.013*"define" + 0.011*"speak" + 0.011*"mean" + 0.010*"nationality" + 0.009*"term"
topic #36 (0.025): 0.012*"system" + 0.012*"displaystyle" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.007*"function" + 0.006*"value" + 0.006*"method" + 0.005*"set" + 0.005*"term"
topic #18 (0.025): 0.051*"village" + 0.047*"population" + 0.028*"town" + 0.025*"region" + 0.025*"municipality" + 0.015*"census" + 0.015*"province" + 0.013*"local" + 0.013*"central" + 0.011*"rural"
topic diff=0.035545, rho=0.130371
PROGRESS: pass 8, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #34 (0.025): 0.037*"church" + 0.031*"student" + 0.023*"child" + 0.021*"college" + 0.013*"grade" + 0.013*"bishop" + 0.012*"education" + 0.011*"teacher" + 0.009*"attend" + 0.008*"catholic"
topic #21 (0.025): 0.011*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"back" + 0.006*"try" + 0.006*"kill" + 0.005*"want" + 0.005*"life" + 0.005*"help" + 0.005*"friend"
topic #1 (0.025): 0.119*"film" + 0.023*"star" + 0.017*"direct" + 0.013*"movie" + 0.010*"story" + 0.008*"character" + 0.008*"director" + 0.007*"role" + 0.007*"production" + 0.007*"produce"
topic #24 (0.025): 0.056*"art" + 0.029*"museum" + 0.027*"artist" + 0.018*"painting" + 0.014*"exhibition" + 0.013*"award" + 0.012*"design" + 0.010*"collection" + 0.010*"paint" + 0.008*"study"
topic #31 (0.025): 0.066*"australian" + 0.042*"australia" + 0.021*"cricket" + 0.020*"match" + 0.018*"sydney" + 0.015*"south_wale" + 0.015*"test" + 0.015*"melbourne" + 0.014*"wale" + 0.013*"class"
topic diff=0.038996, rho=0.130371
PROGRESS: pass 8, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #32 (0.025): 0.015*"study" + 0.011*"social" + 0.011*"research" + 0.009*"society" + 0.007*"theory" + 0.006*"idea" + 0.006*"science" + 0.006*"life" + 0.005*"individual" + 0.005*"human"
topic #36 (0.025): 0.012*"system" + 0.011*"displaystyle" + 0.008*"example" + 0.007*"datum" + 0.007*"point" + 0.006*"function" + 0.006*"value" + 0.006*"method" + 0.005*"set" + 0.005*"term"
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.008*"tree" + 0.007*"mountain" + 0.007*"animal" + 0.006*"bird" + 0.006*"site" + 0.006*"park" + 0.005*"region" + 0.005*"river"
topic #22 (0.025): 0.032*"park" + 0.028*"road" + 0.021*"street" + 0.017*"bridge" + 0.011*"construction" + 0.009*"plan" + 0.008*"project" + 0.007*"north" + 0.007*"facility" + 0.007*"close"
topic #24 (0.025): 0.055*"art" + 0.029*"museum" + 0.027*"artist" + 0.017*"painting" + 0.013*"exhibition" + 0.013*"design" + 0.013*"award" + 0.011*"collection" + 0.010*"paint" + 0.008*"study"
topic diff=0.033676, rho=0.130371
PROGRESS: pass 8, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #10 (0.025): 0.017*"king" + 0.016*"son" + 0.009*"death" + 0.008*"war" + 0.007*"brother" + 0.007*"father" + 0.007*"daughter" + 0.006*"royal" + 0.006*"kingdom" + 0.006*"great"
topic #18 (0.025): 0.052*"village" + 0.047*"population" + 0.030*"town" + 0.026*"municipality" + 0.026*"region" + 0.015*"province" + 0.015*"census" + 0.014*"local" + 0.012*"central" + 0.012*"rural"
topic #33 (0.025): 0.042*"french" + 0.039*"company" + 0.032*"theatre" + 0.020*"production" + 0.019*"store" + 0.017*"opera" + 0.016*"theater" + 0.016*"hotel" + 0.011*"produce" + 0.011*"restaurant"
topic #31 (0.025): 0.068*"australian" + 0.041*"australia" + 0.021*"cricket" + 0.018*"match" + 0.018*"sydney" + 0.015*"melbourne" + 0.015*"test" + 0.015*"south_wale" + 0.014*"wale" + 0.012*"class"
topic #27 (0.025): 0.016*"cell" + 0.011*"protein" + 0.009*"gene" + 0.007*"human" + 0.007*"structure" + 0.006*"material" + 0.006*"increase" + 0.005*"chemical" + 0.005*"process" + 0.005*"temperature"
topic diff=0.036464, rho=0.130371
PROGRESS: pass 8, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.025): 0.052*"language" + 0.029*"japanese" + 0.025*"word" + 0.024*"chinese" + 0.015*"refer" + 0.012*"define" + 0.011*"mean" + 0.011*"speak" + 0.009*"english" + 0.009*"dialect"
topic #4 (0.025): 0.023*"battle" + 0.022*"attack" + 0.021*"force" + 0.018*"fight" + 0.015*"army" + 0.013*"war" + 0.011*"kill" + 0.011*"fire" + 0.010*"troop" + 0.009*"soldier"
topic #12 (0.025): 0.069*"line" + 0.068*"station" + 0.041*"railway" + 0.029*"train" + 0.020*"operate" + 0.018*"bus" + 0.017*"airport" + 0.017*"route" + 0.016*"rail" + 0.014*"passenger"
topic #8 (0.025): 0.039*"album" + 0.037*"song" + 0.033*"music" + 0.026*"band" + 0.016*"single" + 0.011*"track" + 0.011*"perform" + 0.009*"tour" + 0.008*"chart" + 0.007*"video"
topic #38 (0.025): 0.050*"station" + 0.042*"radio" + 0.022*"channel" + 0.021*"broadcast" + 0.020*"network" + 0.015*"jewish" + 0.014*"news" + 0.014*"television" + 0.014*"air" + 0.013*"program"
topic diff=0.035295, rho=0.130371
PROGRESS: pass 8, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.025): 0.170*"german" + 0.075*"russian" + 0.042*"polish" + 0.042*"dutch" + 0.034*"soviet" + 0.024*"commune" + 0.021*"netherland" + 0.020*"danish" + 0.018*"village" + 0.015*"ukrainian"
topic #17 (0.025): 0.034*"company" + 0.012*"business" + 0.009*"market" + 0.008*"product" + 0.007*"industry" + 0.007*"sell" + 0.007*"provide" + 0.007*"purchase" + 0.006*"sale" + 0.006*"increase"
topic #15 (0.025): 0.051*"season" + 0.051*"game" + 0.016*"football" + 0.016*"player" + 0.014*"league" + 0.014*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.010*"point" + 0.010*"college"
topic #9 (0.025): 0.040*"series" + 0.024*"episode" + 0.018*"television" + 0.015*"award" + 0.012*"season" + 0.011*"appear" + 0.010*"role" + 0.008*"character" + 0.008*"tv" + 0.007*"air"
topic #19 (0.025): 0.041*"building" + 0.024*"house" + 0.016*"church" + 0.012*"site" + 0.011*"design" + 0.008*"stone" + 0.008*"th_century" + 0.008*"wall" + 0.007*"room" + 0.007*"tower"
topic diff=0.034434, rho=0.130371
PROGRESS: pass 8, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #7 (0.025): 0.074*"canadian" + 0.057*"irish" + 0.040*"festival" + 0.030*"swedish" + 0.027*"norwegian" + 0.020*"hungarian" + 0.018*"finnish" + 0.014*"quebec" + 0.012*"belgian" + 0.011*"finland"
topic #26 (0.025): 0.069*"election" + 0.055*"party" + 0.041*"vote" + 0.030*"elect" + 0.027*"candidate" + 0.026*"seat" + 0.017*"parliament" + 0.015*"council" + 0.012*"assembly" + 0.011*"government"
topic #29 (0.025): 0.168*"german" + 0.079*"russian" + 0.043*"dutch" + 0.043*"polish" + 0.034*"soviet" + 0.024*"commune" + 0.021*"netherland" + 0.019*"danish" + 0.018*"village" + 0.015*"ukrainian"
topic #33 (0.025): 0.039*"french" + 0.038*"company" + 0.032*"theatre" + 0.021*"production" + 0.018*"store" + 0.015*"opera" + 0.015*"hotel" + 0.015*"theater" + 0.013*"wine" + 0.012*"restaurant"
topic #21 (0.025): 0.011*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"try" + 0.006*"back" + 0.006*"kill" + 0.005*"friend" + 0.005*"want" + 0.005*"life" + 0.005*"help"
topic diff=0.036478, rho=0.130371
PROGRESS: pass 8, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #15 (0.025): 0.052*"season" + 0.050*"game" + 0.016*"football" + 0.016*"player" + 0.015*"league" + 0.013*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.010*"college" + 0.009*"point"
topic #24 (0.025): 0.058*"art" + 0.028*"museum" + 0.027*"artist" + 0.019*"painting" + 0.012*"exhibition" + 0.012*"design" + 0.011*"award" + 0.011*"collection" + 0.010*"paint" + 0.008*"study"
topic #26 (0.025): 0.069*"election" + 0.055*"party" + 0.041*"vote" + 0.030*"elect" + 0.026*"candidate" + 0.026*"seat" + 0.016*"parliament" + 0.016*"council" + 0.012*"assembly" + 0.011*"government"
topic #34 (0.025): 0.048*"church" + 0.028*"student" + 0.023*"child" + 0.020*"college" + 0.014*"bishop" + 0.011*"education" + 0.011*"teacher" + 0.011*"grade" + 0.009*"attend" + 0.008*"catholic"
topic #22 (0.025): 0.030*"park" + 0.029*"road" + 0.022*"street" + 0.018*"bridge" + 0.011*"construction" + 0.009*"plan" + 0.008*"facility" + 0.008*"site" + 0.008*"project" + 0.007*"north"
topic diff=0.039839, rho=0.130371
PROGRESS: pass 8, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.025): 0.040*"building" + 0.025*"house" + 0.016*"church" + 0.011*"site" + 0.011*"design" + 0.008*"stone" + 0.008*"th_century" + 0.008*"wall" + 0.007*"room" + 0.007*"tower"
topic #5 (0.025): 0.008*"temple" + 0.008*"christian" + 0.008*"accord" + 0.007*"tradition" + 0.007*"religious" + 0.007*"life" + 0.006*"ancient" + 0.006*"text" + 0.005*"religion" + 0.005*"say"
topic #31 (0.025): 0.069*"australian" + 0.040*"australia" + 0.019*"cricket" + 0.019*"match" + 0.017*"sydney" + 0.016*"south_wale" + 0.015*"melbourne" + 0.015*"wale" + 0.014*"test" + 0.014*"class"
topic #13 (0.025): 0.023*"university" + 0.017*"student" + 0.016*"program" + 0.014*"college" + 0.013*"research" + 0.013*"award" + 0.011*"international" + 0.011*"science" + 0.011*"study" + 0.011*"education"
topic #32 (0.025): 0.014*"study" + 0.012*"social" + 0.010*"research" + 0.009*"society" + 0.008*"theory" + 0.006*"idea" + 0.005*"science" + 0.005*"human" + 0.005*"life" + 0.005*"individual"
topic diff=0.039195, rho=0.130371
PROGRESS: pass 8, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #25 (0.025): 0.019*"president" + 0.017*"appoint" + 0.013*"law" + 0.012*"elect" + 0.011*"office" + 0.011*"woman" + 0.009*"position" + 0.009*"general" + 0.009*"secretary" + 0.008*"director"
topic #15 (0.025): 0.052*"season" + 0.050*"game" + 0.017*"football" + 0.016*"player" + 0.015*"league" + 0.015*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.010*"college" + 0.010*"baseball"
topic #18 (0.025): 0.053*"village" + 0.047*"population" + 0.030*"town" + 0.025*"region" + 0.025*"municipality" + 0.015*"province" + 0.014*"census" + 0.014*"local" + 0.013*"central" + 0.011*"rural"
topic #36 (0.025): 0.012*"system" + 0.010*"displaystyle" + 0.008*"example" + 0.008*"point" + 0.007*"datum" + 0.007*"function" + 0.006*"set" + 0.006*"value" + 0.005*"method" + 0.005*"term"
topic #31 (0.025): 0.069*"australian" + 0.041*"australia" + 0.019*"match" + 0.019*"sydney" + 0.018*"cricket" + 0.016*"south_wale" + 0.016*"test" + 0.015*"wale" + 0.015*"melbourne" + 0.014*"class"
topic diff=0.033941, rho=0.130371
-8.324 per-word bound, 320.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #35 (0.025): 0.042*"race" + 0.042*"event" + 0.028*"championship" + 0.026*"compete" + 0.020*"finish" + 0.020*"woman" + 0.014*"competition" + 0.013*"sport" + 0.010*"final" + 0.010*"point"
topic #10 (0.025): 0.017*"king" + 0.016*"son" + 0.009*"death" + 0.008*"war" + 0.007*"brother" + 0.007*"daughter" + 0.007*"father" + 0.006*"royal" + 0.006*"kingdom" + 0.006*"french"
topic #38 (0.025): 0.050*"station" + 0.038*"radio" + 0.023*"channel" + 0.021*"broadcast" + 0.019*"network" + 0.017*"news" + 0.015*"tv" + 0.015*"television" + 0.014*"jewish" + 0.014*"air"
topic #26 (0.025): 0.067*"election" + 0.056*"party" + 0.039*"vote" + 0.031*"elect" + 0.025*"candidate" + 0.024*"seat" + 0.018*"parliament" + 0.016*"council" + 0.011*"government" + 0.011*"assembly"
topic #21 (0.025): 0.011*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"back" + 0.006*"try" + 0.006*"kill" + 0.005*"life" + 0.005*"friend" + 0.005*"help" + 0.005*"want"
topic diff=0.031244, rho=0.130371
-8.326 per-word bound, 320.8 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #23 (0.025): 0.016*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"report" + 0.007*"case" + 0.006*"say" + 0.006*"right" + 0.006*"issue" + 0.005*"country"
topic #26 (0.025): 0.068*"election" + 0.056*"party" + 0.039*"vote" + 0.030*"elect" + 0.024*"candidate" + 0.024*"seat" + 0.017*"parliament" + 0.015*"council" + 0.012*"government" + 0.011*"labour"
topic #19 (0.025): 0.039*"building" + 0.025*"house" + 0.016*"church" + 0.012*"site" + 0.011*"design" + 0.009*"stone" + 0.008*"th_century" + 0.008*"wall" + 0.007*"tower" + 0.007*"room"
topic #5 (0.025): 0.008*"accord" + 0.007*"tradition" + 0.007*"temple" + 0.007*"religious" + 0.007*"christian" + 0.007*"life" + 0.006*"ancient" + 0.006*"text" + 0.005*"say" + 0.005*"religion"
topic #13 (0.025): 0.022*"university" + 0.017*"student" + 0.016*"program" + 0.015*"college" + 0.013*"research" + 0.012*"award" + 0.012*"science" + 0.011*"international" + 0.011*"study" + 0.011*"education"
topic diff=0.038927, rho=0.130371
-8.330 per-word bound, 321.7 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 1835 documents into a model of 49835 documents
topic #38 (0.025): 0.050*"station" + 0.038*"radio" + 0.023*"channel" + 0.021*"broadcast" + 0.020*"network" + 0.017*"news" + 0.015*"jewish" + 0.014*"television" + 0.014*"tv" + 0.014*"air"
topic #3 (0.025): 0.015*"design" + 0.012*"power" + 0.011*"engine" + 0.010*"system" + 0.010*"vehicle" + 0.009*"model" + 0.009*"car" + 0.006*"produce" + 0.005*"type" + 0.005*"standard"
topic #7 (0.025): 0.084*"canadian" + 0.054*"irish" + 0.038*"festival" + 0.032*"swedish" + 0.030*"norwegian" + 0.019*"finnish" + 0.017*"hungarian" + 0.015*"quebec" + 0.011*"san_jose" + 0.011*"nhl"
topic #2 (0.025): 0.043*"club" + 0.033*"season" + 0.023*"league" + 0.021*"match" + 0.019*"football" + 0.017*"player" + 0.017*"final" + 0.015*"game" + 0.014*"goal" + 0.014*"score"
topic #33 (0.025): 0.040*"french" + 0.039*"company" + 0.031*"theatre" + 0.022*"production" + 0.019*"store" + 0.015*"hotel" + 0.014*"opera" + 0.014*"theater" + 0.013*"produce" + 0.012*"wine"
topic diff=0.035439, rho=0.130371
-8.247 per-word bound, 303.9 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #1000/49835, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #2000/49835, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #3000/49835, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #4000/49835, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #5000/49835, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #6000/49835, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #7000/49835, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #8000/49835, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #9000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #10000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #10 = documents up to #11000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.040*"french" + 0.039*"company" + 0.032*"theatre" + 0.022*"production" + 0.019*"store" + 0.014*"hotel" + 0.014*"opera" + 0.014*"theater" + 0.014*"wine" + 0.012*"produce"
topic #11 (0.025): 0.066*"game" + 0.025*"player" + 0.013*"version" + 0.012*"computer" + 0.012*"user" + 0.011*"system" + 0.010*"video" + 0.010*"character" + 0.009*"software" + 0.008*"file"
topic #39 (0.025): 0.015*"patient" + 0.013*"treatment" + 0.013*"cause" + 0.011*"disease" + 0.011*"medical" + 0.009*"drug" + 0.008*"health" + 0.007*"increase" + 0.006*"body" + 0.006*"risk"
topic #6 (0.025): 0.056*"specie" + 0.019*"genus" + 0.016*"describe" + 0.013*"white" + 0.013*"plant" + 0.011*"female" + 0.009*"male" + 0.009*"brown" + 0.009*"black" + 0.008*"flower"
topic #36 (0.025): 0.012*"system" + 0.012*"displaystyle" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.007*"function" + 0.006*"set" + 0.006*"method" + 0.006*"value" + 0.005*"case"
topic diff=0.036301, rho=0.129277
PROGRESS: pass 9, dispatched chunk #11 = documents up to #12000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #12 = documents up to #13000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #13 = documents up to #14000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.025): 0.054*"village" + 0.046*"population" + 0.029*"town" + 0.025*"region" + 0.024*"municipality" + 0.015*"census" + 0.015*"province" + 0.014*"local" + 0.013*"central" + 0.011*"rural"
topic #36 (0.025): 0.013*"displaystyle" + 0.012*"system" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.007*"function" + 0.006*"method" + 0.006*"value" + 0.006*"set" + 0.005*"case"
topic #0 (0.025): 0.053*"language" + 0.027*"japanese" + 0.025*"word" + 0.022*"chinese" + 0.014*"refer" + 0.014*"define" + 0.011*"nationality" + 0.011*"speak" + 0.010*"english" + 0.010*"mean"
topic #35 (0.025): 0.044*"event" + 0.040*"race" + 0.029*"championship" + 0.027*"compete" + 0.020*"finish" + 0.018*"woman" + 0.015*"competition" + 0.013*"sport" + 0.011*"final" + 0.010*"olympic"
topic #28 (0.025): 0.040*"county" + 0.028*"river" + 0.026*"age" + 0.022*"town" + 0.019*"north" + 0.018*"km" + 0.018*"population" + 0.014*"household" + 0.013*"south" + 0.012*"route"
topic diff=0.036006, rho=0.129277
PROGRESS: pass 9, dispatched chunk #14 = documents up to #15000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #15 = documents up to #16000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #16 = documents up to #17000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #36 (0.025): 0.014*"displaystyle" + 0.012*"system" + 0.008*"example" + 0.007*"point" + 0.007*"function" + 0.007*"datum" + 0.006*"method" + 0.006*"set" + 0.006*"value" + 0.005*"case"
topic #27 (0.025): 0.016*"cell" + 0.012*"protein" + 0.010*"gene" + 0.007*"structure" + 0.007*"human" + 0.007*"material" + 0.006*"temperature" + 0.005*"process" + 0.005*"increase" + 0.005*"energy"
topic #1 (0.025): 0.119*"film" + 0.023*"star" + 0.017*"direct" + 0.013*"movie" + 0.010*"story" + 0.008*"character" + 0.008*"director" + 0.007*"role" + 0.007*"produce" + 0.007*"production"
topic #15 (0.025): 0.052*"season" + 0.051*"game" + 0.017*"player" + 0.015*"football" + 0.015*"basketball" + 0.015*"league" + 0.013*"coach" + 0.009*"career" + 0.009*"baseball" + 0.009*"college"
topic #31 (0.025): 0.067*"australian" + 0.042*"australia" + 0.020*"cricket" + 0.020*"match" + 0.017*"sydney" + 0.016*"test" + 0.015*"south_wale" + 0.014*"melbourne" + 0.014*"wale" + 0.013*"class"
topic diff=0.032654, rho=0.129277
PROGRESS: pass 9, dispatched chunk #17 = documents up to #18000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #18 = documents up to #19000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #19 = documents up to #20000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.008*"mountain" + 0.008*"tree" + 0.007*"animal" + 0.006*"site" + 0.006*"bird" + 0.005*"region" + 0.005*"park" + 0.005*"river"
topic #24 (0.025): 0.055*"art" + 0.030*"museum" + 0.027*"artist" + 0.019*"painting" + 0.013*"exhibition" + 0.012*"design" + 0.010*"collection" + 0.010*"paint" + 0.010*"award" + 0.008*"study"
topic #7 (0.025): 0.086*"canadian" + 0.050*"irish" + 0.037*"festival" + 0.032*"swedish" + 0.027*"norwegian" + 0.019*"hungarian" + 0.019*"finnish" + 0.015*"quebec" + 0.012*"finland" + 0.012*"belgian"
topic #27 (0.025): 0.016*"cell" + 0.012*"protein" + 0.010*"gene" + 0.007*"structure" + 0.007*"human" + 0.007*"material" + 0.005*"process" + 0.005*"increase" + 0.005*"temperature" + 0.005*"surface"
topic #8 (0.025): 0.038*"album" + 0.037*"song" + 0.034*"music" + 0.026*"band" + 0.016*"single" + 0.011*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic diff=0.034938, rho=0.129277
PROGRESS: pass 9, dispatched chunk #20 = documents up to #21000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #21 = documents up to #22000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #22 = documents up to #23000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #8 (0.025): 0.039*"album" + 0.038*"song" + 0.034*"music" + 0.026*"band" + 0.016*"single" + 0.010*"perform" + 0.010*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"video"
topic #17 (0.025): 0.034*"company" + 0.011*"business" + 0.010*"market" + 0.008*"product" + 0.007*"industry" + 0.007*"sell" + 0.006*"provide" + 0.006*"increase" + 0.006*"purchase" + 0.006*"public"
topic #21 (0.025): 0.012*"say" + 0.009*"get" + 0.008*"tell" + 0.006*"back" + 0.006*"try" + 0.006*"kill" + 0.005*"life" + 0.005*"want" + 0.005*"help" + 0.005*"friend"
topic #13 (0.025): 0.020*"university" + 0.016*"student" + 0.015*"program" + 0.015*"college" + 0.013*"research" + 0.012*"award" + 0.012*"science" + 0.012*"study" + 0.011*"education" + 0.011*"international"
topic #24 (0.025): 0.056*"art" + 0.029*"museum" + 0.027*"artist" + 0.018*"painting" + 0.014*"exhibition" + 0.012*"design" + 0.010*"paint" + 0.010*"collection" + 0.010*"award" + 0.008*"study"
topic diff=0.031909, rho=0.129277
PROGRESS: pass 9, dispatched chunk #23 = documents up to #24000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #24 = documents up to #25000/49835, outstanding queue size 10
PROGRESS: pass 9, dispatched chunk #25 = documents up to #26000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #26 (0.025): 0.068*"election" + 0.052*"party" + 0.040*"vote" + 0.031*"elect" + 0.026*"seat" + 0.026*"candidate" + 0.017*"parliament" + 0.017*"council" + 0.012*"government" + 0.012*"assembly"
topic #11 (0.025): 0.065*"game" + 0.023*"player" + 0.014*"version" + 0.012*"computer" + 0.011*"user" + 0.011*"system" + 0.011*"character" + 0.010*"software" + 0.010*"card" + 0.010*"video"
topic #16 (0.025): 0.024*"war" + 0.023*"military" + 0.022*"army" + 0.019*"unit" + 0.017*"aircraft" + 0.016*"operation" + 0.014*"command" + 0.014*"air" + 0.012*"force" + 0.011*"officer"
topic #10 (0.025): 0.017*"king" + 0.016*"son" + 0.009*"death" + 0.008*"war" + 0.007*"brother" + 0.007*"father" + 0.007*"daughter" + 0.006*"royal" + 0.006*"kingdom" + 0.006*"french"
topic #35 (0.025): 0.043*"event" + 0.038*"race" + 0.028*"championship" + 0.027*"compete" + 0.019*"finish" + 0.019*"woman" + 0.014*"competition" + 0.013*"sport" + 0.011*"final" + 0.009*"winner"
topic diff=0.035349, rho=0.129277
PROGRESS: pass 9, dispatched chunk #26 = documents up to #27000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #27 = documents up to #28000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #28 = documents up to #29000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.025): 0.120*"film" + 0.023*"star" + 0.017*"direct" + 0.013*"movie" + 0.010*"story" + 0.008*"director" + 0.008*"character" + 0.007*"role" + 0.007*"production" + 0.007*"produce"
topic #14 (0.025): 0.014*"water" + 0.009*"forest" + 0.008*"mountain" + 0.007*"tree" + 0.007*"animal" + 0.006*"bird" + 0.006*"site" + 0.006*"park" + 0.006*"region" + 0.005*"river"
topic #39 (0.025): 0.015*"patient" + 0.013*"treatment" + 0.012*"cause" + 0.012*"disease" + 0.011*"medical" + 0.010*"drug" + 0.008*"health" + 0.008*"virus" + 0.007*"body" + 0.006*"increase"
topic #35 (0.025): 0.042*"event" + 0.039*"race" + 0.029*"championship" + 0.026*"compete" + 0.019*"finish" + 0.019*"woman" + 0.014*"competition" + 0.013*"sport" + 0.011*"final" + 0.010*"olympic"
topic #26 (0.025): 0.068*"election" + 0.051*"party" + 0.039*"vote" + 0.030*"elect" + 0.027*"seat" + 0.026*"candidate" + 0.017*"parliament" + 0.016*"council" + 0.012*"assembly" + 0.011*"government"
topic diff=0.029872, rho=0.129277
PROGRESS: pass 9, dispatched chunk #29 = documents up to #30000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #30 = documents up to #31000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #31 = documents up to #32000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #24 (0.025): 0.055*"art" + 0.030*"museum" + 0.027*"artist" + 0.018*"painting" + 0.013*"exhibition" + 0.013*"design" + 0.011*"collection" + 0.010*"paint" + 0.010*"award" + 0.008*"study"
topic #5 (0.025): 0.008*"accord" + 0.007*"tradition" + 0.007*"life" + 0.007*"religious" + 0.007*"temple" + 0.006*"christian" + 0.006*"ancient" + 0.006*"text" + 0.005*"religion" + 0.005*"flag"
topic #36 (0.025): 0.012*"system" + 0.011*"displaystyle" + 0.008*"example" + 0.007*"datum" + 0.007*"point" + 0.006*"function" + 0.006*"value" + 0.005*"case" + 0.005*"method" + 0.005*"set"
topic #12 (0.025): 0.069*"station" + 0.069*"line" + 0.041*"railway" + 0.029*"train" + 0.020*"operate" + 0.019*"bus" + 0.018*"airport" + 0.017*"route" + 0.015*"rail" + 0.014*"passenger"
topic #0 (0.025): 0.053*"language" + 0.031*"japanese" + 0.026*"chinese" + 0.024*"word" + 0.015*"refer" + 0.011*"define" + 0.011*"mean" + 0.010*"speak" + 0.009*"english" + 0.009*"romanian"
topic diff=0.032707, rho=0.129277
PROGRESS: pass 9, dispatched chunk #32 = documents up to #33000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #33 = documents up to #34000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #34 = documents up to #35000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #0 (0.025): 0.051*"language" + 0.030*"japanese" + 0.026*"chinese" + 0.024*"word" + 0.015*"refer" + 0.011*"define" + 0.011*"mean" + 0.011*"speak" + 0.009*"dialect" + 0.009*"english"
topic #11 (0.025): 0.065*"game" + 0.023*"player" + 0.014*"version" + 0.012*"character" + 0.011*"computer" + 0.011*"system" + 0.011*"user" + 0.010*"software" + 0.010*"card" + 0.009*"video"
topic #6 (0.025): 0.056*"specie" + 0.018*"genus" + 0.016*"describe" + 0.014*"white" + 0.013*"plant" + 0.009*"black" + 0.009*"flower" + 0.009*"brown" + 0.008*"female" + 0.008*"male"
topic #35 (0.025): 0.043*"event" + 0.039*"race" + 0.030*"championship" + 0.028*"compete" + 0.021*"woman" + 0.019*"finish" + 0.014*"competition" + 0.013*"sport" + 0.011*"final" + 0.010*"olympic"
topic #23 (0.025): 0.017*"government" + 0.010*"law" + 0.009*"court" + 0.008*"act" + 0.007*"case" + 0.007*"say" + 0.007*"report" + 0.007*"right" + 0.006*"issue" + 0.005*"police"
topic diff=0.031825, rho=0.129277
PROGRESS: pass 9, dispatched chunk #35 = documents up to #36000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #36 = documents up to #37000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #37 = documents up to #38000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #33 (0.025): 0.041*"french" + 0.037*"company" + 0.032*"theatre" + 0.021*"production" + 0.018*"store" + 0.017*"opera" + 0.016*"hotel" + 0.015*"theater" + 0.015*"wine" + 0.011*"produce"
topic #2 (0.025): 0.044*"club" + 0.033*"season" + 0.024*"league" + 0.021*"match" + 0.020*"football" + 0.017*"player" + 0.017*"final" + 0.014*"game" + 0.013*"goal" + 0.013*"score"
topic #36 (0.025): 0.012*"displaystyle" + 0.011*"system" + 0.008*"example" + 0.007*"datum" + 0.007*"point" + 0.006*"function" + 0.006*"set" + 0.006*"value" + 0.005*"case" + 0.005*"term"
topic #15 (0.025): 0.051*"season" + 0.051*"game" + 0.016*"player" + 0.016*"football" + 0.014*"league" + 0.014*"basketball" + 0.013*"coach" + 0.010*"conference" + 0.010*"point" + 0.010*"sign"
topic #16 (0.025): 0.024*"war" + 0.022*"army" + 0.022*"military" + 0.019*"unit" + 0.018*"aircraft" + 0.016*"operation" + 0.014*"air" + 0.014*"command" + 0.012*"force" + 0.011*"officer"
topic diff=0.030750, rho=0.129277
PROGRESS: pass 9, dispatched chunk #38 = documents up to #39000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #39 = documents up to #40000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #40 = documents up to #41000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #29 (0.025): 0.173*"german" + 0.079*"russian" + 0.043*"polish" + 0.043*"dutch" + 0.035*"soviet" + 0.023*"commune" + 0.021*"netherland" + 0.019*"danish" + 0.017*"village" + 0.016*"ukrainian"
topic #32 (0.025): 0.013*"study" + 0.012*"social" + 0.010*"research" + 0.008*"society" + 0.007*"theory" + 0.007*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"individual" + 0.005*"science"
topic #13 (0.025): 0.022*"university" + 0.017*"student" + 0.015*"program" + 0.014*"college" + 0.014*"research" + 0.013*"award" + 0.012*"science" + 0.012*"study" + 0.011*"international" + 0.011*"education"
topic #39 (0.025): 0.014*"patient" + 0.013*"cause" + 0.013*"treatment" + 0.012*"disease" + 0.011*"medical" + 0.009*"drug" + 0.008*"health" + 0.007*"increase" + 0.007*"body" + 0.007*"risk"
topic #1 (0.025): 0.117*"film" + 0.025*"star" + 0.017*"direct" + 0.013*"movie" + 0.010*"story" + 0.008*"character" + 0.008*"role" + 0.008*"director" + 0.007*"produce" + 0.007*"production"
topic diff=0.032978, rho=0.129277
PROGRESS: pass 9, dispatched chunk #41 = documents up to #42000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #42 = documents up to #43000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #43 = documents up to #44000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #18 (0.025): 0.052*"village" + 0.047*"population" + 0.032*"town" + 0.025*"region" + 0.025*"municipality" + 0.016*"province" + 0.014*"census" + 0.014*"local" + 0.012*"central" + 0.011*"rural"
topic #13 (0.025): 0.023*"university" + 0.017*"student" + 0.015*"program" + 0.014*"college" + 0.014*"research" + 0.013*"award" + 0.012*"study" + 0.012*"science" + 0.011*"international" + 0.011*"education"
topic #17 (0.025): 0.034*"company" + 0.013*"business" + 0.009*"market" + 0.008*"product" + 0.007*"sell" + 0.007*"industry" + 0.007*"increase" + 0.006*"provide" + 0.006*"purchase" + 0.006*"sale"
topic #36 (0.025): 0.012*"system" + 0.011*"displaystyle" + 0.008*"example" + 0.007*"point" + 0.007*"datum" + 0.006*"function" + 0.006*"set" + 0.005*"method" + 0.005*"process" + 0.005*"case"
topic #29 (0.025): 0.199*"german" + 0.078*"russian" + 0.045*"dutch" + 0.039*"polish" + 0.033*"soviet" + 0.023*"netherland" + 0.022*"commune" + 0.019*"danish" + 0.016*"village" + 0.014*"ukrainian"
topic diff=0.036671, rho=0.129277
PROGRESS: pass 9, dispatched chunk #44 = documents up to #45000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #45 = documents up to #46000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #46 = documents up to #47000/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #36 (0.025): 0.012*"system" + 0.010*"displaystyle" + 0.008*"point" + 0.008*"example" + 0.007*"datum" + 0.006*"function" + 0.006*"set" + 0.005*"method" + 0.005*"value" + 0.005*"case"
topic #28 (0.025): 0.042*"county" + 0.031*"river" + 0.025*"age" + 0.022*"town" + 0.020*"north" + 0.019*"km" + 0.018*"population" + 0.014*"household" + 0.014*"route" + 0.014*"south"
topic #8 (0.025): 0.040*"album" + 0.038*"song" + 0.033*"music" + 0.025*"band" + 0.016*"single" + 0.011*"perform" + 0.011*"track" + 0.008*"tour" + 0.008*"chart" + 0.007*"singer"
topic #12 (0.025): 0.072*"line" + 0.070*"station" + 0.039*"railway" + 0.029*"train" + 0.020*"operate" + 0.018*"airport" + 0.018*"bus" + 0.016*"route" + 0.015*"rail" + 0.014*"passenger"
topic #20 (0.025): 0.047*"book" + 0.040*"publish" + 0.015*"novel" + 0.014*"author" + 0.013*"magazine" + 0.013*"writer" + 0.011*"story" + 0.010*"editor" + 0.009*"publication" + 0.008*"history"
topic diff=0.031532, rho=0.129277
PROGRESS: pass 9, dispatched chunk #47 = documents up to #48000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #48 = documents up to #49000/49835, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #49 = documents up to #49835/49835, outstanding queue size 9
merging changes from 3000 documents into a model of 49835 documents
topic #1 (0.025): 0.120*"film" + 0.025*"star" + 0.018*"direct" + 0.013*"movie" + 0.010*"story" + 0.008*"director" + 0.008*"character" + 0.007*"role" + 0.007*"produce" + 0.007*"award"
topic #6 (0.025): 0.056*"specie" + 0.018*"genus" + 0.016*"describe" + 0.014*"white" + 0.013*"plant" + 0.010*"brown" + 0.009*"flower" + 0.009*"black" + 0.008*"male" + 0.008*"female"
topic #37 (0.025): 0.040*"ship" + 0.031*"island" + 0.013*"port" + 0.012*"sea" + 0.011*"coast" + 0.011*"boat" + 0.011*"crew" + 0.010*"fleet" + 0.010*"vessel" + 0.010*"navy"
topic #4 (0.025): 0.023*"battle" + 0.022*"attack" + 0.020*"force" + 0.016*"fight" + 0.015*"army" + 0.013*"war" + 0.011*"fire" + 0.011*"kill" + 0.010*"troop" + 0.009*"soldier"
topic #11 (0.025): 0.064*"game" + 0.022*"player" + 0.014*"version" + 0.013*"computer" + 0.011*"user" + 0.011*"system" + 0.010*"software" + 0.010*"video" + 0.010*"character" + 0.008*"code"
topic diff=0.030236, rho=0.129277
-8.311 per-word bound, 317.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 4000 documents into a model of 49835 documents
topic #35 (0.025): 0.042*"event" + 0.042*"race" + 0.029*"championship" + 0.026*"compete" + 0.021*"woman" + 0.019*"finish" + 0.014*"competition" + 0.013*"sport" + 0.011*"final" + 0.010*"winner"
topic #18 (0.025): 0.053*"village" + 0.048*"population" + 0.030*"town" + 0.025*"region" + 0.024*"municipality" + 0.015*"province" + 0.014*"census" + 0.013*"local" + 0.012*"central" + 0.011*"rural"
topic #39 (0.025): 0.014*"patient" + 0.013*"cause" + 0.012*"treatment" + 0.012*"disease" + 0.011*"medical" + 0.010*"drug" + 0.009*"health" + 0.007*"body" + 0.007*"increase" + 0.007*"risk"
topic #34 (0.025): 0.049*"church" + 0.028*"student" + 0.023*"child" + 0.018*"college" + 0.013*"bishop" + 0.013*"grade" + 0.011*"education" + 0.011*"teacher" + 0.009*"catholic" + 0.008*"attend"
topic #17 (0.025): 0.035*"company" + 0.012*"business" + 0.009*"market" + 0.007*"industry" + 0.007*"product" + 0.007*"sell" + 0.006*"increase" + 0.006*"provide" + 0.006*"purchase" + 0.006*"cost"
topic diff=0.030715, rho=0.129277
-8.311 per-word bound, 317.6 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 3000 documents into a model of 49835 documents
topic #19 (0.025): 0.040*"building" + 0.025*"house" + 0.015*"church" + 0.012*"site" + 0.011*"design" + 0.009*"stone" + 0.008*"th_century" + 0.008*"wall" + 0.007*"room" + 0.007*"tower"
topic #30 (0.025): 0.021*"black" + 0.014*"gold" + 0.011*"mine" + 0.010*"wear" + 0.009*"white" + 0.009*"produce" + 0.006*"iron" + 0.006*"red" + 0.005*"steel" + 0.005*"mining"
topic #29 (0.025): 0.183*"german" + 0.081*"russian" + 0.043*"polish" + 0.043*"dutch" + 0.034*"soviet" + 0.021*"netherland" + 0.021*"commune" + 0.019*"danish" + 0.017*"village" + 0.014*"west"
topic #28 (0.025): 0.043*"county" + 0.029*"river" + 0.025*"age" + 0.021*"town" + 0.019*"north" + 0.018*"km" + 0.018*"population" + 0.014*"south" + 0.014*"household" + 0.013*"route"
topic #32 (0.025): 0.013*"study" + 0.011*"social" + 0.009*"research" + 0.008*"society" + 0.007*"theory" + 0.006*"idea" + 0.005*"human" + 0.005*"life" + 0.005*"influence" + 0.005*"experience"
topic diff=0.032895, rho=0.129277
-8.318 per-word bound, 319.1 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
merging changes from 835 documents into a model of 49835 documents
topic #18 (0.025): 0.052*"village" + 0.047*"population" + 0.031*"town" + 0.025*"region" + 0.023*"municipality" + 0.017*"province" + 0.014*"census" + 0.014*"central" + 0.013*"local" + 0.011*"spanish"
topic #15 (0.025): 0.052*"season" + 0.051*"game" + 0.018*"player" + 0.016*"football" + 0.014*"basketball" + 0.014*"league" + 0.012*"coach" + 0.009*"conference" + 0.009*"yard" + 0.009*"point"
topic #19 (0.025): 0.040*"building" + 0.022*"house" + 0.012*"church" + 0.011*"site" + 0.011*"design" + 0.010*"stone" + 0.010*"wall" + 0.008*"room" + 0.008*"monument" + 0.007*"floor"
topic #11 (0.025): 0.063*"game" + 0.024*"player" + 0.013*"version" + 0.012*"computer" + 0.011*"system" + 0.011*"user" + 0.011*"comment" + 0.009*"character" + 0.009*"file" + 0.009*"software"
topic #1 (0.025): 0.121*"film" + 0.025*"star" + 0.017*"direct" + 0.013*"movie" + 0.010*"story" + 0.009*"director" + 0.008*"character" + 0.008*"role" + 0.007*"produce" + 0.007*"production"
topic diff=0.043035, rho=0.129277
-8.172 per-word bound, 288.3 perplexity estimate based on a held-out corpus of 835 documents with 113713 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=26262, num_topics=40, decay=0.5, chunksize=1000) in 2333.40s', 'datetime': '2022-02-07T01:02:38.773005', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows
1 batches submitted to accumulate stats from 64 documents (2187 virtual)
2 batches submitted to accumulate stats from 128 documents (6627 virtual)
3 batches submitted to accumulate stats from 192 documents (7985 virtual)
5 batches submitted to accumulate stats from 320 documents (8806 virtual)
6 batches submitted to accumulate stats from 384 documents (12892 virtual)
9 batches submitted to accumulate stats from 576 documents (14088 virtual)
10 batches submitted to accumulate stats from 640 documents (15806 virtual)
11 batches submitted to accumulate stats from 704 documents (16479 virtual)
12 batches submitted to accumulate stats from 768 documents (21412 virtual)
13 batches submitted to accumulate stats from 832 documents (24200 virtual)
16 batches submitted to accumulate stats from 1024 documents (25550 virtual)
17 batches submitted to accumulate stats from 1088 documents (26193 virtual)
18 batches submitted to accumulate stats from 1152 documents (30275 virtual)
19 batches submitted to accumulate stats from 1216 documents (30476 virtual)
20 batches submitted to accumulate stats from 1280 documents (33795 virtual)
21 batches submitted to accumulate stats from 1344 documents (34977 virtual)
22 batches submitted to accumulate stats from 1408 documents (36542 virtual)
23 batches submitted to accumulate stats from 1472 documents (41191 virtual)
24 batches submitted to accumulate stats from 1536 documents (42943 virtual)
25 batches submitted to accumulate stats from 1600 documents (43581 virtual)
26 batches submitted to accumulate stats from 1664 documents (43804 virtual)
27 batches submitted to accumulate stats from 1728 documents (44982 virtual)
28 batches submitted to accumulate stats from 1792 documents (45279 virtual)
29 batches submitted to accumulate stats from 1856 documents (56224 virtual)
30 batches submitted to accumulate stats from 1920 documents (56795 virtual)
32 batches submitted to accumulate stats from 2048 documents (56627 virtual)
33 batches submitted to accumulate stats from 2112 documents (56653 virtual)
35 batches submitted to accumulate stats from 2240 documents (58622 virtual)
36 batches submitted to accumulate stats from 2304 documents (61502 virtual)
37 batches submitted to accumulate stats from 2368 documents (63505 virtual)
39 batches submitted to accumulate stats from 2496 documents (68601 virtual)
41 batches submitted to accumulate stats from 2624 documents (68703 virtual)
43 batches submitted to accumulate stats from 2752 documents (73935 virtual)
45 batches submitted to accumulate stats from 2880 documents (76508 virtual)
46 batches submitted to accumulate stats from 2944 documents (79630 virtual)
47 batches submitted to accumulate stats from 3008 documents (80649 virtual)
48 batches submitted to accumulate stats from 3072 documents (86174 virtual)
51 batches submitted to accumulate stats from 3264 documents (86072 virtual)
52 batches submitted to accumulate stats from 3328 documents (89849 virtual)
53 batches submitted to accumulate stats from 3392 documents (90653 virtual)
54 batches submitted to accumulate stats from 3456 documents (91884 virtual)
55 batches submitted to accumulate stats from 3520 documents (95638 virtual)
56 batches submitted to accumulate stats from 3584 documents (98469 virtual)
57 batches submitted to accumulate stats from 3648 documents (101438 virtual)
58 batches submitted to accumulate stats from 3712 documents (104768 virtual)
59 batches submitted to accumulate stats from 3776 documents (105765 virtual)
61 batches submitted to accumulate stats from 3904 documents (105512 virtual)
62 batches submitted to accumulate stats from 3968 documents (106980 virtual)
64 batches submitted to accumulate stats from 4096 documents (107932 virtual)
65 batches submitted to accumulate stats from 4160 documents (109181 virtual)
68 batches submitted to accumulate stats from 4352 documents (108519 virtual)
69 batches submitted to accumulate stats from 4416 documents (121546 virtual)
70 batches submitted to accumulate stats from 4480 documents (124612 virtual)
71 batches submitted to accumulate stats from 4544 documents (127583 virtual)
72 batches submitted to accumulate stats from 4608 documents (129583 virtual)
73 batches submitted to accumulate stats from 4672 documents (132103 virtual)
76 batches submitted to accumulate stats from 4864 documents (133352 virtual)
77 batches submitted to accumulate stats from 4928 documents (137667 virtual)
78 batches submitted to accumulate stats from 4992 documents (139880 virtual)
79 batches submitted to accumulate stats from 5056 documents (140763 virtual)
81 batches submitted to accumulate stats from 5184 documents (141876 virtual)
83 batches submitted to accumulate stats from 5312 documents (142153 virtual)
85 batches submitted to accumulate stats from 5440 documents (143076 virtual)
86 batches submitted to accumulate stats from 5504 documents (144055 virtual)
87 batches submitted to accumulate stats from 5568 documents (145348 virtual)
90 batches submitted to accumulate stats from 5760 documents (146339 virtual)
91 batches submitted to accumulate stats from 5824 documents (149945 virtual)
92 batches submitted to accumulate stats from 5888 documents (150800 virtual)
93 batches submitted to accumulate stats from 5952 documents (151087 virtual)
94 batches submitted to accumulate stats from 6016 documents (157245 virtual)
95 batches submitted to accumulate stats from 6080 documents (160055 virtual)
96 batches submitted to accumulate stats from 6144 documents (160603 virtual)
97 batches submitted to accumulate stats from 6208 documents (163090 virtual)
98 batches submitted to accumulate stats from 6272 documents (164073 virtual)
99 batches submitted to accumulate stats from 6336 documents (166609 virtual)
100 batches submitted to accumulate stats from 6400 documents (168101 virtual)
101 batches submitted to accumulate stats from 6464 documents (168193 virtual)
103 batches submitted to accumulate stats from 6592 documents (171574 virtual)
106 batches submitted to accumulate stats from 6784 documents (172835 virtual)
107 batches submitted to accumulate stats from 6848 documents (172998 virtual)
108 batches submitted to accumulate stats from 6912 documents (175030 virtual)
109 batches submitted to accumulate stats from 6976 documents (175394 virtual)
110 batches submitted to accumulate stats from 7040 documents (176726 virtual)
112 batches submitted to accumulate stats from 7168 documents (174971 virtual)
113 batches submitted to accumulate stats from 7232 documents (178092 virtual)
114 batches submitted to accumulate stats from 7296 documents (178578 virtual)
116 batches submitted to accumulate stats from 7424 documents (183507 virtual)
117 batches submitted to accumulate stats from 7488 documents (185451 virtual)
119 batches submitted to accumulate stats from 7616 documents (187828 virtual)
120 batches submitted to accumulate stats from 7680 documents (189514 virtual)
121 batches submitted to accumulate stats from 7744 documents (190292 virtual)
123 batches submitted to accumulate stats from 7872 documents (189992 virtual)
124 batches submitted to accumulate stats from 7936 documents (190218 virtual)
125 batches submitted to accumulate stats from 8000 documents (192179 virtual)
126 batches submitted to accumulate stats from 8064 documents (193292 virtual)
127 batches submitted to accumulate stats from 8128 documents (195947 virtual)
129 batches submitted to accumulate stats from 8256 documents (197071 virtual)
130 batches submitted to accumulate stats from 8320 documents (200648 virtual)
131 batches submitted to accumulate stats from 8384 documents (203905 virtual)
132 batches submitted to accumulate stats from 8448 documents (203992 virtual)
135 batches submitted to accumulate stats from 8640 documents (206438 virtual)
136 batches submitted to accumulate stats from 8704 documents (206963 virtual)
139 batches submitted to accumulate stats from 8896 documents (204072 virtual)
140 batches submitted to accumulate stats from 8960 documents (216405 virtual)
141 batches submitted to accumulate stats from 9024 documents (219655 virtual)
143 batches submitted to accumulate stats from 9152 documents (222753 virtual)
144 batches submitted to accumulate stats from 9216 documents (225686 virtual)
145 batches submitted to accumulate stats from 9280 documents (232219 virtual)
146 batches submitted to accumulate stats from 9344 documents (234137 virtual)
149 batches submitted to accumulate stats from 9536 documents (234504 virtual)
150 batches submitted to accumulate stats from 9600 documents (237121 virtual)
151 batches submitted to accumulate stats from 9664 documents (240677 virtual)
152 batches submitted to accumulate stats from 9728 documents (242570 virtual)
153 batches submitted to accumulate stats from 9792 documents (243103 virtual)
154 batches submitted to accumulate stats from 9856 documents (245314 virtual)
155 batches submitted to accumulate stats from 9920 documents (245937 virtual)
156 batches submitted to accumulate stats from 9984 documents (246579 virtual)
157 batches submitted to accumulate stats from 10048 documents (248718 virtual)
158 batches submitted to accumulate stats from 10112 documents (251595 virtual)
161 batches submitted to accumulate stats from 10304 documents (251975 virtual)
162 batches submitted to accumulate stats from 10368 documents (253771 virtual)
163 batches submitted to accumulate stats from 10432 documents (256774 virtual)
164 batches submitted to accumulate stats from 10496 documents (259325 virtual)
166 batches submitted to accumulate stats from 10624 documents (259644 virtual)
168 batches submitted to accumulate stats from 10752 documents (258966 virtual)
170 batches submitted to accumulate stats from 10880 documents (259612 virtual)
171 batches submitted to accumulate stats from 10944 documents (261495 virtual)
172 batches submitted to accumulate stats from 11008 documents (263959 virtual)
173 batches submitted to accumulate stats from 11072 documents (264120 virtual)
174 batches submitted to accumulate stats from 11136 documents (268674 virtual)
175 batches submitted to accumulate stats from 11200 documents (272083 virtual)
176 batches submitted to accumulate stats from 11264 documents (273484 virtual)
177 batches submitted to accumulate stats from 11328 documents (276433 virtual)
178 batches submitted to accumulate stats from 11392 documents (276637 virtual)
179 batches submitted to accumulate stats from 11456 documents (282224 virtual)
180 batches submitted to accumulate stats from 11520 documents (283356 virtual)
181 batches submitted to accumulate stats from 11584 documents (287999 virtual)
182 batches submitted to accumulate stats from 11648 documents (289261 virtual)
185 batches submitted to accumulate stats from 11840 documents (289345 virtual)
186 batches submitted to accumulate stats from 11904 documents (290663 virtual)
187 batches submitted to accumulate stats from 11968 documents (293672 virtual)
190 batches submitted to accumulate stats from 12160 documents (293399 virtual)
192 batches submitted to accumulate stats from 12288 documents (293664 virtual)
193 batches submitted to accumulate stats from 12352 documents (296130 virtual)
194 batches submitted to accumulate stats from 12416 documents (297675 virtual)
195 batches submitted to accumulate stats from 12480 documents (299883 virtual)
196 batches submitted to accumulate stats from 12544 documents (301455 virtual)
197 batches submitted to accumulate stats from 12608 documents (304064 virtual)
198 batches submitted to accumulate stats from 12672 documents (308746 virtual)
199 batches submitted to accumulate stats from 12736 documents (309384 virtual)
200 batches submitted to accumulate stats from 12800 documents (314302 virtual)
202 batches submitted to accumulate stats from 12928 documents (314229 virtual)
203 batches submitted to accumulate stats from 12992 documents (314502 virtual)
207 batches submitted to accumulate stats from 13248 documents (315172 virtual)
209 batches submitted to accumulate stats from 13376 documents (316685 virtual)
210 batches submitted to accumulate stats from 13440 documents (317243 virtual)
211 batches submitted to accumulate stats from 13504 documents (319241 virtual)
212 batches submitted to accumulate stats from 13568 documents (320379 virtual)
214 batches submitted to accumulate stats from 13696 documents (324272 virtual)
215 batches submitted to accumulate stats from 13760 documents (325131 virtual)
216 batches submitted to accumulate stats from 13824 documents (328956 virtual)
217 batches submitted to accumulate stats from 13888 documents (331877 virtual)
218 batches submitted to accumulate stats from 13952 documents (334227 virtual)
219 batches submitted to accumulate stats from 14016 documents (335778 virtual)
220 batches submitted to accumulate stats from 14080 documents (336955 virtual)
221 batches submitted to accumulate stats from 14144 documents (341026 virtual)
222 batches submitted to accumulate stats from 14208 documents (341774 virtual)
223 batches submitted to accumulate stats from 14272 documents (345299 virtual)
226 batches submitted to accumulate stats from 14464 documents (349627 virtual)
228 batches submitted to accumulate stats from 14592 documents (350208 virtual)
229 batches submitted to accumulate stats from 14656 documents (352507 virtual)
231 batches submitted to accumulate stats from 14784 documents (352083 virtual)
232 batches submitted to accumulate stats from 14848 documents (354249 virtual)
233 batches submitted to accumulate stats from 14912 documents (358206 virtual)
234 batches submitted to accumulate stats from 14976 documents (359270 virtual)
235 batches submitted to accumulate stats from 15040 documents (362122 virtual)
237 batches submitted to accumulate stats from 15168 documents (361959 virtual)
239 batches submitted to accumulate stats from 15296 documents (361337 virtual)
240 batches submitted to accumulate stats from 15360 documents (363756 virtual)
241 batches submitted to accumulate stats from 15424 documents (364973 virtual)
242 batches submitted to accumulate stats from 15488 documents (365761 virtual)
245 batches submitted to accumulate stats from 15680 documents (366560 virtual)
247 batches submitted to accumulate stats from 15808 documents (369106 virtual)
248 batches submitted to accumulate stats from 15872 documents (371119 virtual)
249 batches submitted to accumulate stats from 15936 documents (373248 virtual)
250 batches submitted to accumulate stats from 16000 documents (376007 virtual)
251 batches submitted to accumulate stats from 16064 documents (377066 virtual)
252 batches submitted to accumulate stats from 16128 documents (378841 virtual)
253 batches submitted to accumulate stats from 16192 documents (383753 virtual)
254 batches submitted to accumulate stats from 16256 documents (385624 virtual)
255 batches submitted to accumulate stats from 16320 documents (391956 virtual)
256 batches submitted to accumulate stats from 16384 documents (396395 virtual)
257 batches submitted to accumulate stats from 16448 documents (399424 virtual)
258 batches submitted to accumulate stats from 16512 documents (408257 virtual)
259 batches submitted to accumulate stats from 16576 documents (415933 virtual)
260 batches submitted to accumulate stats from 16640 documents (417362 virtual)
261 batches submitted to accumulate stats from 16704 documents (419353 virtual)
262 batches submitted to accumulate stats from 16768 documents (422622 virtual)
264 batches submitted to accumulate stats from 16896 documents (423372 virtual)
265 batches submitted to accumulate stats from 16960 documents (424063 virtual)
266 batches submitted to accumulate stats from 17024 documents (427411 virtual)
267 batches submitted to accumulate stats from 17088 documents (429946 virtual)
268 batches submitted to accumulate stats from 17152 documents (430651 virtual)
269 batches submitted to accumulate stats from 17216 documents (434722 virtual)
270 batches submitted to accumulate stats from 17280 documents (437739 virtual)
271 batches submitted to accumulate stats from 17344 documents (439205 virtual)
272 batches submitted to accumulate stats from 17408 documents (440337 virtual)
273 batches submitted to accumulate stats from 17472 documents (449640 virtual)
274 batches submitted to accumulate stats from 17536 documents (451994 virtual)
276 batches submitted to accumulate stats from 17664 documents (453067 virtual)
277 batches submitted to accumulate stats from 17728 documents (455591 virtual)
278 batches submitted to accumulate stats from 17792 documents (459550 virtual)
280 batches submitted to accumulate stats from 17920 documents (460224 virtual)
282 batches submitted to accumulate stats from 18048 documents (460087 virtual)
286 batches submitted to accumulate stats from 18304 documents (458341 virtual)
287 batches submitted to accumulate stats from 18368 documents (459058 virtual)
288 batches submitted to accumulate stats from 18432 documents (460468 virtual)
289 batches submitted to accumulate stats from 18496 documents (463510 virtual)
290 batches submitted to accumulate stats from 18560 documents (467593 virtual)
292 batches submitted to accumulate stats from 18688 documents (469822 virtual)
293 batches submitted to accumulate stats from 18752 documents (470440 virtual)
295 batches submitted to accumulate stats from 18880 documents (471727 virtual)
296 batches submitted to accumulate stats from 18944 documents (473093 virtual)
297 batches submitted to accumulate stats from 19008 documents (474035 virtual)
300 batches submitted to accumulate stats from 19200 documents (473647 virtual)
301 batches submitted to accumulate stats from 19264 documents (477033 virtual)
303 batches submitted to accumulate stats from 19392 documents (481172 virtual)
304 batches submitted to accumulate stats from 19456 documents (484320 virtual)
305 batches submitted to accumulate stats from 19520 documents (484538 virtual)
306 batches submitted to accumulate stats from 19584 documents (485774 virtual)
307 batches submitted to accumulate stats from 19648 documents (493676 virtual)
308 batches submitted to accumulate stats from 19712 documents (493929 virtual)
309 batches submitted to accumulate stats from 19776 documents (494047 virtual)
311 batches submitted to accumulate stats from 19904 documents (492503 virtual)
313 batches submitted to accumulate stats from 20032 documents (493415 virtual)
314 batches submitted to accumulate stats from 20096 documents (493928 virtual)
315 batches submitted to accumulate stats from 20160 documents (494803 virtual)
316 batches submitted to accumulate stats from 20224 documents (496102 virtual)
317 batches submitted to accumulate stats from 20288 documents (497766 virtual)
318 batches submitted to accumulate stats from 20352 documents (506326 virtual)
319 batches submitted to accumulate stats from 20416 documents (508100 virtual)
320 batches submitted to accumulate stats from 20480 documents (511781 virtual)
321 batches submitted to accumulate stats from 20544 documents (513394 virtual)
322 batches submitted to accumulate stats from 20608 documents (521649 virtual)
324 batches submitted to accumulate stats from 20736 documents (523050 virtual)
325 batches submitted to accumulate stats from 20800 documents (523439 virtual)
326 batches submitted to accumulate stats from 20864 documents (524111 virtual)
327 batches submitted to accumulate stats from 20928 documents (526621 virtual)
328 batches submitted to accumulate stats from 20992 documents (529167 virtual)
329 batches submitted to accumulate stats from 21056 documents (530580 virtual)
330 batches submitted to accumulate stats from 21120 documents (531397 virtual)
333 batches submitted to accumulate stats from 21312 documents (532403 virtual)
334 batches submitted to accumulate stats from 21376 documents (532610 virtual)
335 batches submitted to accumulate stats from 21440 documents (533492 virtual)
336 batches submitted to accumulate stats from 21504 documents (536910 virtual)
337 batches submitted to accumulate stats from 21568 documents (538221 virtual)
338 batches submitted to accumulate stats from 21632 documents (542877 virtual)
339 batches submitted to accumulate stats from 21696 documents (546550 virtual)
341 batches submitted to accumulate stats from 21824 documents (547396 virtual)
344 batches submitted to accumulate stats from 22016 documents (545782 virtual)
345 batches submitted to accumulate stats from 22080 documents (550265 virtual)
346 batches submitted to accumulate stats from 22144 documents (550334 virtual)
349 batches submitted to accumulate stats from 22336 documents (549101 virtual)
350 batches submitted to accumulate stats from 22400 documents (551699 virtual)
351 batches submitted to accumulate stats from 22464 documents (554979 virtual)
352 batches submitted to accumulate stats from 22528 documents (555971 virtual)
353 batches submitted to accumulate stats from 22592 documents (557403 virtual)
354 batches submitted to accumulate stats from 22656 documents (559560 virtual)
355 batches submitted to accumulate stats from 22720 documents (563095 virtual)
356 batches submitted to accumulate stats from 22784 documents (563653 virtual)
357 batches submitted to accumulate stats from 22848 documents (564896 virtual)
358 batches submitted to accumulate stats from 22912 documents (566393 virtual)
359 batches submitted to accumulate stats from 22976 documents (568690 virtual)
361 batches submitted to accumulate stats from 23104 documents (571429 virtual)
362 batches submitted to accumulate stats from 23168 documents (576289 virtual)
363 batches submitted to accumulate stats from 23232 documents (578132 virtual)
365 batches submitted to accumulate stats from 23360 documents (578356 virtual)
367 batches submitted to accumulate stats from 23488 documents (580701 virtual)
368 batches submitted to accumulate stats from 23552 documents (581962 virtual)
370 batches submitted to accumulate stats from 23680 documents (581741 virtual)
371 batches submitted to accumulate stats from 23744 documents (583089 virtual)
372 batches submitted to accumulate stats from 23808 documents (583749 virtual)
373 batches submitted to accumulate stats from 23872 documents (586423 virtual)
375 batches submitted to accumulate stats from 24000 documents (588064 virtual)
378 batches submitted to accumulate stats from 24192 documents (589170 virtual)
380 batches submitted to accumulate stats from 24320 documents (593729 virtual)
382 batches submitted to accumulate stats from 24448 documents (597606 virtual)
383 batches submitted to accumulate stats from 24512 documents (599128 virtual)
384 batches submitted to accumulate stats from 24576 documents (602209 virtual)
385 batches submitted to accumulate stats from 24640 documents (602670 virtual)
386 batches submitted to accumulate stats from 24704 documents (611623 virtual)
387 batches submitted to accumulate stats from 24768 documents (612974 virtual)
388 batches submitted to accumulate stats from 24832 documents (614119 virtual)
389 batches submitted to accumulate stats from 24896 documents (614742 virtual)
390 batches submitted to accumulate stats from 24960 documents (617043 virtual)
392 batches submitted to accumulate stats from 25088 documents (624390 virtual)
393 batches submitted to accumulate stats from 25152 documents (630077 virtual)
394 batches submitted to accumulate stats from 25216 documents (631743 virtual)
395 batches submitted to accumulate stats from 25280 documents (632418 virtual)
396 batches submitted to accumulate stats from 25344 documents (636187 virtual)
400 batches submitted to accumulate stats from 25600 documents (635194 virtual)
401 batches submitted to accumulate stats from 25664 documents (636594 virtual)
402 batches submitted to accumulate stats from 25728 documents (640451 virtual)
404 batches submitted to accumulate stats from 25856 documents (640977 virtual)
405 batches submitted to accumulate stats from 25920 documents (642517 virtual)
407 batches submitted to accumulate stats from 26048 documents (641790 virtual)
409 batches submitted to accumulate stats from 26176 documents (643799 virtual)
411 batches submitted to accumulate stats from 26304 documents (644311 virtual)
412 batches submitted to accumulate stats from 26368 documents (644786 virtual)
413 batches submitted to accumulate stats from 26432 documents (645437 virtual)
415 batches submitted to accumulate stats from 26560 documents (646145 virtual)
416 batches submitted to accumulate stats from 26624 documents (648073 virtual)
418 batches submitted to accumulate stats from 26752 documents (647502 virtual)
419 batches submitted to accumulate stats from 26816 documents (653985 virtual)
420 batches submitted to accumulate stats from 26880 documents (655882 virtual)
422 batches submitted to accumulate stats from 27008 documents (662959 virtual)
423 batches submitted to accumulate stats from 27072 documents (672188 virtual)
425 batches submitted to accumulate stats from 27200 documents (671385 virtual)
426 batches submitted to accumulate stats from 27264 documents (673853 virtual)
428 batches submitted to accumulate stats from 27392 documents (675180 virtual)
429 batches submitted to accumulate stats from 27456 documents (676303 virtual)
430 batches submitted to accumulate stats from 27520 documents (677699 virtual)
431 batches submitted to accumulate stats from 27584 documents (679677 virtual)
432 batches submitted to accumulate stats from 27648 documents (683184 virtual)
433 batches submitted to accumulate stats from 27712 documents (683405 virtual)
434 batches submitted to accumulate stats from 27776 documents (688550 virtual)
437 batches submitted to accumulate stats from 27968 documents (685702 virtual)
438 batches submitted to accumulate stats from 28032 documents (685850 virtual)
440 batches submitted to accumulate stats from 28160 documents (687883 virtual)
441 batches submitted to accumulate stats from 28224 documents (688423 virtual)
442 batches submitted to accumulate stats from 28288 documents (688791 virtual)
443 batches submitted to accumulate stats from 28352 documents (689050 virtual)
445 batches submitted to accumulate stats from 28480 documents (689640 virtual)
446 batches submitted to accumulate stats from 28544 documents (696973 virtual)
447 batches submitted to accumulate stats from 28608 documents (699299 virtual)
448 batches submitted to accumulate stats from 28672 documents (700973 virtual)
449 batches submitted to accumulate stats from 28736 documents (702852 virtual)
452 batches submitted to accumulate stats from 28928 documents (703615 virtual)
453 batches submitted to accumulate stats from 28992 documents (706370 virtual)
455 batches submitted to accumulate stats from 29120 documents (704957 virtual)
456 batches submitted to accumulate stats from 29184 documents (708701 virtual)
457 batches submitted to accumulate stats from 29248 documents (711237 virtual)
458 batches submitted to accumulate stats from 29312 documents (713569 virtual)
459 batches submitted to accumulate stats from 29376 documents (718348 virtual)
460 batches submitted to accumulate stats from 29440 documents (720807 virtual)
461 batches submitted to accumulate stats from 29504 documents (723180 virtual)
462 batches submitted to accumulate stats from 29568 documents (723427 virtual)
463 batches submitted to accumulate stats from 29632 documents (727244 virtual)
464 batches submitted to accumulate stats from 29696 documents (729319 virtual)
465 batches submitted to accumulate stats from 29760 documents (736938 virtual)
467 batches submitted to accumulate stats from 29888 documents (739405 virtual)
468 batches submitted to accumulate stats from 29952 documents (743262 virtual)
469 batches submitted to accumulate stats from 30016 documents (744827 virtual)
470 batches submitted to accumulate stats from 30080 documents (746723 virtual)
471 batches submitted to accumulate stats from 30144 documents (755647 virtual)
473 batches submitted to accumulate stats from 30272 documents (756611 virtual)
474 batches submitted to accumulate stats from 30336 documents (756889 virtual)
477 batches submitted to accumulate stats from 30528 documents (756262 virtual)
479 batches submitted to accumulate stats from 30656 documents (755214 virtual)
480 batches submitted to accumulate stats from 30720 documents (760420 virtual)
483 batches submitted to accumulate stats from 30912 documents (760836 virtual)
484 batches submitted to accumulate stats from 30976 documents (761610 virtual)
485 batches submitted to accumulate stats from 31040 documents (768829 virtual)
486 batches submitted to accumulate stats from 31104 documents (770401 virtual)
487 batches submitted to accumulate stats from 31168 documents (774057 virtual)
488 batches submitted to accumulate stats from 31232 documents (777668 virtual)
490 batches submitted to accumulate stats from 31360 documents (776961 virtual)
491 batches submitted to accumulate stats from 31424 documents (788207 virtual)
492 batches submitted to accumulate stats from 31488 documents (791765 virtual)
495 batches submitted to accumulate stats from 31680 documents (794550 virtual)
496 batches submitted to accumulate stats from 31744 documents (799015 virtual)
497 batches submitted to accumulate stats from 31808 documents (799475 virtual)
498 batches submitted to accumulate stats from 31872 documents (804676 virtual)
499 batches submitted to accumulate stats from 31936 documents (806384 virtual)
500 batches submitted to accumulate stats from 32000 documents (806750 virtual)
502 batches submitted to accumulate stats from 32128 documents (811175 virtual)
503 batches submitted to accumulate stats from 32192 documents (817101 virtual)
506 batches submitted to accumulate stats from 32384 documents (817074 virtual)
507 batches submitted to accumulate stats from 32448 documents (822000 virtual)
508 batches submitted to accumulate stats from 32512 documents (831560 virtual)
509 batches submitted to accumulate stats from 32576 documents (835537 virtual)
510 batches submitted to accumulate stats from 32640 documents (837435 virtual)
511 batches submitted to accumulate stats from 32704 documents (838809 virtual)
512 batches submitted to accumulate stats from 32768 documents (840021 virtual)
513 batches submitted to accumulate stats from 32832 documents (840276 virtual)
514 batches submitted to accumulate stats from 32896 documents (841782 virtual)
515 batches submitted to accumulate stats from 32960 documents (843862 virtual)
516 batches submitted to accumulate stats from 33024 documents (846078 virtual)
517 batches submitted to accumulate stats from 33088 documents (848078 virtual)
518 batches submitted to accumulate stats from 33152 documents (849570 virtual)
519 batches submitted to accumulate stats from 33216 documents (850330 virtual)
520 batches submitted to accumulate stats from 33280 documents (851782 virtual)
521 batches submitted to accumulate stats from 33344 documents (853577 virtual)
522 batches submitted to accumulate stats from 33408 documents (854055 virtual)
523 batches submitted to accumulate stats from 33472 documents (854139 virtual)
524 batches submitted to accumulate stats from 33536 documents (854826 virtual)
525 batches submitted to accumulate stats from 33600 documents (855386 virtual)
526 batches submitted to accumulate stats from 33664 documents (862233 virtual)
530 batches submitted to accumulate stats from 33920 documents (867676 virtual)
531 batches submitted to accumulate stats from 33984 documents (873028 virtual)
532 batches submitted to accumulate stats from 34048 documents (874094 virtual)
533 batches submitted to accumulate stats from 34112 documents (874868 virtual)
534 batches submitted to accumulate stats from 34176 documents (880306 virtual)
535 batches submitted to accumulate stats from 34240 documents (883086 virtual)
536 batches submitted to accumulate stats from 34304 documents (884068 virtual)
537 batches submitted to accumulate stats from 34368 documents (885829 virtual)
538 batches submitted to accumulate stats from 34432 documents (886383 virtual)
539 batches submitted to accumulate stats from 34496 documents (886524 virtual)
540 batches submitted to accumulate stats from 34560 documents (887091 virtual)
541 batches submitted to accumulate stats from 34624 documents (889643 virtual)
542 batches submitted to accumulate stats from 34688 documents (891085 virtual)
543 batches submitted to accumulate stats from 34752 documents (895745 virtual)
544 batches submitted to accumulate stats from 34816 documents (898586 virtual)
546 batches submitted to accumulate stats from 34944 documents (897659 virtual)
547 batches submitted to accumulate stats from 35008 documents (898474 virtual)
548 batches submitted to accumulate stats from 35072 documents (901761 virtual)
549 batches submitted to accumulate stats from 35136 documents (903587 virtual)
551 batches submitted to accumulate stats from 35264 documents (904172 virtual)
552 batches submitted to accumulate stats from 35328 documents (904427 virtual)
558 batches submitted to accumulate stats from 35712 documents (907123 virtual)
559 batches submitted to accumulate stats from 35776 documents (908357 virtual)
560 batches submitted to accumulate stats from 35840 documents (911341 virtual)
561 batches submitted to accumulate stats from 35904 documents (917158 virtual)
562 batches submitted to accumulate stats from 35968 documents (919212 virtual)
563 batches submitted to accumulate stats from 36032 documents (920227 virtual)
564 batches submitted to accumulate stats from 36096 documents (920479 virtual)
565 batches submitted to accumulate stats from 36160 documents (923044 virtual)
567 batches submitted to accumulate stats from 36288 documents (924830 virtual)
569 batches submitted to accumulate stats from 36416 documents (924859 virtual)
571 batches submitted to accumulate stats from 36544 documents (930197 virtual)
572 batches submitted to accumulate stats from 36608 documents (930346 virtual)
574 batches submitted to accumulate stats from 36736 documents (930979 virtual)
575 batches submitted to accumulate stats from 36800 documents (934927 virtual)
578 batches submitted to accumulate stats from 36992 documents (940556 virtual)
579 batches submitted to accumulate stats from 37056 documents (940800 virtual)
580 batches submitted to accumulate stats from 37120 documents (942701 virtual)
581 batches submitted to accumulate stats from 37184 documents (951705 virtual)
583 batches submitted to accumulate stats from 37312 documents (959113 virtual)
584 batches submitted to accumulate stats from 37376 documents (964060 virtual)
585 batches submitted to accumulate stats from 37440 documents (967792 virtual)
588 batches submitted to accumulate stats from 37632 documents (967406 virtual)
590 batches submitted to accumulate stats from 37760 documents (969216 virtual)
591 batches submitted to accumulate stats from 37824 documents (969533 virtual)
592 batches submitted to accumulate stats from 37888 documents (971168 virtual)
593 batches submitted to accumulate stats from 37952 documents (973823 virtual)
594 batches submitted to accumulate stats from 38016 documents (977701 virtual)
596 batches submitted to accumulate stats from 38144 documents (977252 virtual)
597 batches submitted to accumulate stats from 38208 documents (978204 virtual)
598 batches submitted to accumulate stats from 38272 documents (979685 virtual)
599 batches submitted to accumulate stats from 38336 documents (981335 virtual)
600 batches submitted to accumulate stats from 38400 documents (987189 virtual)
602 batches submitted to accumulate stats from 38528 documents (992018 virtual)
604 batches submitted to accumulate stats from 38656 documents (992279 virtual)
605 batches submitted to accumulate stats from 38720 documents (992406 virtual)
606 batches submitted to accumulate stats from 38784 documents (996559 virtual)
607 batches submitted to accumulate stats from 38848 documents (999683 virtual)
609 batches submitted to accumulate stats from 38976 documents (998847 virtual)
613 batches submitted to accumulate stats from 39232 documents (999125 virtual)
614 batches submitted to accumulate stats from 39296 documents (1000763 virtual)
615 batches submitted to accumulate stats from 39360 documents (1004631 virtual)
616 batches submitted to accumulate stats from 39424 documents (1008501 virtual)
617 batches submitted to accumulate stats from 39488 documents (1011573 virtual)
618 batches submitted to accumulate stats from 39552 documents (1015596 virtual)
619 batches submitted to accumulate stats from 39616 documents (1016586 virtual)
620 batches submitted to accumulate stats from 39680 documents (1019415 virtual)
623 batches submitted to accumulate stats from 39872 documents (1016991 virtual)
624 batches submitted to accumulate stats from 39936 documents (1018724 virtual)
625 batches submitted to accumulate stats from 40000 documents (1022951 virtual)
626 batches submitted to accumulate stats from 40064 documents (1023815 virtual)
628 batches submitted to accumulate stats from 40192 documents (1028189 virtual)
629 batches submitted to accumulate stats from 40256 documents (1030610 virtual)
630 batches submitted to accumulate stats from 40320 documents (1030618 virtual)
631 batches submitted to accumulate stats from 40384 documents (1031551 virtual)
633 batches submitted to accumulate stats from 40512 documents (1030911 virtual)
634 batches submitted to accumulate stats from 40576 documents (1031654 virtual)
636 batches submitted to accumulate stats from 40704 documents (1031667 virtual)
637 batches submitted to accumulate stats from 40768 documents (1035726 virtual)
638 batches submitted to accumulate stats from 40832 documents (1037150 virtual)
641 batches submitted to accumulate stats from 41024 documents (1038413 virtual)
643 batches submitted to accumulate stats from 41152 documents (1039244 virtual)
644 batches submitted to accumulate stats from 41216 documents (1043215 virtual)
645 batches submitted to accumulate stats from 41280 documents (1047261 virtual)
646 batches submitted to accumulate stats from 41344 documents (1049282 virtual)
647 batches submitted to accumulate stats from 41408 documents (1061137 virtual)
649 batches submitted to accumulate stats from 41536 documents (1062088 virtual)
651 batches submitted to accumulate stats from 41664 documents (1063501 virtual)
652 batches submitted to accumulate stats from 41728 documents (1065332 virtual)
653 batches submitted to accumulate stats from 41792 documents (1068419 virtual)
654 batches submitted to accumulate stats from 41856 documents (1073660 virtual)
655 batches submitted to accumulate stats from 41920 documents (1078416 virtual)
656 batches submitted to accumulate stats from 41984 documents (1078441 virtual)
657 batches submitted to accumulate stats from 42048 documents (1078612 virtual)
659 batches submitted to accumulate stats from 42176 documents (1080983 virtual)
660 batches submitted to accumulate stats from 42240 documents (1083283 virtual)
662 batches submitted to accumulate stats from 42368 documents (1084397 virtual)
663 batches submitted to accumulate stats from 42432 documents (1088851 virtual)
665 batches submitted to accumulate stats from 42560 documents (1087625 virtual)
668 batches submitted to accumulate stats from 42752 documents (1088151 virtual)
669 batches submitted to accumulate stats from 42816 documents (1092187 virtual)
670 batches submitted to accumulate stats from 42880 documents (1095506 virtual)
671 batches submitted to accumulate stats from 42944 documents (1097596 virtual)
673 batches submitted to accumulate stats from 43072 documents (1097357 virtual)
674 batches submitted to accumulate stats from 43136 documents (1099080 virtual)
675 batches submitted to accumulate stats from 43200 documents (1099682 virtual)
676 batches submitted to accumulate stats from 43264 documents (1104320 virtual)
677 batches submitted to accumulate stats from 43328 documents (1107597 virtual)
678 batches submitted to accumulate stats from 43392 documents (1108639 virtual)
679 batches submitted to accumulate stats from 43456 documents (1111030 virtual)
681 batches submitted to accumulate stats from 43584 documents (1116222 virtual)
683 batches submitted to accumulate stats from 43712 documents (1116352 virtual)
686 batches submitted to accumulate stats from 43904 documents (1116233 virtual)
687 batches submitted to accumulate stats from 43968 documents (1117926 virtual)
688 batches submitted to accumulate stats from 44032 documents (1118168 virtual)
689 batches submitted to accumulate stats from 44096 documents (1122063 virtual)
691 batches submitted to accumulate stats from 44224 documents (1123544 virtual)
694 batches submitted to accumulate stats from 44416 documents (1123459 virtual)
695 batches submitted to accumulate stats from 44480 documents (1124005 virtual)
696 batches submitted to accumulate stats from 44544 documents (1128770 virtual)
697 batches submitted to accumulate stats from 44608 documents (1130021 virtual)
698 batches submitted to accumulate stats from 44672 documents (1138189 virtual)
699 batches submitted to accumulate stats from 44736 documents (1140525 virtual)
700 batches submitted to accumulate stats from 44800 documents (1140871 virtual)
701 batches submitted to accumulate stats from 44864 documents (1142577 virtual)
702 batches submitted to accumulate stats from 44928 documents (1145281 virtual)
703 batches submitted to accumulate stats from 44992 documents (1150795 virtual)
705 batches submitted to accumulate stats from 45120 documents (1152323 virtual)
706 batches submitted to accumulate stats from 45184 documents (1155383 virtual)
709 batches submitted to accumulate stats from 45376 documents (1154923 virtual)
710 batches submitted to accumulate stats from 45440 documents (1159985 virtual)
711 batches submitted to accumulate stats from 45504 documents (1162486 virtual)
712 batches submitted to accumulate stats from 45568 documents (1162661 virtual)
713 batches submitted to accumulate stats from 45632 documents (1165313 virtual)
714 batches submitted to accumulate stats from 45696 documents (1166037 virtual)
715 batches submitted to accumulate stats from 45760 documents (1170208 virtual)
716 batches submitted to accumulate stats from 45824 documents (1174673 virtual)
717 batches submitted to accumulate stats from 45888 documents (1184777 virtual)
718 batches submitted to accumulate stats from 45952 documents (1185816 virtual)
719 batches submitted to accumulate stats from 46016 documents (1186560 virtual)
720 batches submitted to accumulate stats from 46080 documents (1191840 virtual)
721 batches submitted to accumulate stats from 46144 documents (1195701 virtual)
722 batches submitted to accumulate stats from 46208 documents (1205885 virtual)
723 batches submitted to accumulate stats from 46272 documents (1209888 virtual)
725 batches submitted to accumulate stats from 46400 documents (1210626 virtual)
727 batches submitted to accumulate stats from 46528 documents (1211198 virtual)
728 batches submitted to accumulate stats from 46592 documents (1215879 virtual)
730 batches submitted to accumulate stats from 46720 documents (1218132 virtual)
731 batches submitted to accumulate stats from 46784 documents (1222020 virtual)
733 batches submitted to accumulate stats from 46912 documents (1224726 virtual)
734 batches submitted to accumulate stats from 46976 documents (1232971 virtual)
735 batches submitted to accumulate stats from 47040 documents (1237948 virtual)
736 batches submitted to accumulate stats from 47104 documents (1241024 virtual)
737 batches submitted to accumulate stats from 47168 documents (1244768 virtual)
738 batches submitted to accumulate stats from 47232 documents (1246649 virtual)
739 batches submitted to accumulate stats from 47296 documents (1251074 virtual)
740 batches submitted to accumulate stats from 47360 documents (1252486 virtual)
741 batches submitted to accumulate stats from 47424 documents (1253089 virtual)
742 batches submitted to accumulate stats from 47488 documents (1253253 virtual)
744 batches submitted to accumulate stats from 47616 documents (1252834 virtual)
745 batches submitted to accumulate stats from 47680 documents (1254813 virtual)
747 batches submitted to accumulate stats from 47808 documents (1256626 virtual)
748 batches submitted to accumulate stats from 47872 documents (1258621 virtual)
751 batches submitted to accumulate stats from 48064 documents (1261161 virtual)
752 batches submitted to accumulate stats from 48128 documents (1264084 virtual)
753 batches submitted to accumulate stats from 48192 documents (1269233 virtual)
754 batches submitted to accumulate stats from 48256 documents (1273424 virtual)
756 batches submitted to accumulate stats from 48384 documents (1275257 virtual)
757 batches submitted to accumulate stats from 48448 documents (1277964 virtual)
758 batches submitted to accumulate stats from 48512 documents (1278771 virtual)
760 batches submitted to accumulate stats from 48640 documents (1280125 virtual)
761 batches submitted to accumulate stats from 48704 documents (1281130 virtual)
762 batches submitted to accumulate stats from 48768 documents (1281506 virtual)
763 batches submitted to accumulate stats from 48832 documents (1283894 virtual)
764 batches submitted to accumulate stats from 48896 documents (1287402 virtual)
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
3 accumulators retrieved from output queue
accumulated word occurrence stats for 3848565 virtual documents
K=40, Coherence Score: 0.6177364326821039
ALL DONE!
