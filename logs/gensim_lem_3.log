using symmetric alpha at 0.3333333333333333
using symmetric eta at 0.3333333333333333
using serial LDA version on this node
running online LDA training, 3 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.186*"see" + 0.154*"area" + 0.135*"back" + 0.109*"court" + 0.086*"come" + 0.059*"build" + 0.048*"school" + 0.047*"set" + 0.031*"original" + 0.028*"royal"
topic #1 (0.333): 0.197*"film" + 0.173*"player" + 0.113*"long" + 0.083*"court" + 0.066*"build" + 0.057*"school" + 0.050*"role" + 0.039*"come" + 0.037*"short" + 0.035*"see"
topic #2 (0.333): 0.213*"film" + 0.136*"school" + 0.090*"area" + 0.090*"back" + 0.072*"international" + 0.064*"royal" + 0.059*"come" + 0.051*"set" + 0.044*"build" + 0.042*"short"
topic diff=0.692438, rho=1.000000
-3.248 per-word bound, 9.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.181*"see" + 0.160*"area" + 0.154*"back" + 0.121*"court" + 0.093*"come" + 0.057*"build" + 0.045*"set" + 0.043*"school" + 0.029*"original" + 0.023*"royal"
topic #1 (0.333): 0.208*"player" + 0.180*"film" + 0.130*"long" + 0.067*"build" + 0.060*"court" + 0.050*"role" + 0.045*"school" + 0.044*"short" + 0.039*"original" + 0.035*"come"
topic #2 (0.333): 0.242*"film" + 0.149*"school" + 0.079*"area" + 0.074*"international" + 0.071*"royal" + 0.067*"back" + 0.052*"set" + 0.052*"come" + 0.046*"build" + 0.044*"short"
topic diff=0.165443, rho=0.377964
-3.175 per-word bound, 9.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.179*"see" + 0.167*"back" + 0.165*"area" + 0.127*"court" + 0.097*"come" + 0.055*"build" + 0.043*"set" + 0.039*"school" + 0.025*"original" + 0.020*"role"
topic #1 (0.333): 0.234*"player" + 0.154*"film" + 0.143*"long" + 0.068*"build" + 0.053*"short" + 0.051*"original" + 0.049*"role" + 0.043*"court" + 0.040*"international" + 0.036*"school"
topic #2 (0.333): 0.267*"film" + 0.160*"school" + 0.075*"royal" + 0.075*"international" + 0.070*"area" + 0.054*"set" + 0.049*"back" + 0.047*"build" + 0.046*"come" + 0.044*"short"
topic diff=0.148743, rho=0.353553
-3.123 per-word bound, 8.7 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.177*"see" + 0.175*"back" + 0.169*"area" + 0.131*"court" + 0.099*"come" + 0.054*"build" + 0.042*"set" + 0.037*"school" + 0.023*"original" + 0.020*"role"
topic #1 (0.333): 0.255*"player" + 0.152*"long" + 0.126*"film" + 0.070*"build" + 0.062*"original" + 0.060*"short" + 0.048*"role" + 0.047*"international" + 0.033*"come" + 0.032*"court"
topic #2 (0.333): 0.288*"film" + 0.167*"school" + 0.077*"royal" + 0.074*"international" + 0.062*"area" + 0.055*"set" + 0.048*"build" + 0.044*"short" + 0.042*"come" + 0.042*"role"
topic diff=0.129639, rho=0.333333
-3.080 per-word bound, 8.5 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.180*"back" + 0.176*"see" + 0.173*"area" + 0.133*"court" + 0.100*"come" + 0.053*"build" + 0.041*"set" + 0.034*"school" + 0.021*"role" + 0.020*"original"
topic #1 (0.333): 0.269*"player" + 0.158*"long" + 0.100*"film" + 0.071*"build" + 0.071*"original" + 0.067*"short" + 0.055*"international" + 0.047*"role" + 0.036*"come" + 0.027*"set"
topic #2 (0.333): 0.306*"film" + 0.174*"school" + 0.080*"royal" + 0.071*"international" + 0.056*"set" + 0.055*"area" + 0.048*"build" + 0.043*"short" + 0.043*"role" + 0.038*"come"
topic diff=0.117558, rho=0.316228
-3.047 per-word bound, 8.3 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.184*"back" + 0.177*"area" + 0.175*"see" + 0.134*"court" + 0.100*"come" + 0.053*"build" + 0.041*"set" + 0.033*"school" + 0.021*"role" + 0.019*"original"
topic #1 (0.333): 0.274*"player" + 0.159*"long" + 0.077*"original" + 0.077*"film" + 0.072*"international" + 0.071*"build" + 0.071*"short" + 0.046*"role" + 0.043*"come" + 0.029*"set"
topic #2 (0.333): 0.323*"film" + 0.180*"school" + 0.082*"royal" + 0.065*"international" + 0.055*"set" + 0.049*"area" + 0.049*"build" + 0.044*"role" + 0.043*"short" + 0.033*"come"
topic diff=0.116449, rho=0.301511
-3.022 per-word bound, 8.1 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.188*"back" + 0.181*"area" + 0.174*"see" + 0.135*"court" + 0.100*"come" + 0.053*"build" + 0.040*"set" + 0.032*"school" + 0.021*"role" + 0.017*"original"
topic #1 (0.333): 0.272*"player" + 0.157*"long" + 0.092*"international" + 0.082*"original" + 0.073*"short" + 0.070*"build" + 0.059*"film" + 0.051*"come" + 0.045*"role" + 0.031*"set"
topic #2 (0.333): 0.338*"film" + 0.186*"school" + 0.085*"royal" + 0.056*"international" + 0.055*"set" + 0.049*"build" + 0.044*"role" + 0.044*"area" + 0.042*"short" + 0.030*"come"
topic diff=0.106722, rho=0.288675
-3.002 per-word bound, 8.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.333): 0.190*"back" + 0.184*"area" + 0.174*"see" + 0.136*"court" + 0.099*"come" + 0.053*"build" + 0.040*"set" + 0.031*"school" + 0.021*"role" + 0.015*"original"
topic #1 (0.333): 0.269*"player" + 0.155*"long" + 0.113*"international" + 0.084*"original" + 0.073*"short" + 0.070*"build" + 0.057*"come" + 0.046*"film" + 0.040*"role" + 0.035*"set"
topic #2 (0.333): 0.351*"film" + 0.192*"school" + 0.087*"royal" + 0.054*"set" + 0.049*"build" + 0.047*"role" + 0.046*"international" + 0.043*"short" + 0.039*"area" + 0.027*"come"
topic diff=0.102160, rho=0.277350
-2.989 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=3, decay=0.5, chunksize=20) in 0.18s', 'datetime': '2022-02-06T15:41:52.414732', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
K=3, Coherence Score: 0.5503595963830591
Starting K=4
using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 8 passes over the supplied corpus of 100 documents, updating every 140 documents, evaluating every ~100 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.154*"area" + 0.137*"see" + 0.132*"back" + 0.127*"court" + 0.074*"build" + 0.070*"come" + 0.057*"royal" + 0.054*"set" + 0.050*"school" + 0.041*"role"
topic #1 (0.250): 0.357*"player" + 0.096*"long" + 0.075*"build" + 0.074*"court" + 0.073*"short" + 0.073*"come" + 0.052*"school" + 0.051*"original" + 0.030*"film" + 0.030*"international"
topic #2 (0.250): 0.303*"film" + 0.170*"school" + 0.080*"area" + 0.074*"international" + 0.057*"come" + 0.045*"short" + 0.045*"royal" + 0.045*"set" + 0.040*"build" + 0.038*"role"
topic #3 (0.250): 0.222*"back" + 0.161*"see" + 0.155*"film" + 0.111*"long" + 0.068*"area" + 0.065*"come" + 0.050*"original" + 0.032*"short" + 0.030*"set" + 0.023*"build"
topic diff=1.067418, rho=1.000000
-3.098 per-word bound, 8.6 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.169*"area" + 0.137*"court" + 0.131*"see" + 0.129*"back" + 0.077*"build" + 0.076*"come" + 0.061*"royal" + 0.050*"set" + 0.047*"school" + 0.040*"role"
topic #1 (0.250): 0.385*"player" + 0.096*"long" + 0.078*"short" + 0.077*"come" + 0.075*"build" + 0.060*"original" + 0.048*"court" + 0.038*"international" + 0.037*"school" + 0.031*"set"
topic #2 (0.250): 0.329*"film" + 0.181*"school" + 0.080*"international" + 0.067*"area" + 0.048*"come" + 0.047*"set" + 0.047*"short" + 0.040*"role" + 0.039*"royal" + 0.036*"build"
topic #3 (0.250): 0.244*"back" + 0.181*"see" + 0.129*"film" + 0.120*"long" + 0.062*"come" + 0.056*"area" + 0.055*"original" + 0.032*"short" + 0.031*"set" + 0.022*"build"
topic diff=0.200071, rho=0.377964
-3.044 per-word bound, 8.2 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.181*"area" + 0.143*"court" + 0.121*"back" + 0.120*"see" + 0.081*"come" + 0.081*"build" + 0.069*"royal" + 0.048*"set" + 0.048*"school" + 0.038*"role"
topic #1 (0.250): 0.399*"player" + 0.088*"long" + 0.084*"come" + 0.081*"short" + 0.074*"build" + 0.072*"original" + 0.048*"international" + 0.033*"court" + 0.033*"set" + 0.026*"school"
topic #2 (0.250): 0.352*"film" + 0.188*"school" + 0.083*"international" + 0.056*"area" + 0.049*"set" + 0.048*"short" + 0.046*"role" + 0.040*"come" + 0.032*"build" + 0.030*"royal"
topic #3 (0.250): 0.263*"back" + 0.205*"see" + 0.125*"long" + 0.108*"film" + 0.062*"come" + 0.049*"original" + 0.047*"area" + 0.033*"set" + 0.032*"short" + 0.021*"build"
topic diff=0.188703, rho=0.353553
-3.005 per-word bound, 8.0 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.193*"area" + 0.149*"court" + 0.112*"back" + 0.108*"see" + 0.086*"build" + 0.084*"come" + 0.075*"royal" + 0.049*"school" + 0.046*"set" + 0.037*"role"
topic #1 (0.250): 0.401*"player" + 0.090*"come" + 0.083*"short" + 0.081*"long" + 0.078*"original" + 0.072*"build" + 0.062*"international" + 0.035*"set" + 0.024*"court" + 0.020*"role"
topic #2 (0.250): 0.372*"film" + 0.193*"school" + 0.083*"international" + 0.050*"set" + 0.049*"role" + 0.049*"short" + 0.046*"area" + 0.032*"come" + 0.028*"build" + 0.027*"original"
topic #3 (0.250): 0.281*"back" + 0.231*"see" + 0.122*"long" + 0.082*"film" + 0.065*"come" + 0.044*"original" + 0.042*"area" + 0.035*"set" + 0.031*"short" + 0.020*"build"
topic diff=0.170666, rho=0.333333
-2.979 per-word bound, 7.9 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.204*"area" + 0.155*"court" + 0.103*"back" + 0.095*"see" + 0.091*"build" + 0.083*"come" + 0.082*"royal" + 0.051*"school" + 0.042*"set" + 0.037*"role"
topic #1 (0.250): 0.387*"player" + 0.099*"come" + 0.088*"international" + 0.081*"short" + 0.078*"original" + 0.076*"long" + 0.069*"build" + 0.038*"set" + 0.021*"role" + 0.017*"court"
topic #2 (0.250): 0.394*"film" + 0.198*"school" + 0.076*"international" + 0.053*"set" + 0.051*"role" + 0.050*"short" + 0.038*"area" + 0.029*"original" + 0.025*"come" + 0.024*"build"
topic #3 (0.250): 0.292*"back" + 0.254*"see" + 0.115*"long" + 0.071*"come" + 0.061*"film" + 0.041*"original" + 0.040*"area" + 0.038*"set" + 0.030*"short" + 0.018*"build"
topic diff=0.165278, rho=0.316228
-2.966 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.214*"area" + 0.161*"court" + 0.096*"build" + 0.095*"back" + 0.087*"royal" + 0.082*"see" + 0.082*"come" + 0.054*"school" + 0.039*"set" + 0.037*"role"
topic #1 (0.250): 0.371*"player" + 0.119*"international" + 0.103*"come" + 0.080*"short" + 0.079*"original" + 0.071*"long" + 0.065*"build" + 0.038*"set" + 0.022*"role" + 0.013*"court"
topic #2 (0.250): 0.415*"film" + 0.202*"school" + 0.064*"international" + 0.055*"set" + 0.052*"role" + 0.051*"short" + 0.032*"area" + 0.031*"original" + 0.021*"build" + 0.020*"come"
topic #3 (0.250): 0.299*"back" + 0.271*"see" + 0.108*"long" + 0.078*"come" + 0.044*"film" + 0.041*"set" + 0.039*"area" + 0.038*"original" + 0.028*"short" + 0.017*"build"
topic diff=0.149026, rho=0.301511
-2.959 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.222*"area" + 0.166*"court" + 0.102*"build" + 0.092*"royal" + 0.087*"back" + 0.079*"come" + 0.071*"see" + 0.056*"school" + 0.038*"role" + 0.036*"set"
topic #1 (0.250): 0.353*"player" + 0.146*"international" + 0.102*"come" + 0.080*"short" + 0.078*"original" + 0.071*"long" + 0.061*"build" + 0.043*"set" + 0.021*"role" + 0.010*"court"
topic #2 (0.250): 0.434*"film" + 0.207*"school" + 0.055*"set" + 0.054*"role" + 0.052*"short" + 0.051*"international" + 0.032*"original" + 0.027*"area" + 0.018*"build" + 0.018*"long"
topic #3 (0.250): 0.307*"back" + 0.284*"see" + 0.100*"long" + 0.085*"come" + 0.043*"set" + 0.039*"area" + 0.035*"original" + 0.033*"film" + 0.025*"short" + 0.014*"build"
topic diff=0.138334, rho=0.288675
-2.961 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #20/100, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #40/100, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #60/100, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #80/100, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #100/100, outstanding queue size 5
topic #0 (0.250): 0.229*"area" + 0.170*"court" + 0.107*"build" + 0.096*"royal" + 0.080*"back" + 0.077*"come" + 0.063*"see" + 0.059*"school" + 0.038*"role" + 0.034*"set"
topic #1 (0.250): 0.340*"player" + 0.162*"international" + 0.102*"come" + 0.080*"short" + 0.079*"original" + 0.072*"long" + 0.059*"build" + 0.046*"set" + 0.021*"role" + 0.008*"court"
topic #2 (0.250): 0.448*"film" + 0.209*"school" + 0.055*"role" + 0.055*"set" + 0.054*"short" + 0.041*"international" + 0.033*"original" + 0.023*"area" + 0.020*"long" + 0.016*"build"
topic #3 (0.250): 0.313*"back" + 0.292*"see" + 0.092*"long" + 0.092*"come" + 0.046*"set" + 0.040*"area" + 0.033*"original" + 0.025*"film" + 0.023*"short" + 0.012*"school"
topic diff=0.123266, rho=0.277350
-2.961 per-word bound, 7.8 perplexity estimate based on a held-out corpus of 20 documents with 39 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=16, num_topics=4, decay=0.5, chunksize=20) in 0.17s', 'datetime': '2022-02-06T15:41:52.702925', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
serializing accumulator to return to master...
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
serializing accumulator to return to master...
accumulator serialized
7 accumulators retrieved from output queue
accumulated word occurrence stats for 65 virtual documents
K=4, Coherence Score: 0.550359596383059
ALL DONE!
