using symmetric alpha at 0.05
using symmetric eta at 0.05
using serial LDA version on this node
running online LDA training, 20 topics, 4 passes over the supplied corpus of 996 documents, updating every 700 documents, evaluating every ~996 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #15 (0.050): 0.004*"also" + 0.004*"first" + 0.003*"one" + 0.003*"may" + 0.003*"new" + 0.002*"two" + 0.002*"species" + 0.002*"time" + 0.002*"people" + 0.002*"would"
topic #5 (0.050): 0.004*"two" + 0.004*"also" + 0.004*"first" + 0.003*"one" + 0.003*"would" + 0.002*"three" + 0.002*"new" + 0.002*"time" + 0.002*"met" + 0.002*"band"
topic #11 (0.050): 0.004*"one" + 0.003*"also" + 0.003*"time" + 0.003*"city" + 0.002*"new" + 0.002*"first" + 0.002*"would" + 0.002*"two" + 0.002*"may" + 0.002*"borg"
topic #7 (0.050): 0.004*"also" + 0.003*"one" + 0.003*"two" + 0.003*"first" + 0.002*"park" + 0.002*"year" + 0.002*"new" + 0.002*"since" + 0.002*"world" + 0.002*"station"
topic #9 (0.050): 0.004*"one" + 0.003*"also" + 0.003*"first" + 0.002*"university" + 0.002*"new" + 0.002*"board" + 0.002*"two" + 0.002*"company" + 0.002*"line" + 0.002*"would"
topic diff=9.783338, rho=1.000000
-15.017 per-word bound, 33146.9 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
merging changes from 296 documents into a model of 996 documents
topic #14 (0.050): 0.005*"also" + 0.004*"first" + 0.004*"one" + 0.002*"line" + 0.002*"two" + 0.002*"school" + 0.002*"university" + 0.002*"time" + 0.002*"national" + 0.002*"new"
topic #13 (0.050): 0.005*"first" + 0.005*"also" + 0.003*"time" + 0.003*"one" + 0.003*"game" + 0.002*"part" + 0.002*"two" + 0.002*"would" + 0.002*"new" + 0.002*"line"
topic #19 (0.050): 0.004*"also" + 0.004*"one" + 0.004*"first" + 0.003*"school" + 0.003*"year" + 0.003*"new" + 0.002*"national" + 0.002*"th" + 0.002*"two" + 0.002*"years"
topic #11 (0.050): 0.004*"one" + 0.003*"also" + 0.003*"time" + 0.003*"game" + 0.002*"city" + 0.002*"two" + 0.002*"first" + 0.002*"new" + 0.002*"would" + 0.002*"john"
topic #16 (0.050): 0.005*"also" + 0.004*"first" + 0.004*"one" + 0.003*"two" + 0.002*"years" + 0.002*"line" + 0.002*"university" + 0.002*"would" + 0.002*"district" + 0.002*"album"
topic diff=2.672853, rho=0.353553
-10.912 per-word bound, 1926.9 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #12 (0.050): 0.004*"also" + 0.003*"town" + 0.003*"city" + 0.003*"two" + 0.003*"finchley" + 0.003*"first" + 0.003*"north" + 0.002*"district" + 0.002*"railway" + 0.002*"one"
topic #11 (0.050): 0.004*"one" + 0.003*"also" + 0.003*"time" + 0.003*"city" + 0.002*"two" + 0.002*"game" + 0.002*"first" + 0.002*"building" + 0.002*"vassar" + 0.002*"new"
topic #9 (0.050): 0.004*"one" + 0.004*"university" + 0.003*"also" + 0.002*"first" + 0.002*"new" + 0.002*"company" + 0.002*"line" + 0.002*"board" + 0.002*"two" + 0.002*"game"
topic #17 (0.050): 0.005*"one" + 0.004*"also" + 0.003*"film" + 0.002*"two" + 0.002*"first" + 0.002*"may" + 0.002*"air" + 0.002*"new" + 0.002*"station" + 0.002*"three"
topic #0 (0.050): 0.007*"amazing_race" + 0.005*"also" + 0.004*"first" + 0.003*"one" + 0.003*"league" + 0.003*"season" + 0.003*"team" + 0.003*"club" + 0.002*"new" + 0.002*"time"
topic diff=0.993828, rho=0.289157
-11.013 per-word bound, 2066.5 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
merging changes from 296 documents into a model of 996 documents
topic #11 (0.050): 0.004*"one" + 0.003*"time" + 0.003*"also" + 0.003*"game" + 0.002*"city" + 0.002*"first" + 0.002*"community" + 0.002*"two" + 0.002*"would" + 0.002*"imageshack"
topic #17 (0.050): 0.004*"one" + 0.004*"winner" + 0.003*"also" + 0.003*"film" + 0.003*"patients" + 0.003*"center" + 0.002*"first" + 0.002*"history" + 0.002*"american" + 0.002*"two"
topic #18 (0.050): 0.005*"line" + 0.004*"one" + 0.003*"th" + 0.003*"first" + 0.003*"station" + 0.003*"east" + 0.002*"film" + 0.002*"new" + 0.002*"kilometres_mi" + 0.002*"city"
topic #13 (0.050): 0.005*"also" + 0.004*"first" + 0.003*"line" + 0.003*"one" + 0.003*"two" + 0.003*"time" + 0.003*"part" + 0.002*"west" + 0.002*"station" + 0.002*"route"
topic #9 (0.050): 0.005*"board" + 0.004*"one" + 0.004*"film" + 0.003*"also" + 0.003*"university" + 0.002*"first" + 0.002*"new" + 0.002*"channel" + 0.002*"frampton" + 0.002*"time"
topic diff=1.072005, rho=0.289157
-9.728 per-word bound, 847.8 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #12 (0.050): 0.007*"railway" + 0.007*"met" + 0.006*"district" + 0.004*"also" + 0.004*"station" + 0.004*"line" + 0.004*"london" + 0.004*"city" + 0.004*"north" + 0.003*"new"
topic #8 (0.050): 0.005*"school" + 0.004*"students" + 0.004*"one" + 0.003*"two" + 0.003*"county" + 0.003*"displaystyle" + 0.003*"part" + 0.003*"state" + 0.003*"first" + 0.003*"average"
topic #1 (0.050): 0.006*"would" + 0.005*"season" + 0.005*"also" + 0.005*"game" + 0.005*"first" + 0.005*"two" + 0.004*"team" + 0.004*"year" + 0.004*"played" + 0.004*"one"
topic #10 (0.050): 0.004*"tanuki" + 0.003*"two" + 0.003*"time" + 0.003*"first" + 0.003*"ian" + 0.003*"also" + 0.002*"samantha" + 0.002*"world" + 0.002*"rabbit" + 0.002*"event"
topic #2 (0.050): 0.007*"game" + 0.005*"also" + 0.005*"one" + 0.003*"would" + 0.003*"first" + 0.003*"two" + 0.003*"may" + 0.003*"athletes" + 0.003*"world" + 0.003*"time"
topic diff=0.904290, rho=0.277778
-9.860 per-word bound, 929.1 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
merging changes from 296 documents into a model of 996 documents
topic #4 (0.050): 0.007*"squadron" + 0.006*"cocaine" + 0.005*"pce" + 0.005*"may" + 0.003*"children" + 0.003*"de" + 0.003*"also" + 0.003*"studies" + 0.003*"effects" + 0.003*"raf"
topic #7 (0.050): 0.004*"displaystyle" + 0.004*"final" + 0.004*"losing" + 0.004*"also" + 0.004*"year" + 0.004*"first" + 0.004*"reached" + 0.004*"two" + 0.003*"one" + 0.003*"three"
topic #9 (0.050): 0.005*"film" + 0.005*"board" + 0.004*"one" + 0.003*"frampton" + 0.003*"university" + 0.003*"also" + 0.003*"channel" + 0.002*"zorns_lemma" + 0.002*"first" + 0.002*"new"
topic #11 (0.050): 0.003*"one" + 0.003*"imageshack" + 0.003*"church" + 0.003*"also" + 0.003*"building" + 0.003*"time" + 0.003*"theatre" + 0.003*"two" + 0.002*"first" + 0.002*"service"
topic #3 (0.050): 0.004*"also" + 0.003*"film" + 0.003*"national" + 0.003*"physical" + 0.003*"event" + 0.003*"two" + 0.002*"events" + 0.002*"mental" + 0.002*"since" + 0.002*"later"
topic diff=0.876340, rho=0.277778
-9.225 per-word bound, 598.6 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #13 (0.050): 0.005*"also" + 0.004*"first" + 0.003*"line" + 0.003*"one" + 0.003*"two" + 0.003*"route" + 0.003*"west" + 0.003*"part" + 0.002*"station" + 0.002*"time"
topic #3 (0.050): 0.004*"also" + 0.004*"physical" + 0.003*"event" + 0.003*"national" + 0.003*"mental" + 0.003*"laws" + 0.003*"film" + 0.003*"events" + 0.003*"davidson" + 0.003*"two"
topic #10 (0.050): 0.004*"tanuki" + 0.004*"ian" + 0.003*"first" + 0.003*"samantha" + 0.003*"time" + 0.003*"two" + 0.003*"world" + 0.003*"university" + 0.003*"event" + 0.003*"member"
topic #6 (0.050): 0.005*"film" + 0.004*"also" + 0.004*"jhelum" + 0.004*"album" + 0.003*"british" + 0.003*"first" + 0.003*"th" + 0.003*"shire" + 0.003*"one" + 0.003*"battle"
topic #15 (0.050): 0.006*"also" + 0.006*"species" + 0.004*"may" + 0.003*"first" + 0.003*"brown_algae" + 0.002*"known" + 0.002*"one" + 0.002*"two" + 0.002*"new" + 0.002*"state"
topic diff=0.761091, rho=0.267644
-9.385 per-word bound, 668.5 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
merging changes from 296 documents into a model of 996 documents
topic #19 (0.050): 0.004*"school" + 0.004*"also" + 0.004*"first" + 0.004*"one" + 0.003*"year" + 0.003*"th" + 0.003*"music" + 0.002*"house" + 0.002*"national" + 0.002*"years"
topic #5 (0.050): 0.008*"band" + 0.007*"album" + 0.005*"released" + 0.005*"first" + 0.005*"two" + 0.004*"single" + 0.004*"new" + 0.004*"also" + 0.004*"one" + 0.003*"song"
topic #4 (0.050): 0.011*"squadron" + 0.006*"cocaine" + 0.005*"may" + 0.005*"pce" + 0.005*"raf" + 0.004*"de" + 0.003*"children" + 0.003*"aircraft" + 0.003*"effects" + 0.003*"also"
topic #12 (0.050): 0.007*"railway" + 0.006*"finchley" + 0.005*"met" + 0.005*"district" + 0.004*"station" + 0.004*"city" + 0.004*"also" + 0.004*"angre" + 0.004*"north" + 0.004*"london"
topic #3 (0.050): 0.004*"also" + 0.004*"national" + 0.004*"film" + 0.003*"physical" + 0.003*"events" + 0.003*"event" + 0.003*"since" + 0.003*"mental" + 0.003*"two" + 0.002*"laws"
topic diff=0.720917, rho=0.267644
-8.980 per-word bound, 504.8 perplexity estimate based on a held-out corpus of 96 documents with 18935 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=32647, num_topics=20, decay=0.5, chunksize=100) in 29.97s', 'datetime': '2022-02-06T15:19:54.406580', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 1 minutes
Note: Perplexity estimate based on a held-out corpus of 4 documents


## With Lemmatization
using symmetric alpha at 0.25
using symmetric eta at 0.25
using serial LDA version on this node
running online LDA training, 4 topics, 10 passes over the supplied corpus of 996 documents, updating every 700 documents, evaluating every ~996 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 0, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 0, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 0, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 0, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 0, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.006*"game" + 0.005*"film" + 0.005*"season" + 0.005*"station" + 0.004*"event" + 0.004*"specie" + 0.004*"study" + 0.004*"district" + 0.004*"population" + 0.003*"company"
topic #1 (0.250): 0.004*"building" + 0.004*"child" + 0.004*"season" + 0.004*"design" + 0.004*"game" + 0.004*"player" + 0.004*"club" + 0.003*"program" + 0.003*"football" + 0.003*"university"
topic #2 (0.250): 0.009*"game" + 0.005*"player" + 0.004*"railway" + 0.004*"station" + 0.004*"song" + 0.004*"album" + 0.004*"child" + 0.003*"meet" + 0.003*"music" + 0.003*"season"
topic #3 (0.250): 0.005*"film" + 0.005*"district" + 0.004*"company" + 0.004*"street" + 0.003*"game" + 0.003*"album" + 0.003*"story" + 0.003*"meet" + 0.003*"village" + 0.003*"specie"
topic diff=0.691411, rho=1.000000
-7.493 per-word bound, 180.1 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.008*"game" + 0.006*"film" + 0.004*"station" + 0.004*"season" + 0.004*"star" + 0.004*"district" + 0.004*"event" + 0.004*"title" + 0.004*"house" + 0.003*"company"
topic #1 (0.250): 0.004*"building" + 0.004*"design" + 0.004*"season" + 0.004*"game" + 0.003*"child" + 0.003*"band" + 0.003*"player" + 0.003*"house" + 0.003*"woman" + 0.003*"club"
topic #2 (0.250): 0.011*"game" + 0.005*"album" + 0.005*"song" + 0.005*"player" + 0.004*"season" + 0.004*"event" + 0.004*"station" + 0.003*"band" + 0.003*"music" + 0.003*"run"
topic #3 (0.250): 0.006*"film" + 0.004*"squadron" + 0.004*"company" + 0.004*"district" + 0.004*"game" + 0.004*"street" + 0.003*"album" + 0.003*"book" + 0.003*"story" + 0.003*"art"
topic diff=0.206230, rho=0.353553
-7.355 per-word bound, 163.7 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 1, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 1, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 1, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 1, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 1, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.007*"game" + 0.006*"film" + 0.005*"season" + 0.005*"specie" + 0.004*"event" + 0.004*"station" + 0.004*"star" + 0.004*"district" + 0.004*"population" + 0.004*"study"
topic #1 (0.250): 0.005*"building" + 0.004*"child" + 0.004*"design" + 0.004*"club" + 0.004*"season" + 0.003*"house" + 0.003*"football" + 0.003*"program" + 0.003*"player" + 0.003*"station"
topic #2 (0.250): 0.012*"game" + 0.006*"album" + 0.006*"player" + 0.006*"song" + 0.004*"music" + 0.004*"season" + 0.004*"band" + 0.004*"event" + 0.004*"station" + 0.003*"series"
topic #3 (0.250): 0.006*"film" + 0.005*"company" + 0.004*"street" + 0.004*"district" + 0.004*"story" + 0.004*"squadron" + 0.003*"meet" + 0.003*"book" + 0.003*"attack" + 0.003*"art"
topic diff=0.123490, rho=0.289157
-7.362 per-word bound, 164.5 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.006*"film" + 0.006*"game" + 0.006*"star" + 0.005*"title" + 0.005*"event" + 0.004*"specie" + 0.004*"season" + 0.004*"district" + 0.004*"student" + 0.004*"station"
topic #1 (0.250): 0.006*"building" + 0.005*"house" + 0.004*"design" + 0.004*"station" + 0.004*"town" + 0.004*"woman" + 0.004*"child" + 0.004*"bridge" + 0.004*"season" + 0.003*"football"
topic #2 (0.250): 0.016*"game" + 0.009*"album" + 0.008*"song" + 0.006*"band" + 0.006*"player" + 0.005*"music" + 0.005*"season" + 0.004*"event" + 0.004*"week" + 0.003*"series"
topic #3 (0.250): 0.008*"film" + 0.006*"squadron" + 0.005*"company" + 0.004*"district" + 0.004*"street" + 0.004*"attack" + 0.004*"book" + 0.003*"story" + 0.003*"art" + 0.003*"british"
topic diff=0.181238, rho=0.289157
-7.211 per-word bound, 148.2 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 2, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 2, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 2, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 2, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 2, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.006*"film" + 0.006*"specie" + 0.005*"star" + 0.005*"event" + 0.005*"student" + 0.005*"game" + 0.004*"title" + 0.004*"study" + 0.004*"population" + 0.004*"season"
topic #1 (0.250): 0.006*"building" + 0.005*"house" + 0.005*"child" + 0.004*"station" + 0.004*"design" + 0.004*"town" + 0.004*"church" + 0.004*"park" + 0.004*"woman" + 0.004*"road"
topic #2 (0.250): 0.016*"game" + 0.008*"album" + 0.008*"song" + 0.007*"player" + 0.007*"band" + 0.005*"season" + 0.005*"music" + 0.004*"event" + 0.004*"club" + 0.004*"sport"
topic #3 (0.250): 0.008*"film" + 0.006*"company" + 0.005*"squadron" + 0.005*"district" + 0.004*"street" + 0.004*"attack" + 0.004*"meet" + 0.004*"government" + 0.003*"book" + 0.003*"story"
topic diff=0.129461, rho=0.277778
-7.241 per-word bound, 151.3 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.006*"star" + 0.006*"film" + 0.006*"event" + 0.005*"title" + 0.005*"specie" + 0.005*"game" + 0.005*"student" + 0.005*"lose" + 0.005*"final" + 0.005*"season"
topic #1 (0.250): 0.007*"building" + 0.006*"house" + 0.006*"station" + 0.005*"design" + 0.004*"bridge" + 0.004*"town" + 0.004*"park" + 0.004*"church" + 0.004*"child" + 0.004*"center"
topic #2 (0.250): 0.021*"game" + 0.012*"album" + 0.009*"song" + 0.008*"band" + 0.007*"player" + 0.007*"season" + 0.006*"music" + 0.005*"week" + 0.004*"event" + 0.004*"track"
topic #3 (0.250): 0.008*"film" + 0.007*"squadron" + 0.006*"company" + 0.005*"attack" + 0.004*"story" + 0.004*"street" + 0.004*"district" + 0.003*"book" + 0.003*"order" + 0.003*"government"
topic diff=0.190641, rho=0.277778
-7.125 per-word bound, 139.6 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 3, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 3, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 3, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 3, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 3, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.006*"specie" + 0.006*"event" + 0.006*"star" + 0.006*"student" + 0.005*"film" + 0.005*"study" + 0.004*"season" + 0.004*"university" + 0.004*"title" + 0.004*"research"
topic #1 (0.250): 0.007*"building" + 0.006*"station" + 0.006*"house" + 0.005*"child" + 0.005*"design" + 0.005*"park" + 0.005*"church" + 0.004*"district" + 0.004*"road" + 0.004*"town"
topic #2 (0.250): 0.020*"game" + 0.011*"album" + 0.009*"song" + 0.008*"player" + 0.008*"band" + 0.007*"season" + 0.007*"music" + 0.005*"club" + 0.004*"event" + 0.004*"week"
topic #3 (0.250): 0.008*"film" + 0.006*"company" + 0.005*"squadron" + 0.005*"attack" + 0.005*"story" + 0.004*"meet" + 0.004*"street" + 0.004*"district" + 0.004*"government" + 0.004*"battle"
topic diff=0.129195, rho=0.267644
-7.162 per-word bound, 143.2 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.007*"star" + 0.006*"specie" + 0.006*"film" + 0.006*"event" + 0.006*"student" + 0.006*"title" + 0.005*"lose" + 0.005*"final" + 0.005*"study" + 0.005*"reach"
topic #1 (0.250): 0.008*"building" + 0.007*"house" + 0.007*"station" + 0.006*"town" + 0.005*"bridge" + 0.005*"design" + 0.005*"park" + 0.005*"church" + 0.005*"district" + 0.004*"center"
topic #2 (0.250): 0.023*"game" + 0.012*"album" + 0.010*"song" + 0.009*"band" + 0.008*"season" + 0.008*"player" + 0.007*"music" + 0.005*"week" + 0.005*"club" + 0.005*"event"
topic #3 (0.250): 0.009*"film" + 0.007*"squadron" + 0.007*"company" + 0.005*"attack" + 0.004*"british" + 0.004*"story" + 0.004*"ship" + 0.004*"order" + 0.004*"government" + 0.004*"army"
topic diff=0.169870, rho=0.267644
-7.078 per-word bound, 135.1 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 4, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 4, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 4, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 4, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 4, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.007*"specie" + 0.006*"student" + 0.006*"star" + 0.006*"event" + 0.006*"study" + 0.005*"film" + 0.005*"research" + 0.005*"university" + 0.005*"title" + 0.004*"final"
topic #1 (0.250): 0.008*"building" + 0.007*"station" + 0.007*"house" + 0.005*"district" + 0.005*"town" + 0.005*"child" + 0.005*"park" + 0.005*"church" + 0.005*"design" + 0.005*"population"
topic #2 (0.250): 0.022*"game" + 0.012*"album" + 0.010*"song" + 0.009*"band" + 0.009*"season" + 0.009*"player" + 0.007*"music" + 0.006*"club" + 0.005*"week" + 0.005*"event"
topic #3 (0.250): 0.009*"film" + 0.007*"company" + 0.006*"squadron" + 0.005*"attack" + 0.005*"story" + 0.004*"meet" + 0.004*"ship" + 0.004*"battle" + 0.004*"order" + 0.004*"government"
topic diff=0.129461, rho=0.258544
-7.118 per-word bound, 138.9 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.007*"star" + 0.006*"specie" + 0.006*"student" + 0.006*"event" + 0.006*"title" + 0.006*"lose" + 0.006*"final" + 0.005*"film" + 0.005*"study" + 0.005*"reach"
topic #1 (0.250): 0.008*"house" + 0.008*"building" + 0.008*"station" + 0.007*"town" + 0.006*"district" + 0.005*"bridge" + 0.005*"park" + 0.005*"water" + 0.005*"church" + 0.005*"design"
topic #2 (0.250): 0.024*"game" + 0.013*"album" + 0.011*"song" + 0.010*"band" + 0.009*"season" + 0.008*"player" + 0.008*"music" + 0.006*"club" + 0.006*"week" + 0.005*"event"
topic #3 (0.250): 0.010*"film" + 0.007*"squadron" + 0.007*"company" + 0.006*"attack" + 0.005*"british" + 0.005*"ship" + 0.004*"story" + 0.004*"order" + 0.004*"war" + 0.004*"army"
topic diff=0.159234, rho=0.258544
-7.049 per-word bound, 132.5 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 5, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 5, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 5, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 5, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 5, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.007*"specie" + 0.007*"student" + 0.006*"star" + 0.006*"study" + 0.006*"event" + 0.005*"research" + 0.005*"university" + 0.005*"title" + 0.005*"final" + 0.005*"lose"
topic #1 (0.250): 0.008*"station" + 0.008*"building" + 0.008*"house" + 0.007*"district" + 0.006*"town" + 0.005*"population" + 0.005*"church" + 0.005*"park" + 0.005*"road" + 0.005*"child"
topic #2 (0.250): 0.023*"game" + 0.012*"album" + 0.010*"song" + 0.010*"season" + 0.010*"band" + 0.009*"player" + 0.008*"music" + 0.006*"club" + 0.005*"week" + 0.005*"series"
topic #3 (0.250): 0.009*"film" + 0.008*"company" + 0.006*"squadron" + 0.006*"attack" + 0.005*"story" + 0.004*"ship" + 0.004*"order" + 0.004*"british" + 0.004*"battle" + 0.004*"war"
topic diff=0.128895, rho=0.250313
-7.089 per-word bound, 136.2 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.007*"star" + 0.007*"student" + 0.006*"specie" + 0.006*"event" + 0.006*"lose" + 0.006*"final" + 0.006*"title" + 0.006*"study" + 0.005*"reach" + 0.005*"film"
topic #1 (0.250): 0.009*"house" + 0.008*"station" + 0.008*"building" + 0.007*"town" + 0.007*"district" + 0.005*"bridge" + 0.005*"water" + 0.005*"park" + 0.005*"church" + 0.005*"population"
topic #2 (0.250): 0.025*"game" + 0.013*"album" + 0.011*"song" + 0.011*"band" + 0.010*"season" + 0.008*"player" + 0.008*"music" + 0.006*"club" + 0.006*"week" + 0.005*"event"
topic #3 (0.250): 0.010*"film" + 0.008*"company" + 0.008*"squadron" + 0.006*"attack" + 0.005*"british" + 0.005*"ship" + 0.004*"order" + 0.004*"story" + 0.004*"war" + 0.004*"son"
topic diff=0.151754, rho=0.250313
-7.029 per-word bound, 130.6 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 6, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 6, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 6, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 6, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 6, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.007*"specie" + 0.007*"student" + 0.006*"study" + 0.006*"star" + 0.006*"event" + 0.005*"research" + 0.005*"university" + 0.005*"lose" + 0.005*"final" + 0.005*"title"
topic #1 (0.250): 0.009*"station" + 0.008*"house" + 0.008*"building" + 0.007*"district" + 0.007*"town" + 0.006*"population" + 0.005*"church" + 0.005*"park" + 0.005*"road" + 0.005*"south"
topic #2 (0.250): 0.024*"game" + 0.013*"album" + 0.011*"season" + 0.011*"song" + 0.010*"band" + 0.009*"player" + 0.008*"music" + 0.007*"club" + 0.005*"series" + 0.005*"football"
topic #3 (0.250): 0.010*"film" + 0.008*"company" + 0.006*"squadron" + 0.006*"attack" + 0.005*"ship" + 0.005*"story" + 0.005*"british" + 0.005*"order" + 0.004*"war" + 0.004*"battle"
topic diff=0.126396, rho=0.242821
-7.068 per-word bound, 134.2 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.007*"star" + 0.007*"student" + 0.007*"specie" + 0.006*"study" + 0.006*"event" + 0.006*"lose" + 0.006*"final" + 0.006*"title" + 0.005*"research" + 0.005*"reach"
topic #1 (0.250): 0.009*"house" + 0.009*"station" + 0.008*"building" + 0.008*"town" + 0.007*"district" + 0.006*"water" + 0.006*"bridge" + 0.005*"population" + 0.005*"park" + 0.005*"church"
topic #2 (0.250): 0.025*"game" + 0.013*"album" + 0.011*"song" + 0.011*"band" + 0.011*"season" + 0.008*"player" + 0.008*"music" + 0.007*"club" + 0.006*"week" + 0.005*"series"
topic #3 (0.250): 0.011*"film" + 0.008*"company" + 0.008*"squadron" + 0.006*"attack" + 0.005*"british" + 0.005*"ship" + 0.005*"order" + 0.005*"war" + 0.004*"son" + 0.004*"story"
topic diff=0.145882, rho=0.242821
-7.012 per-word bound, 129.1 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 7, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 7, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 7, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 7, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 7, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.007*"specie" + 0.007*"student" + 0.007*"study" + 0.006*"star" + 0.006*"event" + 0.005*"research" + 0.005*"university" + 0.005*"lose" + 0.005*"program" + 0.005*"final"
topic #1 (0.250): 0.010*"station" + 0.008*"house" + 0.008*"building" + 0.008*"district" + 0.007*"town" + 0.006*"population" + 0.006*"church" + 0.005*"park" + 0.005*"railway" + 0.005*"water"
topic #2 (0.250): 0.025*"game" + 0.013*"album" + 0.011*"season" + 0.011*"song" + 0.010*"band" + 0.009*"player" + 0.008*"music" + 0.007*"club" + 0.006*"football" + 0.006*"series"
topic #3 (0.250): 0.011*"film" + 0.009*"company" + 0.007*"squadron" + 0.006*"attack" + 0.005*"ship" + 0.005*"british" + 0.005*"order" + 0.005*"war" + 0.005*"story" + 0.005*"battle"
topic diff=0.122999, rho=0.235965
-7.050 per-word bound, 132.6 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.007*"star" + 0.007*"student" + 0.006*"specie" + 0.006*"study" + 0.006*"lose" + 0.006*"event" + 0.006*"final" + 0.005*"title" + 0.005*"research" + 0.005*"reach"
topic #1 (0.250): 0.009*"house" + 0.009*"station" + 0.008*"building" + 0.008*"district" + 0.008*"town" + 0.006*"water" + 0.006*"population" + 0.006*"bridge" + 0.005*"park" + 0.005*"church"
topic #2 (0.250): 0.026*"game" + 0.013*"album" + 0.011*"song" + 0.011*"season" + 0.011*"band" + 0.009*"player" + 0.009*"music" + 0.007*"club" + 0.006*"week" + 0.006*"series"
topic #3 (0.250): 0.012*"film" + 0.009*"company" + 0.008*"squadron" + 0.006*"attack" + 0.006*"british" + 0.005*"ship" + 0.005*"order" + 0.005*"war" + 0.005*"son" + 0.004*"design"
topic diff=0.139173, rho=0.235965
-7.000 per-word bound, 128.0 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 8, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 8, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 8, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 8, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 8, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 700 documents into a model of 996 documents
topic #0 (0.250): 0.007*"study" + 0.007*"student" + 0.007*"specie" + 0.006*"star" + 0.006*"event" + 0.005*"research" + 0.005*"program" + 0.005*"university" + 0.005*"lose" + 0.004*"final"
topic #1 (0.250): 0.010*"station" + 0.009*"house" + 0.009*"district" + 0.008*"building" + 0.007*"town" + 0.007*"population" + 0.006*"railway" + 0.006*"church" + 0.006*"park" + 0.006*"water"
topic #2 (0.250): 0.025*"game" + 0.013*"album" + 0.012*"season" + 0.011*"song" + 0.010*"band" + 0.009*"player" + 0.009*"music" + 0.007*"club" + 0.006*"football" + 0.006*"series"
topic #3 (0.250): 0.011*"film" + 0.009*"company" + 0.007*"squadron" + 0.006*"attack" + 0.005*"ship" + 0.005*"british" + 0.005*"order" + 0.005*"war" + 0.005*"son" + 0.005*"battle"
topic diff=0.118702, rho=0.229658
-7.038 per-word bound, 131.4 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 296 documents into a model of 996 documents
topic #0 (0.250): 0.007*"student" + 0.007*"star" + 0.007*"study" + 0.006*"specie" + 0.006*"lose" + 0.006*"event" + 0.005*"final" + 0.005*"title" + 0.005*"research" + 0.005*"reach"
topic #1 (0.250): 0.010*"house" + 0.010*"station" + 0.009*"building" + 0.008*"district" + 0.008*"town" + 0.006*"water" + 0.006*"population" + 0.006*"bridge" + 0.005*"park" + 0.005*"church"
topic #2 (0.250): 0.026*"game" + 0.013*"album" + 0.011*"season" + 0.011*"song" + 0.011*"band" + 0.009*"player" + 0.009*"music" + 0.007*"club" + 0.006*"week" + 0.006*"series"
topic #3 (0.250): 0.012*"film" + 0.009*"company" + 0.008*"squadron" + 0.007*"attack" + 0.006*"british" + 0.006*"ship" + 0.005*"order" + 0.005*"war" + 0.005*"son" + 0.005*"design"
topic diff=0.132108, rho=0.229658
-6.992 per-word bound, 127.3 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #100/996, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #200/996, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #300/996, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #400/996, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #500/996, outstanding queue size 5
PROGRESS: pass 9, dispatched chunk #5 = documents up to #600/996, outstanding queue size 6
PROGRESS: pass 9, dispatched chunk #6 = documents up to #700/996, outstanding queue size 7
PROGRESS: pass 9, dispatched chunk #7 = documents up to #800/996, outstanding queue size 8
PROGRESS: pass 9, dispatched chunk #8 = documents up to #900/996, outstanding queue size 9
PROGRESS: pass 9, dispatched chunk #9 = documents up to #996/996, outstanding queue size 10
merging changes from 796 documents into a model of 996 documents
topic #0 (0.250): 0.007*"study" + 0.007*"student" + 0.007*"specie" + 0.006*"star" + 0.006*"event" + 0.005*"lose" + 0.005*"research" + 0.005*"program" + 0.005*"final" + 0.005*"university"
topic #1 (0.250): 0.010*"station" + 0.009*"house" + 0.009*"district" + 0.009*"building" + 0.008*"town" + 0.007*"population" + 0.006*"railway" + 0.006*"water" + 0.006*"park" + 0.006*"street"
topic #2 (0.250): 0.026*"game" + 0.013*"album" + 0.012*"season" + 0.011*"song" + 0.010*"band" + 0.009*"player" + 0.009*"music" + 0.007*"club" + 0.006*"series" + 0.006*"football"
topic #3 (0.250): 0.012*"film" + 0.009*"company" + 0.007*"squadron" + 0.006*"attack" + 0.005*"ship" + 0.005*"british" + 0.005*"order" + 0.005*"war" + 0.005*"son" + 0.005*"battle"
topic diff=0.100211, rho=0.223831
-6.996 per-word bound, 127.7 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
merging changes from 200 documents into a model of 996 documents
topic #0 (0.250): 0.007*"student" + 0.007*"study" + 0.007*"specie" + 0.006*"star" + 0.005*"title" + 0.005*"research" + 0.005*"event" + 0.005*"lose" + 0.005*"final" + 0.005*"university"
topic #1 (0.250): 0.010*"house" + 0.009*"station" + 0.009*"town" + 0.009*"building" + 0.008*"district" + 0.007*"water" + 0.006*"bridge" + 0.006*"population" + 0.006*"church" + 0.006*"park"
topic #2 (0.250): 0.023*"game" + 0.014*"album" + 0.012*"band" + 0.012*"song" + 0.012*"season" + 0.009*"music" + 0.009*"player" + 0.007*"club" + 0.006*"series" + 0.006*"football"
topic #3 (0.250): 0.013*"film" + 0.009*"squadron" + 0.009*"company" + 0.007*"attack" + 0.006*"british" + 0.006*"ship" + 0.005*"order" + 0.005*"war" + 0.005*"army" + 0.005*"son"
topic diff=0.158796, rho=0.223831
-7.058 per-word bound, 133.2 perplexity estimate based on a held-out corpus of 96 documents with 7671 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=1970, num_topics=4, decay=0.5, chunksize=100) in 3.52s', 'datetime': '2022-02-06T15:19:58.281748', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 0 minutes
Note: Log likelihood is per-word ELBO
Note: Perplexity estimate based on a held-out corpus of 4 documents
Starting K=3
