using symmetric alpha at 0.05
using symmetric eta at 0.05
using serial LDA version on this node
running online LDA training, 20 topics, 20 passes over the supplied corpus of 996 documents, updating every 1400 documents, evaluating every ~996 documents, iterating 50x with a convergence threshold of 0.001000
training LDA model using 7 processes
PROGRESS: pass 0, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 0, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 0, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 0, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 0, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #15 (0.050): 0.005*"also" + 0.003*"first" + 0.003*"one" + 0.003*"may" + 0.002*"new" + 0.002*"two" + 0.002*"time" + 0.002*"species" + 0.002*"people" + 0.002*"would"
topic #5 (0.050): 0.004*"also" + 0.004*"two" + 0.004*"first" + 0.003*"one" + 0.003*"would" + 0.003*"band" + 0.002*"new" + 0.002*"three" + 0.002*"album" + 0.002*"time"
topic #11 (0.050): 0.004*"one" + 0.003*"also" + 0.003*"time" + 0.003*"game" + 0.003*"city" + 0.002*"first" + 0.002*"two" + 0.002*"new" + 0.002*"would" + 0.002*"years"
topic #7 (0.050): 0.004*"also" + 0.004*"one" + 0.003*"two" + 0.003*"first" + 0.002*"year" + 0.002*"bridge" + 0.002*"new" + 0.002*"three" + 0.002*"station" + 0.002*"since"
topic #9 (0.050): 0.004*"one" + 0.003*"also" + 0.003*"first" + 0.002*"university" + 0.002*"new" + 0.002*"two" + 0.002*"would" + 0.002*"company" + 0.002*"species" + 0.002*"film"
topic diff=8.663399, rho=1.000000
-11.124 per-word bound, 2232.5 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 1, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 1, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 1, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 1, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 1, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #13 (0.050): 0.005*"also" + 0.005*"first" + 0.004*"game" + 0.003*"one" + 0.003*"time" + 0.003*"line" + 0.002*"two" + 0.002*"part" + 0.002*"new" + 0.002*"west"
topic #3 (0.050): 0.005*"also" + 0.003*"national" + 0.003*"first" + 0.002*"two" + 0.002*"later" + 0.002*"year" + 0.002*"street" + 0.002*"years" + 0.002*"one" + 0.002*"second"
topic #15 (0.050): 0.005*"also" + 0.004*"may" + 0.004*"species" + 0.003*"first" + 0.003*"one" + 0.003*"squadron" + 0.003*"new" + 0.002*"two" + 0.002*"known" + 0.002*"people"
topic #8 (0.050): 0.003*"displaystyle" + 0.003*"one" + 0.003*"school" + 0.003*"two" + 0.003*"state" + 0.003*"first" + 0.002*"part" + 0.002*"also" + 0.002*"university" + 0.002*"team"
topic #7 (0.050): 0.004*"also" + 0.004*"one" + 0.003*"two" + 0.003*"first" + 0.003*"displaystyle" + 0.003*"bridge" + 0.002*"park" + 0.002*"year" + 0.002*"three" + 0.002*"since"
topic diff=1.331948, rho=0.378506
-10.440 per-word bound, 1389.4 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 2, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 2, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 2, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 2, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 2, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #1 (0.050): 0.006*"would" + 0.005*"also" + 0.005*"game" + 0.004*"first" + 0.004*"two" + 0.004*"team" + 0.004*"season" + 0.003*"one" + 0.003*"university" + 0.003*"year"
topic #16 (0.050): 0.005*"one" + 0.005*"also" + 0.004*"first" + 0.003*"population" + 0.003*"years" + 0.002*"two" + 0.002*"line" + 0.002*"house" + 0.002*"university" + 0.002*"building"
topic #0 (0.050): 0.008*"borg" + 0.007*"amazing_race" + 0.005*"season" + 0.005*"also" + 0.004*"team" + 0.004*"league" + 0.004*"first" + 0.003*"club" + 0.003*"star_trek" + 0.003*"one"
topic #8 (0.050): 0.004*"school" + 0.004*"displaystyle" + 0.003*"one" + 0.003*"state" + 0.003*"two" + 0.003*"students" + 0.003*"first" + 0.002*"county" + 0.002*"part" + 0.002*"university"
topic #5 (0.050): 0.005*"band" + 0.005*"two" + 0.004*"first" + 0.004*"also" + 0.004*"one" + 0.004*"laudrup" + 0.004*"new" + 0.003*"would" + 0.003*"album" + 0.003*"time"
topic diff=1.195971, rho=0.353996
-10.055 per-word bound, 1064.1 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 3, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 3, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 3, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 3, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 3, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #10 (0.050): 0.003*"first" + 0.003*"two" + 0.003*"university" + 0.003*"also" + 0.003*"united_states" + 0.003*"world" + 0.003*"tanuki" + 0.003*"time" + 0.002*"event" + 0.002*"house"
topic #18 (0.050): 0.004*"line" + 0.004*"squadron" + 0.003*"film" + 0.003*"one" + 0.003*"south" + 0.003*"city" + 0.003*"th" + 0.003*"east" + 0.002*"kilometres_mi" + 0.002*"first"
topic #4 (0.050): 0.006*"cocaine" + 0.005*"de" + 0.005*"pce" + 0.004*"may" + 0.003*"also" + 0.003*"one" + 0.003*"children" + 0.003*"studies" + 0.003*"first" + 0.003*"effects"
topic #3 (0.050): 0.004*"also" + 0.004*"street" + 0.003*"national" + 0.003*"physical" + 0.003*"event" + 0.003*"two" + 0.003*"first" + 0.002*"mental" + 0.002*"laws" + 0.002*"davidson"
topic #1 (0.050): 0.007*"would" + 0.005*"also" + 0.005*"first" + 0.005*"game" + 0.005*"season" + 0.004*"team" + 0.004*"great_danes" + 0.004*"two" + 0.004*"university" + 0.003*"one"
topic diff=1.036590, rho=0.333704
-9.828 per-word bound, 909.0 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 4, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 4, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 4, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 4, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 4, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #18 (0.050): 0.005*"line" + 0.004*"squadron" + 0.003*"film" + 0.003*"south" + 0.003*"one" + 0.003*"city" + 0.003*"th" + 0.003*"kilometres_mi" + 0.003*"east" + 0.003*"roosevelt"
topic #17 (0.050): 0.005*"one" + 0.005*"film" + 0.004*"also" + 0.003*"center" + 0.002*"winner" + 0.002*"air" + 0.002*"patients" + 0.002*"work" + 0.002*"first" + 0.002*"station"
topic #19 (0.050): 0.005*"school" + 0.004*"also" + 0.004*"first" + 0.004*"year" + 0.004*"one" + 0.003*"management" + 0.003*"company" + 0.002*"two" + 0.002*"th" + 0.002*"years"
topic #11 (0.050): 0.004*"one" + 0.004*"vassar" + 0.003*"city" + 0.002*"enamel" + 0.002*"collection" + 0.002*"also" + 0.002*"two" + 0.002*"church" + 0.002*"first" + 0.002*"building"
topic #13 (0.050): 0.006*"game" + 0.005*"also" + 0.004*"first" + 0.003*"one" + 0.003*"route" + 0.003*"ian" + 0.003*"two" + 0.003*"line" + 0.002*"nowe" + 0.002*"time"
topic diff=0.883174, rho=0.316544
-9.683 per-word bound, 821.8 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 5, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 5, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 5, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 5, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 5, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #7 (0.050): 0.005*"displaystyle" + 0.004*"one" + 0.004*"first" + 0.004*"park" + 0.003*"final" + 0.003*"bridge" + 0.003*"two" + 0.003*"also" + 0.003*"losing" + 0.003*"year"
topic #3 (0.050): 0.005*"street" + 0.004*"also" + 0.004*"physical" + 0.004*"national" + 0.003*"event" + 0.003*"mental" + 0.003*"laws" + 0.003*"davidson" + 0.003*"events" + 0.003*"two"
topic #8 (0.050): 0.006*"school" + 0.005*"displaystyle" + 0.004*"students" + 0.004*"one" + 0.004*"county" + 0.003*"state" + 0.003*"goldberg" + 0.003*"carter" + 0.003*"weir_high" + 0.003*"university"
topic #1 (0.050): 0.008*"would" + 0.006*"season" + 0.006*"first" + 0.005*"team" + 0.005*"game" + 0.005*"also" + 0.005*"great_danes" + 0.004*"university" + 0.004*"played" + 0.004*"two"
topic #13 (0.050): 0.007*"game" + 0.005*"also" + 0.004*"first" + 0.003*"one" + 0.003*"route" + 0.003*"ian" + 0.003*"nowe" + 0.003*"two" + 0.002*"line" + 0.002*"samantha"
topic diff=0.741094, rho=0.301786
-9.586 per-word bound, 768.7 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 6, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 6, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 6, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 6, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 6, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #2 (0.050): 0.006*"game" + 0.005*"also" + 0.005*"one" + 0.004*"athletes" + 0.003*"sport_psychology" + 0.003*"performance" + 0.003*"would" + 0.003*"time" + 0.003*"first" + 0.003*"sports"
topic #14 (0.050): 0.006*"also" + 0.004*"hiv" + 0.004*"indonesia" + 0.003*"school" + 0.003*"national" + 0.003*"first" + 0.003*"university" + 0.003*"westover" + 0.003*"one" + 0.003*"sattler"
topic #17 (0.050): 0.005*"film" + 0.005*"also" + 0.005*"one" + 0.003*"winner" + 0.003*"center" + 0.003*"air" + 0.003*"patients" + 0.002*"work" + 0.002*"first" + 0.002*"flavipes"
topic #15 (0.050): 0.007*"species" + 0.005*"squadron" + 0.005*"also" + 0.005*"may" + 0.003*"brown_algae" + 0.003*"new" + 0.003*"two" + 0.003*"first" + 0.003*"symbiodinium" + 0.003*"raf"
topic #0 (0.050): 0.012*"borg" + 0.009*"amazing_race" + 0.006*"season" + 0.005*"league" + 0.005*"star_trek" + 0.005*"also" + 0.004*"first" + 0.004*"team" + 0.004*"club" + 0.003*"episode"
topic diff=0.615264, rho=0.288916
-9.520 per-word bound, 734.3 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 7, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 7, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 7, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 7, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 7, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #18 (0.050): 0.006*"line" + 0.005*"squadron" + 0.004*"film" + 0.003*"south" + 0.003*"kilometres_mi" + 0.003*"east" + 0.003*"city" + 0.003*"roosevelt" + 0.003*"th" + 0.003*"islands"
topic #12 (0.050): 0.009*"railway" + 0.008*"met" + 0.007*"district" + 0.007*"station" + 0.007*"line" + 0.005*"north" + 0.005*"london" + 0.005*"finchley" + 0.004*"town" + 0.004*"city"
topic #15 (0.050): 0.007*"species" + 0.006*"squadron" + 0.005*"also" + 0.005*"may" + 0.003*"brown_algae" + 0.003*"new" + 0.003*"two" + 0.003*"symbiodinium" + 0.003*"first" + 0.003*"raf"
topic #11 (0.050): 0.004*"vassar" + 0.004*"one" + 0.003*"enamel" + 0.003*"collection" + 0.003*"church" + 0.003*"th_century" + 0.002*"two" + 0.002*"building" + 0.002*"also" + 0.002*"high"
topic #4 (0.050): 0.007*"cocaine" + 0.006*"de" + 0.006*"pce" + 0.005*"may" + 0.004*"children" + 0.004*"also" + 0.003*"effects" + 0.003*"studies" + 0.003*"one" + 0.003*"found"
topic diff=0.506553, rho=0.277564
-9.475 per-word bound, 711.4 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 8, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 8, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 8, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 8, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 8, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #18 (0.050): 0.006*"line" + 0.005*"squadron" + 0.004*"film" + 0.004*"south" + 0.003*"kilometres_mi" + 0.003*"east" + 0.003*"city" + 0.003*"roosevelt" + 0.003*"th" + 0.003*"islands"
topic #19 (0.050): 0.006*"school" + 0.004*"also" + 0.004*"year" + 0.004*"first" + 0.004*"management" + 0.003*"one" + 0.003*"company" + 0.003*"university" + 0.003*"th" + 0.003*"work"
topic #7 (0.050): 0.006*"displaystyle" + 0.004*"first" + 0.004*"one" + 0.004*"park" + 0.004*"final" + 0.003*"bridge" + 0.003*"losing" + 0.003*"two" + 0.003*"also" + 0.003*"year"
topic #13 (0.050): 0.007*"game" + 0.005*"also" + 0.004*"first" + 0.003*"route" + 0.003*"one" + 0.003*"ian" + 0.003*"nowe" + 0.002*"samantha" + 0.002*"two" + 0.002*"drakengard"
topic #16 (0.050): 0.005*"one" + 0.005*"also" + 0.004*"population" + 0.003*"first" + 0.003*"years" + 0.003*"people" + 0.002*"house" + 0.002*"age" + 0.002*"area" + 0.002*"building"
topic diff=0.413768, rho=0.267452
-9.443 per-word bound, 695.9 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 9, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 9, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 9, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 9, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 9, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #8 (0.050): 0.007*"displaystyle" + 0.006*"school" + 0.005*"students" + 0.004*"county" + 0.004*"one" + 0.004*"goldberg" + 0.003*"state" + 0.003*"carter" + 0.003*"weir_high" + 0.003*"university"
topic #11 (0.050): 0.004*"vassar" + 0.003*"one" + 0.003*"enamel" + 0.003*"collection" + 0.003*"church" + 0.003*"th_century" + 0.002*"two" + 0.002*"include" + 0.002*"building" + 0.002*"high"
topic #6 (0.050): 0.006*"album" + 0.005*"th" + 0.005*"lennon" + 0.004*"film" + 0.004*"also" + 0.004*"one" + 0.003*"first" + 0.003*"battle" + 0.003*"british" + 0.003*"two"
topic #12 (0.050): 0.009*"railway" + 0.009*"met" + 0.007*"station" + 0.007*"district" + 0.007*"line" + 0.005*"north" + 0.005*"london" + 0.005*"finchley" + 0.004*"town" + 0.004*"road"
topic #4 (0.050): 0.007*"cocaine" + 0.006*"de" + 0.006*"pce" + 0.005*"may" + 0.004*"children" + 0.004*"also" + 0.004*"effects" + 0.003*"studies" + 0.003*"one" + 0.003*"found"
topic diff=0.336568, rho=0.258371
-9.420 per-word bound, 685.1 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 10, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 10, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 10, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 10, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 10, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #19 (0.050): 0.006*"school" + 0.004*"also" + 0.004*"year" + 0.004*"first" + 0.004*"management" + 0.003*"one" + 0.003*"company" + 0.003*"university" + 0.003*"work" + 0.003*"director"
topic #7 (0.050): 0.006*"displaystyle" + 0.004*"first" + 0.004*"park" + 0.004*"one" + 0.004*"final" + 0.003*"losing" + 0.003*"bridge" + 0.003*"two" + 0.003*"year" + 0.003*"also"
topic #8 (0.050): 0.007*"displaystyle" + 0.006*"school" + 0.005*"students" + 0.005*"county" + 0.004*"one" + 0.004*"goldberg" + 0.004*"carter" + 0.004*"weir_high" + 0.003*"state" + 0.003*"table"
topic #18 (0.050): 0.006*"line" + 0.005*"squadron" + 0.004*"south" + 0.004*"east" + 0.004*"kilometres_mi" + 0.004*"film" + 0.004*"islands" + 0.003*"city" + 0.003*"roosevelt" + 0.003*"th"
topic #1 (0.050): 0.008*"would" + 0.008*"season" + 0.007*"team" + 0.006*"first" + 0.006*"played" + 0.005*"game" + 0.005*"also" + 0.005*"great_danes" + 0.005*"university" + 0.005*"year"
topic diff=0.272718, rho=0.250156
-9.404 per-word bound, 677.3 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 11, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 11, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 11, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 11, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 11, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #6 (0.050): 0.005*"album" + 0.005*"th" + 0.005*"lennon" + 0.004*"film" + 0.004*"also" + 0.004*"one" + 0.003*"battle" + 0.003*"first" + 0.003*"british" + 0.003*"two"
topic #10 (0.050): 0.005*"hayden" + 0.004*"united_states" + 0.004*"election" + 0.003*"tanuki" + 0.003*"university" + 0.003*"member" + 0.003*"house" + 0.003*"world" + 0.003*"first" + 0.003*"born"
topic #14 (0.050): 0.006*"also" + 0.004*"hiv" + 0.004*"indonesia" + 0.004*"school" + 0.004*"national" + 0.003*"university" + 0.003*"westover" + 0.003*"sattler" + 0.003*"first" + 0.002*"one"
topic #15 (0.050): 0.008*"species" + 0.006*"squadron" + 0.005*"also" + 0.005*"may" + 0.003*"brown_algae" + 0.003*"new" + 0.003*"symbiodinium" + 0.003*"two" + 0.003*"raf" + 0.003*"found"
topic #13 (0.050): 0.007*"game" + 0.005*"also" + 0.004*"first" + 0.004*"route" + 0.003*"one" + 0.003*"ian" + 0.003*"nowe" + 0.003*"samantha" + 0.002*"drakengard" + 0.002*"two"
topic diff=0.220725, rho=0.242678
-9.391 per-word bound, 671.6 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 12, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 12, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 12, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 12, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 12, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #15 (0.050): 0.008*"species" + 0.006*"squadron" + 0.005*"also" + 0.005*"may" + 0.003*"brown_algae" + 0.003*"new" + 0.003*"symbiodinium" + 0.003*"two" + 0.003*"found" + 0.003*"raf"
topic #13 (0.050): 0.007*"game" + 0.005*"also" + 0.004*"first" + 0.004*"route" + 0.003*"one" + 0.003*"ian" + 0.003*"nowe" + 0.003*"samantha" + 0.002*"drakengard" + 0.002*"two"
topic #7 (0.050): 0.006*"displaystyle" + 0.004*"first" + 0.004*"park" + 0.004*"one" + 0.004*"final" + 0.003*"losing" + 0.003*"bridge" + 0.003*"two" + 0.003*"year" + 0.003*"also"
topic #1 (0.050): 0.009*"season" + 0.008*"would" + 0.008*"team" + 0.007*"first" + 0.006*"played" + 0.005*"game" + 0.005*"also" + 0.005*"football" + 0.005*"university" + 0.005*"year"
topic #17 (0.050): 0.005*"film" + 0.005*"also" + 0.005*"one" + 0.004*"winner" + 0.003*"center" + 0.003*"patients" + 0.003*"air" + 0.003*"work" + 0.003*"flavipes" + 0.002*"uba"
topic diff=0.178441, rho=0.235833
-9.382 per-word bound, 667.3 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 13, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 13, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 13, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 13, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 13, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #16 (0.050): 0.005*"one" + 0.005*"population" + 0.005*"also" + 0.003*"first" + 0.003*"years" + 0.003*"people" + 0.003*"area" + 0.003*"age" + 0.002*"house" + 0.002*"building"
topic #10 (0.050): 0.005*"hayden" + 0.004*"election" + 0.004*"united_states" + 0.004*"tanuki" + 0.003*"university" + 0.003*"member" + 0.003*"house" + 0.003*"born" + 0.003*"elected" + 0.003*"world"
topic #1 (0.050): 0.009*"season" + 0.008*"team" + 0.008*"would" + 0.007*"first" + 0.006*"played" + 0.006*"game" + 0.005*"football" + 0.005*"also" + 0.005*"year" + 0.005*"university"
topic #0 (0.050): 0.014*"borg" + 0.010*"amazing_race" + 0.005*"star_trek" + 0.005*"season" + 0.004*"also" + 0.004*"league" + 0.004*"first" + 0.004*"episode" + 0.003*"club" + 0.003*"time"
topic #6 (0.050): 0.005*"album" + 0.005*"th" + 0.005*"lennon" + 0.004*"film" + 0.004*"also" + 0.004*"one" + 0.003*"battle" + 0.003*"british" + 0.003*"first" + 0.003*"two"
topic diff=0.144403, rho=0.229537
-9.375 per-word bound, 664.1 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 14, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 14, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 14, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 14, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 14, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #5 (0.050): 0.008*"band" + 0.006*"laudrup" + 0.006*"album" + 0.005*"also" + 0.005*"released" + 0.004*"two" + 0.004*"first" + 0.004*"one" + 0.004*"new" + 0.004*"single"
topic #16 (0.050): 0.005*"one" + 0.005*"population" + 0.005*"also" + 0.003*"first" + 0.003*"years" + 0.003*"people" + 0.003*"area" + 0.003*"age" + 0.003*"house" + 0.003*"building"
topic #6 (0.050): 0.005*"album" + 0.005*"th" + 0.005*"lennon" + 0.005*"film" + 0.004*"one" + 0.004*"also" + 0.003*"battle" + 0.003*"british" + 0.003*"first" + 0.003*"two"
topic #3 (0.050): 0.005*"street" + 0.005*"physical" + 0.004*"event" + 0.004*"also" + 0.004*"mental" + 0.004*"laws" + 0.004*"national" + 0.004*"davidson" + 0.004*"events" + 0.003*"two"
topic #4 (0.050): 0.008*"cocaine" + 0.006*"de" + 0.006*"pce" + 0.005*"may" + 0.004*"children" + 0.004*"effects" + 0.004*"also" + 0.004*"studies" + 0.003*"one" + 0.003*"crack"
topic diff=0.116995, rho=0.223719
-9.370 per-word bound, 661.5 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 15, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 15, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 15, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 15, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 15, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #1 (0.050): 0.009*"season" + 0.009*"team" + 0.008*"would" + 0.007*"played" + 0.007*"first" + 0.006*"game" + 0.005*"football" + 0.005*"year" + 0.005*"also" + 0.005*"university"
topic #3 (0.050): 0.005*"physical" + 0.005*"street" + 0.004*"event" + 0.004*"also" + 0.004*"mental" + 0.004*"laws" + 0.004*"national" + 0.004*"davidson" + 0.004*"events" + 0.003*"two"
topic #7 (0.050): 0.006*"displaystyle" + 0.005*"first" + 0.004*"park" + 0.004*"one" + 0.004*"final" + 0.004*"losing" + 0.003*"bridge" + 0.003*"two" + 0.003*"year" + 0.003*"reached"
topic #13 (0.050): 0.008*"game" + 0.005*"also" + 0.004*"first" + 0.004*"route" + 0.003*"one" + 0.003*"ian" + 0.003*"nowe" + 0.003*"samantha" + 0.002*"drakengard" + 0.002*"festival"
topic #10 (0.050): 0.005*"hayden" + 0.004*"election" + 0.004*"united_states" + 0.004*"tanuki" + 0.003*"university" + 0.003*"member" + 0.003*"elected" + 0.003*"house" + 0.003*"born" + 0.003*"world"
topic diff=0.095079, rho=0.218322
-9.365 per-word bound, 659.4 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 16, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 16, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 16, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 16, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 16, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #6 (0.050): 0.005*"album" + 0.005*"th" + 0.005*"lennon" + 0.005*"film" + 0.004*"one" + 0.004*"also" + 0.003*"battle" + 0.003*"british" + 0.003*"first" + 0.003*"two"
topic #9 (0.050): 0.006*"board" + 0.005*"game" + 0.004*"film" + 0.003*"tactics_advance" + 0.003*"final_fantasy" + 0.003*"university" + 0.003*"new" + 0.003*"line" + 0.003*"ship" + 0.003*"one"
topic #13 (0.050): 0.008*"game" + 0.005*"also" + 0.004*"first" + 0.004*"route" + 0.003*"one" + 0.003*"ian" + 0.003*"nowe" + 0.003*"samantha" + 0.002*"drakengard" + 0.002*"festival"
topic #5 (0.050): 0.008*"band" + 0.006*"album" + 0.006*"laudrup" + 0.005*"released" + 0.005*"also" + 0.004*"first" + 0.004*"two" + 0.004*"one" + 0.004*"single" + 0.004*"new"
topic #18 (0.050): 0.006*"line" + 0.005*"squadron" + 0.004*"islands" + 0.004*"east" + 0.004*"south" + 0.004*"island" + 0.004*"kilometres_mi" + 0.004*"film" + 0.004*"city" + 0.003*"roosevelt"
topic diff=0.077493, rho=0.213298
-9.361 per-word bound, 657.7 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 17, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 17, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 17, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 17, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 17, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #13 (0.050): 0.008*"game" + 0.005*"also" + 0.004*"first" + 0.004*"route" + 0.003*"one" + 0.003*"ian" + 0.003*"nowe" + 0.003*"samantha" + 0.002*"drakengard" + 0.002*"festival"
topic #3 (0.050): 0.005*"physical" + 0.005*"street" + 0.004*"event" + 0.004*"also" + 0.004*"mental" + 0.004*"laws" + 0.004*"national" + 0.004*"davidson" + 0.004*"events" + 0.003*"two"
topic #0 (0.050): 0.014*"borg" + 0.010*"amazing_race" + 0.006*"star_trek" + 0.005*"season" + 0.004*"also" + 0.004*"episode" + 0.004*"league" + 0.004*"first" + 0.003*"time" + 0.003*"club"
topic #2 (0.050): 0.006*"game" + 0.005*"also" + 0.005*"one" + 0.005*"athletes" + 0.004*"sport_psychology" + 0.003*"performance" + 0.003*"would" + 0.003*"sports" + 0.003*"time" + 0.003*"state"
topic #18 (0.050): 0.006*"line" + 0.005*"squadron" + 0.004*"islands" + 0.004*"east" + 0.004*"south" + 0.004*"island" + 0.004*"kilometres_mi" + 0.004*"film" + 0.004*"city" + 0.003*"roosevelt"
topic diff=0.063398, rho=0.208605
-9.358 per-word bound, 656.2 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 18, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 18, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 18, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 18, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 18, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #11 (0.050): 0.005*"vassar" + 0.004*"enamel" + 0.004*"collection" + 0.003*"church" + 0.003*"one" + 0.003*"th_century" + 0.003*"include" + 0.003*"two" + 0.002*"building" + 0.002*"period"
topic #19 (0.050): 0.007*"school" + 0.004*"also" + 0.004*"year" + 0.004*"management" + 0.004*"first" + 0.004*"university" + 0.003*"company" + 0.003*"one" + 0.003*"work" + 0.003*"director"
topic #4 (0.050): 0.008*"cocaine" + 0.007*"de" + 0.006*"pce" + 0.005*"may" + 0.004*"children" + 0.004*"effects" + 0.004*"studies" + 0.004*"also" + 0.003*"one" + 0.003*"crack"
topic #1 (0.050): 0.009*"season" + 0.009*"team" + 0.008*"would" + 0.007*"played" + 0.007*"first" + 0.006*"game" + 0.006*"football" + 0.005*"year" + 0.005*"also" + 0.005*"league"
topic #10 (0.050): 0.005*"hayden" + 0.005*"election" + 0.004*"united_states" + 0.004*"member" + 0.004*"elected" + 0.004*"university" + 0.004*"tanuki" + 0.003*"party" + 0.003*"born" + 0.003*"house"
topic diff=0.052172, rho=0.204209
-9.355 per-word bound, 654.9 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
PROGRESS: pass 19, dispatched chunk #0 = documents up to #200/996, outstanding queue size 1
PROGRESS: pass 19, dispatched chunk #1 = documents up to #400/996, outstanding queue size 2
PROGRESS: pass 19, dispatched chunk #2 = documents up to #600/996, outstanding queue size 3
PROGRESS: pass 19, dispatched chunk #3 = documents up to #800/996, outstanding queue size 4
PROGRESS: pass 19, dispatched chunk #4 = documents up to #996/996, outstanding queue size 5
topic #0 (0.050): 0.015*"borg" + 0.011*"amazing_race" + 0.006*"star_trek" + 0.005*"season" + 0.004*"also" + 0.004*"episode" + 0.004*"first" + 0.004*"league" + 0.003*"time" + 0.003*"film"
topic #7 (0.050): 0.006*"displaystyle" + 0.005*"park" + 0.005*"first" + 0.004*"one" + 0.004*"final" + 0.004*"losing" + 0.003*"bridge" + 0.003*"two" + 0.003*"year" + 0.003*"cricket"
topic #12 (0.050): 0.009*"railway" + 0.009*"met" + 0.008*"station" + 0.008*"district" + 0.008*"line" + 0.006*"north" + 0.006*"london" + 0.005*"road" + 0.005*"east" + 0.005*"city"
topic #15 (0.050): 0.009*"species" + 0.006*"squadron" + 0.005*"also" + 0.005*"may" + 0.004*"brown_algae" + 0.003*"new" + 0.003*"symbiodinium" + 0.003*"found" + 0.003*"raf" + 0.003*"two"
topic #18 (0.050): 0.006*"line" + 0.005*"squadron" + 0.004*"islands" + 0.004*"island" + 0.004*"east" + 0.004*"south" + 0.004*"kilometres_mi" + 0.004*"film" + 0.004*"city" + 0.003*"roosevelt"
topic diff=0.043171, rho=0.200080
-9.353 per-word bound, 653.7 perplexity estimate based on a held-out corpus of 196 documents with 39665 words
LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=32647, num_topics=20, decay=0.5, chunksize=200) in 51.10s', 'datetime': '2022-02-06T04:03:34.349105', 'gensim': '4.1.2', 'python': '3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:32:32) \n[GCC 9.3.0]', 'platform': 'Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'event': 'created'}
Time taken = 1 minutes
Note: Perplexity estimate based on a held-out corpus of 4 documents


## With Lemmatization
